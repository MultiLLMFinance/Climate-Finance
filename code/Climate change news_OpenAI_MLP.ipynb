{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing OpenAI embedding vectors from string format...\n",
      "Sample Title embedding shape: (1536,)\n",
      "Sample Fulltext embedding shape: (1536,)\n",
      "Loaded 838 climate change news articles spanning from 07/11/2014 to 24/09/2024\n",
      "Class distribution for short-term prediction: {0: 431, 1: 407}\n",
      "Class distribution for long-term prediction: {1: 440, 0: 398}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_84\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_312 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_228 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_168 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_313 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_229 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_169 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_314 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_230 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_315 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.8068 - accuracy: 0.5594\n",
      "Epoch 1: val_loss improved from inf to 0.69006, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 30ms/step - loss: 0.8234 - accuracy: 0.5486 - val_loss: 0.6901 - val_accuracy: 0.5420\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8083 - accuracy: 0.5398\n",
      "Epoch 2: val_loss did not improve from 0.69006\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.8029 - accuracy: 0.5411 - val_loss: 0.6916 - val_accuracy: 0.5420\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7821 - accuracy: 0.5511\n",
      "Epoch 3: val_loss did not improve from 0.69006\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7708 - accuracy: 0.5586 - val_loss: 0.6968 - val_accuracy: 0.5420\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6701 - accuracy: 0.6108\n",
      "Epoch 4: val_loss did not improve from 0.69006\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6667 - accuracy: 0.6209 - val_loss: 0.7042 - val_accuracy: 0.5420\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6135 - accuracy: 0.6818\n",
      "Epoch 5: val_loss did not improve from 0.69006\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6350 - accuracy: 0.6633 - val_loss: 0.7124 - val_accuracy: 0.5420\n",
      "Epoch 6/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.6230 - accuracy: 0.6406\n",
      "Epoch 6: val_loss did not improve from 0.69006\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6227 - accuracy: 0.6484 - val_loss: 0.7218 - val_accuracy: 0.5420\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5642 - accuracy: 0.7131\n",
      "Epoch 7: val_loss did not improve from 0.69006\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5618 - accuracy: 0.7107 - val_loss: 0.7312 - val_accuracy: 0.5420\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5646 - accuracy: 0.7188\n",
      "Epoch 8: val_loss did not improve from 0.69006\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5498 - accuracy: 0.7307 - val_loss: 0.7424 - val_accuracy: 0.5420\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5037 - accuracy: 0.7528\n",
      "Epoch 9: val_loss did not improve from 0.69006\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5074 - accuracy: 0.7481 - val_loss: 0.7518 - val_accuracy: 0.5420\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4636 - accuracy: 0.7926\n",
      "Epoch 10: val_loss did not improve from 0.69006\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4613 - accuracy: 0.7930 - val_loss: 0.7622 - val_accuracy: 0.5420\n",
      "Epoch 11/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.4741 - accuracy: 0.7656\n",
      "Epoch 11: val_loss did not improve from 0.69006\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4693 - accuracy: 0.7756 - val_loss: 0.7727 - val_accuracy: 0.5420\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4522 - accuracy: 0.8040\n",
      "Epoch 12: val_loss did not improve from 0.69006\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4672 - accuracy: 0.7905 - val_loss: 0.7834 - val_accuracy: 0.5420\n",
      "Epoch 13/100\n",
      " 9/13 [===================>..........] - ETA: 0s - loss: 0.3986 - accuracy: 0.7986\n",
      "Epoch 13: val_loss did not improve from 0.69006\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4075 - accuracy: 0.8030 - val_loss: 0.7938 - val_accuracy: 0.5420\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4254 - accuracy: 0.8097\n",
      "Epoch 14: val_loss did not improve from 0.69006\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4349 - accuracy: 0.8005 - val_loss: 0.8013 - val_accuracy: 0.5420\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4182 - accuracy: 0.7898\n",
      "Epoch 15: val_loss did not improve from 0.69006\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4189 - accuracy: 0.7955 - val_loss: 0.8089 - val_accuracy: 0.5420\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3805 - accuracy: 0.8267Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69006\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.3953 - accuracy: 0.8204 - val_loss: 0.8181 - val_accuracy: 0.5420\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.5338\n",
      "Test AUC for Layer 1: 0.4796\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_85\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_316 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_231 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_170 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_317 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_232 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_171 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_318 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_233 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_319 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.8643 - accuracy: 0.5542\n",
      "Epoch 1: val_loss improved from inf to 0.69397, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 26ms/step - loss: 0.8750 - accuracy: 0.5451 - val_loss: 0.6940 - val_accuracy: 0.4511\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8269 - accuracy: 0.5566\n",
      "Epoch 2: val_loss did not improve from 0.69397\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.8187 - accuracy: 0.5602 - val_loss: 0.6945 - val_accuracy: 0.4511\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7968 - accuracy: 0.5742\n",
      "Epoch 3: val_loss did not improve from 0.69397\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7912 - accuracy: 0.5789 - val_loss: 0.6948 - val_accuracy: 0.4436\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7231 - accuracy: 0.6367\n",
      "Epoch 4: val_loss did not improve from 0.69397\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7139 - accuracy: 0.6429 - val_loss: 0.6951 - val_accuracy: 0.4511\n",
      "Epoch 5/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6256 - accuracy: 0.6438\n",
      "Epoch 5: val_loss did not improve from 0.69397\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6322 - accuracy: 0.6410 - val_loss: 0.6949 - val_accuracy: 0.4436\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5798 - accuracy: 0.7012\n",
      "Epoch 6: val_loss did not improve from 0.69397\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5782 - accuracy: 0.7030 - val_loss: 0.6946 - val_accuracy: 0.4586\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5771 - accuracy: 0.7105\n",
      "Epoch 7: val_loss did not improve from 0.69397\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5771 - accuracy: 0.7105 - val_loss: 0.6942 - val_accuracy: 0.4812\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5767 - accuracy: 0.6992\n",
      "Epoch 8: val_loss did not improve from 0.69397\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5764 - accuracy: 0.7011 - val_loss: 0.6945 - val_accuracy: 0.5188\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5076 - accuracy: 0.7402\n",
      "Epoch 9: val_loss improved from 0.69397 to 0.69385, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5062 - accuracy: 0.7387 - val_loss: 0.6939 - val_accuracy: 0.5038\n",
      "Epoch 10/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4963 - accuracy: 0.7542\n",
      "Epoch 10: val_loss did not improve from 0.69385\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4911 - accuracy: 0.7632 - val_loss: 0.6943 - val_accuracy: 0.4962\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4975 - accuracy: 0.7500\n",
      "Epoch 11: val_loss did not improve from 0.69385\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4932 - accuracy: 0.7481 - val_loss: 0.6947 - val_accuracy: 0.5038\n",
      "Epoch 12/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4470 - accuracy: 0.7937\n",
      "Epoch 12: val_loss did not improve from 0.69385\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4497 - accuracy: 0.7970 - val_loss: 0.6950 - val_accuracy: 0.5038\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4021 - accuracy: 0.8340\n",
      "Epoch 13: val_loss did not improve from 0.69385\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4062 - accuracy: 0.8289 - val_loss: 0.6955 - val_accuracy: 0.4962\n",
      "Epoch 14/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3893 - accuracy: 0.8099\n",
      "Epoch 14: val_loss did not improve from 0.69385\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4100 - accuracy: 0.8064 - val_loss: 0.6956 - val_accuracy: 0.5038\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3899 - accuracy: 0.8340\n",
      "Epoch 15: val_loss did not improve from 0.69385\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3917 - accuracy: 0.8308 - val_loss: 0.6963 - val_accuracy: 0.5338\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3827 - accuracy: 0.8301\n",
      "Epoch 16: val_loss did not improve from 0.69385\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3869 - accuracy: 0.8289 - val_loss: 0.6971 - val_accuracy: 0.5188\n",
      "Epoch 17/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3646 - accuracy: 0.8613\n",
      "Epoch 17: val_loss did not improve from 0.69385\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3664 - accuracy: 0.8590 - val_loss: 0.6987 - val_accuracy: 0.5038\n",
      "Epoch 18/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3202 - accuracy: 0.8926\n",
      "Epoch 18: val_loss did not improve from 0.69385\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3172 - accuracy: 0.8947 - val_loss: 0.7011 - val_accuracy: 0.5038\n",
      "Epoch 19/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3095 - accuracy: 0.8789\n",
      "Epoch 19: val_loss did not improve from 0.69385\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3090 - accuracy: 0.8759 - val_loss: 0.7031 - val_accuracy: 0.4962\n",
      "Epoch 20/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2965 - accuracy: 0.8809\n",
      "Epoch 20: val_loss did not improve from 0.69385\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2922 - accuracy: 0.8853 - val_loss: 0.7063 - val_accuracy: 0.4962\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3160 - accuracy: 0.8797\n",
      "Epoch 21: val_loss did not improve from 0.69385\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3160 - accuracy: 0.8797 - val_loss: 0.7089 - val_accuracy: 0.4887\n",
      "Epoch 22/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2836 - accuracy: 0.9102\n",
      "Epoch 22: val_loss did not improve from 0.69385\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2868 - accuracy: 0.9117 - val_loss: 0.7141 - val_accuracy: 0.4887\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2808 - accuracy: 0.8891\n",
      "Epoch 23: val_loss did not improve from 0.69385\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2808 - accuracy: 0.8891 - val_loss: 0.7180 - val_accuracy: 0.4737\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2237 - accuracy: 0.9286Restoring model weights from the end of the best epoch: 9.\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.69385\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2237 - accuracy: 0.9286 - val_loss: 0.7228 - val_accuracy: 0.4887\n",
      "Epoch 24: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.6139\n",
      "Test AUC for Layer 2: 0.6063\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_86\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_320 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_234 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_172 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_321 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_235 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_173 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_322 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_236 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_323 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.9621 - accuracy: 0.4984\n",
      "Epoch 1: val_loss improved from inf to 0.69175, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 24ms/step - loss: 0.9617 - accuracy: 0.5045 - val_loss: 0.6918 - val_accuracy: 0.5545\n",
      "Epoch 2/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.7977 - accuracy: 0.5444\n",
      "Epoch 2: val_loss improved from 0.69175 to 0.69078, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 0.8037 - accuracy: 0.5405 - val_loss: 0.6908 - val_accuracy: 0.5644\n",
      "Epoch 3/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.7933 - accuracy: 0.5437\n",
      "Epoch 3: val_loss improved from 0.69078 to 0.69053, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7940 - accuracy: 0.5420 - val_loss: 0.6905 - val_accuracy: 0.5545\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7063 - accuracy: 0.6126\n",
      "Epoch 4: val_loss improved from 0.69053 to 0.69040, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 0.7063 - accuracy: 0.6126 - val_loss: 0.6904 - val_accuracy: 0.5149\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6683 - accuracy: 0.6411\n",
      "Epoch 5: val_loss improved from 0.69040 to 0.69003, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 0.6683 - accuracy: 0.6411 - val_loss: 0.6900 - val_accuracy: 0.5149\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6316 - accuracy: 0.6562\n",
      "Epoch 6: val_loss improved from 0.69003 to 0.68928, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 0.6316 - accuracy: 0.6562 - val_loss: 0.6893 - val_accuracy: 0.5347\n",
      "Epoch 7/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5849 - accuracy: 0.6969\n",
      "Epoch 7: val_loss improved from 0.68928 to 0.68922, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5900 - accuracy: 0.6922 - val_loss: 0.6892 - val_accuracy: 0.5050\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5776 - accuracy: 0.6907\n",
      "Epoch 8: val_loss improved from 0.68922 to 0.68899, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5776 - accuracy: 0.6907 - val_loss: 0.6890 - val_accuracy: 0.5149\n",
      "Epoch 9/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5520 - accuracy: 0.7109\n",
      "Epoch 9: val_loss improved from 0.68899 to 0.68796, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 0.5515 - accuracy: 0.7117 - val_loss: 0.6880 - val_accuracy: 0.5248\n",
      "Epoch 10/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5352 - accuracy: 0.7469\n",
      "Epoch 10: val_loss improved from 0.68796 to 0.68729, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5315 - accuracy: 0.7477 - val_loss: 0.6873 - val_accuracy: 0.5149\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4977 - accuracy: 0.7628\n",
      "Epoch 11: val_loss improved from 0.68729 to 0.68726, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4977 - accuracy: 0.7628 - val_loss: 0.6873 - val_accuracy: 0.5050\n",
      "Epoch 12/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4889 - accuracy: 0.7569\n",
      "Epoch 12: val_loss did not improve from 0.68726\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.4984 - accuracy: 0.7508 - val_loss: 0.6878 - val_accuracy: 0.4950\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4295 - accuracy: 0.8048\n",
      "Epoch 13: val_loss improved from 0.68726 to 0.68725, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 0.4295 - accuracy: 0.8048 - val_loss: 0.6872 - val_accuracy: 0.4851\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4099 - accuracy: 0.8108\n",
      "Epoch 14: val_loss did not improve from 0.68725\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4099 - accuracy: 0.8108 - val_loss: 0.6878 - val_accuracy: 0.5347\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4149 - accuracy: 0.8288\n",
      "Epoch 15: val_loss did not improve from 0.68725\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4149 - accuracy: 0.8288 - val_loss: 0.6873 - val_accuracy: 0.5446\n",
      "Epoch 16/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4148 - accuracy: 0.7951\n",
      "Epoch 16: val_loss did not improve from 0.68725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4135 - accuracy: 0.7943 - val_loss: 0.6873 - val_accuracy: 0.5347\n",
      "Epoch 17/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3901 - accuracy: 0.8574\n",
      "Epoch 17: val_loss did not improve from 0.68725\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3786 - accuracy: 0.8589 - val_loss: 0.6892 - val_accuracy: 0.5248\n",
      "Epoch 18/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3575 - accuracy: 0.8503\n",
      "Epoch 18: val_loss did not improve from 0.68725\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3561 - accuracy: 0.8514 - val_loss: 0.6924 - val_accuracy: 0.4950\n",
      "Epoch 19/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3649 - accuracy: 0.8500\n",
      "Epoch 19: val_loss did not improve from 0.68725\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3668 - accuracy: 0.8514 - val_loss: 0.6904 - val_accuracy: 0.5050\n",
      "Epoch 20/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3221 - accuracy: 0.8724\n",
      "Epoch 20: val_loss did not improve from 0.68725\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3221 - accuracy: 0.8724 - val_loss: 0.6920 - val_accuracy: 0.5248\n",
      "Epoch 21/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3181 - accuracy: 0.8672\n",
      "Epoch 21: val_loss did not improve from 0.68725\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3011 - accuracy: 0.8844 - val_loss: 0.6924 - val_accuracy: 0.5149\n",
      "Epoch 22/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2916 - accuracy: 0.9069\n",
      "Epoch 22: val_loss did not improve from 0.68725\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2916 - accuracy: 0.9069 - val_loss: 0.6956 - val_accuracy: 0.5149\n",
      "Epoch 23/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.9189\n",
      "Epoch 23: val_loss did not improve from 0.68725\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2594 - accuracy: 0.9189 - val_loss: 0.7011 - val_accuracy: 0.4950\n",
      "Epoch 24/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2820 - accuracy: 0.8979\n",
      "Epoch 24: val_loss did not improve from 0.68725\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2820 - accuracy: 0.8979 - val_loss: 0.7071 - val_accuracy: 0.5149\n",
      "Epoch 25/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2758 - accuracy: 0.9062\n",
      "Epoch 25: val_loss did not improve from 0.68725\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2679 - accuracy: 0.9129 - val_loss: 0.7104 - val_accuracy: 0.5347\n",
      "Epoch 26/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2401 - accuracy: 0.9174\n",
      "Epoch 26: val_loss did not improve from 0.68725\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2401 - accuracy: 0.9174 - val_loss: 0.7195 - val_accuracy: 0.5347\n",
      "Epoch 27/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2360 - accuracy: 0.9264\n",
      "Epoch 27: val_loss did not improve from 0.68725\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2360 - accuracy: 0.9264 - val_loss: 0.7261 - val_accuracy: 0.5545\n",
      "Epoch 28/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2500 - accuracy: 0.9121Restoring model weights from the end of the best epoch: 13.\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.68725\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2388 - accuracy: 0.9219 - val_loss: 0.7328 - val_accuracy: 0.5545\n",
      "Epoch 28: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4714\n",
      "Test AUC for Layer 3: 0.4719\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5397\n",
      "Average Test AUC across all layers: 0.5193\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_87\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_324 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_237 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_174 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_325 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_238 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_175 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_326 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_239 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_327 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.8750 - accuracy: 0.4812\n",
      "Epoch 1: val_loss improved from inf to 0.67772, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 30ms/step - loss: 0.8797 - accuracy: 0.4888 - val_loss: 0.6777 - val_accuracy: 0.6718\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8320 - accuracy: 0.5199\n",
      "Epoch 2: val_loss improved from 0.67772 to 0.67091, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.8227 - accuracy: 0.5112 - val_loss: 0.6709 - val_accuracy: 0.6718\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6965 - accuracy: 0.5966\n",
      "Epoch 3: val_loss improved from 0.67091 to 0.66505, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.7239 - accuracy: 0.5810 - val_loss: 0.6651 - val_accuracy: 0.6718\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6903 - accuracy: 0.6136\n",
      "Epoch 4: val_loss improved from 0.66505 to 0.66176, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.6861 - accuracy: 0.6135 - val_loss: 0.6618 - val_accuracy: 0.6718\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6645 - accuracy: 0.6619\n",
      "Epoch 5: val_loss improved from 0.66176 to 0.66077, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.6559 - accuracy: 0.6658 - val_loss: 0.6608 - val_accuracy: 0.6718\n",
      "Epoch 6/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.5705 - accuracy: 0.6812\n",
      "Epoch 6: val_loss improved from 0.66077 to 0.65974, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_1.weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 19ms/step - loss: 0.5879 - accuracy: 0.6783 - val_loss: 0.6597 - val_accuracy: 0.6718\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5964 - accuracy: 0.6903\n",
      "Epoch 7: val_loss improved from 0.65974 to 0.65949, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5772 - accuracy: 0.7032 - val_loss: 0.6595 - val_accuracy: 0.6718\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5656 - accuracy: 0.6903\n",
      "Epoch 8: val_loss did not improve from 0.65949\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5631 - accuracy: 0.6833 - val_loss: 0.6596 - val_accuracy: 0.6718\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5368 - accuracy: 0.7131\n",
      "Epoch 9: val_loss did not improve from 0.65949\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5244 - accuracy: 0.7207 - val_loss: 0.6611 - val_accuracy: 0.6718\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5026 - accuracy: 0.7670\n",
      "Epoch 10: val_loss did not improve from 0.65949\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4922 - accuracy: 0.7731 - val_loss: 0.6619 - val_accuracy: 0.6718\n",
      "Epoch 11/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5132 - accuracy: 0.7614\n",
      "Epoch 11: val_loss did not improve from 0.65949\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5176 - accuracy: 0.7506 - val_loss: 0.6623 - val_accuracy: 0.6718\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4298 - accuracy: 0.8040\n",
      "Epoch 12: val_loss did not improve from 0.65949\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4298 - accuracy: 0.8105 - val_loss: 0.6641 - val_accuracy: 0.6718\n",
      "Epoch 13/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4093 - accuracy: 0.8267\n",
      "Epoch 13: val_loss did not improve from 0.65949\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4084 - accuracy: 0.8279 - val_loss: 0.6680 - val_accuracy: 0.6718\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3996 - accuracy: 0.8324\n",
      "Epoch 14: val_loss did not improve from 0.65949\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3962 - accuracy: 0.8304 - val_loss: 0.6703 - val_accuracy: 0.6870\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4076 - accuracy: 0.8295\n",
      "Epoch 15: val_loss did not improve from 0.65949\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3990 - accuracy: 0.8354 - val_loss: 0.6747 - val_accuracy: 0.7023\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4074 - accuracy: 0.8182\n",
      "Epoch 16: val_loss did not improve from 0.65949\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4083 - accuracy: 0.8180 - val_loss: 0.6772 - val_accuracy: 0.6718\n",
      "Epoch 17/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3631 - accuracy: 0.8608\n",
      "Epoch 17: val_loss did not improve from 0.65949\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3493 - accuracy: 0.8753 - val_loss: 0.6800 - val_accuracy: 0.6412\n",
      "Epoch 18/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3430 - accuracy: 0.8665\n",
      "Epoch 18: val_loss did not improve from 0.65949\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3501 - accuracy: 0.8603 - val_loss: 0.6858 - val_accuracy: 0.5649\n",
      "Epoch 19/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3112 - accuracy: 0.8665\n",
      "Epoch 19: val_loss did not improve from 0.65949\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3225 - accuracy: 0.8529 - val_loss: 0.6894 - val_accuracy: 0.5115\n",
      "Epoch 20/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3110 - accuracy: 0.8977\n",
      "Epoch 20: val_loss did not improve from 0.65949\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3095 - accuracy: 0.8953 - val_loss: 0.6967 - val_accuracy: 0.4504\n",
      "Epoch 21/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2872 - accuracy: 0.9176\n",
      "Epoch 21: val_loss did not improve from 0.65949\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.2822 - accuracy: 0.9202 - val_loss: 0.7028 - val_accuracy: 0.4046\n",
      "Epoch 22/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3168 - accuracy: 0.8778Restoring model weights from the end of the best epoch: 7.\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.65949\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3171 - accuracy: 0.8703 - val_loss: 0.7070 - val_accuracy: 0.4046\n",
      "Epoch 22: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.7368\n",
      "Test AUC for Layer 1: 0.5569\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_88\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_328 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_240 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_176 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_329 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_241 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_177 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_330 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_242 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_331 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8860 - accuracy: 0.5137\n",
      "Epoch 1: val_loss improved from inf to 0.66522, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 26ms/step - loss: 0.8973 - accuracy: 0.5113 - val_loss: 0.6652 - val_accuracy: 0.7368\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8390 - accuracy: 0.5332\n",
      "Epoch 2: val_loss improved from 0.66522 to 0.64546, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.8330 - accuracy: 0.5376 - val_loss: 0.6455 - val_accuracy: 0.7368\n",
      "Epoch 3/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.7727 - accuracy: 0.5625\n",
      "Epoch 3: val_loss improved from 0.64546 to 0.63323, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.7703 - accuracy: 0.5714 - val_loss: 0.6332 - val_accuracy: 0.7368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7335 - accuracy: 0.6128\n",
      "Epoch 4: val_loss improved from 0.63323 to 0.62323, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.7335 - accuracy: 0.6128 - val_loss: 0.6232 - val_accuracy: 0.7368\n",
      "Epoch 5/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6477 - accuracy: 0.6479\n",
      "Epoch 5: val_loss improved from 0.62323 to 0.61691, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6447 - accuracy: 0.6410 - val_loss: 0.6169 - val_accuracy: 0.7368\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6223 - accuracy: 0.6680\n",
      "Epoch 6: val_loss improved from 0.61691 to 0.61412, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6178 - accuracy: 0.6654 - val_loss: 0.6141 - val_accuracy: 0.7368\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6056 - accuracy: 0.7070\n",
      "Epoch 7: val_loss improved from 0.61412 to 0.61243, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6061 - accuracy: 0.7030 - val_loss: 0.6124 - val_accuracy: 0.7368\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5670 - accuracy: 0.6934\n",
      "Epoch 8: val_loss improved from 0.61243 to 0.60969, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5642 - accuracy: 0.6955 - val_loss: 0.6097 - val_accuracy: 0.7368\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5237 - accuracy: 0.7387\n",
      "Epoch 9: val_loss improved from 0.60969 to 0.60835, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5237 - accuracy: 0.7387 - val_loss: 0.6083 - val_accuracy: 0.7368\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4930 - accuracy: 0.7598\n",
      "Epoch 10: val_loss improved from 0.60835 to 0.60788, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4971 - accuracy: 0.7556 - val_loss: 0.6079 - val_accuracy: 0.7368\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4825 - accuracy: 0.7598\n",
      "Epoch 11: val_loss improved from 0.60788 to 0.60786, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4873 - accuracy: 0.7575 - val_loss: 0.6079 - val_accuracy: 0.7368\n",
      "Epoch 12/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4364 - accuracy: 0.7979\n",
      "Epoch 12: val_loss did not improve from 0.60786\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4313 - accuracy: 0.8008 - val_loss: 0.6092 - val_accuracy: 0.7368\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4410 - accuracy: 0.8008\n",
      "Epoch 13: val_loss did not improve from 0.60786\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4410 - accuracy: 0.8008 - val_loss: 0.6108 - val_accuracy: 0.7368\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4341 - accuracy: 0.8105\n",
      "Epoch 14: val_loss did not improve from 0.60786\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4339 - accuracy: 0.8102 - val_loss: 0.6128 - val_accuracy: 0.7368\n",
      "Epoch 15/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3817 - accuracy: 0.8411\n",
      "Epoch 15: val_loss did not improve from 0.60786\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3825 - accuracy: 0.8327 - val_loss: 0.6167 - val_accuracy: 0.7368\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3821 - accuracy: 0.8438\n",
      "Epoch 16: val_loss did not improve from 0.60786\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3798 - accuracy: 0.8459 - val_loss: 0.6174 - val_accuracy: 0.7368\n",
      "Epoch 17/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3742 - accuracy: 0.8317\n",
      "Epoch 17: val_loss did not improve from 0.60786\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3645 - accuracy: 0.8402 - val_loss: 0.6182 - val_accuracy: 0.7368\n",
      "Epoch 18/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3332 - accuracy: 0.8789\n",
      "Epoch 18: val_loss did not improve from 0.60786\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3346 - accuracy: 0.8778 - val_loss: 0.6207 - val_accuracy: 0.7293\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3251 - accuracy: 0.8853\n",
      "Epoch 19: val_loss did not improve from 0.60786\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3251 - accuracy: 0.8853 - val_loss: 0.6229 - val_accuracy: 0.7293\n",
      "Epoch 20/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2999 - accuracy: 0.9004\n",
      "Epoch 20: val_loss did not improve from 0.60786\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2990 - accuracy: 0.9023 - val_loss: 0.6229 - val_accuracy: 0.7293\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3413 - accuracy: 0.8628\n",
      "Epoch 21: val_loss did not improve from 0.60786\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3413 - accuracy: 0.8628 - val_loss: 0.6234 - val_accuracy: 0.7218\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.9004\n",
      "Epoch 22: val_loss did not improve from 0.60786\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2665 - accuracy: 0.9004 - val_loss: 0.6246 - val_accuracy: 0.7068\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.9173\n",
      "Epoch 23: val_loss did not improve from 0.60786\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2514 - accuracy: 0.9173 - val_loss: 0.6261 - val_accuracy: 0.7068\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2714 - accuracy: 0.9004\n",
      "Epoch 24: val_loss did not improve from 0.60786\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2714 - accuracy: 0.9004 - val_loss: 0.6282 - val_accuracy: 0.6842\n",
      "Epoch 25/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2505 - accuracy: 0.9219\n",
      "Epoch 25: val_loss did not improve from 0.60786\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2529 - accuracy: 0.9192 - val_loss: 0.6342 - val_accuracy: 0.6541\n",
      "Epoch 26/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2504 - accuracy: 0.9303Restoring model weights from the end of the best epoch: 11.\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.60786\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2503 - accuracy: 0.9305 - val_loss: 0.6352 - val_accuracy: 0.6617\n",
      "Epoch 26: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5050\n",
      "Test AUC for Layer 2: 0.5314\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_332 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_243 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " dropout_178 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_333 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_244 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_179 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_334 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_245 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_335 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9179 - accuracy: 0.4685\n",
      "Epoch 1: val_loss improved from inf to 0.69289, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 23ms/step - loss: 0.9179 - accuracy: 0.4685 - val_loss: 0.6929 - val_accuracy: 0.4950\n",
      "Epoch 2/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.7820 - accuracy: 0.5750\n",
      "Epoch 2: val_loss did not improve from 0.69289\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7785 - accuracy: 0.5781 - val_loss: 0.6946 - val_accuracy: 0.4950\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7250 - accuracy: 0.5916\n",
      "Epoch 3: val_loss did not improve from 0.69289\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7250 - accuracy: 0.5916 - val_loss: 0.6976 - val_accuracy: 0.4950\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7029 - accuracy: 0.6276\n",
      "Epoch 4: val_loss did not improve from 0.69289\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.7029 - accuracy: 0.6276 - val_loss: 0.7014 - val_accuracy: 0.4950\n",
      "Epoch 5/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.6482 - accuracy: 0.6476\n",
      "Epoch 5: val_loss did not improve from 0.69289\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6640 - accuracy: 0.6336 - val_loss: 0.7053 - val_accuracy: 0.4950\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6060 - accuracy: 0.6727\n",
      "Epoch 6: val_loss did not improve from 0.69289\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6060 - accuracy: 0.6727 - val_loss: 0.7105 - val_accuracy: 0.4950\n",
      "Epoch 7/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.5435 - accuracy: 0.7227\n",
      "Epoch 7: val_loss did not improve from 0.69289\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5609 - accuracy: 0.7132 - val_loss: 0.7151 - val_accuracy: 0.4950\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5570 - accuracy: 0.6877\n",
      "Epoch 8: val_loss did not improve from 0.69289\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5570 - accuracy: 0.6877 - val_loss: 0.7205 - val_accuracy: 0.4950\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5039 - accuracy: 0.7613\n",
      "Epoch 9: val_loss did not improve from 0.69289\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5039 - accuracy: 0.7613 - val_loss: 0.7266 - val_accuracy: 0.4950\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4866 - accuracy: 0.7613\n",
      "Epoch 10: val_loss did not improve from 0.69289\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4866 - accuracy: 0.7613 - val_loss: 0.7312 - val_accuracy: 0.4950\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4937 - accuracy: 0.7553\n",
      "Epoch 11: val_loss did not improve from 0.69289\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4937 - accuracy: 0.7553 - val_loss: 0.7373 - val_accuracy: 0.4950\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4413 - accuracy: 0.8063\n",
      "Epoch 12: val_loss did not improve from 0.69289\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4413 - accuracy: 0.8063 - val_loss: 0.7404 - val_accuracy: 0.4950\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4497 - accuracy: 0.7973\n",
      "Epoch 13: val_loss did not improve from 0.69289\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4497 - accuracy: 0.7973 - val_loss: 0.7382 - val_accuracy: 0.4950\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4059 - accuracy: 0.8123\n",
      "Epoch 14: val_loss did not improve from 0.69289\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4059 - accuracy: 0.8123 - val_loss: 0.7413 - val_accuracy: 0.4950\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3911 - accuracy: 0.8243\n",
      "Epoch 15: val_loss did not improve from 0.69289\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3911 - accuracy: 0.8243 - val_loss: 0.7427 - val_accuracy: 0.4950\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3441 - accuracy: 0.8619Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69289\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3441 - accuracy: 0.8619 - val_loss: 0.7429 - val_accuracy: 0.4950\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5571\n",
      "Test AUC for Layer 3: 0.5294\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5996\n",
      "Average Test AUC across all layers: 0.5392\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_90\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_336 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_246 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_180 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_337 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_247 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_181 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_338 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_248 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " dense_339 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8783 - accuracy: 0.4830\n",
      "Epoch 1: val_loss improved from inf to 0.69426, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 30ms/step - loss: 0.8784 - accuracy: 0.4838 - val_loss: 0.6943 - val_accuracy: 0.4580\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7951 - accuracy: 0.5369\n",
      "Epoch 2: val_loss did not improve from 0.69426\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.7984 - accuracy: 0.5337 - val_loss: 0.6960 - val_accuracy: 0.4580\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7213 - accuracy: 0.6222\n",
      "Epoch 3: val_loss did not improve from 0.69426\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.7368 - accuracy: 0.6135 - val_loss: 0.6970 - val_accuracy: 0.4580\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6701 - accuracy: 0.6506\n",
      "Epoch 4: val_loss did not improve from 0.69426\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6750 - accuracy: 0.6459 - val_loss: 0.6982 - val_accuracy: 0.4580\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6783 - accuracy: 0.6165\n",
      "Epoch 5: val_loss did not improve from 0.69426\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6837 - accuracy: 0.6234 - val_loss: 0.6996 - val_accuracy: 0.4580\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6168 - accuracy: 0.6534\n",
      "Epoch 6: val_loss did not improve from 0.69426\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6387 - accuracy: 0.6284 - val_loss: 0.7000 - val_accuracy: 0.4580\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6079 - accuracy: 0.6733\n",
      "Epoch 7: val_loss did not improve from 0.69426\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6038 - accuracy: 0.6708 - val_loss: 0.7004 - val_accuracy: 0.4580\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5814 - accuracy: 0.6875\n",
      "Epoch 8: val_loss did not improve from 0.69426\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5792 - accuracy: 0.6833 - val_loss: 0.7006 - val_accuracy: 0.4580\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5275 - accuracy: 0.7330\n",
      "Epoch 9: val_loss did not improve from 0.69426\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5472 - accuracy: 0.7207 - val_loss: 0.7019 - val_accuracy: 0.4580\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4974 - accuracy: 0.7528\n",
      "Epoch 10: val_loss did not improve from 0.69426\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5070 - accuracy: 0.7481 - val_loss: 0.7030 - val_accuracy: 0.4580\n",
      "Epoch 11/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5232 - accuracy: 0.7216\n",
      "Epoch 11: val_loss did not improve from 0.69426\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5331 - accuracy: 0.7132 - val_loss: 0.7033 - val_accuracy: 0.4580\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4511 - accuracy: 0.7841\n",
      "Epoch 12: val_loss did not improve from 0.69426\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4433 - accuracy: 0.7905 - val_loss: 0.7033 - val_accuracy: 0.4580\n",
      "Epoch 13/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4647 - accuracy: 0.7841\n",
      "Epoch 13: val_loss did not improve from 0.69426\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4707 - accuracy: 0.7756 - val_loss: 0.7027 - val_accuracy: 0.4580\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4464 - accuracy: 0.7642\n",
      "Epoch 14: val_loss did not improve from 0.69426\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4345 - accuracy: 0.7756 - val_loss: 0.7038 - val_accuracy: 0.4580\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4147 - accuracy: 0.8182\n",
      "Epoch 15: val_loss did not improve from 0.69426\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4190 - accuracy: 0.8180 - val_loss: 0.7041 - val_accuracy: 0.4580\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4307 - accuracy: 0.7955Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69426\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4237 - accuracy: 0.8005 - val_loss: 0.7056 - val_accuracy: 0.4580\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.4662\n",
      "Test AUC for Layer 1: 0.5266\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_340 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_249 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_182 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_341 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_250 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_183 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_342 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_251 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_343 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.8455 - accuracy: 0.5167\n",
      "Epoch 1: val_loss improved from inf to 0.69278, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 26ms/step - loss: 0.8734 - accuracy: 0.5169 - val_loss: 0.6928 - val_accuracy: 0.5414\n",
      "Epoch 2/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.7694 - accuracy: 0.5625\n",
      "Epoch 2: val_loss improved from 0.69278 to 0.69260, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7531 - accuracy: 0.5714 - val_loss: 0.6926 - val_accuracy: 0.5338\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7442 - accuracy: 0.5879\n",
      "Epoch 3: val_loss improved from 0.69260 to 0.69246, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7526 - accuracy: 0.5808 - val_loss: 0.6925 - val_accuracy: 0.5338\n",
      "Epoch 4/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6960 - accuracy: 0.6062\n",
      "Epoch 4: val_loss did not improve from 0.69246\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6918 - accuracy: 0.6071 - val_loss: 0.6927 - val_accuracy: 0.5338\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6602 - accuracy: 0.6191\n",
      "Epoch 5: val_loss did not improve from 0.69246\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6577 - accuracy: 0.6184 - val_loss: 0.6933 - val_accuracy: 0.5338\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6188 - accuracy: 0.6719\n",
      "Epoch 6: val_loss did not improve from 0.69246\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6277 - accuracy: 0.6654 - val_loss: 0.6945 - val_accuracy: 0.5338\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6230 - accuracy: 0.6719\n",
      "Epoch 7: val_loss did not improve from 0.69246\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6154 - accuracy: 0.6767 - val_loss: 0.6958 - val_accuracy: 0.5338\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5504 - accuracy: 0.7051\n",
      "Epoch 8: val_loss did not improve from 0.69246\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5511 - accuracy: 0.7049 - val_loss: 0.6980 - val_accuracy: 0.5338\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5507 - accuracy: 0.7090\n",
      "Epoch 9: val_loss did not improve from 0.69246\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5461 - accuracy: 0.7143 - val_loss: 0.7012 - val_accuracy: 0.5338\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4985 - accuracy: 0.7441\n",
      "Epoch 10: val_loss did not improve from 0.69246\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4997 - accuracy: 0.7462 - val_loss: 0.7030 - val_accuracy: 0.5338\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4528 - accuracy: 0.7754\n",
      "Epoch 11: val_loss did not improve from 0.69246\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4543 - accuracy: 0.7726 - val_loss: 0.7058 - val_accuracy: 0.5338\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4546 - accuracy: 0.7793\n",
      "Epoch 12: val_loss did not improve from 0.69246\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4602 - accuracy: 0.7744 - val_loss: 0.7105 - val_accuracy: 0.5338\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4653 - accuracy: 0.7734\n",
      "Epoch 13: val_loss did not improve from 0.69246\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4606 - accuracy: 0.7763 - val_loss: 0.7143 - val_accuracy: 0.5338\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4240 - accuracy: 0.8125\n",
      "Epoch 14: val_loss did not improve from 0.69246\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4258 - accuracy: 0.8102 - val_loss: 0.7175 - val_accuracy: 0.5338\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4191 - accuracy: 0.8086\n",
      "Epoch 15: val_loss did not improve from 0.69246\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4231 - accuracy: 0.8008 - val_loss: 0.7184 - val_accuracy: 0.5338\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3920 - accuracy: 0.8281\n",
      "Epoch 16: val_loss did not improve from 0.69246\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3968 - accuracy: 0.8289 - val_loss: 0.7207 - val_accuracy: 0.5338\n",
      "Epoch 17/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3803 - accuracy: 0.8477\n",
      "Epoch 17: val_loss did not improve from 0.69246\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3847 - accuracy: 0.8477 - val_loss: 0.7249 - val_accuracy: 0.5338\n",
      "Epoch 18/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4110 - accuracy: 0.8242Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.69246\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4132 - accuracy: 0.8271 - val_loss: 0.7304 - val_accuracy: 0.5338\n",
      "Epoch 18: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5545\n",
      "Test AUC for Layer 2: 0.4345\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_92\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_344 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_252 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_184 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_345 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_253 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_185 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_346 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_254 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_347 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8715 - accuracy: 0.5300\n",
      "Epoch 1: val_loss improved from inf to 0.69434, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 22ms/step - loss: 0.8715 - accuracy: 0.5300 - val_loss: 0.6943 - val_accuracy: 0.4455\n",
      "Epoch 2/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.8214 - accuracy: 0.5609\n",
      "Epoch 2: val_loss did not improve from 0.69434\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.8187 - accuracy: 0.5616 - val_loss: 0.6960 - val_accuracy: 0.4356\n",
      "Epoch 3/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.7800 - accuracy: 0.5677\n",
      "Epoch 3: val_loss did not improve from 0.69434\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.7819 - accuracy: 0.5676 - val_loss: 0.6989 - val_accuracy: 0.4455\n",
      "Epoch 4/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.7269 - accuracy: 0.5901\n",
      "Epoch 4: val_loss did not improve from 0.69434\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.7393 - accuracy: 0.5811 - val_loss: 0.7044 - val_accuracy: 0.4455\n",
      "Epoch 5/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6846 - accuracy: 0.5891\n",
      "Epoch 5: val_loss did not improve from 0.69434\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6847 - accuracy: 0.5916 - val_loss: 0.7116 - val_accuracy: 0.4455\n",
      "Epoch 6/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.6227 - accuracy: 0.6736\n",
      "Epoch 6: val_loss did not improve from 0.69434\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6209 - accuracy: 0.6742 - val_loss: 0.7209 - val_accuracy: 0.4455\n",
      "Epoch 7/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.6197 - accuracy: 0.6953\n",
      "Epoch 7: val_loss did not improve from 0.69434\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6288 - accuracy: 0.6772 - val_loss: 0.7310 - val_accuracy: 0.4455\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5927 - accuracy: 0.6907\n",
      "Epoch 8: val_loss did not improve from 0.69434\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5927 - accuracy: 0.6907 - val_loss: 0.7430 - val_accuracy: 0.4455\n",
      "Epoch 9/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5976 - accuracy: 0.6875\n",
      "Epoch 9: val_loss did not improve from 0.69434\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6069 - accuracy: 0.6802 - val_loss: 0.7618 - val_accuracy: 0.4455\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5408 - accuracy: 0.7177\n",
      "Epoch 10: val_loss did not improve from 0.69434\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5408 - accuracy: 0.7177 - val_loss: 0.7789 - val_accuracy: 0.4455\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5130 - accuracy: 0.7432\n",
      "Epoch 11: val_loss did not improve from 0.69434\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5130 - accuracy: 0.7432 - val_loss: 0.7942 - val_accuracy: 0.4455\n",
      "Epoch 12/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5113 - accuracy: 0.7391\n",
      "Epoch 12: val_loss did not improve from 0.69434\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5148 - accuracy: 0.7372 - val_loss: 0.8154 - val_accuracy: 0.4455\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4743 - accuracy: 0.7658\n",
      "Epoch 13: val_loss did not improve from 0.69434\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4743 - accuracy: 0.7658 - val_loss: 0.8359 - val_accuracy: 0.4455\n",
      "Epoch 14/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.4494 - accuracy: 0.7629\n",
      "Epoch 14: val_loss did not improve from 0.69434\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4479 - accuracy: 0.7688 - val_loss: 0.8447 - val_accuracy: 0.4455\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4527 - accuracy: 0.7898\n",
      "Epoch 15: val_loss did not improve from 0.69434\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4527 - accuracy: 0.7898 - val_loss: 0.8541 - val_accuracy: 0.4455\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4245 - accuracy: 0.8003Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69434\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4245 - accuracy: 0.8003 - val_loss: 0.8583 - val_accuracy: 0.4455\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5429\n",
      "Test AUC for Layer 3: 0.4515\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5212\n",
      "Average Test AUC across all layers: 0.4709\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_93\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_348 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_255 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_186 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_349 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_256 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_187 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_350 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_257 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_351 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.9337 - accuracy: 0.4563\n",
      "Epoch 1: val_loss improved from inf to 0.68715, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 31ms/step - loss: 0.9095 - accuracy: 0.4738 - val_loss: 0.6872 - val_accuracy: 0.6641\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7934 - accuracy: 0.5597\n",
      "Epoch 2: val_loss improved from 0.68715 to 0.67900, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.7844 - accuracy: 0.5711 - val_loss: 0.6790 - val_accuracy: 0.6718\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7690 - accuracy: 0.5312\n",
      "Epoch 3: val_loss improved from 0.67900 to 0.67322, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.7396 - accuracy: 0.5536 - val_loss: 0.6732 - val_accuracy: 0.6718\n",
      "Epoch 4/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.6912 - accuracy: 0.5656\n",
      "Epoch 4: val_loss improved from 0.67322 to 0.66947, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.6936 - accuracy: 0.5736 - val_loss: 0.6695 - val_accuracy: 0.6718\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6218 - accuracy: 0.6449\n",
      "Epoch 5: val_loss improved from 0.66947 to 0.66635, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.6232 - accuracy: 0.6434 - val_loss: 0.6664 - val_accuracy: 0.6718\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6607 - accuracy: 0.6534\n",
      "Epoch 6: val_loss improved from 0.66635 to 0.66411, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 18ms/step - loss: 0.6520 - accuracy: 0.6584 - val_loss: 0.6641 - val_accuracy: 0.6718\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5747 - accuracy: 0.6932\n",
      "Epoch 7: val_loss improved from 0.66411 to 0.66145, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.5760 - accuracy: 0.6883 - val_loss: 0.6615 - val_accuracy: 0.6718\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5341 - accuracy: 0.7244\n",
      "Epoch 8: val_loss improved from 0.66145 to 0.66090, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5425 - accuracy: 0.7207 - val_loss: 0.6609 - val_accuracy: 0.6718\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5482 - accuracy: 0.7045\n",
      "Epoch 9: val_loss improved from 0.66090 to 0.65987, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.5569 - accuracy: 0.7032 - val_loss: 0.6599 - val_accuracy: 0.6718\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5562 - accuracy: 0.7131\n",
      "Epoch 10: val_loss improved from 0.65987 to 0.65895, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5496 - accuracy: 0.7232 - val_loss: 0.6589 - val_accuracy: 0.6718\n",
      "Epoch 11/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4864 - accuracy: 0.7614\n",
      "Epoch 11: val_loss improved from 0.65895 to 0.65866, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.4810 - accuracy: 0.7581 - val_loss: 0.6587 - val_accuracy: 0.6718\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4936 - accuracy: 0.7699\n",
      "Epoch 12: val_loss did not improve from 0.65866\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4923 - accuracy: 0.7656 - val_loss: 0.6591 - val_accuracy: 0.6718\n",
      "Epoch 13/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4552 - accuracy: 0.8011\n",
      "Epoch 13: val_loss did not improve from 0.65866\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4511 - accuracy: 0.8005 - val_loss: 0.6599 - val_accuracy: 0.6718\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4456 - accuracy: 0.7727\n",
      "Epoch 14: val_loss did not improve from 0.65866\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4441 - accuracy: 0.7830 - val_loss: 0.6598 - val_accuracy: 0.6718\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4132 - accuracy: 0.8040\n",
      "Epoch 15: val_loss did not improve from 0.65866\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4258 - accuracy: 0.7980 - val_loss: 0.6609 - val_accuracy: 0.6718\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4471 - accuracy: 0.7898\n",
      "Epoch 16: val_loss did not improve from 0.65866\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4315 - accuracy: 0.8030 - val_loss: 0.6627 - val_accuracy: 0.6718\n",
      "Epoch 17/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4094 - accuracy: 0.8523\n",
      "Epoch 17: val_loss did not improve from 0.65866\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4096 - accuracy: 0.8454 - val_loss: 0.6639 - val_accuracy: 0.6641\n",
      "Epoch 18/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4110 - accuracy: 0.8097\n",
      "Epoch 18: val_loss did not improve from 0.65866\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4261 - accuracy: 0.8030 - val_loss: 0.6655 - val_accuracy: 0.6489\n",
      "Epoch 19/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3668 - accuracy: 0.8438\n",
      "Epoch 19: val_loss did not improve from 0.65866\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3613 - accuracy: 0.8504 - val_loss: 0.6670 - val_accuracy: 0.6412\n",
      "Epoch 20/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3456 - accuracy: 0.8551\n",
      "Epoch 20: val_loss did not improve from 0.65866\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3546 - accuracy: 0.8529 - val_loss: 0.6684 - val_accuracy: 0.6565\n",
      "Epoch 21/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3366 - accuracy: 0.8693\n",
      "Epoch 21: val_loss did not improve from 0.65866\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3275 - accuracy: 0.8778 - val_loss: 0.6691 - val_accuracy: 0.6412\n",
      "Epoch 22/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3467 - accuracy: 0.8580\n",
      "Epoch 22: val_loss did not improve from 0.65866\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3381 - accuracy: 0.8703 - val_loss: 0.6703 - val_accuracy: 0.6260\n",
      "Epoch 23/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3254 - accuracy: 0.8835\n",
      "Epoch 23: val_loss did not improve from 0.65866\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3280 - accuracy: 0.8853 - val_loss: 0.6708 - val_accuracy: 0.6183\n",
      "Epoch 24/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3341 - accuracy: 0.8665\n",
      "Epoch 24: val_loss did not improve from 0.65866\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3357 - accuracy: 0.8703 - val_loss: 0.6712 - val_accuracy: 0.6031\n",
      "Epoch 25/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2953 - accuracy: 0.9034\n",
      "Epoch 25: val_loss did not improve from 0.65866\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.2972 - accuracy: 0.8928 - val_loss: 0.6718 - val_accuracy: 0.5878\n",
      "Epoch 26/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2808 - accuracy: 0.9091Restoring model weights from the end of the best epoch: 11.\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.65866\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.2842 - accuracy: 0.9027 - val_loss: 0.6727 - val_accuracy: 0.5954\n",
      "Epoch 26: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.7368\n",
      "Test AUC for Layer 1: 0.4309\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_94\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_352 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_258 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_188 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_353 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_259 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_189 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_354 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_260 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " dense_355 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8415 - accuracy: 0.5547\n",
      "Epoch 1: val_loss improved from inf to 0.71341, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8436 - accuracy: 0.5508 - val_loss: 0.7134 - val_accuracy: 0.2632\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8196 - accuracy: 0.5684\n",
      "Epoch 2: val_loss did not improve from 0.71341\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.8145 - accuracy: 0.5733 - val_loss: 0.7374 - val_accuracy: 0.2632\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7435 - accuracy: 0.5957\n",
      "Epoch 3: val_loss did not improve from 0.71341\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7346 - accuracy: 0.6015 - val_loss: 0.7620 - val_accuracy: 0.2632\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6662 - accuracy: 0.6582\n",
      "Epoch 4: val_loss did not improve from 0.71341\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6618 - accuracy: 0.6635 - val_loss: 0.7871 - val_accuracy: 0.2632\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6773 - accuracy: 0.6348\n",
      "Epoch 5: val_loss did not improve from 0.71341\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6776 - accuracy: 0.6353 - val_loss: 0.8122 - val_accuracy: 0.2632\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6247 - accuracy: 0.6898\n",
      "Epoch 6: val_loss did not improve from 0.71341\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6247 - accuracy: 0.6898 - val_loss: 0.8397 - val_accuracy: 0.2632\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5766 - accuracy: 0.7148\n",
      "Epoch 7: val_loss did not improve from 0.71341\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5849 - accuracy: 0.7068 - val_loss: 0.8667 - val_accuracy: 0.2632\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5768 - accuracy: 0.7129\n",
      "Epoch 8: val_loss did not improve from 0.71341\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5790 - accuracy: 0.7105 - val_loss: 0.8947 - val_accuracy: 0.2632\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5132 - accuracy: 0.7380\n",
      "Epoch 9: val_loss did not improve from 0.71341\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5148 - accuracy: 0.7387 - val_loss: 0.9215 - val_accuracy: 0.2632\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4952 - accuracy: 0.7656\n",
      "Epoch 10: val_loss did not improve from 0.71341\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4911 - accuracy: 0.7669 - val_loss: 0.9536 - val_accuracy: 0.2632\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5001 - accuracy: 0.7444\n",
      "Epoch 11: val_loss did not improve from 0.71341\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5001 - accuracy: 0.7444 - val_loss: 0.9777 - val_accuracy: 0.2632\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4833 - accuracy: 0.7669\n",
      "Epoch 12: val_loss did not improve from 0.71341\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4833 - accuracy: 0.7669 - val_loss: 1.0069 - val_accuracy: 0.2632\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4389 - accuracy: 0.7930\n",
      "Epoch 13: val_loss did not improve from 0.71341\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4408 - accuracy: 0.7914 - val_loss: 1.0286 - val_accuracy: 0.2632\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3881 - accuracy: 0.8281\n",
      "Epoch 14: val_loss did not improve from 0.71341\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3998 - accuracy: 0.8214 - val_loss: 1.0530 - val_accuracy: 0.2632\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4148 - accuracy: 0.8164\n",
      "Epoch 15: val_loss did not improve from 0.71341\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4202 - accuracy: 0.8102 - val_loss: 1.0770 - val_accuracy: 0.2632\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4023 - accuracy: 0.8029Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.71341\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3977 - accuracy: 0.8139 - val_loss: 1.0996 - val_accuracy: 0.2632\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4950\n",
      "Test AUC for Layer 2: 0.5667\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_95\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_356 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_261 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_190 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_357 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_262 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_191 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_358 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_263 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_359 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.9223 - accuracy: 0.4753\n",
      "Epoch 1: val_loss improved from inf to 0.69400, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 24ms/step - loss: 0.9185 - accuracy: 0.4775 - val_loss: 0.6940 - val_accuracy: 0.4950\n",
      "Epoch 2/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.7491 - accuracy: 0.5953\n",
      "Epoch 2: val_loss did not improve from 0.69400\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7475 - accuracy: 0.5961 - val_loss: 0.6955 - val_accuracy: 0.4950\n",
      "Epoch 3/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.6963 - accuracy: 0.6181\n",
      "Epoch 3: val_loss did not improve from 0.69400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6974 - accuracy: 0.6156 - val_loss: 0.6968 - val_accuracy: 0.4950\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6715 - accuracy: 0.6201\n",
      "Epoch 4: val_loss did not improve from 0.69400\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6715 - accuracy: 0.6201 - val_loss: 0.6987 - val_accuracy: 0.4950\n",
      "Epoch 5/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.6329 - accuracy: 0.6510\n",
      "Epoch 5: val_loss did not improve from 0.69400\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6252 - accuracy: 0.6652 - val_loss: 0.6998 - val_accuracy: 0.4950\n",
      "Epoch 6/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.6450 - accuracy: 0.6349\n",
      "Epoch 6: val_loss did not improve from 0.69400\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6455 - accuracy: 0.6381 - val_loss: 0.7005 - val_accuracy: 0.4950\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6273 - accuracy: 0.6667\n",
      "Epoch 7: val_loss did not improve from 0.69400\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6273 - accuracy: 0.6667 - val_loss: 0.7014 - val_accuracy: 0.4950\n",
      "Epoch 8/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.5676 - accuracy: 0.7096\n",
      "Epoch 8: val_loss did not improve from 0.69400\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5743 - accuracy: 0.7072 - val_loss: 0.7019 - val_accuracy: 0.4950\n",
      "Epoch 9/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5453 - accuracy: 0.7220\n",
      "Epoch 9: val_loss did not improve from 0.69400\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5529 - accuracy: 0.7177 - val_loss: 0.7023 - val_accuracy: 0.4950\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5093 - accuracy: 0.7523\n",
      "Epoch 10: val_loss did not improve from 0.69400\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5093 - accuracy: 0.7523 - val_loss: 0.7029 - val_accuracy: 0.4950\n",
      "Epoch 11/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.4789 - accuracy: 0.8145\n",
      "Epoch 11: val_loss did not improve from 0.69400\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4815 - accuracy: 0.8168 - val_loss: 0.7039 - val_accuracy: 0.4851\n",
      "Epoch 12/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4847 - accuracy: 0.7747\n",
      "Epoch 12: val_loss did not improve from 0.69400\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4868 - accuracy: 0.7703 - val_loss: 0.7043 - val_accuracy: 0.4851\n",
      "Epoch 13/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4742 - accuracy: 0.7604\n",
      "Epoch 13: val_loss did not improve from 0.69400\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4693 - accuracy: 0.7598 - val_loss: 0.7043 - val_accuracy: 0.4752\n",
      "Epoch 14/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4537 - accuracy: 0.7743\n",
      "Epoch 14: val_loss did not improve from 0.69400\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4523 - accuracy: 0.7748 - val_loss: 0.7069 - val_accuracy: 0.4752\n",
      "Epoch 15/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.4099 - accuracy: 0.8033\n",
      "Epoch 15: val_loss did not improve from 0.69400\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4056 - accuracy: 0.8063 - val_loss: 0.7090 - val_accuracy: 0.4950\n",
      "Epoch 16/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3976 - accuracy: 0.8184Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69400\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4181 - accuracy: 0.8033 - val_loss: 0.7107 - val_accuracy: 0.4752\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5571\n",
      "Test AUC for Layer 3: 0.4012\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5963\n",
      "Average Test AUC across all layers: 0.4662\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Title and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Created XGBoost model\n",
      "Training XGBoost model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XGBClassifier.fit() got an unexpected keyword argument 'eval_metric'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 811\u001b[0m\n\u001b[0;32m    808\u001b[0m predictor \u001b[38;5;241m=\u001b[39m ClimateNewsOpenAIPredictor(csv_path)\n\u001b[0;32m    810\u001b[0m \u001b[38;5;66;03m# Run the complete analysis\u001b[39;00m\n\u001b[1;32m--> 811\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefine_time_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_all_combinations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalysis complete! Results saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenAI_MLP_XGB\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 605\u001b[0m, in \u001b[0;36mClimateNewsOpenAIPredictor.run_all_combinations\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m embedding_col \u001b[38;5;129;01min\u001b[39;00m embedding_cols:\n\u001b[0;32m    604\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m label_col \u001b[38;5;129;01min\u001b[39;00m label_cols:\n\u001b[1;32m--> 605\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;66;03m# Print summary\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 544\u001b[0m, in \u001b[0;36mClimateNewsOpenAIPredictor.train_and_evaluate_model\u001b[1;34m(self, text_col, label_col, model_type)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining XGBoost model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    543\u001b[0m \u001b[38;5;66;03m# Train model with early stopping\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m model, evals_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_xgb_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[0;32m    549\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[0;32m    552\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[8], line 325\u001b[0m, in \u001b[0;36mClimateNewsOpenAIPredictor.train_xgb_model\u001b[1;34m(self, model, X_train, y_train, X_val, y_val, early_stopping_rounds)\u001b[0m\n\u001b[0;32m    322\u001b[0m eval_set \u001b[38;5;241m=\u001b[39m [(X_train, y_train), (X_val, y_val)]\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# Train the model with the scikit-learn interface\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# Get the evaluation results\u001b[39;00m\n\u001b[0;32m    334\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m: []},\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[0;32m    337\u001b[0m }\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: XGBClassifier.fit() got an unexpected keyword argument 'eval_metric'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, model_type, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class ClimateNewsOpenAIPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'OpenAI_MLP_XGB/visualizations_mlp/COP'\n",
    "        self.xgb_viz_dir = 'OpenAI_MLP_XGB/visualizations_xgb/COP'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs(self.xgb_viz_dir, exist_ok=True)\n",
    "        os.makedirs('OpenAI_MLP_XGB/visualizations_summary/COP', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert embeddings to numpy arrays\n",
    "        self.data['Title_embedding'] = self.data['Title_embedding_vector'].apply(parse_embedding)\n",
    "        self.data['Fulltext_embedding'] = self.data['Full_text_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_title_embedding = self.data['Title_embedding'].iloc[0]\n",
    "        sample_fulltext_embedding = self.data['Fulltext_embedding'].iloc[0]\n",
    "        \n",
    "        print(f\"Sample Title embedding shape: {sample_title_embedding.shape}\")\n",
    "        print(f\"Sample Fulltext embedding shape: {sample_fulltext_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2014-2020), validation (2020-2021), test (2021-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2020-10-31'),\n",
    "            'val_start': pd.Timestamp('2020-11-01'),\n",
    "            'val_end': pd.Timestamp('2021-10-31'),\n",
    "            'test_start': pd.Timestamp('2021-11-01'),\n",
    "            'test_end': pd.Timestamp('2022-10-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2014-2021), validation (2021-2022), test (2022-2023)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2021-10-31'),\n",
    "            'val_start': pd.Timestamp('2021-11-01'),\n",
    "            'val_end': pd.Timestamp('2022-10-31'),\n",
    "            'test_start': pd.Timestamp('2022-11-01'),\n",
    "            'test_end': pd.Timestamp('2023-10-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2014-2022), validation (2022-2023), test (2023-2024)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2022-10-31'),\n",
    "            'val_start': pd.Timestamp('2022-11-01'),\n",
    "            'val_end': pd.Timestamp('2023-10-31'),\n",
    "            'test_start': pd.Timestamp('2023-11-01'),\n",
    "            'test_end': pd.Timestamp('2024-11-01')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, text_col, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            text_col: The column containing the embeddings ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data[text_col].values)\n",
    "        X_val = np.stack(val_data[text_col].values)\n",
    "        X_test = np.stack(test_data[text_col].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def get_xgb_parameters(self):\n",
    "        \"\"\"\n",
    "        Get XGBoost parameters and parameter tuning space optimized for high-dimensional embeddings.\n",
    "        Uses a single parameter set for both Title and Full text embeddings since they have the same dimension (1536).\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of base parameters and parameter grid for tuning\n",
    "        \"\"\"\n",
    "        # Setup XGBoost base parameters optimized for high-dimensional embeddings\n",
    "        base_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 3,\n",
    "            'learning_rate': 0.03,\n",
    "            'subsample': 0.7,          # Row subsampling to prevent overfitting\n",
    "            'colsample_bytree': 0.7,   # Column subsampling to handle high dimensionality\n",
    "            'min_child_weight': 3,     # Prevents overfitting on high-dimensional embeddings\n",
    "            'reg_alpha': 0.5,          # L1 regularization\n",
    "            'reg_lambda': 1.0,         # L2 regularization\n",
    "            'random_state': 42\n",
    "        }\n",
    "        \n",
    "        # Parameter grid for tuning\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'max_depth': [2, 3, 4],\n",
    "            'learning_rate': [0.01, 0.03, 0.05],\n",
    "            'min_child_weight': [2, 3, 5],\n",
    "            'subsample': [0.6, 0.7, 0.8],\n",
    "            'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "            'reg_alpha': [0.1, 0.5, 1.0],\n",
    "            'reg_lambda': [1.0, 2.0, 3.0]\n",
    "        }\n",
    "        \n",
    "        return base_params, param_grid\n",
    "    \n",
    "    def create_xgb_model(self):\n",
    "        \"\"\"\n",
    "        Create an XGBoost model for document-level embeddings.\n",
    "        \n",
    "        Returns:\n",
    "            XGBoost model\n",
    "        \"\"\"\n",
    "        # Get base parameters\n",
    "        base_params, _ = self.get_xgb_parameters()\n",
    "        \n",
    "        # Create XGBoost model with additional required parameters\n",
    "        base_params['use_label_encoder'] = False  # Avoid deprecation warning\n",
    "        base_params['eval_metric'] = 'auc'        # Explicitly set eval metric\n",
    "        \n",
    "        model = xgb.XGBClassifier(**base_params)\n",
    "        \n",
    "        print(f\"Created XGBoost model\")\n",
    "        return model\n",
    "    \n",
    "    def train_xgb_model(self, model, X_train, y_train, X_val, y_val, early_stopping_rounds=10):\n",
    "        \"\"\"\n",
    "        Train XGBoost model with early stopping using scikit-learn API.\n",
    "        \n",
    "        Args:\n",
    "            model: XGBoost model\n",
    "            X_train, y_train: Training data\n",
    "            X_val, y_val: Validation data\n",
    "            early_stopping_rounds: Early stopping patience\n",
    "        \n",
    "        Returns:\n",
    "            Trained model, training history\n",
    "        \"\"\"\n",
    "        # Set up evaluation set for early stopping\n",
    "        eval_set = [(X_train, y_train), (X_val, y_val)]\n",
    "        \n",
    "        # Train the model with the scikit-learn interface\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric='auc',\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            verbose=True\n",
    "        )\n",
    "    \n",
    "        # Get the evaluation results\n",
    "        evals_result = {\n",
    "            'train': {'error': [], 'auc': []},\n",
    "            'validation': {'error': [], 'auc': []}\n",
    "        }\n",
    "    \n",
    "        # Extract evaluation results if available\n",
    "        if hasattr(model, 'evals_result'):\n",
    "            results = model.evals_result()\n",
    "            if results:\n",
    "                # Map the results to our expected format\n",
    "                for i, metric in enumerate(['error', 'auc']):\n",
    "                    if f'validation_{i}' in results:\n",
    "                        validation_key = f'validation_{i}'\n",
    "                        if 'auc' in results[validation_key]:\n",
    "                            evals_result['validation']['auc'] = results[validation_key]['auc']\n",
    "                        if 'error' in results[validation_key]:\n",
    "                            evals_result['validation']['error'] = results[validation_key]['error']\n",
    "                \n",
    "                    if f'train_{i}' in results:\n",
    "                        train_key = f'train_{i}'\n",
    "                        if 'auc' in results[train_key]:\n",
    "                            evals_result['train']['auc'] = results[train_key]['auc']\n",
    "                        if 'error' in results[train_key]:\n",
    "                            evals_result['train']['error'] = results[train_key]['error']\n",
    "    \n",
    "        return model, evals_result\n",
    "    \n",
    "    def plot_xgb_learning_curves(self, evals_result, text_col, label_col, layer_num):\n",
    "        \"\"\"\n",
    "        Plot XGBoost final learning curves.\n",
    "        \n",
    "        Args:\n",
    "            evals_result: Results from training\n",
    "            text_col: Text column used\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "        \"\"\"\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        \n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        \n",
    "        # Plot training error\n",
    "        plt.plot(evals_result['train']['error'], label='Training Error', color='blue')\n",
    "        plt.plot(evals_result['validation']['error'], label='Validation Error', color='orange')\n",
    "        plt.title(f'XGBoost Error Curves ({display_text}, {label_col}, Layer {layer_num})')\n",
    "        plt.xlabel('Boosting Rounds')\n",
    "        plt.ylabel('Error')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        \n",
    "        # Plot AUC\n",
    "        plt.plot(evals_result['train']['auc'], label='Training AUC', color='blue')\n",
    "        plt.plot(evals_result['validation']['auc'], label='Validation AUC', color='orange')\n",
    "        plt.title(f'XGBoost AUC Curves ({display_text}, {label_col}, Layer {layer_num})')\n",
    "        plt.xlabel('Boosting Rounds')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plt.savefig(f\"{self.xgb_viz_dir}/xgb_{display_text.replace(' ', '_')}_{label_col}_layer_{layer_num}.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, model_type, text_col, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            model_type: Model type (MLP)\n",
    "            text_col: Text column used\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_{model_type.lower()}_{display_text.replace(' ', '_')}_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, text_col, label_col, model_type):\n",
    "        \"\"\"\n",
    "        Train and evaluate a model for a specific text column and label column.\n",
    "        \n",
    "        Args:\n",
    "            text_col: The embedding column to use ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "            model_type: The model type to use ('MLP' or 'XGBoost')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        combination_key = f\"{model_type}|{display_text}|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training {model_type} model for {display_text} and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        visualization_dir = self.mlp_viz_dir if model_type == 'MLP' else self.xgb_viz_dir\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, text_col, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            if model_type == 'MLP':\n",
    "                # MLP model training\n",
    "                model = self.create_mlp_model((1536,))\n",
    "                print(f\"Created MLP model for {display_text} ({label_col})\")\n",
    "                model.summary()\n",
    "                \n",
    "                # Setup callbacks\n",
    "                plot_callback = PlotLearningCallback(model_type, display_text, label_col, i+1, visualization_dir)\n",
    "                early_stopping = EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=15,\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                model_checkpoint = ModelCheckpoint(\n",
    "                    filepath=f\"{visualization_dir}/best_{model_type.lower()}_{display_text.lower().replace(' ', '_')}_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                    monitor='val_loss',\n",
    "                    save_weights_only=True,\n",
    "                    save_best_only=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Train model\n",
    "                print(f\"Training MLP model...\")\n",
    "                batch_size = 32  # Larger batch size for embeddings\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                y_pred_proba = model.predict(X_test)\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                # Create final learning curve visualization\n",
    "                self.plot_final_learning_curves(history, model_type, text_col, label_col, i+1, visualization_dir)\n",
    "                \n",
    "                # Store training history\n",
    "                training_history = history.history\n",
    "                \n",
    "            else:  # XGBoost\n",
    "                # XGBoost model training\n",
    "                model = self.create_xgb_model()\n",
    "                print(f\"Training XGBoost model...\")\n",
    "                \n",
    "                # Train model with early stopping\n",
    "                model, evals_result = self.train_xgb_model(\n",
    "                    model,\n",
    "                    X_train, y_train,\n",
    "                    X_val, y_val,\n",
    "                    early_stopping_rounds=10\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                # Plot learning curves\n",
    "                self.plot_xgb_learning_curves(evals_result, text_col, label_col, i+1)\n",
    "                \n",
    "                # Store training history\n",
    "                training_history = evals_result\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'model_type': model_type,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': training_history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for all combinations of text inputs, label columns, and model types.\"\"\"\n",
    "        # Define all combinations\n",
    "        embedding_cols = ['Title_embedding', 'Fulltext_embedding']\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        model_types = ['MLP', 'XGBoost']\n",
    "        \n",
    "        # Run analysis for each combination\n",
    "        for model_type in model_types:\n",
    "            for embedding_col in embedding_cols:\n",
    "                for label_col in label_cols:\n",
    "                    self.train_and_evaluate_model(embedding_col, label_col, model_type)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Model': model_type,\n",
    "                'Text': text_col,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing all model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Split by model type\n",
    "        mlp_data = df[df['Model'] == 'MLP']\n",
    "        xgb_data = df[df['Model'] == 'XGBoost']\n",
    "        \n",
    "        # 1. Performance comparison for MLP\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot accuracy and AUC bars for MLP\n",
    "        x = np.arange(len(mlp_data))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, mlp_data['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, mlp_data['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('MLP Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with OpenAI Embeddings')\n",
    "        labels = [f\"{row['Text']} + {row['Label']}\" for _, row in mlp_data.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(mlp_data['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(mlp_data['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(mlp_data['Avg Accuracy'].max(), mlp_data['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/COP', \"mlp_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Performance comparison for XGBoost\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot accuracy and AUC bars for XGBoost\n",
    "        x = np.arange(len(xgb_data))\n",
    "        \n",
    "        plt.bar(x - width/2, xgb_data['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, xgb_data['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('XGBoost Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of XGBoost Models with OpenAI Embeddings')\n",
    "        labels = [f\"{row['Text']} + {row['Label']}\" for _, row in xgb_data.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(xgb_data['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(xgb_data['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(xgb_data['Avg Accuracy'].max(), xgb_data['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/COP', \"xgb_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"XGBoost summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP and XGBoost models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        mlp_layer_data = []\n",
    "        xgb_layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Model': model_type,\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                if model_type == 'MLP':\n",
    "                    mlp_layer_data.append(layer_info)\n",
    "                else:  # XGBoost\n",
    "                    xgb_layer_data.append(layer_info)\n",
    "        \n",
    "        # Create visualizations for each model type\n",
    "        self._create_model_layer_visualization(mlp_layer_data, 'MLP')\n",
    "        self._create_model_layer_visualization(xgb_layer_data, 'XGBoost')\n",
    "    \n",
    "    def _create_model_layer_visualization(self, layer_data, model_type):\n",
    "        \"\"\"Create layer-specific visualizations for a given model type.\"\"\"\n",
    "        if not layer_data:\n",
    "            print(f\"No layer data available for {model_type}\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} Accuracy by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} AUC by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/COP', f\"{model_type.lower()}_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"{model_type} layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['OpenAI_MLP_XGB/visualizations_mlp/COP', 'OpenAI_MLP_XGB/visualizations_xgb/COP', \n",
    "                      'OpenAI_MLP_XGB/visualizations_summary/COP']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/wall_street_news_semantics_SP500_database/wall_street_news_semantics_COP_completed_openai.csv'\n",
    "    \n",
    "    # Initialize the predictor with OpenAI embeddings\n",
    "    predictor = ClimateNewsOpenAIPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'OpenAI_MLP_XGB' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing OpenAI embedding vectors from string format...\n",
      "Sample Title embedding shape: (1536,)\n",
      "Sample Fulltext embedding shape: (1536,)\n",
      "Loaded 838 climate change news articles spanning from 07/11/2014 to 24/09/2024\n",
      "Class distribution for short-term prediction: {0: 431, 1: 407}\n",
      "Class distribution for long-term prediction: {1: 440, 0: 398}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_121\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_460 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_339 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_242 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_461 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_340 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_243 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_462 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_341 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_463 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.8693 - accuracy: 0.5250\n",
      "Epoch 1: val_loss improved from inf to 0.69090, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 30ms/step - loss: 0.8741 - accuracy: 0.5187 - val_loss: 0.6909 - val_accuracy: 0.5420\n",
      "Epoch 2/100\n",
      " 9/13 [===================>..........] - ETA: 0s - loss: 0.8645 - accuracy: 0.5312\n",
      "Epoch 2: val_loss improved from 0.69090 to 0.69089, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.8347 - accuracy: 0.5586 - val_loss: 0.6909 - val_accuracy: 0.5420\n",
      "Epoch 3/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.7691 - accuracy: 0.6094\n",
      "Epoch 3: val_loss did not improve from 0.69089\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7593 - accuracy: 0.6060 - val_loss: 0.6910 - val_accuracy: 0.5420\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6995 - accuracy: 0.6080\n",
      "Epoch 4: val_loss did not improve from 0.69089\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6980 - accuracy: 0.6185 - val_loss: 0.6913 - val_accuracy: 0.5420\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6530 - accuracy: 0.6392\n",
      "Epoch 5: val_loss did not improve from 0.69089\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6592 - accuracy: 0.6359 - val_loss: 0.6918 - val_accuracy: 0.5420\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6100 - accuracy: 0.6761\n",
      "Epoch 6: val_loss did not improve from 0.69089\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6081 - accuracy: 0.6883 - val_loss: 0.6923 - val_accuracy: 0.5420\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6000 - accuracy: 0.6648\n",
      "Epoch 7: val_loss did not improve from 0.69089\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5909 - accuracy: 0.6758 - val_loss: 0.6928 - val_accuracy: 0.5420\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5502 - accuracy: 0.7358\n",
      "Epoch 8: val_loss did not improve from 0.69089\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5586 - accuracy: 0.7257 - val_loss: 0.6931 - val_accuracy: 0.5420\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5449 - accuracy: 0.7500\n",
      "Epoch 9: val_loss did not improve from 0.69089\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5440 - accuracy: 0.7481 - val_loss: 0.6934 - val_accuracy: 0.5420\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4964 - accuracy: 0.7472\n",
      "Epoch 10: val_loss did not improve from 0.69089\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4944 - accuracy: 0.7581 - val_loss: 0.6936 - val_accuracy: 0.5420\n",
      "Epoch 11/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.4721 - accuracy: 0.7781\n",
      "Epoch 11: val_loss did not improve from 0.69089\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4702 - accuracy: 0.7805 - val_loss: 0.6938 - val_accuracy: 0.5420\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4871 - accuracy: 0.7955\n",
      "Epoch 12: val_loss did not improve from 0.69089\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4841 - accuracy: 0.7955 - val_loss: 0.6944 - val_accuracy: 0.5420\n",
      "Epoch 13/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4213 - accuracy: 0.8097\n",
      "Epoch 13: val_loss did not improve from 0.69089\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4176 - accuracy: 0.8180 - val_loss: 0.6951 - val_accuracy: 0.5420\n",
      "Epoch 14/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.4199 - accuracy: 0.8062\n",
      "Epoch 14: val_loss did not improve from 0.69089\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4296 - accuracy: 0.8005 - val_loss: 0.6952 - val_accuracy: 0.5420\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3834 - accuracy: 0.8494\n",
      "Epoch 15: val_loss did not improve from 0.69089\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3912 - accuracy: 0.8404 - val_loss: 0.6952 - val_accuracy: 0.5420\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3939 - accuracy: 0.8210\n",
      "Epoch 16: val_loss did not improve from 0.69089\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3990 - accuracy: 0.8229 - val_loss: 0.6959 - val_accuracy: 0.5420\n",
      "Epoch 17/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3366 - accuracy: 0.8693Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.69089\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3406 - accuracy: 0.8703 - val_loss: 0.6965 - val_accuracy: 0.5344\n",
      "Epoch 17: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.5338\n",
      "Test AUC for Layer 1: 0.5095\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_122\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_464 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_342 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_244 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_465 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_343 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_245 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_466 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_344 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_467 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.8529 - accuracy: 0.5063\n",
      "Epoch 1: val_loss improved from inf to 0.69204, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8729 - accuracy: 0.5019 - val_loss: 0.6920 - val_accuracy: 0.5338\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7644 - accuracy: 0.5684\n",
      "Epoch 2: val_loss did not improve from 0.69204\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7664 - accuracy: 0.5695 - val_loss: 0.6942 - val_accuracy: 0.5338\n",
      "Epoch 3/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.7813 - accuracy: 0.5437\n",
      "Epoch 3: val_loss did not improve from 0.69204\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.7656 - accuracy: 0.5526 - val_loss: 0.7016 - val_accuracy: 0.5338\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6987 - accuracy: 0.6230\n",
      "Epoch 4: val_loss did not improve from 0.69204\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7055 - accuracy: 0.6184 - val_loss: 0.7138 - val_accuracy: 0.5338\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6430 - accuracy: 0.6230\n",
      "Epoch 5: val_loss did not improve from 0.69204\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6492 - accuracy: 0.6184 - val_loss: 0.7282 - val_accuracy: 0.5338\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6220 - accuracy: 0.6289\n",
      "Epoch 6: val_loss did not improve from 0.69204\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6188 - accuracy: 0.6353 - val_loss: 0.7465 - val_accuracy: 0.5338\n",
      "Epoch 7/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6233 - accuracy: 0.6687\n",
      "Epoch 7: val_loss did not improve from 0.69204\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.6189 - accuracy: 0.6729 - val_loss: 0.7647 - val_accuracy: 0.5338\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5522 - accuracy: 0.7129\n",
      "Epoch 8: val_loss did not improve from 0.69204\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5619 - accuracy: 0.7030 - val_loss: 0.7861 - val_accuracy: 0.5338\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5080 - accuracy: 0.7363\n",
      "Epoch 9: val_loss did not improve from 0.69204\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5052 - accuracy: 0.7368 - val_loss: 0.8067 - val_accuracy: 0.5338\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4992 - accuracy: 0.7480\n",
      "Epoch 10: val_loss did not improve from 0.69204\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5015 - accuracy: 0.7462 - val_loss: 0.8300 - val_accuracy: 0.5338\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4875 - accuracy: 0.7598\n",
      "Epoch 11: val_loss did not improve from 0.69204\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4933 - accuracy: 0.7575 - val_loss: 0.8472 - val_accuracy: 0.5338\n",
      "Epoch 12/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4453 - accuracy: 0.7875\n",
      "Epoch 12: val_loss did not improve from 0.69204\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4431 - accuracy: 0.7857 - val_loss: 0.8652 - val_accuracy: 0.5338\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4086 - accuracy: 0.8125\n",
      "Epoch 13: val_loss did not improve from 0.69204\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4058 - accuracy: 0.8177 - val_loss: 0.8862 - val_accuracy: 0.5338\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4329 - accuracy: 0.8105\n",
      "Epoch 14: val_loss did not improve from 0.69204\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4385 - accuracy: 0.8083 - val_loss: 0.9047 - val_accuracy: 0.5338\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3949 - accuracy: 0.8301\n",
      "Epoch 15: val_loss did not improve from 0.69204\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3949 - accuracy: 0.8289 - val_loss: 0.9145 - val_accuracy: 0.5338\n",
      "Epoch 16/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.3737 - accuracy: 0.8417Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69204\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.3774 - accuracy: 0.8421 - val_loss: 0.9328 - val_accuracy: 0.5338\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5545\n",
      "Test AUC for Layer 2: 0.4952\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_123\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_468 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_345 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_246 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_469 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_346 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_247 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_470 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_347 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_471 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.8847 - accuracy: 0.4906\n",
      "Epoch 1: val_loss improved from inf to 0.68800, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 23ms/step - loss: 0.8884 - accuracy: 0.4910 - val_loss: 0.6880 - val_accuracy: 0.5545\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8133 - accuracy: 0.5210\n",
      "Epoch 2: val_loss improved from 0.68800 to 0.68662, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.8133 - accuracy: 0.5210 - val_loss: 0.6866 - val_accuracy: 0.5545\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7433 - accuracy: 0.5586\n",
      "Epoch 3: val_loss did not improve from 0.68662\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7433 - accuracy: 0.5586 - val_loss: 0.6893 - val_accuracy: 0.5545\n",
      "Epoch 4/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.7366 - accuracy: 0.5789\n",
      "Epoch 4: val_loss did not improve from 0.68662\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7385 - accuracy: 0.5796 - val_loss: 0.6954 - val_accuracy: 0.5545\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6517 - accuracy: 0.6426\n",
      "Epoch 5: val_loss did not improve from 0.68662\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6517 - accuracy: 0.6426 - val_loss: 0.7026 - val_accuracy: 0.5545\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6206 - accuracy: 0.6577\n",
      "Epoch 6: val_loss did not improve from 0.68662\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6206 - accuracy: 0.6577 - val_loss: 0.7134 - val_accuracy: 0.5545\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5760 - accuracy: 0.6847\n",
      "Epoch 7: val_loss did not improve from 0.68662\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5760 - accuracy: 0.6847 - val_loss: 0.7240 - val_accuracy: 0.5545\n",
      "Epoch 8/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5391 - accuracy: 0.7220\n",
      "Epoch 8: val_loss did not improve from 0.68662\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5339 - accuracy: 0.7312 - val_loss: 0.7365 - val_accuracy: 0.5545\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5389 - accuracy: 0.7282\n",
      "Epoch 9: val_loss did not improve from 0.68662\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5389 - accuracy: 0.7282 - val_loss: 0.7502 - val_accuracy: 0.5545\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5092 - accuracy: 0.7568\n",
      "Epoch 10: val_loss did not improve from 0.68662\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5092 - accuracy: 0.7568 - val_loss: 0.7612 - val_accuracy: 0.5545\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4917 - accuracy: 0.7688\n",
      "Epoch 11: val_loss did not improve from 0.68662\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4917 - accuracy: 0.7688 - val_loss: 0.7715 - val_accuracy: 0.5545\n",
      "Epoch 12/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4356 - accuracy: 0.8191\n",
      "Epoch 12: val_loss did not improve from 0.68662\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.4366 - accuracy: 0.8198 - val_loss: 0.7842 - val_accuracy: 0.5545\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4580 - accuracy: 0.7823\n",
      "Epoch 13: val_loss did not improve from 0.68662\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4580 - accuracy: 0.7823 - val_loss: 0.7958 - val_accuracy: 0.5545\n",
      "Epoch 14/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4180 - accuracy: 0.8344\n",
      "Epoch 14: val_loss did not improve from 0.68662\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4211 - accuracy: 0.8318 - val_loss: 0.8050 - val_accuracy: 0.5545\n",
      "Epoch 15/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4064 - accuracy: 0.8203\n",
      "Epoch 15: val_loss did not improve from 0.68662\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.4049 - accuracy: 0.8198 - val_loss: 0.8114 - val_accuracy: 0.5545\n",
      "Epoch 16/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4030 - accuracy: 0.8313\n",
      "Epoch 16: val_loss did not improve from 0.68662\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4027 - accuracy: 0.8303 - val_loss: 0.8155 - val_accuracy: 0.5545\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3801 - accuracy: 0.8363Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.68662\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3801 - accuracy: 0.8363 - val_loss: 0.8238 - val_accuracy: 0.5545\n",
      "Epoch 17: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4000\n",
      "Test AUC for Layer 3: 0.4779\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4961\n",
      "Average Test AUC across all layers: 0.4942\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_124\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_472 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_348 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_248 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_473 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_349 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_249 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_474 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_350 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_475 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.8752 - accuracy: 0.4969\n",
      "Epoch 1: val_loss improved from inf to 0.72254, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 30ms/step - loss: 0.8811 - accuracy: 0.4838 - val_loss: 0.7225 - val_accuracy: 0.3282\n",
      "Epoch 2/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.7732 - accuracy: 0.5562\n",
      "Epoch 2: val_loss did not improve from 0.72254\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7860 - accuracy: 0.5586 - val_loss: 0.7506 - val_accuracy: 0.3282\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7570 - accuracy: 0.5568\n",
      "Epoch 3: val_loss did not improve from 0.72254\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7689 - accuracy: 0.5511 - val_loss: 0.7804 - val_accuracy: 0.3282\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6545 - accuracy: 0.6250\n",
      "Epoch 4: val_loss did not improve from 0.72254\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6644 - accuracy: 0.6160 - val_loss: 0.8087 - val_accuracy: 0.3282\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6530 - accuracy: 0.6335\n",
      "Epoch 5: val_loss did not improve from 0.72254\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6507 - accuracy: 0.6484 - val_loss: 0.8356 - val_accuracy: 0.3282\n",
      "Epoch 6/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.6011 - accuracy: 0.6906\n",
      "Epoch 6: val_loss did not improve from 0.72254\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6295 - accuracy: 0.6783 - val_loss: 0.8620 - val_accuracy: 0.3282\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5822 - accuracy: 0.6989\n",
      "Epoch 7: val_loss did not improve from 0.72254\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5797 - accuracy: 0.7057 - val_loss: 0.8841 - val_accuracy: 0.3282\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5575 - accuracy: 0.6960\n",
      "Epoch 8: val_loss did not improve from 0.72254\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5644 - accuracy: 0.6933 - val_loss: 0.9034 - val_accuracy: 0.3282\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5302 - accuracy: 0.7159\n",
      "Epoch 9: val_loss did not improve from 0.72254\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5154 - accuracy: 0.7307 - val_loss: 0.9182 - val_accuracy: 0.3282\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5166 - accuracy: 0.7585\n",
      "Epoch 10: val_loss did not improve from 0.72254\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5093 - accuracy: 0.7681 - val_loss: 0.9306 - val_accuracy: 0.3282\n",
      "Epoch 11/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5254 - accuracy: 0.7386\n",
      "Epoch 11: val_loss did not improve from 0.72254\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5234 - accuracy: 0.7481 - val_loss: 0.9442 - val_accuracy: 0.3282\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4519 - accuracy: 0.8011\n",
      "Epoch 12: val_loss did not improve from 0.72254\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4748 - accuracy: 0.7805 - val_loss: 0.9597 - val_accuracy: 0.3282\n",
      "Epoch 13/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4430 - accuracy: 0.8040\n",
      "Epoch 13: val_loss did not improve from 0.72254\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4411 - accuracy: 0.8080 - val_loss: 0.9692 - val_accuracy: 0.3282\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3936 - accuracy: 0.8381\n",
      "Epoch 14: val_loss did not improve from 0.72254\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3889 - accuracy: 0.8454 - val_loss: 0.9779 - val_accuracy: 0.3282\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3965 - accuracy: 0.8466\n",
      "Epoch 15: val_loss did not improve from 0.72254\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3861 - accuracy: 0.8504 - val_loss: 0.9808 - val_accuracy: 0.3282\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3705 - accuracy: 0.8438Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.72254\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3654 - accuracy: 0.8479 - val_loss: 0.9885 - val_accuracy: 0.3282\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.2632\n",
      "Test AUC for Layer 1: 0.3854\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_125\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_476 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_351 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_250 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_477 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_352 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_251 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_478 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_353 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_479 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.9054 - accuracy: 0.5021\n",
      "Epoch 1: val_loss improved from inf to 0.71472, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 26ms/step - loss: 0.9008 - accuracy: 0.5000 - val_loss: 0.7147 - val_accuracy: 0.2632\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7943 - accuracy: 0.5430\n",
      "Epoch 2: val_loss did not improve from 0.71472\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7994 - accuracy: 0.5376 - val_loss: 0.7227 - val_accuracy: 0.2632\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7131 - accuracy: 0.6074\n",
      "Epoch 3: val_loss did not improve from 0.71472\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7213 - accuracy: 0.6015 - val_loss: 0.7300 - val_accuracy: 0.2632\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6938 - accuracy: 0.6289\n",
      "Epoch 4: val_loss did not improve from 0.71472\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7071 - accuracy: 0.6222 - val_loss: 0.7321 - val_accuracy: 0.2632\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6676 - accuracy: 0.6504\n",
      "Epoch 5: val_loss did not improve from 0.71472\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6654 - accuracy: 0.6523 - val_loss: 0.7339 - val_accuracy: 0.2632\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6072 - accuracy: 0.6641\n",
      "Epoch 6: val_loss did not improve from 0.71472\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6071 - accuracy: 0.6617 - val_loss: 0.7311 - val_accuracy: 0.2632\n",
      "Epoch 7/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5689 - accuracy: 0.6896\n",
      "Epoch 7: val_loss did not improve from 0.71472\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5833 - accuracy: 0.6786 - val_loss: 0.7312 - val_accuracy: 0.2632\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5414 - accuracy: 0.7148\n",
      "Epoch 8: val_loss did not improve from 0.71472\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5386 - accuracy: 0.7143 - val_loss: 0.7264 - val_accuracy: 0.2632\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5079 - accuracy: 0.7637\n",
      "Epoch 9: val_loss did not improve from 0.71472\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5091 - accuracy: 0.7613 - val_loss: 0.7217 - val_accuracy: 0.2782\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4889 - accuracy: 0.7422\n",
      "Epoch 10: val_loss did not improve from 0.71472\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4949 - accuracy: 0.7368 - val_loss: 0.7147 - val_accuracy: 0.2632\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5041 - accuracy: 0.7500\n",
      "Epoch 11: val_loss improved from 0.71472 to 0.70516, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.5067 - accuracy: 0.7462 - val_loss: 0.7052 - val_accuracy: 0.3910\n",
      "Epoch 12/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4473 - accuracy: 0.8167\n",
      "Epoch 12: val_loss improved from 0.70516 to 0.70046, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4491 - accuracy: 0.8139 - val_loss: 0.7005 - val_accuracy: 0.4286\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4192 - accuracy: 0.7949\n",
      "Epoch 13: val_loss improved from 0.70046 to 0.69573, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4138 - accuracy: 0.8008 - val_loss: 0.6957 - val_accuracy: 0.4737\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3806 - accuracy: 0.8477\n",
      "Epoch 14: val_loss improved from 0.69573 to 0.69247, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.3852 - accuracy: 0.8459 - val_loss: 0.6925 - val_accuracy: 0.4887\n",
      "Epoch 15/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4256 - accuracy: 0.7979\n",
      "Epoch 15: val_loss improved from 0.69247 to 0.68971, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.4233 - accuracy: 0.7951 - val_loss: 0.6897 - val_accuracy: 0.4962\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4124 - accuracy: 0.8047\n",
      "Epoch 16: val_loss improved from 0.68971 to 0.68847, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4146 - accuracy: 0.8026 - val_loss: 0.6885 - val_accuracy: 0.4962\n",
      "Epoch 17/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3720 - accuracy: 0.8398\n",
      "Epoch 17: val_loss did not improve from 0.68847\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3717 - accuracy: 0.8421 - val_loss: 0.6918 - val_accuracy: 0.4962\n",
      "Epoch 18/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3641 - accuracy: 0.8496\n",
      "Epoch 18: val_loss did not improve from 0.68847\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3659 - accuracy: 0.8496 - val_loss: 0.6956 - val_accuracy: 0.4737\n",
      "Epoch 19/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3337 - accuracy: 0.8652\n",
      "Epoch 19: val_loss did not improve from 0.68847\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3375 - accuracy: 0.8647 - val_loss: 0.6971 - val_accuracy: 0.4586\n",
      "Epoch 20/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2855 - accuracy: 0.9023\n",
      "Epoch 20: val_loss did not improve from 0.68847\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.2853 - accuracy: 0.9023 - val_loss: 0.7017 - val_accuracy: 0.4586\n",
      "Epoch 21/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2810 - accuracy: 0.9102\n",
      "Epoch 21: val_loss did not improve from 0.68847\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.2789 - accuracy: 0.9135 - val_loss: 0.7076 - val_accuracy: 0.4361\n",
      "Epoch 22/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2839 - accuracy: 0.8945\n",
      "Epoch 22: val_loss did not improve from 0.68847\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.2826 - accuracy: 0.8947 - val_loss: 0.7178 - val_accuracy: 0.4511\n",
      "Epoch 23/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2726 - accuracy: 0.9062\n",
      "Epoch 23: val_loss did not improve from 0.68847\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.2771 - accuracy: 0.9041 - val_loss: 0.7276 - val_accuracy: 0.4286\n",
      "Epoch 24/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.2797 - accuracy: 0.9021\n",
      "Epoch 24: val_loss did not improve from 0.68847\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.2835 - accuracy: 0.8947 - val_loss: 0.7349 - val_accuracy: 0.4286\n",
      "Epoch 25/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2595 - accuracy: 0.9062\n",
      "Epoch 25: val_loss did not improve from 0.68847\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.2601 - accuracy: 0.9060 - val_loss: 0.7479 - val_accuracy: 0.4211\n",
      "Epoch 26/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2365 - accuracy: 0.9180\n",
      "Epoch 26: val_loss did not improve from 0.68847\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2386 - accuracy: 0.9154 - val_loss: 0.7622 - val_accuracy: 0.3910\n",
      "Epoch 27/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2265 - accuracy: 0.9434\n",
      "Epoch 27: val_loss did not improve from 0.68847\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.2245 - accuracy: 0.9455 - val_loss: 0.7760 - val_accuracy: 0.3759\n",
      "Epoch 28/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2298 - accuracy: 0.9277\n",
      "Epoch 28: val_loss did not improve from 0.68847\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.2314 - accuracy: 0.9267 - val_loss: 0.7792 - val_accuracy: 0.3835\n",
      "Epoch 29/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2152 - accuracy: 0.9414\n",
      "Epoch 29: val_loss did not improve from 0.68847\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2161 - accuracy: 0.9380 - val_loss: 0.7864 - val_accuracy: 0.3910\n",
      "Epoch 30/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.1984 - accuracy: 0.9492\n",
      "Epoch 30: val_loss did not improve from 0.68847\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.1984 - accuracy: 0.9492 - val_loss: 0.7950 - val_accuracy: 0.4060\n",
      "Epoch 31/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.1915 - accuracy: 0.9492Restoring model weights from the end of the best epoch: 16.\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.68847\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.1904 - accuracy: 0.9492 - val_loss: 0.8064 - val_accuracy: 0.4060\n",
      "Epoch 31: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5347\n",
      "Test AUC for Layer 2: 0.4827\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_126\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_480 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_354 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_252 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_481 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_355 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_253 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_482 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_356 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_483 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.8824 - accuracy: 0.5000\n",
      "Epoch 1: val_loss improved from inf to 0.69454, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 22ms/step - loss: 0.8774 - accuracy: 0.5045 - val_loss: 0.6945 - val_accuracy: 0.4950\n",
      "Epoch 2/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.8022 - accuracy: 0.5359\n",
      "Epoch 2: val_loss did not improve from 0.69454\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7994 - accuracy: 0.5405 - val_loss: 0.6967 - val_accuracy: 0.4950\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7395 - accuracy: 0.5736\n",
      "Epoch 3: val_loss did not improve from 0.69454\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7395 - accuracy: 0.5736 - val_loss: 0.6992 - val_accuracy: 0.4950\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6619 - accuracy: 0.6321\n",
      "Epoch 4: val_loss did not improve from 0.69454\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6619 - accuracy: 0.6321 - val_loss: 0.7015 - val_accuracy: 0.4950\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6474 - accuracy: 0.6471\n",
      "Epoch 5: val_loss did not improve from 0.69454\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6474 - accuracy: 0.6471 - val_loss: 0.7036 - val_accuracy: 0.4950\n",
      "Epoch 6/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.6331 - accuracy: 0.6299\n",
      "Epoch 6: val_loss did not improve from 0.69454\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6310 - accuracy: 0.6351 - val_loss: 0.7057 - val_accuracy: 0.4950\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5768 - accuracy: 0.7027\n",
      "Epoch 7: val_loss did not improve from 0.69454\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5768 - accuracy: 0.7027 - val_loss: 0.7074 - val_accuracy: 0.4950\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5171 - accuracy: 0.7568\n",
      "Epoch 8: val_loss did not improve from 0.69454\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5171 - accuracy: 0.7568 - val_loss: 0.7073 - val_accuracy: 0.4950\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4864 - accuracy: 0.7703\n",
      "Epoch 9: val_loss did not improve from 0.69454\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.4864 - accuracy: 0.7703 - val_loss: 0.7087 - val_accuracy: 0.4950\n",
      "Epoch 10/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5266 - accuracy: 0.7385\n",
      "Epoch 10: val_loss did not improve from 0.69454\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5257 - accuracy: 0.7372 - val_loss: 0.7088 - val_accuracy: 0.4950\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4778 - accuracy: 0.7673\n",
      "Epoch 11: val_loss did not improve from 0.69454\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4778 - accuracy: 0.7673 - val_loss: 0.7097 - val_accuracy: 0.4950\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4654 - accuracy: 0.7658\n",
      "Epoch 12: val_loss did not improve from 0.69454\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4654 - accuracy: 0.7658 - val_loss: 0.7115 - val_accuracy: 0.4950\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4394 - accuracy: 0.7973\n",
      "Epoch 13: val_loss did not improve from 0.69454\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4394 - accuracy: 0.7973 - val_loss: 0.7112 - val_accuracy: 0.4950\n",
      "Epoch 14/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4131 - accuracy: 0.8250\n",
      "Epoch 14: val_loss did not improve from 0.69454\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.4143 - accuracy: 0.8183 - val_loss: 0.7126 - val_accuracy: 0.4950\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3795 - accuracy: 0.8453\n",
      "Epoch 15: val_loss did not improve from 0.69454\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3795 - accuracy: 0.8453 - val_loss: 0.7151 - val_accuracy: 0.4950\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3701 - accuracy: 0.8498Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69454\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3701 - accuracy: 0.8498 - val_loss: 0.7180 - val_accuracy: 0.5347\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5571\n",
      "Test AUC for Layer 3: 0.3267\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4517\n",
      "Average Test AUC across all layers: 0.3983\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_127\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_484 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_357 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_254 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_485 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_358 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " dropout_255 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_486 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_359 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_487 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.8926 - accuracy: 0.5625\n",
      "Epoch 1: val_loss improved from inf to 0.69842, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 31ms/step - loss: 0.9099 - accuracy: 0.5461 - val_loss: 0.6984 - val_accuracy: 0.4580\n",
      "Epoch 2/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.7964 - accuracy: 0.5500\n",
      "Epoch 2: val_loss did not improve from 0.69842\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.7840 - accuracy: 0.5586 - val_loss: 0.7067 - val_accuracy: 0.4580\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7779 - accuracy: 0.5994\n",
      "Epoch 3: val_loss did not improve from 0.69842\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7693 - accuracy: 0.6085 - val_loss: 0.7163 - val_accuracy: 0.4580\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6656 - accuracy: 0.6364\n",
      "Epoch 4: val_loss did not improve from 0.69842\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6599 - accuracy: 0.6359 - val_loss: 0.7253 - val_accuracy: 0.4580\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6244 - accuracy: 0.6335\n",
      "Epoch 5: val_loss did not improve from 0.69842\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6222 - accuracy: 0.6459 - val_loss: 0.7352 - val_accuracy: 0.4580\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6468 - accuracy: 0.6562\n",
      "Epoch 6: val_loss did not improve from 0.69842\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6669 - accuracy: 0.6409 - val_loss: 0.7444 - val_accuracy: 0.4580\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5709 - accuracy: 0.6960\n",
      "Epoch 7: val_loss did not improve from 0.69842\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5786 - accuracy: 0.6933 - val_loss: 0.7543 - val_accuracy: 0.4580\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5108 - accuracy: 0.7358\n",
      "Epoch 8: val_loss did not improve from 0.69842\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5170 - accuracy: 0.7232 - val_loss: 0.7635 - val_accuracy: 0.4580\n",
      "Epoch 9/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5540 - accuracy: 0.6849\n",
      "Epoch 9: val_loss did not improve from 0.69842\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5562 - accuracy: 0.6808 - val_loss: 0.7725 - val_accuracy: 0.4580\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5086 - accuracy: 0.7386\n",
      "Epoch 10: val_loss did not improve from 0.69842\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5033 - accuracy: 0.7406 - val_loss: 0.7799 - val_accuracy: 0.4580\n",
      "Epoch 11/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4681 - accuracy: 0.7699\n",
      "Epoch 11: val_loss did not improve from 0.69842\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4676 - accuracy: 0.7731 - val_loss: 0.7859 - val_accuracy: 0.4580\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4561 - accuracy: 0.7756\n",
      "Epoch 12: val_loss did not improve from 0.69842\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4567 - accuracy: 0.7756 - val_loss: 0.7914 - val_accuracy: 0.4580\n",
      "Epoch 13/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.4624 - accuracy: 0.8094\n",
      "Epoch 13: val_loss did not improve from 0.69842\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4503 - accuracy: 0.8130 - val_loss: 0.7978 - val_accuracy: 0.4580\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4060 - accuracy: 0.8295\n",
      "Epoch 14: val_loss did not improve from 0.69842\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3968 - accuracy: 0.8354 - val_loss: 0.8004 - val_accuracy: 0.4580\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4272 - accuracy: 0.8097\n",
      "Epoch 15: val_loss did not improve from 0.69842\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4188 - accuracy: 0.8130 - val_loss: 0.8034 - val_accuracy: 0.4580\n",
      "Epoch 16/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.4230 - accuracy: 0.8062Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69842\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4108 - accuracy: 0.8180 - val_loss: 0.8077 - val_accuracy: 0.4580\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.4662\n",
      "Test AUC for Layer 1: 0.4639\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_128\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_488 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_360 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_256 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_489 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_361 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_257 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_490 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_362 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_491 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.9138 - accuracy: 0.4808\n",
      "Epoch 1: val_loss improved from inf to 0.70404, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 27ms/step - loss: 0.8687 - accuracy: 0.5038 - val_loss: 0.7040 - val_accuracy: 0.4662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.7561 - accuracy: 0.5750\n",
      "Epoch 2: val_loss did not improve from 0.70404\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7590 - accuracy: 0.5752 - val_loss: 0.7172 - val_accuracy: 0.4662\n",
      "Epoch 3/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.7363 - accuracy: 0.5871\n",
      "Epoch 3: val_loss did not improve from 0.70404\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7388 - accuracy: 0.5827 - val_loss: 0.7324 - val_accuracy: 0.4662\n",
      "Epoch 4/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.7091 - accuracy: 0.6138\n",
      "Epoch 4: val_loss did not improve from 0.70404\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7117 - accuracy: 0.6109 - val_loss: 0.7465 - val_accuracy: 0.4662\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6225 - accuracy: 0.6816\n",
      "Epoch 5: val_loss did not improve from 0.70404\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6219 - accuracy: 0.6861 - val_loss: 0.7612 - val_accuracy: 0.4662\n",
      "Epoch 6/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6682 - accuracy: 0.5979\n",
      "Epoch 6: val_loss did not improve from 0.70404\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6617 - accuracy: 0.6071 - val_loss: 0.7744 - val_accuracy: 0.4662\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5573 - accuracy: 0.6992\n",
      "Epoch 7: val_loss did not improve from 0.70404\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5518 - accuracy: 0.7049 - val_loss: 0.7842 - val_accuracy: 0.4662\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5710 - accuracy: 0.7090\n",
      "Epoch 8: val_loss did not improve from 0.70404\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5629 - accuracy: 0.7105 - val_loss: 0.7941 - val_accuracy: 0.4662\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5270 - accuracy: 0.7480\n",
      "Epoch 9: val_loss did not improve from 0.70404\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5298 - accuracy: 0.7462 - val_loss: 0.8041 - val_accuracy: 0.4662\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5038 - accuracy: 0.7305\n",
      "Epoch 10: val_loss did not improve from 0.70404\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4987 - accuracy: 0.7331 - val_loss: 0.8124 - val_accuracy: 0.4662\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5078 - accuracy: 0.7500\n",
      "Epoch 11: val_loss did not improve from 0.70404\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5056 - accuracy: 0.7500 - val_loss: 0.8213 - val_accuracy: 0.4662\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4617 - accuracy: 0.7930\n",
      "Epoch 12: val_loss did not improve from 0.70404\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4608 - accuracy: 0.7914 - val_loss: 0.8264 - val_accuracy: 0.4662\n",
      "Epoch 13/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.4492 - accuracy: 0.8013\n",
      "Epoch 13: val_loss did not improve from 0.70404\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4544 - accuracy: 0.7914 - val_loss: 0.8296 - val_accuracy: 0.4662\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4327 - accuracy: 0.8102\n",
      "Epoch 14: val_loss did not improve from 0.70404\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4327 - accuracy: 0.8102 - val_loss: 0.8361 - val_accuracy: 0.4662\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4076 - accuracy: 0.8145\n",
      "Epoch 15: val_loss did not improve from 0.70404\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4017 - accuracy: 0.8214 - val_loss: 0.8418 - val_accuracy: 0.4662\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4501 - accuracy: 0.7812Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70404\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4527 - accuracy: 0.7782 - val_loss: 0.8489 - val_accuracy: 0.4662\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4455\n",
      "Test AUC for Layer 2: 0.4329\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_129\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_492 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_363 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_258 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_493 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_364 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_259 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_494 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_365 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_495 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.9461 - accuracy: 0.4859\n",
      "Epoch 1: val_loss improved from inf to 0.69301, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 23ms/step - loss: 0.9438 - accuracy: 0.4865 - val_loss: 0.6930 - val_accuracy: 0.5248\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8267 - accuracy: 0.5646\n",
      "Epoch 2: val_loss did not improve from 0.69301\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.8267 - accuracy: 0.5646 - val_loss: 0.6952 - val_accuracy: 0.4554\n",
      "Epoch 3/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.7951 - accuracy: 0.5757\n",
      "Epoch 3: val_loss did not improve from 0.69301\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7795 - accuracy: 0.5856 - val_loss: 0.6978 - val_accuracy: 0.4455\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7298 - accuracy: 0.6051\n",
      "Epoch 4: val_loss did not improve from 0.69301\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7298 - accuracy: 0.6051 - val_loss: 0.7019 - val_accuracy: 0.4455\n",
      "Epoch 5/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.6719 - accuracy: 0.6649\n",
      "Epoch 5: val_loss did not improve from 0.69301\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6832 - accuracy: 0.6562 - val_loss: 0.7072 - val_accuracy: 0.4455\n",
      "Epoch 6/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.6315 - accuracy: 0.6562\n",
      "Epoch 6: val_loss did not improve from 0.69301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6548 - accuracy: 0.6471 - val_loss: 0.7148 - val_accuracy: 0.4455\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6330 - accuracy: 0.6411\n",
      "Epoch 7: val_loss did not improve from 0.69301\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6330 - accuracy: 0.6411 - val_loss: 0.7217 - val_accuracy: 0.4455\n",
      "Epoch 8/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.5871 - accuracy: 0.6840\n",
      "Epoch 8: val_loss did not improve from 0.69301\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5864 - accuracy: 0.6832 - val_loss: 0.7299 - val_accuracy: 0.4455\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5675 - accuracy: 0.7117\n",
      "Epoch 9: val_loss did not improve from 0.69301\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5675 - accuracy: 0.7117 - val_loss: 0.7407 - val_accuracy: 0.4455\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5580 - accuracy: 0.7282\n",
      "Epoch 10: val_loss did not improve from 0.69301\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5580 - accuracy: 0.7282 - val_loss: 0.7537 - val_accuracy: 0.4455\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5295 - accuracy: 0.7312\n",
      "Epoch 11: val_loss did not improve from 0.69301\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5295 - accuracy: 0.7312 - val_loss: 0.7689 - val_accuracy: 0.4455\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5069 - accuracy: 0.7357\n",
      "Epoch 12: val_loss did not improve from 0.69301\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5069 - accuracy: 0.7357 - val_loss: 0.7798 - val_accuracy: 0.4455\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4912 - accuracy: 0.7417\n",
      "Epoch 13: val_loss did not improve from 0.69301\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4912 - accuracy: 0.7417 - val_loss: 0.7890 - val_accuracy: 0.4455\n",
      "Epoch 14/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.4420 - accuracy: 0.7923\n",
      "Epoch 14: val_loss did not improve from 0.69301\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4519 - accuracy: 0.7838 - val_loss: 0.7939 - val_accuracy: 0.4455\n",
      "Epoch 15/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4289 - accuracy: 0.8142\n",
      "Epoch 15: val_loss did not improve from 0.69301\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4372 - accuracy: 0.8018 - val_loss: 0.7972 - val_accuracy: 0.4455\n",
      "Epoch 16/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4216 - accuracy: 0.7953Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69301\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4198 - accuracy: 0.7973 - val_loss: 0.8077 - val_accuracy: 0.4455\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5857\n",
      "Test AUC for Layer 3: 0.6122\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4991\n",
      "Average Test AUC across all layers: 0.5030\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_130\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_496 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_366 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_260 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_497 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_367 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_261 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_498 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_368 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_499 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      " 9/13 [===================>..........] - ETA: 0s - loss: 0.8670 - accuracy: 0.4722\n",
      "Epoch 1: val_loss improved from inf to 0.67720, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 31ms/step - loss: 0.8741 - accuracy: 0.4564 - val_loss: 0.6772 - val_accuracy: 0.6718\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7935 - accuracy: 0.5341\n",
      "Epoch 2: val_loss improved from 0.67720 to 0.66312, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.7760 - accuracy: 0.5387 - val_loss: 0.6631 - val_accuracy: 0.6718\n",
      "Epoch 3/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.7172 - accuracy: 0.6031\n",
      "Epoch 3: val_loss improved from 0.66312 to 0.65329, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.7194 - accuracy: 0.6035 - val_loss: 0.6533 - val_accuracy: 0.6718\n",
      "Epoch 4/100\n",
      " 9/13 [===================>..........] - ETA: 0s - loss: 0.7050 - accuracy: 0.5764\n",
      "Epoch 4: val_loss improved from 0.65329 to 0.64662, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.6956 - accuracy: 0.5810 - val_loss: 0.6466 - val_accuracy: 0.6718\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6404 - accuracy: 0.6534\n",
      "Epoch 5: val_loss improved from 0.64662 to 0.64207, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.6404 - accuracy: 0.6534 - val_loss: 0.6421 - val_accuracy: 0.6718\n",
      "Epoch 6/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.6027 - accuracy: 0.6687\n",
      "Epoch 6: val_loss improved from 0.64207 to 0.63923, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.6058 - accuracy: 0.6559 - val_loss: 0.6392 - val_accuracy: 0.6718\n",
      "Epoch 7/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.6004 - accuracy: 0.6625\n",
      "Epoch 7: val_loss improved from 0.63923 to 0.63722, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5898 - accuracy: 0.6708 - val_loss: 0.6372 - val_accuracy: 0.6718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.5418 - accuracy: 0.7281\n",
      "Epoch 8: val_loss improved from 0.63722 to 0.63531, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.5539 - accuracy: 0.7107 - val_loss: 0.6353 - val_accuracy: 0.6718\n",
      "Epoch 9/100\n",
      " 9/13 [===================>..........] - ETA: 0s - loss: 0.5146 - accuracy: 0.7465\n",
      "Epoch 9: val_loss improved from 0.63531 to 0.63440, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.5355 - accuracy: 0.7257 - val_loss: 0.6344 - val_accuracy: 0.6718\n",
      "Epoch 10/100\n",
      " 9/13 [===================>..........] - ETA: 0s - loss: 0.5067 - accuracy: 0.7431\n",
      "Epoch 10: val_loss improved from 0.63440 to 0.63380, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.4957 - accuracy: 0.7556 - val_loss: 0.6338 - val_accuracy: 0.6718\n",
      "Epoch 11/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5037 - accuracy: 0.7386\n",
      "Epoch 11: val_loss improved from 0.63380 to 0.63328, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5088 - accuracy: 0.7431 - val_loss: 0.6333 - val_accuracy: 0.6718\n",
      "Epoch 12/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.4381 - accuracy: 0.7937\n",
      "Epoch 12: val_loss improved from 0.63328 to 0.63287, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.4307 - accuracy: 0.8030 - val_loss: 0.6329 - val_accuracy: 0.6718\n",
      "Epoch 13/100\n",
      " 9/13 [===================>..........] - ETA: 0s - loss: 0.4273 - accuracy: 0.8229\n",
      "Epoch 13: val_loss improved from 0.63287 to 0.63272, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.4379 - accuracy: 0.7980 - val_loss: 0.6327 - val_accuracy: 0.6718\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4518 - accuracy: 0.7926\n",
      "Epoch 14: val_loss improved from 0.63272 to 0.63250, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.4449 - accuracy: 0.8005 - val_loss: 0.6325 - val_accuracy: 0.6718\n",
      "Epoch 15/100\n",
      " 9/13 [===================>..........] - ETA: 0s - loss: 0.3951 - accuracy: 0.8438\n",
      "Epoch 15: val_loss did not improve from 0.63250\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4152 - accuracy: 0.8304 - val_loss: 0.6326 - val_accuracy: 0.6718\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4009 - accuracy: 0.8295\n",
      "Epoch 16: val_loss did not improve from 0.63250\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4015 - accuracy: 0.8279 - val_loss: 0.6326 - val_accuracy: 0.6718\n",
      "Epoch 17/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4312 - accuracy: 0.8097\n",
      "Epoch 17: val_loss did not improve from 0.63250\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4346 - accuracy: 0.8005 - val_loss: 0.6326 - val_accuracy: 0.6718\n",
      "Epoch 18/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3742 - accuracy: 0.8466\n",
      "Epoch 18: val_loss did not improve from 0.63250\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3748 - accuracy: 0.8454 - val_loss: 0.6327 - val_accuracy: 0.6718\n",
      "Epoch 19/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3563 - accuracy: 0.8409\n",
      "Epoch 19: val_loss did not improve from 0.63250\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3661 - accuracy: 0.8379 - val_loss: 0.6329 - val_accuracy: 0.6718\n",
      "Epoch 20/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3487 - accuracy: 0.8494\n",
      "Epoch 20: val_loss did not improve from 0.63250\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3475 - accuracy: 0.8479 - val_loss: 0.6332 - val_accuracy: 0.6718\n",
      "Epoch 21/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3086 - accuracy: 0.9062\n",
      "Epoch 21: val_loss did not improve from 0.63250\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3068 - accuracy: 0.9027 - val_loss: 0.6333 - val_accuracy: 0.6718\n",
      "Epoch 22/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3347 - accuracy: 0.8580\n",
      "Epoch 22: val_loss did not improve from 0.63250\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3419 - accuracy: 0.8554 - val_loss: 0.6336 - val_accuracy: 0.6718\n",
      "Epoch 23/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.3310 - accuracy: 0.8687\n",
      "Epoch 23: val_loss did not improve from 0.63250\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3165 - accuracy: 0.8803 - val_loss: 0.6341 - val_accuracy: 0.6718\n",
      "Epoch 24/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3122 - accuracy: 0.8949\n",
      "Epoch 24: val_loss did not improve from 0.63250\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3057 - accuracy: 0.9027 - val_loss: 0.6350 - val_accuracy: 0.6718\n",
      "Epoch 25/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.3070 - accuracy: 0.8969\n",
      "Epoch 25: val_loss did not improve from 0.63250\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.2964 - accuracy: 0.9002 - val_loss: 0.6354 - val_accuracy: 0.6718\n",
      "Epoch 26/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2791 - accuracy: 0.9176\n",
      "Epoch 26: val_loss did not improve from 0.63250\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.2947 - accuracy: 0.8953 - val_loss: 0.6354 - val_accuracy: 0.6718\n",
      "Epoch 27/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2808 - accuracy: 0.9119\n",
      "Epoch 27: val_loss did not improve from 0.63250\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.2780 - accuracy: 0.9152 - val_loss: 0.6357 - val_accuracy: 0.6718\n",
      "Epoch 28/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2499 - accuracy: 0.9148\n",
      "Epoch 28: val_loss did not improve from 0.63250\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.2517 - accuracy: 0.9152 - val_loss: 0.6357 - val_accuracy: 0.6718\n",
      "Epoch 29/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2718 - accuracy: 0.8892Restoring model weights from the end of the best epoch: 14.\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.63250\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.2746 - accuracy: 0.8953 - val_loss: 0.6364 - val_accuracy: 0.6718\n",
      "Epoch 29: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.7368\n",
      "Test AUC for Layer 1: 0.5073\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_131\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_500 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_369 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_262 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_501 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_370 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_263 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_502 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_371 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_503 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.8671 - accuracy: 0.5793\n",
      "Epoch 1: val_loss improved from inf to 0.72401, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 27ms/step - loss: 0.8784 - accuracy: 0.5714 - val_loss: 0.7240 - val_accuracy: 0.2632\n",
      "Epoch 2/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.7641 - accuracy: 0.6049\n",
      "Epoch 2: val_loss did not improve from 0.72401\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.7580 - accuracy: 0.6034 - val_loss: 0.7656 - val_accuracy: 0.2632\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7714 - accuracy: 0.5625\n",
      "Epoch 3: val_loss did not improve from 0.72401\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7737 - accuracy: 0.5639 - val_loss: 0.8066 - val_accuracy: 0.2632\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.6055\n",
      "Epoch 4: val_loss did not improve from 0.72401\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6929 - accuracy: 0.6071 - val_loss: 0.8574 - val_accuracy: 0.2632\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6662 - accuracy: 0.6289\n",
      "Epoch 5: val_loss did not improve from 0.72401\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6674 - accuracy: 0.6297 - val_loss: 0.9042 - val_accuracy: 0.2632\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6307 - accuracy: 0.6418\n",
      "Epoch 6: val_loss did not improve from 0.72401\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6214 - accuracy: 0.6617 - val_loss: 0.9463 - val_accuracy: 0.2632\n",
      "Epoch 7/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5406 - accuracy: 0.7318\n",
      "Epoch 7: val_loss did not improve from 0.72401\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5643 - accuracy: 0.7162 - val_loss: 0.9901 - val_accuracy: 0.2632\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5849 - accuracy: 0.7090\n",
      "Epoch 8: val_loss did not improve from 0.72401\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5749 - accuracy: 0.7143 - val_loss: 1.0341 - val_accuracy: 0.2632\n",
      "Epoch 9/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4912 - accuracy: 0.7630\n",
      "Epoch 9: val_loss did not improve from 0.72401\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4682 - accuracy: 0.7763 - val_loss: 1.0882 - val_accuracy: 0.2632\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4778 - accuracy: 0.7754\n",
      "Epoch 10: val_loss did not improve from 0.72401\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4842 - accuracy: 0.7688 - val_loss: 1.1428 - val_accuracy: 0.2632\n",
      "Epoch 11/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4843 - accuracy: 0.7740\n",
      "Epoch 11: val_loss did not improve from 0.72401\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4945 - accuracy: 0.7632 - val_loss: 1.1919 - val_accuracy: 0.2632\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4610 - accuracy: 0.8008\n",
      "Epoch 12: val_loss did not improve from 0.72401\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4610 - accuracy: 0.8008 - val_loss: 1.2426 - val_accuracy: 0.2632\n",
      "Epoch 13/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4459 - accuracy: 0.7771\n",
      "Epoch 13: val_loss did not improve from 0.72401\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4470 - accuracy: 0.7782 - val_loss: 1.2845 - val_accuracy: 0.2632\n",
      "Epoch 14/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4163 - accuracy: 0.8221\n",
      "Epoch 14: val_loss did not improve from 0.72401\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4030 - accuracy: 0.8327 - val_loss: 1.3197 - val_accuracy: 0.2632\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3993 - accuracy: 0.8252\n",
      "Epoch 15: val_loss did not improve from 0.72401\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3993 - accuracy: 0.8252 - val_loss: 1.3474 - val_accuracy: 0.2632\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3912 - accuracy: 0.8164Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.72401\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3910 - accuracy: 0.8158 - val_loss: 1.3732 - val_accuracy: 0.2632\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4950\n",
      "Test AUC for Layer 2: 0.4722\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_132\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_504 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_372 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_264 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_505 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_373 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_265 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_506 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_374 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_507 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.8752 - accuracy: 0.4816\n",
      "Epoch 1: val_loss improved from inf to 0.69511, saving model to OpenAI_MLP_XGB/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_3.weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 2s 23ms/step - loss: 0.8551 - accuracy: 0.4895 - val_loss: 0.6951 - val_accuracy: 0.4950\n",
      "Epoch 2/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.7751 - accuracy: 0.5717\n",
      "Epoch 2: val_loss did not improve from 0.69511\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7722 - accuracy: 0.5826 - val_loss: 0.6969 - val_accuracy: 0.4950\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7433 - accuracy: 0.5766\n",
      "Epoch 3: val_loss did not improve from 0.69511\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.7433 - accuracy: 0.5766 - val_loss: 0.6985 - val_accuracy: 0.4950\n",
      "Epoch 4/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.6630 - accuracy: 0.6530\n",
      "Epoch 4: val_loss did not improve from 0.69511\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6695 - accuracy: 0.6456 - val_loss: 0.6996 - val_accuracy: 0.4950\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6572 - accuracy: 0.6471\n",
      "Epoch 5: val_loss did not improve from 0.69511\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6572 - accuracy: 0.6471 - val_loss: 0.7002 - val_accuracy: 0.4950\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6120 - accuracy: 0.6772\n",
      "Epoch 6: val_loss did not improve from 0.69511\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6120 - accuracy: 0.6772 - val_loss: 0.7005 - val_accuracy: 0.4950\n",
      "Epoch 7/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.6073 - accuracy: 0.6728\n",
      "Epoch 7: val_loss did not improve from 0.69511\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5915 - accuracy: 0.6877 - val_loss: 0.6990 - val_accuracy: 0.4950\n",
      "Epoch 8/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.5549 - accuracy: 0.7243\n",
      "Epoch 8: val_loss did not improve from 0.69511\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5448 - accuracy: 0.7297 - val_loss: 0.6991 - val_accuracy: 0.4950\n",
      "Epoch 9/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4995 - accuracy: 0.7625\n",
      "Epoch 9: val_loss did not improve from 0.69511\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5012 - accuracy: 0.7598 - val_loss: 0.6992 - val_accuracy: 0.5347\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5071 - accuracy: 0.7462\n",
      "Epoch 10: val_loss did not improve from 0.69511\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5071 - accuracy: 0.7462 - val_loss: 0.6989 - val_accuracy: 0.4356\n",
      "Epoch 11/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4436 - accuracy: 0.8109\n",
      "Epoch 11: val_loss did not improve from 0.69511\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4454 - accuracy: 0.8018 - val_loss: 0.6992 - val_accuracy: 0.4455\n",
      "Epoch 12/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4887 - accuracy: 0.7604\n",
      "Epoch 12: val_loss did not improve from 0.69511\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4760 - accuracy: 0.7688 - val_loss: 0.7026 - val_accuracy: 0.4554\n",
      "Epoch 13/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4221 - accuracy: 0.8092\n",
      "Epoch 13: val_loss did not improve from 0.69511\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4204 - accuracy: 0.8108 - val_loss: 0.7058 - val_accuracy: 0.4455\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4274 - accuracy: 0.8228\n",
      "Epoch 14: val_loss did not improve from 0.69511\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4274 - accuracy: 0.8228 - val_loss: 0.7086 - val_accuracy: 0.4554\n",
      "Epoch 15/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4110 - accuracy: 0.8194\n",
      "Epoch 15: val_loss did not improve from 0.69511\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4052 - accuracy: 0.8258 - val_loss: 0.7138 - val_accuracy: 0.4554\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3952 - accuracy: 0.8348Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69511\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3952 - accuracy: 0.8348 - val_loss: 0.7181 - val_accuracy: 0.4554\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5571\n",
      "Test AUC for Layer 3: 0.5666\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5963\n",
      "Average Test AUC across all layers: 0.5153\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Title and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.44777\n",
      "[1]\tvalidation_0-auc:0.45704\n",
      "[2]\tvalidation_0-auc:0.47770\n",
      "[3]\tvalidation_0-auc:0.44237\n",
      "[4]\tvalidation_0-auc:0.47430\n",
      "[5]\tvalidation_0-auc:0.44085\n",
      "[6]\tvalidation_0-auc:0.45070\n",
      "[7]\tvalidation_0-auc:0.45845\n",
      "[8]\tvalidation_0-auc:0.48310\n",
      "[9]\tvalidation_0-auc:0.51197\n",
      "[10]\tvalidation_0-auc:0.52535\n",
      "[11]\tvalidation_0-auc:0.50939\n",
      "[12]\tvalidation_0-auc:0.50915\n",
      "[13]\tvalidation_0-auc:0.50047\n",
      "[14]\tvalidation_0-auc:0.51221\n",
      "[15]\tvalidation_0-auc:0.50070\n",
      "[16]\tvalidation_0-auc:0.49624\n",
      "[17]\tvalidation_0-auc:0.51033\n",
      "[18]\tvalidation_0-auc:0.50892\n",
      "[19]\tvalidation_0-auc:0.50423\n",
      "[20]\tvalidation_0-auc:0.49624\n",
      "[21]\tvalidation_0-auc:0.47512\n",
      "[22]\tvalidation_0-auc:0.48568\n",
      "[23]\tvalidation_0-auc:0.48286\n",
      "[24]\tvalidation_0-auc:0.47864\n",
      "[25]\tvalidation_0-auc:0.48404\n",
      "[26]\tvalidation_0-auc:0.48310\n",
      "[27]\tvalidation_0-auc:0.50235\n",
      "[28]\tvalidation_0-auc:0.50211\n",
      "[29]\tvalidation_0-auc:0.50399\n",
      "[30]\tvalidation_0-auc:0.50446\n",
      "[31]\tvalidation_0-auc:0.51338\n",
      "[32]\tvalidation_0-auc:0.51878\n",
      "[33]\tvalidation_0-auc:0.51080\n",
      "[34]\tvalidation_0-auc:0.51972\n",
      "[35]\tvalidation_0-auc:0.51854\n",
      "[36]\tvalidation_0-auc:0.50493\n",
      "[37]\tvalidation_0-auc:0.50540\n",
      "[38]\tvalidation_0-auc:0.50681\n",
      "[39]\tvalidation_0-auc:0.50164\n",
      "[40]\tvalidation_0-auc:0.50423\n",
      "[41]\tvalidation_0-auc:0.50352\n",
      "[42]\tvalidation_0-auc:0.49906\n",
      "[43]\tvalidation_0-auc:0.49742\n",
      "[44]\tvalidation_0-auc:0.49225\n",
      "[45]\tvalidation_0-auc:0.48779\n",
      "[46]\tvalidation_0-auc:0.49730\n",
      "[47]\tvalidation_0-auc:0.49190\n",
      "[48]\tvalidation_0-auc:0.49014\n",
      "[49]\tvalidation_0-auc:0.48357\n",
      "[50]\tvalidation_0-auc:0.48685\n",
      "[51]\tvalidation_0-auc:0.48404\n",
      "[52]\tvalidation_0-auc:0.48099\n",
      "[53]\tvalidation_0-auc:0.48310\n",
      "[54]\tvalidation_0-auc:0.48028\n",
      "[55]\tvalidation_0-auc:0.47277\n",
      "[56]\tvalidation_0-auc:0.46643\n",
      "[57]\tvalidation_0-auc:0.46150\n",
      "[58]\tvalidation_0-auc:0.45845\n",
      "[59]\tvalidation_0-auc:0.46080\n",
      "[60]\tvalidation_0-auc:0.46244\n",
      "[61]\tvalidation_0-auc:0.46643\n",
      "[62]\tvalidation_0-auc:0.46362\n",
      "[63]\tvalidation_0-auc:0.46150\n",
      "[64]\tvalidation_0-auc:0.45892\n",
      "[65]\tvalidation_0-auc:0.46408\n",
      "[66]\tvalidation_0-auc:0.46831\n",
      "[67]\tvalidation_0-auc:0.46432\n",
      "[68]\tvalidation_0-auc:0.45962\n",
      "[69]\tvalidation_0-auc:0.46221\n",
      "[70]\tvalidation_0-auc:0.46103\n",
      "[71]\tvalidation_0-auc:0.46948\n",
      "[72]\tvalidation_0-auc:0.46455\n",
      "[73]\tvalidation_0-auc:0.45869\n",
      "[74]\tvalidation_0-auc:0.45376\n",
      "[75]\tvalidation_0-auc:0.45681\n",
      "[76]\tvalidation_0-auc:0.45446\n",
      "[77]\tvalidation_0-auc:0.45728\n",
      "[78]\tvalidation_0-auc:0.46033\n",
      "[79]\tvalidation_0-auc:0.46033\n",
      "[80]\tvalidation_0-auc:0.46268\n",
      "[81]\tvalidation_0-auc:0.46432\n",
      "[82]\tvalidation_0-auc:0.46831\n",
      "[83]\tvalidation_0-auc:0.46502\n",
      "[84]\tvalidation_0-auc:0.46714\n",
      "[85]\tvalidation_0-auc:0.47113\n",
      "[86]\tvalidation_0-auc:0.46878\n",
      "[87]\tvalidation_0-auc:0.46854\n",
      "[88]\tvalidation_0-auc:0.47230\n",
      "[89]\tvalidation_0-auc:0.46549\n",
      "[90]\tvalidation_0-auc:0.46479\n",
      "[91]\tvalidation_0-auc:0.46808\n",
      "[92]\tvalidation_0-auc:0.47066\n",
      "[93]\tvalidation_0-auc:0.47347\n",
      "[94]\tvalidation_0-auc:0.47183\n",
      "[95]\tvalidation_0-auc:0.47160\n",
      "[96]\tvalidation_0-auc:0.46714\n",
      "[97]\tvalidation_0-auc:0.46901\n",
      "[98]\tvalidation_0-auc:0.46925\n",
      "[99]\tvalidation_0-auc:0.47512\n",
      "Test Accuracy for Layer 1: 0.5188\n",
      "Test AUC for Layer 1: 0.5582\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.49523\n",
      "[1]\tvalidation_0-auc:0.38823\n",
      "[2]\tvalidation_0-auc:0.38130\n",
      "[3]\tvalidation_0-auc:0.39539\n",
      "[4]\tvalidation_0-auc:0.40572\n",
      "[5]\tvalidation_0-auc:0.46365\n",
      "[6]\tvalidation_0-auc:0.46615\n",
      "[7]\tvalidation_0-auc:0.46729\n",
      "[8]\tvalidation_0-auc:0.47388\n",
      "[9]\tvalidation_0-auc:0.48228\n",
      "[10]\tvalidation_0-auc:0.47478\n",
      "[11]\tvalidation_0-auc:0.49364\n",
      "[12]\tvalidation_0-auc:0.49364\n",
      "[13]\tvalidation_0-auc:0.50931\n",
      "[14]\tvalidation_0-auc:0.49500\n",
      "[15]\tvalidation_0-auc:0.49114\n",
      "[16]\tvalidation_0-auc:0.48523\n",
      "[17]\tvalidation_0-auc:0.49409\n",
      "[18]\tvalidation_0-auc:0.49886\n",
      "[19]\tvalidation_0-auc:0.49977\n",
      "[20]\tvalidation_0-auc:0.50477\n",
      "[21]\tvalidation_0-auc:0.50023\n",
      "[22]\tvalidation_0-auc:0.51090\n",
      "[23]\tvalidation_0-auc:0.51340\n",
      "[24]\tvalidation_0-auc:0.51045\n",
      "[25]\tvalidation_0-auc:0.51386\n",
      "[26]\tvalidation_0-auc:0.52272\n",
      "[27]\tvalidation_0-auc:0.53862\n",
      "[28]\tvalidation_0-auc:0.52908\n",
      "[29]\tvalidation_0-auc:0.51386\n",
      "[30]\tvalidation_0-auc:0.50591\n",
      "[31]\tvalidation_0-auc:0.50068\n",
      "[32]\tvalidation_0-auc:0.49500\n",
      "[33]\tvalidation_0-auc:0.48728\n",
      "[34]\tvalidation_0-auc:0.48682\n",
      "[35]\tvalidation_0-auc:0.47774\n",
      "[36]\tvalidation_0-auc:0.46729\n",
      "[37]\tvalidation_0-auc:0.46320\n",
      "[38]\tvalidation_0-auc:0.46456\n",
      "[39]\tvalidation_0-auc:0.46956\n",
      "[40]\tvalidation_0-auc:0.46184\n",
      "[41]\tvalidation_0-auc:0.46115\n",
      "[42]\tvalidation_0-auc:0.45684\n",
      "[43]\tvalidation_0-auc:0.45593\n",
      "[44]\tvalidation_0-auc:0.45139\n",
      "[45]\tvalidation_0-auc:0.45434\n",
      "[46]\tvalidation_0-auc:0.45593\n",
      "[47]\tvalidation_0-auc:0.45411\n",
      "[48]\tvalidation_0-auc:0.45661\n",
      "[49]\tvalidation_0-auc:0.45502\n",
      "[50]\tvalidation_0-auc:0.45525\n",
      "[51]\tvalidation_0-auc:0.45002\n",
      "[52]\tvalidation_0-auc:0.44866\n",
      "[53]\tvalidation_0-auc:0.45797\n",
      "[54]\tvalidation_0-auc:0.45116\n",
      "[55]\tvalidation_0-auc:0.44457\n",
      "[56]\tvalidation_0-auc:0.44775\n",
      "[57]\tvalidation_0-auc:0.44434\n",
      "[58]\tvalidation_0-auc:0.44434\n",
      "[59]\tvalidation_0-auc:0.43980\n",
      "[60]\tvalidation_0-auc:0.44184\n",
      "[61]\tvalidation_0-auc:0.44048\n",
      "[62]\tvalidation_0-auc:0.44412\n",
      "[63]\tvalidation_0-auc:0.44571\n",
      "[64]\tvalidation_0-auc:0.45252\n",
      "[65]\tvalidation_0-auc:0.45593\n",
      "[66]\tvalidation_0-auc:0.45661\n",
      "[67]\tvalidation_0-auc:0.45570\n",
      "[68]\tvalidation_0-auc:0.45025\n",
      "[69]\tvalidation_0-auc:0.45411\n",
      "[70]\tvalidation_0-auc:0.45002\n",
      "[71]\tvalidation_0-auc:0.45661\n",
      "[72]\tvalidation_0-auc:0.45843\n",
      "[73]\tvalidation_0-auc:0.45298\n",
      "[74]\tvalidation_0-auc:0.45275\n",
      "[75]\tvalidation_0-auc:0.44548\n",
      "[76]\tvalidation_0-auc:0.44457\n",
      "[77]\tvalidation_0-auc:0.44321\n",
      "[78]\tvalidation_0-auc:0.44730\n",
      "[79]\tvalidation_0-auc:0.45320\n",
      "[80]\tvalidation_0-auc:0.45229\n",
      "[81]\tvalidation_0-auc:0.45570\n",
      "[82]\tvalidation_0-auc:0.45638\n",
      "[83]\tvalidation_0-auc:0.45252\n",
      "[84]\tvalidation_0-auc:0.45229\n",
      "[85]\tvalidation_0-auc:0.44775\n",
      "[86]\tvalidation_0-auc:0.44866\n",
      "[87]\tvalidation_0-auc:0.44616\n",
      "[88]\tvalidation_0-auc:0.44275\n",
      "[89]\tvalidation_0-auc:0.43866\n",
      "[90]\tvalidation_0-auc:0.44184\n",
      "[91]\tvalidation_0-auc:0.44343\n",
      "[92]\tvalidation_0-auc:0.44048\n",
      "[93]\tvalidation_0-auc:0.44457\n",
      "[94]\tvalidation_0-auc:0.44366\n",
      "[95]\tvalidation_0-auc:0.43912\n",
      "[96]\tvalidation_0-auc:0.43821\n",
      "[97]\tvalidation_0-auc:0.43844\n",
      "[98]\tvalidation_0-auc:0.43889\n",
      "[99]\tvalidation_0-auc:0.43503\n",
      "Test Accuracy for Layer 2: 0.4851\n",
      "Test AUC for Layer 2: 0.5234\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.60377\n",
      "[1]\tvalidation_0-auc:0.60040\n",
      "[2]\tvalidation_0-auc:0.62302\n",
      "[3]\tvalidation_0-auc:0.63611\n",
      "[4]\tvalidation_0-auc:0.62123\n",
      "[5]\tvalidation_0-auc:0.62044\n",
      "[6]\tvalidation_0-auc:0.57242\n",
      "[7]\tvalidation_0-auc:0.58413\n",
      "[8]\tvalidation_0-auc:0.57976\n",
      "[9]\tvalidation_0-auc:0.60000\n",
      "[10]\tvalidation_0-auc:0.58214\n",
      "[11]\tvalidation_0-auc:0.58214\n",
      "[12]\tvalidation_0-auc:0.59762\n",
      "[13]\tvalidation_0-auc:0.58333\n",
      "[14]\tvalidation_0-auc:0.59087\n",
      "[15]\tvalidation_0-auc:0.57698\n",
      "[16]\tvalidation_0-auc:0.57540\n",
      "[17]\tvalidation_0-auc:0.59087\n",
      "[18]\tvalidation_0-auc:0.58651\n",
      "[19]\tvalidation_0-auc:0.58135\n",
      "[20]\tvalidation_0-auc:0.57738\n",
      "[21]\tvalidation_0-auc:0.57976\n",
      "[22]\tvalidation_0-auc:0.57976\n",
      "[23]\tvalidation_0-auc:0.56548\n",
      "[24]\tvalidation_0-auc:0.54405\n",
      "[25]\tvalidation_0-auc:0.54325\n",
      "[26]\tvalidation_0-auc:0.54722\n",
      "[27]\tvalidation_0-auc:0.53492\n",
      "[28]\tvalidation_0-auc:0.53095\n",
      "[29]\tvalidation_0-auc:0.52857\n",
      "[30]\tvalidation_0-auc:0.53492\n",
      "[31]\tvalidation_0-auc:0.52143\n",
      "[32]\tvalidation_0-auc:0.50516\n",
      "[33]\tvalidation_0-auc:0.50000\n",
      "[34]\tvalidation_0-auc:0.48532\n",
      "[35]\tvalidation_0-auc:0.46508\n",
      "[36]\tvalidation_0-auc:0.46746\n",
      "[37]\tvalidation_0-auc:0.46548\n",
      "[38]\tvalidation_0-auc:0.45675\n",
      "[39]\tvalidation_0-auc:0.46111\n",
      "[40]\tvalidation_0-auc:0.45635\n",
      "[41]\tvalidation_0-auc:0.45476\n",
      "[42]\tvalidation_0-auc:0.44563\n",
      "[43]\tvalidation_0-auc:0.43810\n",
      "[44]\tvalidation_0-auc:0.44246\n",
      "[45]\tvalidation_0-auc:0.43452\n",
      "[46]\tvalidation_0-auc:0.42103\n",
      "[47]\tvalidation_0-auc:0.41429\n",
      "[48]\tvalidation_0-auc:0.42937\n",
      "[49]\tvalidation_0-auc:0.42262\n",
      "[50]\tvalidation_0-auc:0.42262\n",
      "[51]\tvalidation_0-auc:0.42579\n",
      "[52]\tvalidation_0-auc:0.44563\n",
      "[53]\tvalidation_0-auc:0.43770\n",
      "[54]\tvalidation_0-auc:0.44524\n",
      "[55]\tvalidation_0-auc:0.44683\n",
      "[56]\tvalidation_0-auc:0.44683\n",
      "[57]\tvalidation_0-auc:0.43929\n",
      "[58]\tvalidation_0-auc:0.45079\n",
      "[59]\tvalidation_0-auc:0.44405\n",
      "[60]\tvalidation_0-auc:0.45238\n",
      "[61]\tvalidation_0-auc:0.45040\n",
      "[62]\tvalidation_0-auc:0.45278\n",
      "[63]\tvalidation_0-auc:0.44921\n",
      "[64]\tvalidation_0-auc:0.45000\n",
      "[65]\tvalidation_0-auc:0.45357\n",
      "[66]\tvalidation_0-auc:0.46230\n",
      "[67]\tvalidation_0-auc:0.46667\n",
      "[68]\tvalidation_0-auc:0.46786\n",
      "[69]\tvalidation_0-auc:0.47897\n",
      "[70]\tvalidation_0-auc:0.48135\n",
      "[71]\tvalidation_0-auc:0.47897\n",
      "[72]\tvalidation_0-auc:0.47619\n",
      "[73]\tvalidation_0-auc:0.46905\n",
      "[74]\tvalidation_0-auc:0.46389\n",
      "[75]\tvalidation_0-auc:0.46190\n",
      "[76]\tvalidation_0-auc:0.46071\n",
      "[77]\tvalidation_0-auc:0.45595\n",
      "[78]\tvalidation_0-auc:0.46071\n",
      "[79]\tvalidation_0-auc:0.46548\n",
      "[80]\tvalidation_0-auc:0.46746\n",
      "[81]\tvalidation_0-auc:0.46865\n",
      "[82]\tvalidation_0-auc:0.46587\n",
      "[83]\tvalidation_0-auc:0.46389\n",
      "[84]\tvalidation_0-auc:0.45754\n",
      "[85]\tvalidation_0-auc:0.45556\n",
      "[86]\tvalidation_0-auc:0.45635\n",
      "[87]\tvalidation_0-auc:0.45952\n",
      "[88]\tvalidation_0-auc:0.46190\n",
      "[89]\tvalidation_0-auc:0.46468\n",
      "[90]\tvalidation_0-auc:0.46468\n",
      "[91]\tvalidation_0-auc:0.46389\n",
      "[92]\tvalidation_0-auc:0.45913\n",
      "[93]\tvalidation_0-auc:0.45675\n",
      "[94]\tvalidation_0-auc:0.46151\n",
      "[95]\tvalidation_0-auc:0.46429\n",
      "[96]\tvalidation_0-auc:0.46667\n",
      "[97]\tvalidation_0-auc:0.46984\n",
      "[98]\tvalidation_0-auc:0.46627\n",
      "[99]\tvalidation_0-auc:0.46349\n",
      "Test Accuracy for Layer 3: 0.5000\n",
      "Test AUC for Layer 3: 0.4796\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5013\n",
      "Average Test AUC across all layers: 0.5204\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Title and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.50436\n",
      "[1]\tvalidation_0-auc:0.50938\n",
      "[2]\tvalidation_0-auc:0.57003\n",
      "[3]\tvalidation_0-auc:0.57056\n",
      "[4]\tvalidation_0-auc:0.57413\n",
      "[5]\tvalidation_0-auc:0.56792\n",
      "[6]\tvalidation_0-auc:0.58430\n",
      "[7]\tvalidation_0-auc:0.57585\n",
      "[8]\tvalidation_0-auc:0.56290\n",
      "[9]\tvalidation_0-auc:0.53938\n",
      "[10]\tvalidation_0-auc:0.55893\n",
      "[11]\tvalidation_0-auc:0.56554\n",
      "[12]\tvalidation_0-auc:0.55206\n",
      "[13]\tvalidation_0-auc:0.54228\n",
      "[14]\tvalidation_0-auc:0.54255\n",
      "[15]\tvalidation_0-auc:0.55206\n",
      "[16]\tvalidation_0-auc:0.55814\n",
      "[17]\tvalidation_0-auc:0.55233\n",
      "[18]\tvalidation_0-auc:0.54598\n",
      "[19]\tvalidation_0-auc:0.54308\n",
      "[20]\tvalidation_0-auc:0.54625\n",
      "[21]\tvalidation_0-auc:0.54413\n",
      "[22]\tvalidation_0-auc:0.55497\n",
      "[23]\tvalidation_0-auc:0.55338\n",
      "[24]\tvalidation_0-auc:0.54730\n",
      "[25]\tvalidation_0-auc:0.54202\n",
      "[26]\tvalidation_0-auc:0.53990\n",
      "[27]\tvalidation_0-auc:0.54096\n",
      "[28]\tvalidation_0-auc:0.54625\n",
      "[29]\tvalidation_0-auc:0.53753\n",
      "[30]\tvalidation_0-auc:0.53990\n",
      "[31]\tvalidation_0-auc:0.54757\n",
      "[32]\tvalidation_0-auc:0.55999\n",
      "[33]\tvalidation_0-auc:0.55233\n",
      "[34]\tvalidation_0-auc:0.54651\n",
      "[35]\tvalidation_0-auc:0.54889\n",
      "[36]\tvalidation_0-auc:0.53990\n",
      "[37]\tvalidation_0-auc:0.54255\n",
      "[38]\tvalidation_0-auc:0.54836\n",
      "[39]\tvalidation_0-auc:0.55021\n",
      "[40]\tvalidation_0-auc:0.55285\n",
      "[41]\tvalidation_0-auc:0.55180\n",
      "[42]\tvalidation_0-auc:0.54942\n",
      "[43]\tvalidation_0-auc:0.54308\n",
      "[44]\tvalidation_0-auc:0.53832\n",
      "[45]\tvalidation_0-auc:0.53938\n",
      "[46]\tvalidation_0-auc:0.54202\n",
      "[47]\tvalidation_0-auc:0.54863\n",
      "[48]\tvalidation_0-auc:0.55285\n",
      "[49]\tvalidation_0-auc:0.55074\n",
      "[50]\tvalidation_0-auc:0.54678\n",
      "[51]\tvalidation_0-auc:0.54730\n",
      "[52]\tvalidation_0-auc:0.54413\n",
      "[53]\tvalidation_0-auc:0.54783\n",
      "[54]\tvalidation_0-auc:0.54334\n",
      "[55]\tvalidation_0-auc:0.54017\n",
      "[56]\tvalidation_0-auc:0.54149\n",
      "[57]\tvalidation_0-auc:0.54017\n",
      "[58]\tvalidation_0-auc:0.54149\n",
      "[59]\tvalidation_0-auc:0.53858\n",
      "[60]\tvalidation_0-auc:0.53832\n",
      "[61]\tvalidation_0-auc:0.54175\n",
      "[62]\tvalidation_0-auc:0.54413\n",
      "[63]\tvalidation_0-auc:0.53858\n",
      "[64]\tvalidation_0-auc:0.53911\n",
      "[65]\tvalidation_0-auc:0.53726\n",
      "[66]\tvalidation_0-auc:0.53938\n",
      "[67]\tvalidation_0-auc:0.53700\n",
      "[68]\tvalidation_0-auc:0.53145\n",
      "[69]\tvalidation_0-auc:0.53436\n",
      "[70]\tvalidation_0-auc:0.53198\n",
      "[71]\tvalidation_0-auc:0.52907\n",
      "[72]\tvalidation_0-auc:0.52748\n",
      "[73]\tvalidation_0-auc:0.53303\n",
      "[74]\tvalidation_0-auc:0.53118\n",
      "[75]\tvalidation_0-auc:0.52881\n",
      "[76]\tvalidation_0-auc:0.53118\n",
      "[77]\tvalidation_0-auc:0.52960\n",
      "[78]\tvalidation_0-auc:0.53251\n",
      "[79]\tvalidation_0-auc:0.53092\n",
      "[80]\tvalidation_0-auc:0.53118\n",
      "[81]\tvalidation_0-auc:0.53673\n",
      "[82]\tvalidation_0-auc:0.53726\n",
      "[83]\tvalidation_0-auc:0.53753\n",
      "[84]\tvalidation_0-auc:0.53647\n",
      "[85]\tvalidation_0-auc:0.53594\n",
      "[86]\tvalidation_0-auc:0.53383\n",
      "[87]\tvalidation_0-auc:0.53647\n",
      "[88]\tvalidation_0-auc:0.54175\n",
      "[89]\tvalidation_0-auc:0.53911\n",
      "[90]\tvalidation_0-auc:0.54096\n",
      "[91]\tvalidation_0-auc:0.54149\n",
      "[92]\tvalidation_0-auc:0.54017\n",
      "[93]\tvalidation_0-auc:0.54017\n",
      "[94]\tvalidation_0-auc:0.54360\n",
      "[95]\tvalidation_0-auc:0.54070\n",
      "[96]\tvalidation_0-auc:0.54281\n",
      "[97]\tvalidation_0-auc:0.54308\n",
      "[98]\tvalidation_0-auc:0.54519\n",
      "[99]\tvalidation_0-auc:0.54413\n",
      "Test Accuracy for Layer 1: 0.3684\n",
      "Test AUC for Layer 1: 0.4703\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.47449\n",
      "[1]\tvalidation_0-auc:0.47915\n",
      "[2]\tvalidation_0-auc:0.47391\n",
      "[3]\tvalidation_0-auc:0.45394\n",
      "[4]\tvalidation_0-auc:0.45525\n",
      "[5]\tvalidation_0-auc:0.46822\n",
      "[6]\tvalidation_0-auc:0.48863\n",
      "[7]\tvalidation_0-auc:0.50671\n",
      "[8]\tvalidation_0-auc:0.53673\n",
      "[9]\tvalidation_0-auc:0.52711\n",
      "[10]\tvalidation_0-auc:0.52478\n",
      "[11]\tvalidation_0-auc:0.55569\n",
      "[12]\tvalidation_0-auc:0.55452\n",
      "[13]\tvalidation_0-auc:0.56035\n",
      "[14]\tvalidation_0-auc:0.54636\n",
      "[15]\tvalidation_0-auc:0.55569\n",
      "[16]\tvalidation_0-auc:0.55598\n",
      "[17]\tvalidation_0-auc:0.55248\n",
      "[18]\tvalidation_0-auc:0.54548\n",
      "[19]\tvalidation_0-auc:0.53499\n",
      "[20]\tvalidation_0-auc:0.54548\n",
      "[21]\tvalidation_0-auc:0.55452\n",
      "[22]\tvalidation_0-auc:0.55510\n",
      "[23]\tvalidation_0-auc:0.55452\n",
      "[24]\tvalidation_0-auc:0.58047\n",
      "[25]\tvalidation_0-auc:0.57114\n",
      "[26]\tvalidation_0-auc:0.56035\n",
      "[27]\tvalidation_0-auc:0.55743\n",
      "[28]\tvalidation_0-auc:0.56851\n",
      "[29]\tvalidation_0-auc:0.57143\n",
      "[30]\tvalidation_0-auc:0.55889\n",
      "[31]\tvalidation_0-auc:0.55627\n",
      "[32]\tvalidation_0-auc:0.56297\n",
      "[33]\tvalidation_0-auc:0.55539\n",
      "[34]\tvalidation_0-auc:0.55773\n",
      "[35]\tvalidation_0-auc:0.56122\n",
      "[36]\tvalidation_0-auc:0.54840\n",
      "[37]\tvalidation_0-auc:0.54577\n",
      "[38]\tvalidation_0-auc:0.54315\n",
      "[39]\tvalidation_0-auc:0.54082\n",
      "[40]\tvalidation_0-auc:0.53907\n",
      "[41]\tvalidation_0-auc:0.54577\n",
      "[42]\tvalidation_0-auc:0.55364\n",
      "[43]\tvalidation_0-auc:0.55481\n",
      "[44]\tvalidation_0-auc:0.54577\n",
      "[45]\tvalidation_0-auc:0.55102\n",
      "[46]\tvalidation_0-auc:0.55627\n",
      "[47]\tvalidation_0-auc:0.55539\n",
      "[48]\tvalidation_0-auc:0.55015\n",
      "[49]\tvalidation_0-auc:0.55160\n",
      "[50]\tvalidation_0-auc:0.55335\n",
      "[51]\tvalidation_0-auc:0.55889\n",
      "[52]\tvalidation_0-auc:0.55131\n",
      "[53]\tvalidation_0-auc:0.55802\n",
      "[54]\tvalidation_0-auc:0.55073\n",
      "[55]\tvalidation_0-auc:0.54665\n",
      "[56]\tvalidation_0-auc:0.54636\n",
      "[57]\tvalidation_0-auc:0.54694\n",
      "[58]\tvalidation_0-auc:0.54956\n",
      "[59]\tvalidation_0-auc:0.55423\n",
      "[60]\tvalidation_0-auc:0.55219\n",
      "[61]\tvalidation_0-auc:0.55131\n",
      "[62]\tvalidation_0-auc:0.55423\n",
      "[63]\tvalidation_0-auc:0.55190\n",
      "[64]\tvalidation_0-auc:0.54402\n",
      "[65]\tvalidation_0-auc:0.54636\n",
      "[66]\tvalidation_0-auc:0.53819\n",
      "[67]\tvalidation_0-auc:0.54169\n",
      "[68]\tvalidation_0-auc:0.54286\n",
      "[69]\tvalidation_0-auc:0.54694\n",
      "[70]\tvalidation_0-auc:0.54665\n",
      "[71]\tvalidation_0-auc:0.54577\n",
      "[72]\tvalidation_0-auc:0.54840\n",
      "[73]\tvalidation_0-auc:0.54694\n",
      "[74]\tvalidation_0-auc:0.55044\n",
      "[75]\tvalidation_0-auc:0.55131\n",
      "[76]\tvalidation_0-auc:0.55510\n",
      "[77]\tvalidation_0-auc:0.55277\n",
      "[78]\tvalidation_0-auc:0.55015\n",
      "[79]\tvalidation_0-auc:0.55190\n",
      "[80]\tvalidation_0-auc:0.55539\n",
      "[81]\tvalidation_0-auc:0.55714\n",
      "[82]\tvalidation_0-auc:0.54985\n",
      "[83]\tvalidation_0-auc:0.55219\n",
      "[84]\tvalidation_0-auc:0.55510\n",
      "[85]\tvalidation_0-auc:0.56035\n",
      "[86]\tvalidation_0-auc:0.55335\n",
      "[87]\tvalidation_0-auc:0.55102\n",
      "[88]\tvalidation_0-auc:0.55131\n",
      "[89]\tvalidation_0-auc:0.55044\n",
      "[90]\tvalidation_0-auc:0.55714\n",
      "[91]\tvalidation_0-auc:0.55714\n",
      "[92]\tvalidation_0-auc:0.55423\n",
      "[93]\tvalidation_0-auc:0.55452\n",
      "[94]\tvalidation_0-auc:0.55714\n",
      "[95]\tvalidation_0-auc:0.55569\n",
      "[96]\tvalidation_0-auc:0.55569\n",
      "[97]\tvalidation_0-auc:0.55335\n",
      "[98]\tvalidation_0-auc:0.55248\n",
      "[99]\tvalidation_0-auc:0.55394\n",
      "Test Accuracy for Layer 2: 0.5941\n",
      "Test AUC for Layer 2: 0.5663\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.59902\n",
      "[1]\tvalidation_0-auc:0.50196\n",
      "[2]\tvalidation_0-auc:0.54039\n",
      "[3]\tvalidation_0-auc:0.55196\n",
      "[4]\tvalidation_0-auc:0.52588\n",
      "[5]\tvalidation_0-auc:0.48235\n",
      "[6]\tvalidation_0-auc:0.45569\n",
      "[7]\tvalidation_0-auc:0.47255\n",
      "[8]\tvalidation_0-auc:0.49686\n",
      "[9]\tvalidation_0-auc:0.50078\n",
      "[10]\tvalidation_0-auc:0.51373\n",
      "[11]\tvalidation_0-auc:0.52471\n",
      "[12]\tvalidation_0-auc:0.51804\n",
      "[13]\tvalidation_0-auc:0.51412\n",
      "[14]\tvalidation_0-auc:0.51647\n",
      "[15]\tvalidation_0-auc:0.52039\n",
      "[16]\tvalidation_0-auc:0.51098\n",
      "[17]\tvalidation_0-auc:0.49882\n",
      "[18]\tvalidation_0-auc:0.49255\n",
      "[19]\tvalidation_0-auc:0.48039\n",
      "[20]\tvalidation_0-auc:0.51020\n",
      "[21]\tvalidation_0-auc:0.53373\n",
      "[22]\tvalidation_0-auc:0.51608\n",
      "[23]\tvalidation_0-auc:0.51373\n",
      "[24]\tvalidation_0-auc:0.51020\n",
      "[25]\tvalidation_0-auc:0.52078\n",
      "[26]\tvalidation_0-auc:0.51412\n",
      "[27]\tvalidation_0-auc:0.51882\n",
      "[28]\tvalidation_0-auc:0.53059\n",
      "[29]\tvalidation_0-auc:0.53451\n",
      "[30]\tvalidation_0-auc:0.54471\n",
      "[31]\tvalidation_0-auc:0.55412\n",
      "[32]\tvalidation_0-auc:0.54431\n",
      "[33]\tvalidation_0-auc:0.54235\n",
      "[34]\tvalidation_0-auc:0.54667\n",
      "[35]\tvalidation_0-auc:0.53451\n",
      "[36]\tvalidation_0-auc:0.53490\n",
      "[37]\tvalidation_0-auc:0.53412\n",
      "[38]\tvalidation_0-auc:0.53412\n",
      "[39]\tvalidation_0-auc:0.53216\n",
      "[40]\tvalidation_0-auc:0.53647\n",
      "[41]\tvalidation_0-auc:0.53333\n",
      "[42]\tvalidation_0-auc:0.52000\n",
      "[43]\tvalidation_0-auc:0.52275\n",
      "[44]\tvalidation_0-auc:0.52824\n",
      "[45]\tvalidation_0-auc:0.52941\n",
      "[46]\tvalidation_0-auc:0.52392\n",
      "[47]\tvalidation_0-auc:0.52431\n",
      "[48]\tvalidation_0-auc:0.52941\n",
      "[49]\tvalidation_0-auc:0.52784\n",
      "[50]\tvalidation_0-auc:0.53176\n",
      "[51]\tvalidation_0-auc:0.52745\n",
      "[52]\tvalidation_0-auc:0.52784\n",
      "[53]\tvalidation_0-auc:0.52941\n",
      "[54]\tvalidation_0-auc:0.53255\n",
      "[55]\tvalidation_0-auc:0.54157\n",
      "[56]\tvalidation_0-auc:0.54941\n",
      "[57]\tvalidation_0-auc:0.55255\n",
      "[58]\tvalidation_0-auc:0.55922\n",
      "[59]\tvalidation_0-auc:0.54941\n",
      "[60]\tvalidation_0-auc:0.54667\n",
      "[61]\tvalidation_0-auc:0.55216\n",
      "[62]\tvalidation_0-auc:0.55333\n",
      "[63]\tvalidation_0-auc:0.55176\n",
      "[64]\tvalidation_0-auc:0.55412\n",
      "[65]\tvalidation_0-auc:0.55176\n",
      "[66]\tvalidation_0-auc:0.54706\n",
      "[67]\tvalidation_0-auc:0.54980\n",
      "[68]\tvalidation_0-auc:0.54784\n",
      "[69]\tvalidation_0-auc:0.55373\n",
      "[70]\tvalidation_0-auc:0.55294\n",
      "[71]\tvalidation_0-auc:0.55725\n",
      "[72]\tvalidation_0-auc:0.55490\n",
      "[73]\tvalidation_0-auc:0.55647\n",
      "[74]\tvalidation_0-auc:0.55686\n",
      "[75]\tvalidation_0-auc:0.55686\n",
      "[76]\tvalidation_0-auc:0.56039\n",
      "[77]\tvalidation_0-auc:0.55294\n",
      "[78]\tvalidation_0-auc:0.55412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79]\tvalidation_0-auc:0.54941\n",
      "[80]\tvalidation_0-auc:0.54392\n",
      "[81]\tvalidation_0-auc:0.54980\n",
      "[82]\tvalidation_0-auc:0.55686\n",
      "[83]\tvalidation_0-auc:0.55882\n",
      "[84]\tvalidation_0-auc:0.55647\n",
      "[85]\tvalidation_0-auc:0.55216\n",
      "[86]\tvalidation_0-auc:0.54392\n",
      "[87]\tvalidation_0-auc:0.54275\n",
      "[88]\tvalidation_0-auc:0.54235\n",
      "[89]\tvalidation_0-auc:0.54863\n",
      "[90]\tvalidation_0-auc:0.54902\n",
      "[91]\tvalidation_0-auc:0.54627\n",
      "[92]\tvalidation_0-auc:0.54314\n",
      "[93]\tvalidation_0-auc:0.54627\n",
      "[94]\tvalidation_0-auc:0.54549\n",
      "[95]\tvalidation_0-auc:0.54627\n",
      "[96]\tvalidation_0-auc:0.54980\n",
      "[97]\tvalidation_0-auc:0.54588\n",
      "[98]\tvalidation_0-auc:0.54627\n",
      "[99]\tvalidation_0-auc:0.54039\n",
      "Test Accuracy for Layer 3: 0.5286\n",
      "Test AUC for Layer 3: 0.5964\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4970\n",
      "Average Test AUC across all layers: 0.5443\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Full text and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.53063\n",
      "[1]\tvalidation_0-auc:0.53556\n",
      "[2]\tvalidation_0-auc:0.51397\n",
      "[3]\tvalidation_0-auc:0.51538\n",
      "[4]\tvalidation_0-auc:0.43650\n",
      "[5]\tvalidation_0-auc:0.39883\n",
      "[6]\tvalidation_0-auc:0.37207\n",
      "[7]\tvalidation_0-auc:0.35634\n",
      "[8]\tvalidation_0-auc:0.35962\n",
      "[9]\tvalidation_0-auc:0.37887\n",
      "[10]\tvalidation_0-auc:0.38122\n",
      "[11]\tvalidation_0-auc:0.40540\n",
      "[12]\tvalidation_0-auc:0.43897\n",
      "[13]\tvalidation_0-auc:0.43592\n",
      "[14]\tvalidation_0-auc:0.42019\n",
      "[15]\tvalidation_0-auc:0.41502\n",
      "[16]\tvalidation_0-auc:0.41080\n",
      "[17]\tvalidation_0-auc:0.40329\n",
      "[18]\tvalidation_0-auc:0.40282\n",
      "[19]\tvalidation_0-auc:0.42958\n",
      "[20]\tvalidation_0-auc:0.44296\n",
      "[21]\tvalidation_0-auc:0.43474\n",
      "[22]\tvalidation_0-auc:0.44085\n",
      "[23]\tvalidation_0-auc:0.44554\n",
      "[24]\tvalidation_0-auc:0.43756\n",
      "[25]\tvalidation_0-auc:0.44202\n",
      "[26]\tvalidation_0-auc:0.44343\n",
      "[27]\tvalidation_0-auc:0.43615\n",
      "[28]\tvalidation_0-auc:0.44859\n",
      "[29]\tvalidation_0-auc:0.44718\n",
      "[30]\tvalidation_0-auc:0.44765\n",
      "[31]\tvalidation_0-auc:0.44178\n",
      "[32]\tvalidation_0-auc:0.44131\n",
      "[33]\tvalidation_0-auc:0.43826\n",
      "[34]\tvalidation_0-auc:0.43991\n",
      "[35]\tvalidation_0-auc:0.44695\n",
      "[36]\tvalidation_0-auc:0.44390\n",
      "[37]\tvalidation_0-auc:0.44906\n",
      "[38]\tvalidation_0-auc:0.44343\n",
      "[39]\tvalidation_0-auc:0.44789\n",
      "[40]\tvalidation_0-auc:0.43451\n",
      "[41]\tvalidation_0-auc:0.44765\n",
      "[42]\tvalidation_0-auc:0.44930\n",
      "[43]\tvalidation_0-auc:0.44460\n",
      "[44]\tvalidation_0-auc:0.44249\n",
      "[45]\tvalidation_0-auc:0.44155\n",
      "[46]\tvalidation_0-auc:0.44296\n",
      "[47]\tvalidation_0-auc:0.43216\n",
      "[48]\tvalidation_0-auc:0.42934\n",
      "[49]\tvalidation_0-auc:0.42676\n",
      "[50]\tvalidation_0-auc:0.42746\n",
      "[51]\tvalidation_0-auc:0.42700\n",
      "[52]\tvalidation_0-auc:0.41854\n",
      "[53]\tvalidation_0-auc:0.41854\n",
      "[54]\tvalidation_0-auc:0.41901\n",
      "[55]\tvalidation_0-auc:0.41291\n",
      "[56]\tvalidation_0-auc:0.42113\n",
      "[57]\tvalidation_0-auc:0.42746\n",
      "[58]\tvalidation_0-auc:0.42230\n",
      "[59]\tvalidation_0-auc:0.43075\n",
      "[60]\tvalidation_0-auc:0.42676\n",
      "[61]\tvalidation_0-auc:0.41620\n",
      "[62]\tvalidation_0-auc:0.41878\n",
      "[63]\tvalidation_0-auc:0.41784\n",
      "[64]\tvalidation_0-auc:0.41784\n",
      "[65]\tvalidation_0-auc:0.41690\n",
      "[66]\tvalidation_0-auc:0.41972\n",
      "[67]\tvalidation_0-auc:0.41831\n",
      "[68]\tvalidation_0-auc:0.41362\n",
      "[69]\tvalidation_0-auc:0.41197\n",
      "[70]\tvalidation_0-auc:0.41714\n",
      "[71]\tvalidation_0-auc:0.41432\n",
      "[72]\tvalidation_0-auc:0.40822\n",
      "[73]\tvalidation_0-auc:0.40352\n",
      "[74]\tvalidation_0-auc:0.40704\n",
      "[75]\tvalidation_0-auc:0.40352\n",
      "[76]\tvalidation_0-auc:0.39624\n",
      "[77]\tvalidation_0-auc:0.39108\n",
      "[78]\tvalidation_0-auc:0.39624\n",
      "[79]\tvalidation_0-auc:0.39718\n",
      "[80]\tvalidation_0-auc:0.39554\n",
      "[81]\tvalidation_0-auc:0.39296\n",
      "[82]\tvalidation_0-auc:0.39178\n",
      "[83]\tvalidation_0-auc:0.39131\n",
      "[84]\tvalidation_0-auc:0.40047\n",
      "[85]\tvalidation_0-auc:0.39930\n",
      "[86]\tvalidation_0-auc:0.39695\n",
      "[87]\tvalidation_0-auc:0.39718\n",
      "[88]\tvalidation_0-auc:0.39648\n",
      "[89]\tvalidation_0-auc:0.39812\n",
      "[90]\tvalidation_0-auc:0.39296\n",
      "[91]\tvalidation_0-auc:0.39624\n",
      "[92]\tvalidation_0-auc:0.39601\n",
      "[93]\tvalidation_0-auc:0.39413\n",
      "[94]\tvalidation_0-auc:0.39437\n",
      "[95]\tvalidation_0-auc:0.39742\n",
      "[96]\tvalidation_0-auc:0.39366\n",
      "[97]\tvalidation_0-auc:0.39272\n",
      "[98]\tvalidation_0-auc:0.39272\n",
      "[99]\tvalidation_0-auc:0.39437\n",
      "Test Accuracy for Layer 1: 0.5263\n",
      "Test AUC for Layer 1: 0.4980\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.48682\n",
      "[1]\tvalidation_0-auc:0.47194\n",
      "[2]\tvalidation_0-auc:0.47751\n",
      "[3]\tvalidation_0-auc:0.49648\n",
      "[4]\tvalidation_0-auc:0.48239\n",
      "[5]\tvalidation_0-auc:0.51488\n",
      "[6]\tvalidation_0-auc:0.52908\n",
      "[7]\tvalidation_0-auc:0.50795\n",
      "[8]\tvalidation_0-auc:0.51999\n",
      "[9]\tvalidation_0-auc:0.49909\n",
      "[10]\tvalidation_0-auc:0.49727\n",
      "[11]\tvalidation_0-auc:0.49432\n",
      "[12]\tvalidation_0-auc:0.49318\n",
      "[13]\tvalidation_0-auc:0.47978\n",
      "[14]\tvalidation_0-auc:0.46683\n",
      "[15]\tvalidation_0-auc:0.48296\n",
      "[16]\tvalidation_0-auc:0.48773\n",
      "[17]\tvalidation_0-auc:0.48160\n",
      "[18]\tvalidation_0-auc:0.49046\n",
      "[19]\tvalidation_0-auc:0.48637\n",
      "[20]\tvalidation_0-auc:0.48932\n",
      "[21]\tvalidation_0-auc:0.48296\n",
      "[22]\tvalidation_0-auc:0.49250\n",
      "[23]\tvalidation_0-auc:0.50227\n",
      "[24]\tvalidation_0-auc:0.51772\n",
      "[25]\tvalidation_0-auc:0.51318\n",
      "[26]\tvalidation_0-auc:0.50931\n",
      "[27]\tvalidation_0-auc:0.51318\n",
      "[28]\tvalidation_0-auc:0.52544\n",
      "[29]\tvalidation_0-auc:0.52476\n",
      "[30]\tvalidation_0-auc:0.51272\n",
      "[31]\tvalidation_0-auc:0.51068\n",
      "[32]\tvalidation_0-auc:0.51272\n",
      "[33]\tvalidation_0-auc:0.51477\n",
      "[34]\tvalidation_0-auc:0.51068\n",
      "[35]\tvalidation_0-auc:0.51477\n",
      "[36]\tvalidation_0-auc:0.52363\n",
      "[37]\tvalidation_0-auc:0.51999\n",
      "[38]\tvalidation_0-auc:0.51749\n",
      "[39]\tvalidation_0-auc:0.51340\n",
      "[40]\tvalidation_0-auc:0.51454\n",
      "[41]\tvalidation_0-auc:0.50931\n",
      "[42]\tvalidation_0-auc:0.50954\n",
      "[43]\tvalidation_0-auc:0.51295\n",
      "[44]\tvalidation_0-auc:0.51272\n",
      "[45]\tvalidation_0-auc:0.51045\n",
      "[46]\tvalidation_0-auc:0.51658\n",
      "[47]\tvalidation_0-auc:0.51454\n",
      "[48]\tvalidation_0-auc:0.52908\n",
      "[49]\tvalidation_0-auc:0.52976\n",
      "[50]\tvalidation_0-auc:0.53203\n",
      "[51]\tvalidation_0-auc:0.53135\n",
      "[52]\tvalidation_0-auc:0.53271\n",
      "[53]\tvalidation_0-auc:0.53067\n",
      "[54]\tvalidation_0-auc:0.52930\n",
      "[55]\tvalidation_0-auc:0.53385\n",
      "[56]\tvalidation_0-auc:0.53998\n",
      "[57]\tvalidation_0-auc:0.53476\n",
      "[58]\tvalidation_0-auc:0.53226\n",
      "[59]\tvalidation_0-auc:0.53090\n",
      "[60]\tvalidation_0-auc:0.53203\n",
      "[61]\tvalidation_0-auc:0.53249\n",
      "[62]\tvalidation_0-auc:0.53226\n",
      "[63]\tvalidation_0-auc:0.52885\n",
      "[64]\tvalidation_0-auc:0.52658\n",
      "[65]\tvalidation_0-auc:0.52363\n",
      "[66]\tvalidation_0-auc:0.51840\n",
      "[67]\tvalidation_0-auc:0.51658\n",
      "[68]\tvalidation_0-auc:0.51931\n",
      "[69]\tvalidation_0-auc:0.51272\n",
      "[70]\tvalidation_0-auc:0.51090\n",
      "[71]\tvalidation_0-auc:0.51545\n",
      "[72]\tvalidation_0-auc:0.51477\n",
      "[73]\tvalidation_0-auc:0.51090\n",
      "[74]\tvalidation_0-auc:0.51318\n",
      "[75]\tvalidation_0-auc:0.50750\n",
      "[76]\tvalidation_0-auc:0.50363\n",
      "[77]\tvalidation_0-auc:0.50273\n",
      "[78]\tvalidation_0-auc:0.49773\n",
      "[79]\tvalidation_0-auc:0.50659\n",
      "[80]\tvalidation_0-auc:0.50136\n",
      "[81]\tvalidation_0-auc:0.49909\n",
      "[82]\tvalidation_0-auc:0.49818\n",
      "[83]\tvalidation_0-auc:0.50591\n",
      "[84]\tvalidation_0-auc:0.50273\n",
      "[85]\tvalidation_0-auc:0.50136\n",
      "[86]\tvalidation_0-auc:0.49796\n",
      "[87]\tvalidation_0-auc:0.49409\n",
      "[88]\tvalidation_0-auc:0.49341\n",
      "[89]\tvalidation_0-auc:0.49296\n",
      "[90]\tvalidation_0-auc:0.49409\n",
      "[91]\tvalidation_0-auc:0.49159\n",
      "[92]\tvalidation_0-auc:0.49864\n",
      "[93]\tvalidation_0-auc:0.50045\n",
      "[94]\tvalidation_0-auc:0.49705\n",
      "[95]\tvalidation_0-auc:0.50136\n",
      "[96]\tvalidation_0-auc:0.50250\n",
      "[97]\tvalidation_0-auc:0.50114\n",
      "[98]\tvalidation_0-auc:0.50591\n",
      "[99]\tvalidation_0-auc:0.50636\n",
      "Test Accuracy for Layer 2: 0.4455\n",
      "Test AUC for Layer 2: 0.4845\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.51766\n",
      "[1]\tvalidation_0-auc:0.50179\n",
      "[2]\tvalidation_0-auc:0.48254\n",
      "[3]\tvalidation_0-auc:0.42579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\tvalidation_0-auc:0.39861\n",
      "[5]\tvalidation_0-auc:0.43591\n",
      "[6]\tvalidation_0-auc:0.47024\n",
      "[7]\tvalidation_0-auc:0.45437\n",
      "[8]\tvalidation_0-auc:0.43135\n",
      "[9]\tvalidation_0-auc:0.44802\n",
      "[10]\tvalidation_0-auc:0.44683\n",
      "[11]\tvalidation_0-auc:0.43333\n",
      "[12]\tvalidation_0-auc:0.42500\n",
      "[13]\tvalidation_0-auc:0.42540\n",
      "[14]\tvalidation_0-auc:0.42183\n",
      "[15]\tvalidation_0-auc:0.41270\n",
      "[16]\tvalidation_0-auc:0.39683\n",
      "[17]\tvalidation_0-auc:0.37381\n",
      "[18]\tvalidation_0-auc:0.39048\n",
      "[19]\tvalidation_0-auc:0.39683\n",
      "[20]\tvalidation_0-auc:0.39683\n",
      "[21]\tvalidation_0-auc:0.38452\n",
      "[22]\tvalidation_0-auc:0.41270\n",
      "[23]\tvalidation_0-auc:0.41032\n",
      "[24]\tvalidation_0-auc:0.42500\n",
      "[25]\tvalidation_0-auc:0.41667\n",
      "[26]\tvalidation_0-auc:0.42103\n",
      "[27]\tvalidation_0-auc:0.42222\n",
      "[28]\tvalidation_0-auc:0.41310\n",
      "[29]\tvalidation_0-auc:0.40317\n",
      "[30]\tvalidation_0-auc:0.41865\n",
      "[31]\tvalidation_0-auc:0.42063\n",
      "[32]\tvalidation_0-auc:0.40992\n",
      "[33]\tvalidation_0-auc:0.42619\n",
      "[34]\tvalidation_0-auc:0.42024\n",
      "[35]\tvalidation_0-auc:0.41587\n",
      "[36]\tvalidation_0-auc:0.41230\n",
      "[37]\tvalidation_0-auc:0.41111\n",
      "[38]\tvalidation_0-auc:0.41627\n",
      "[39]\tvalidation_0-auc:0.41706\n",
      "[40]\tvalidation_0-auc:0.41984\n",
      "[41]\tvalidation_0-auc:0.41984\n",
      "[42]\tvalidation_0-auc:0.41865\n",
      "[43]\tvalidation_0-auc:0.42500\n",
      "[44]\tvalidation_0-auc:0.43135\n",
      "[45]\tvalidation_0-auc:0.44087\n",
      "[46]\tvalidation_0-auc:0.44206\n",
      "[47]\tvalidation_0-auc:0.43492\n",
      "[48]\tvalidation_0-auc:0.43849\n",
      "[49]\tvalidation_0-auc:0.43095\n",
      "[50]\tvalidation_0-auc:0.42579\n",
      "[51]\tvalidation_0-auc:0.42698\n",
      "[52]\tvalidation_0-auc:0.41706\n",
      "[53]\tvalidation_0-auc:0.41190\n",
      "[54]\tvalidation_0-auc:0.41032\n",
      "[55]\tvalidation_0-auc:0.40516\n",
      "[56]\tvalidation_0-auc:0.39722\n",
      "[57]\tvalidation_0-auc:0.39802\n",
      "[58]\tvalidation_0-auc:0.39921\n",
      "[59]\tvalidation_0-auc:0.39484\n",
      "[60]\tvalidation_0-auc:0.39444\n",
      "[61]\tvalidation_0-auc:0.39603\n",
      "[62]\tvalidation_0-auc:0.39762\n",
      "[63]\tvalidation_0-auc:0.39841\n",
      "[64]\tvalidation_0-auc:0.40000\n",
      "[65]\tvalidation_0-auc:0.40714\n",
      "[66]\tvalidation_0-auc:0.41190\n",
      "[67]\tvalidation_0-auc:0.41230\n",
      "[68]\tvalidation_0-auc:0.41627\n",
      "[69]\tvalidation_0-auc:0.41270\n",
      "[70]\tvalidation_0-auc:0.41310\n",
      "[71]\tvalidation_0-auc:0.40516\n",
      "[72]\tvalidation_0-auc:0.41310\n",
      "[73]\tvalidation_0-auc:0.41071\n",
      "[74]\tvalidation_0-auc:0.40833\n",
      "[75]\tvalidation_0-auc:0.40119\n",
      "[76]\tvalidation_0-auc:0.40476\n",
      "[77]\tvalidation_0-auc:0.39643\n",
      "[78]\tvalidation_0-auc:0.40357\n",
      "[79]\tvalidation_0-auc:0.40992\n",
      "[80]\tvalidation_0-auc:0.41151\n",
      "[81]\tvalidation_0-auc:0.40476\n",
      "[82]\tvalidation_0-auc:0.40556\n",
      "[83]\tvalidation_0-auc:0.40714\n",
      "[84]\tvalidation_0-auc:0.40714\n",
      "[85]\tvalidation_0-auc:0.40635\n",
      "[86]\tvalidation_0-auc:0.40952\n",
      "[87]\tvalidation_0-auc:0.41190\n",
      "[88]\tvalidation_0-auc:0.40556\n",
      "[89]\tvalidation_0-auc:0.40873\n",
      "[90]\tvalidation_0-auc:0.40992\n",
      "[91]\tvalidation_0-auc:0.40833\n",
      "[92]\tvalidation_0-auc:0.40476\n",
      "[93]\tvalidation_0-auc:0.39881\n",
      "[94]\tvalidation_0-auc:0.40238\n",
      "[95]\tvalidation_0-auc:0.40238\n",
      "[96]\tvalidation_0-auc:0.40278\n",
      "[97]\tvalidation_0-auc:0.39841\n",
      "[98]\tvalidation_0-auc:0.40079\n",
      "[99]\tvalidation_0-auc:0.40198\n",
      "Test Accuracy for Layer 3: 0.4571\n",
      "Test AUC for Layer 3: 0.5077\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4763\n",
      "Average Test AUC across all layers: 0.4967\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Full text and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.54889\n",
      "[1]\tvalidation_0-auc:0.50938\n",
      "[2]\tvalidation_0-auc:0.47423\n",
      "[3]\tvalidation_0-auc:0.49194\n",
      "[4]\tvalidation_0-auc:0.46300\n",
      "[5]\tvalidation_0-auc:0.45428\n",
      "[6]\tvalidation_0-auc:0.42653\n",
      "[7]\tvalidation_0-auc:0.44371\n",
      "[8]\tvalidation_0-auc:0.47278\n",
      "[9]\tvalidation_0-auc:0.46538\n",
      "[10]\tvalidation_0-auc:0.46617\n",
      "[11]\tvalidation_0-auc:0.45666\n",
      "[12]\tvalidation_0-auc:0.46934\n",
      "[13]\tvalidation_0-auc:0.47278\n",
      "[14]\tvalidation_0-auc:0.46538\n",
      "[15]\tvalidation_0-auc:0.45851\n",
      "[16]\tvalidation_0-auc:0.44371\n",
      "[17]\tvalidation_0-auc:0.45137\n",
      "[18]\tvalidation_0-auc:0.46379\n",
      "[19]\tvalidation_0-auc:0.46564\n",
      "[20]\tvalidation_0-auc:0.46697\n",
      "[21]\tvalidation_0-auc:0.47833\n",
      "[22]\tvalidation_0-auc:0.48811\n",
      "[23]\tvalidation_0-auc:0.48705\n",
      "[24]\tvalidation_0-auc:0.48969\n",
      "[25]\tvalidation_0-auc:0.49630\n",
      "[26]\tvalidation_0-auc:0.50159\n",
      "[27]\tvalidation_0-auc:0.49656\n",
      "[28]\tvalidation_0-auc:0.49234\n",
      "[29]\tvalidation_0-auc:0.49947\n",
      "[30]\tvalidation_0-auc:0.48573\n",
      "[31]\tvalidation_0-auc:0.48388\n",
      "[32]\tvalidation_0-auc:0.49234\n",
      "[33]\tvalidation_0-auc:0.48362\n",
      "[34]\tvalidation_0-auc:0.49207\n",
      "[35]\tvalidation_0-auc:0.49630\n",
      "[36]\tvalidation_0-auc:0.49841\n",
      "[37]\tvalidation_0-auc:0.49445\n",
      "[38]\tvalidation_0-auc:0.50132\n",
      "[39]\tvalidation_0-auc:0.50079\n",
      "[40]\tvalidation_0-auc:0.50449\n",
      "[41]\tvalidation_0-auc:0.49286\n",
      "[42]\tvalidation_0-auc:0.49075\n",
      "[43]\tvalidation_0-auc:0.49709\n",
      "[44]\tvalidation_0-auc:0.49392\n",
      "[45]\tvalidation_0-auc:0.49736\n",
      "[46]\tvalidation_0-auc:0.50793\n",
      "[47]\tvalidation_0-auc:0.50661\n",
      "[48]\tvalidation_0-auc:0.51242\n",
      "[49]\tvalidation_0-auc:0.51295\n",
      "[50]\tvalidation_0-auc:0.50793\n",
      "[51]\tvalidation_0-auc:0.50740\n",
      "[52]\tvalidation_0-auc:0.50502\n",
      "[53]\tvalidation_0-auc:0.50053\n",
      "[54]\tvalidation_0-auc:0.49841\n",
      "[55]\tvalidation_0-auc:0.50951\n",
      "[56]\tvalidation_0-auc:0.50396\n",
      "[57]\tvalidation_0-auc:0.50634\n",
      "[58]\tvalidation_0-auc:0.50555\n",
      "[59]\tvalidation_0-auc:0.50370\n",
      "[60]\tvalidation_0-auc:0.50185\n",
      "[61]\tvalidation_0-auc:0.50793\n",
      "[62]\tvalidation_0-auc:0.50766\n",
      "[63]\tvalidation_0-auc:0.49604\n",
      "[64]\tvalidation_0-auc:0.49471\n",
      "[65]\tvalidation_0-auc:0.50211\n",
      "[66]\tvalidation_0-auc:0.50211\n",
      "[67]\tvalidation_0-auc:0.50132\n",
      "[68]\tvalidation_0-auc:0.49841\n",
      "[69]\tvalidation_0-auc:0.49656\n",
      "[70]\tvalidation_0-auc:0.49551\n",
      "[71]\tvalidation_0-auc:0.49154\n",
      "[72]\tvalidation_0-auc:0.48414\n",
      "[73]\tvalidation_0-auc:0.49339\n",
      "[74]\tvalidation_0-auc:0.49313\n",
      "[75]\tvalidation_0-auc:0.49286\n",
      "[76]\tvalidation_0-auc:0.49551\n",
      "[77]\tvalidation_0-auc:0.49313\n",
      "[78]\tvalidation_0-auc:0.49604\n",
      "[79]\tvalidation_0-auc:0.50026\n",
      "[80]\tvalidation_0-auc:0.49366\n",
      "[81]\tvalidation_0-auc:0.49815\n",
      "[82]\tvalidation_0-auc:0.50291\n",
      "[83]\tvalidation_0-auc:0.50132\n",
      "[84]\tvalidation_0-auc:0.50132\n",
      "[85]\tvalidation_0-auc:0.50581\n",
      "[86]\tvalidation_0-auc:0.50264\n",
      "[87]\tvalidation_0-auc:0.50555\n",
      "[88]\tvalidation_0-auc:0.50925\n",
      "[89]\tvalidation_0-auc:0.50370\n",
      "[90]\tvalidation_0-auc:0.50211\n",
      "[91]\tvalidation_0-auc:0.50132\n",
      "[92]\tvalidation_0-auc:0.50502\n",
      "[93]\tvalidation_0-auc:0.50423\n",
      "[94]\tvalidation_0-auc:0.50264\n",
      "[95]\tvalidation_0-auc:0.50581\n",
      "[96]\tvalidation_0-auc:0.50529\n",
      "[97]\tvalidation_0-auc:0.50529\n",
      "[98]\tvalidation_0-auc:0.50396\n",
      "[99]\tvalidation_0-auc:0.51189\n",
      "Test Accuracy for Layer 1: 0.3308\n",
      "Test AUC for Layer 1: 0.5093\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.52930\n",
      "[1]\tvalidation_0-auc:0.53557\n",
      "[2]\tvalidation_0-auc:0.57959\n",
      "[3]\tvalidation_0-auc:0.54184\n",
      "[4]\tvalidation_0-auc:0.53848\n",
      "[5]\tvalidation_0-auc:0.54985\n",
      "[6]\tvalidation_0-auc:0.55714\n",
      "[7]\tvalidation_0-auc:0.54431\n",
      "[8]\tvalidation_0-auc:0.54694\n",
      "[9]\tvalidation_0-auc:0.54869\n",
      "[10]\tvalidation_0-auc:0.55015\n",
      "[11]\tvalidation_0-auc:0.54723\n",
      "[12]\tvalidation_0-auc:0.55598\n",
      "[13]\tvalidation_0-auc:0.55889\n",
      "[14]\tvalidation_0-auc:0.54752\n",
      "[15]\tvalidation_0-auc:0.54752\n",
      "[16]\tvalidation_0-auc:0.55131\n",
      "[17]\tvalidation_0-auc:0.55131\n",
      "[18]\tvalidation_0-auc:0.55889\n",
      "[19]\tvalidation_0-auc:0.55190\n",
      "[20]\tvalidation_0-auc:0.54956\n",
      "[21]\tvalidation_0-auc:0.53644\n",
      "[22]\tvalidation_0-auc:0.54198\n",
      "[23]\tvalidation_0-auc:0.54344\n",
      "[24]\tvalidation_0-auc:0.55073\n",
      "[25]\tvalidation_0-auc:0.55569\n",
      "[26]\tvalidation_0-auc:0.55044\n",
      "[27]\tvalidation_0-auc:0.55131\n",
      "[28]\tvalidation_0-auc:0.54461\n",
      "[29]\tvalidation_0-auc:0.54052\n",
      "[30]\tvalidation_0-auc:0.54111\n",
      "[31]\tvalidation_0-auc:0.54869\n",
      "[32]\tvalidation_0-auc:0.54140\n",
      "[33]\tvalidation_0-auc:0.54752\n",
      "[34]\tvalidation_0-auc:0.54402\n",
      "[35]\tvalidation_0-auc:0.54956\n",
      "[36]\tvalidation_0-auc:0.56122\n",
      "[37]\tvalidation_0-auc:0.56239\n",
      "[38]\tvalidation_0-auc:0.56239\n",
      "[39]\tvalidation_0-auc:0.56093\n",
      "[40]\tvalidation_0-auc:0.56647\n",
      "[41]\tvalidation_0-auc:0.55977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42]\tvalidation_0-auc:0.55889\n",
      "[43]\tvalidation_0-auc:0.55160\n",
      "[44]\tvalidation_0-auc:0.54548\n",
      "[45]\tvalidation_0-auc:0.55306\n",
      "[46]\tvalidation_0-auc:0.56618\n",
      "[47]\tvalidation_0-auc:0.56939\n",
      "[48]\tvalidation_0-auc:0.56297\n",
      "[49]\tvalidation_0-auc:0.55656\n",
      "[50]\tvalidation_0-auc:0.56443\n",
      "[51]\tvalidation_0-auc:0.57172\n",
      "[52]\tvalidation_0-auc:0.56618\n",
      "[53]\tvalidation_0-auc:0.56997\n",
      "[54]\tvalidation_0-auc:0.57172\n",
      "[55]\tvalidation_0-auc:0.57289\n",
      "[56]\tvalidation_0-auc:0.57493\n",
      "[57]\tvalidation_0-auc:0.57201\n",
      "[58]\tvalidation_0-auc:0.55948\n",
      "[59]\tvalidation_0-auc:0.55773\n",
      "[60]\tvalidation_0-auc:0.55015\n",
      "[61]\tvalidation_0-auc:0.54898\n",
      "[62]\tvalidation_0-auc:0.56385\n",
      "[63]\tvalidation_0-auc:0.56327\n",
      "[64]\tvalidation_0-auc:0.56181\n",
      "[65]\tvalidation_0-auc:0.56414\n",
      "[66]\tvalidation_0-auc:0.56589\n",
      "[67]\tvalidation_0-auc:0.55918\n",
      "[68]\tvalidation_0-auc:0.56356\n",
      "[69]\tvalidation_0-auc:0.56210\n",
      "[70]\tvalidation_0-auc:0.56093\n",
      "[71]\tvalidation_0-auc:0.56035\n",
      "[72]\tvalidation_0-auc:0.56006\n",
      "[73]\tvalidation_0-auc:0.55802\n",
      "[74]\tvalidation_0-auc:0.55918\n",
      "[75]\tvalidation_0-auc:0.55569\n",
      "[76]\tvalidation_0-auc:0.55714\n",
      "[77]\tvalidation_0-auc:0.55569\n",
      "[78]\tvalidation_0-auc:0.55773\n",
      "[79]\tvalidation_0-auc:0.56122\n",
      "[80]\tvalidation_0-auc:0.56181\n",
      "[81]\tvalidation_0-auc:0.56560\n",
      "[82]\tvalidation_0-auc:0.56297\n",
      "[83]\tvalidation_0-auc:0.57143\n",
      "[84]\tvalidation_0-auc:0.56793\n",
      "[85]\tvalidation_0-auc:0.56531\n",
      "[86]\tvalidation_0-auc:0.56035\n",
      "[87]\tvalidation_0-auc:0.56122\n",
      "[88]\tvalidation_0-auc:0.56531\n",
      "[89]\tvalidation_0-auc:0.56531\n",
      "[90]\tvalidation_0-auc:0.57055\n",
      "[91]\tvalidation_0-auc:0.56851\n",
      "[92]\tvalidation_0-auc:0.56880\n",
      "[93]\tvalidation_0-auc:0.56327\n",
      "[94]\tvalidation_0-auc:0.56385\n",
      "[95]\tvalidation_0-auc:0.56531\n",
      "[96]\tvalidation_0-auc:0.56793\n",
      "[97]\tvalidation_0-auc:0.56327\n",
      "[98]\tvalidation_0-auc:0.56472\n",
      "[99]\tvalidation_0-auc:0.56122\n",
      "Test Accuracy for Layer 2: 0.4950\n",
      "Test AUC for Layer 2: 0.4824\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.47941\n",
      "[1]\tvalidation_0-auc:0.53059\n",
      "[2]\tvalidation_0-auc:0.57059\n",
      "[3]\tvalidation_0-auc:0.59824\n",
      "[4]\tvalidation_0-auc:0.59510\n",
      "[5]\tvalidation_0-auc:0.59941\n",
      "[6]\tvalidation_0-auc:0.58235\n",
      "[7]\tvalidation_0-auc:0.57765\n",
      "[8]\tvalidation_0-auc:0.55255\n",
      "[9]\tvalidation_0-auc:0.56549\n",
      "[10]\tvalidation_0-auc:0.53765\n",
      "[11]\tvalidation_0-auc:0.53373\n",
      "[12]\tvalidation_0-auc:0.53804\n",
      "[13]\tvalidation_0-auc:0.55333\n",
      "[14]\tvalidation_0-auc:0.56941\n",
      "[15]\tvalidation_0-auc:0.54863\n",
      "[16]\tvalidation_0-auc:0.55686\n",
      "[17]\tvalidation_0-auc:0.55765\n",
      "[18]\tvalidation_0-auc:0.56471\n",
      "[19]\tvalidation_0-auc:0.55098\n",
      "[20]\tvalidation_0-auc:0.55804\n",
      "[21]\tvalidation_0-auc:0.55961\n",
      "[22]\tvalidation_0-auc:0.55176\n",
      "[23]\tvalidation_0-auc:0.55608\n",
      "[24]\tvalidation_0-auc:0.57686\n",
      "[25]\tvalidation_0-auc:0.57490\n",
      "[26]\tvalidation_0-auc:0.56980\n",
      "[27]\tvalidation_0-auc:0.56627\n",
      "[28]\tvalidation_0-auc:0.57412\n",
      "[29]\tvalidation_0-auc:0.58275\n",
      "[30]\tvalidation_0-auc:0.59882\n",
      "[31]\tvalidation_0-auc:0.59137\n",
      "[32]\tvalidation_0-auc:0.58314\n",
      "[33]\tvalidation_0-auc:0.57020\n",
      "[34]\tvalidation_0-auc:0.57490\n",
      "[35]\tvalidation_0-auc:0.56510\n",
      "[36]\tvalidation_0-auc:0.56588\n",
      "[37]\tvalidation_0-auc:0.57843\n",
      "[38]\tvalidation_0-auc:0.56941\n",
      "[39]\tvalidation_0-auc:0.57843\n",
      "[40]\tvalidation_0-auc:0.57647\n",
      "[41]\tvalidation_0-auc:0.57765\n",
      "[42]\tvalidation_0-auc:0.56941\n",
      "[43]\tvalidation_0-auc:0.57020\n",
      "[44]\tvalidation_0-auc:0.57255\n",
      "[45]\tvalidation_0-auc:0.57294\n",
      "[46]\tvalidation_0-auc:0.57608\n",
      "[47]\tvalidation_0-auc:0.57961\n",
      "[48]\tvalidation_0-auc:0.57647\n",
      "[49]\tvalidation_0-auc:0.57059\n",
      "[50]\tvalidation_0-auc:0.56510\n",
      "[51]\tvalidation_0-auc:0.55922\n",
      "[52]\tvalidation_0-auc:0.55255\n",
      "[53]\tvalidation_0-auc:0.55765\n",
      "[54]\tvalidation_0-auc:0.55294\n",
      "[55]\tvalidation_0-auc:0.55725\n",
      "[56]\tvalidation_0-auc:0.54824\n",
      "[57]\tvalidation_0-auc:0.54157\n",
      "[58]\tvalidation_0-auc:0.53216\n",
      "[59]\tvalidation_0-auc:0.54039\n",
      "[60]\tvalidation_0-auc:0.54784\n",
      "[61]\tvalidation_0-auc:0.54157\n",
      "[62]\tvalidation_0-auc:0.54353\n",
      "[63]\tvalidation_0-auc:0.54118\n",
      "[64]\tvalidation_0-auc:0.53529\n",
      "[65]\tvalidation_0-auc:0.53843\n",
      "[66]\tvalidation_0-auc:0.53529\n",
      "[67]\tvalidation_0-auc:0.53216\n",
      "[68]\tvalidation_0-auc:0.54157\n",
      "[69]\tvalidation_0-auc:0.53647\n",
      "[70]\tvalidation_0-auc:0.53569\n",
      "[71]\tvalidation_0-auc:0.54706\n",
      "[72]\tvalidation_0-auc:0.55059\n",
      "[73]\tvalidation_0-auc:0.54706\n",
      "[74]\tvalidation_0-auc:0.54824\n",
      "[75]\tvalidation_0-auc:0.54392\n",
      "[76]\tvalidation_0-auc:0.54078\n",
      "[77]\tvalidation_0-auc:0.53843\n",
      "[78]\tvalidation_0-auc:0.53647\n",
      "[79]\tvalidation_0-auc:0.53725\n",
      "[80]\tvalidation_0-auc:0.53294\n",
      "[81]\tvalidation_0-auc:0.53333\n",
      "[82]\tvalidation_0-auc:0.53137\n",
      "[83]\tvalidation_0-auc:0.53059\n",
      "[84]\tvalidation_0-auc:0.53137\n",
      "[85]\tvalidation_0-auc:0.52706\n",
      "[86]\tvalidation_0-auc:0.52353\n",
      "[87]\tvalidation_0-auc:0.52471\n",
      "[88]\tvalidation_0-auc:0.52235\n",
      "[89]\tvalidation_0-auc:0.52000\n",
      "[90]\tvalidation_0-auc:0.52118\n",
      "[91]\tvalidation_0-auc:0.52353\n",
      "[92]\tvalidation_0-auc:0.52157\n",
      "[93]\tvalidation_0-auc:0.52471\n",
      "[94]\tvalidation_0-auc:0.52157\n",
      "[95]\tvalidation_0-auc:0.52118\n",
      "[96]\tvalidation_0-auc:0.52118\n",
      "[97]\tvalidation_0-auc:0.52118\n",
      "[98]\tvalidation_0-auc:0.52039\n",
      "[99]\tvalidation_0-auc:0.52157\n",
      "Test Accuracy for Layer 3: 0.4714\n",
      "Test AUC for Layer 3: 0.4615\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4324\n",
      "Average Test AUC across all layers: 0.4844\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Title + S_label\n",
      "Average Accuracy: 0.4961\n",
      "Average AUC: 0.4942\n",
      "  Layer 1 - Accuracy: 0.5338, AUC: 0.5095\n",
      "  Layer 2 - Accuracy: 0.5545, AUC: 0.4952\n",
      "  Layer 3 - Accuracy: 0.4000, AUC: 0.4779\n",
      "\n",
      "Combination: MLP with Title + L_label\n",
      "Average Accuracy: 0.4517\n",
      "Average AUC: 0.3983\n",
      "  Layer 1 - Accuracy: 0.2632, AUC: 0.3854\n",
      "  Layer 2 - Accuracy: 0.5347, AUC: 0.4827\n",
      "  Layer 3 - Accuracy: 0.5571, AUC: 0.3267\n",
      "\n",
      "Combination: MLP with Full text + S_label\n",
      "Average Accuracy: 0.4991\n",
      "Average AUC: 0.5030\n",
      "  Layer 1 - Accuracy: 0.4662, AUC: 0.4639\n",
      "  Layer 2 - Accuracy: 0.4455, AUC: 0.4329\n",
      "  Layer 3 - Accuracy: 0.5857, AUC: 0.6122\n",
      "\n",
      "Combination: MLP with Full text + L_label\n",
      "Average Accuracy: 0.5963\n",
      "Average AUC: 0.5153\n",
      "  Layer 1 - Accuracy: 0.7368, AUC: 0.5073\n",
      "  Layer 2 - Accuracy: 0.4950, AUC: 0.4722\n",
      "  Layer 3 - Accuracy: 0.5571, AUC: 0.5666\n",
      "\n",
      "Combination: XGBoost with Title + S_label\n",
      "Average Accuracy: 0.5013\n",
      "Average AUC: 0.5204\n",
      "  Layer 1 - Accuracy: 0.5188, AUC: 0.5582\n",
      "  Layer 2 - Accuracy: 0.4851, AUC: 0.5234\n",
      "  Layer 3 - Accuracy: 0.5000, AUC: 0.4796\n",
      "\n",
      "Combination: XGBoost with Title + L_label\n",
      "Average Accuracy: 0.4970\n",
      "Average AUC: 0.5443\n",
      "  Layer 1 - Accuracy: 0.3684, AUC: 0.4703\n",
      "  Layer 2 - Accuracy: 0.5941, AUC: 0.5663\n",
      "  Layer 3 - Accuracy: 0.5286, AUC: 0.5964\n",
      "\n",
      "Combination: XGBoost with Full text + S_label\n",
      "Average Accuracy: 0.4763\n",
      "Average AUC: 0.4967\n",
      "  Layer 1 - Accuracy: 0.5263, AUC: 0.4980\n",
      "  Layer 2 - Accuracy: 0.4455, AUC: 0.4845\n",
      "  Layer 3 - Accuracy: 0.4571, AUC: 0.5077\n",
      "\n",
      "Combination: XGBoost with Full text + L_label\n",
      "Average Accuracy: 0.4324\n",
      "Average AUC: 0.4844\n",
      "  Layer 1 - Accuracy: 0.3308, AUC: 0.5093\n",
      "  Layer 2 - Accuracy: 0.4950, AUC: 0.4824\n",
      "  Layer 3 - Accuracy: 0.4714, AUC: 0.4615\n",
      "MLP summary comparison visualization saved as: OpenAI_MLP_XGB/visualizations_summary/COP\\mlp_performance_comparison.png\n",
      "XGBoost summary comparison visualization saved as: OpenAI_MLP_XGB/visualizations_summary/COP\\xgb_performance_comparison.png\n",
      "MLP layer performance visualization saved as: OpenAI_MLP_XGB/visualizations_summary/COP\\mlp_layer_performance.png\n",
      "XGBoost layer performance visualization saved as: OpenAI_MLP_XGB/visualizations_summary/COP\\xgboost_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'OpenAI_MLP_XGB' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "from xgboost import callback\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, model_type, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class ClimateNewsOpenAIPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'OpenAI_MLP_XGB/visualizations_mlp/COP'\n",
    "        self.xgb_viz_dir = 'OpenAI_MLP_XGB/visualizations_xgb/COP'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs(self.xgb_viz_dir, exist_ok=True)\n",
    "        os.makedirs('OpenAI_MLP_XGB/visualizations_summary/COP', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert embeddings to numpy arrays\n",
    "        self.data['Title_embedding'] = self.data['Title_embedding_vector'].apply(parse_embedding)\n",
    "        self.data['Fulltext_embedding'] = self.data['Full_text_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_title_embedding = self.data['Title_embedding'].iloc[0]\n",
    "        sample_fulltext_embedding = self.data['Fulltext_embedding'].iloc[0]\n",
    "        \n",
    "        print(f\"Sample Title embedding shape: {sample_title_embedding.shape}\")\n",
    "        print(f\"Sample Fulltext embedding shape: {sample_fulltext_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2014-2020), validation (2020-2021), test (2021-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2020-10-31'),\n",
    "            'val_start': pd.Timestamp('2020-11-01'),\n",
    "            'val_end': pd.Timestamp('2021-10-31'),\n",
    "            'test_start': pd.Timestamp('2021-11-01'),\n",
    "            'test_end': pd.Timestamp('2022-10-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2014-2021), validation (2021-2022), test (2022-2023)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2021-10-31'),\n",
    "            'val_start': pd.Timestamp('2021-11-01'),\n",
    "            'val_end': pd.Timestamp('2022-10-31'),\n",
    "            'test_start': pd.Timestamp('2022-11-01'),\n",
    "            'test_end': pd.Timestamp('2023-10-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2014-2022), validation (2022-2023), test (2023-2024)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2022-10-31'),\n",
    "            'val_start': pd.Timestamp('2022-11-01'),\n",
    "            'val_end': pd.Timestamp('2023-10-31'),\n",
    "            'test_start': pd.Timestamp('2023-11-01'),\n",
    "            'test_end': pd.Timestamp('2024-11-01')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, text_col, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            text_col: The column containing the embeddings ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data[text_col].values)\n",
    "        X_val = np.stack(val_data[text_col].values)\n",
    "        X_test = np.stack(test_data[text_col].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def get_xgb_parameters(self):\n",
    "        \"\"\"\n",
    "        Get XGBoost parameters optimized for high-dimensional embeddings.\n",
    "        Uses a single parameter set for both Title and Full text embeddings since they have the same dimension (1536).\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of base parameters\n",
    "        \"\"\"\n",
    "        # Setup XGBoost base parameters optimized for high-dimensional embeddings\n",
    "        base_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 3,\n",
    "            'learning_rate': 0.05,\n",
    "            'subsample': 0.7,          # Row subsampling to prevent overfitting\n",
    "            'colsample_bytree': 0.5,   # Column subsampling to handle high dimensionality\n",
    "            'min_child_weight': 3,     # Prevents overfitting on high-dimensional embeddings\n",
    "            'reg_alpha': 1.0,          # L1 regularization\n",
    "            'reg_lambda': 2.0,         # L2 regularization\n",
    "            'random_state': 42,\n",
    "            'use_label_encoder': False # Avoid deprecation warning\n",
    "        }\n",
    "        \n",
    "        return base_params\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, model_type, text_col, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            model_type: Model type (MLP)\n",
    "            text_col: Text column used\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_{model_type.lower()}_{display_text.replace(' ', '_')}_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, text_col, label_col, model_type):\n",
    "        \"\"\"\n",
    "        Train and evaluate a model for a specific text column and label column.\n",
    "        \n",
    "        Args:\n",
    "            text_col: The embedding column to use ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "            model_type: The model type to use ('MLP' or 'XGBoost')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        combination_key = f\"{model_type}|{display_text}|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training {model_type} model for {display_text} and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        visualization_dir = self.mlp_viz_dir if model_type == 'MLP' else self.xgb_viz_dir\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, text_col, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            if model_type == 'MLP':\n",
    "                # MLP model training\n",
    "                model = self.create_mlp_model((1536,))\n",
    "                print(f\"Created MLP model for {display_text} ({label_col})\")\n",
    "                model.summary()\n",
    "                \n",
    "                # Setup callbacks\n",
    "                plot_callback = PlotLearningCallback(model_type, display_text, label_col, i+1, visualization_dir)\n",
    "                early_stopping = EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=15,\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                model_checkpoint = ModelCheckpoint(\n",
    "                    filepath=f\"{visualization_dir}/best_{model_type.lower()}_{display_text.lower().replace(' ', '_')}_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                    monitor='val_loss',\n",
    "                    save_weights_only=True,\n",
    "                    save_best_only=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Train model\n",
    "                print(f\"Training MLP model...\")\n",
    "                batch_size = 32  # Larger batch size for embeddings\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                y_pred_proba = model.predict(X_test)\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                # Create final learning curve visualization\n",
    "                self.plot_final_learning_curves(history, model_type, text_col, label_col, i+1, visualization_dir)\n",
    "                \n",
    "                # Store training history\n",
    "                training_history = history.history\n",
    "                \n",
    "            else:  # XGBoost - simplified approach to fix API issues\n",
    "                # Get XGBoost parameters\n",
    "                base_params = self.get_xgb_parameters()\n",
    "                \n",
    "                # Create and train XGBoost model\n",
    "                print(f\"Creating and training XGBoost model...\")\n",
    "                model = xgb.XGBClassifier(**base_params)\n",
    "                \n",
    "                # Only use validation set for evaluation (not training set)\n",
    "                eval_set = [(X_val, y_val)]\n",
    "                \n",
    "                # Train the model\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    verbose=True\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                # Create a simple summary for XGBoost (no detailed learning curves available)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.text(0.5, 0.5, f'XGBoost Model Trained Successfully\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\\nAUC: {roc_auc_score(y_test, y_pred_proba):.4f}',\n",
    "                         ha='center', va='center', size=14, fontweight='bold')\n",
    "                plt.title(f'XGBoost Results ({display_text}, {label_col}, Layer {i+1})')\n",
    "                plt.axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{visualization_dir}/xgb_{display_text.replace(' ', '_')}_{label_col}_layer_{i+1}.png\")\n",
    "                plt.close()\n",
    "                \n",
    "                # Use None for training history since detailed learning curves aren't available\n",
    "                training_history = None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'model_type': model_type,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': training_history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for all combinations of text inputs, label columns, and model types.\"\"\"\n",
    "        # Define all combinations\n",
    "        embedding_cols = ['Title_embedding', 'Fulltext_embedding']\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        model_types = ['MLP', 'XGBoost']\n",
    "        \n",
    "        # Run analysis for each combination\n",
    "        for model_type in model_types:\n",
    "            for embedding_col in embedding_cols:\n",
    "                for label_col in label_cols:\n",
    "                    self.train_and_evaluate_model(embedding_col, label_col, model_type)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Model': model_type,\n",
    "                'Text': text_col,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing all model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Split by model type\n",
    "        mlp_data = df[df['Model'] == 'MLP']\n",
    "        xgb_data = df[df['Model'] == 'XGBoost']\n",
    "        \n",
    "        # 1. Performance comparison for MLP\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot accuracy and AUC bars for MLP\n",
    "        x = np.arange(len(mlp_data))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, mlp_data['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, mlp_data['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('MLP Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with OpenAI Embeddings')\n",
    "        labels = [f\"{row['Text']} + {row['Label']}\" for _, row in mlp_data.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(mlp_data['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(mlp_data['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(mlp_data['Avg Accuracy'].max(), mlp_data['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/COP', \"mlp_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Performance comparison for XGBoost\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot accuracy and AUC bars for XGBoost\n",
    "        x = np.arange(len(xgb_data))\n",
    "        \n",
    "        plt.bar(x - width/2, xgb_data['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, xgb_data['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('XGBoost Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of XGBoost Models with OpenAI Embeddings')\n",
    "        labels = [f\"{row['Text']} + {row['Label']}\" for _, row in xgb_data.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(xgb_data['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(xgb_data['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(xgb_data['Avg Accuracy'].max(), xgb_data['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/COP', \"xgb_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"XGBoost summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP and XGBoost models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        mlp_layer_data = []\n",
    "        xgb_layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Model': model_type,\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                if model_type == 'MLP':\n",
    "                    mlp_layer_data.append(layer_info)\n",
    "                else:  # XGBoost\n",
    "                    xgb_layer_data.append(layer_info)\n",
    "        \n",
    "        # Create visualizations for each model type\n",
    "        self._create_model_layer_visualization(mlp_layer_data, 'MLP')\n",
    "        self._create_model_layer_visualization(xgb_layer_data, 'XGBoost')\n",
    "    \n",
    "    def _create_model_layer_visualization(self, layer_data, model_type):\n",
    "        \"\"\"Create layer-specific visualizations for a given model type.\"\"\"\n",
    "        if not layer_data:\n",
    "            print(f\"No layer data available for {model_type}\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} Accuracy by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} AUC by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/COP', f\"{model_type.lower()}_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"{model_type} layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['OpenAI_MLP_XGB/visualizations_mlp/COP', 'OpenAI_MLP_XGB/visualizations_xgb/COP', \n",
    "                      'OpenAI_MLP_XGB/visualizations_summary/COP']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/wall_street_news_semantics_SP500_database/wall_street_news_semantics_COP_completed_openai.csv'\n",
    "    \n",
    "    # Initialize the predictor with OpenAI embeddings\n",
    "    predictor = ClimateNewsOpenAIPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'OpenAI_MLP_XGB' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing OpenAI embedding vectors from string format...\n",
      "Sample Title embedding shape: (1536,)\n",
      "Sample Fulltext embedding shape: (1536,)\n",
      "Loaded 838 climate change news articles spanning from 07/11/2014 to 24/09/2024\n",
      "Class distribution for short-term prediction: {1: 452, 0: 386}\n",
      "Class distribution for long-term prediction: {1: 464, 0: 374}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_133\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_508 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_375 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_266 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_509 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_376 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_267 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_510 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_377 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_511 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8549 - accuracy: 0.5426\n",
      "Epoch 1: val_loss improved from inf to 0.68903, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 30ms/step - loss: 0.8602 - accuracy: 0.5362 - val_loss: 0.6890 - val_accuracy: 0.5649\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8425 - accuracy: 0.4972\n",
      "Epoch 2: val_loss improved from 0.68903 to 0.68646, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.8248 - accuracy: 0.5262 - val_loss: 0.6865 - val_accuracy: 0.5649\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7987 - accuracy: 0.5568\n",
      "Epoch 3: val_loss improved from 0.68646 to 0.68514, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.7963 - accuracy: 0.5561 - val_loss: 0.6851 - val_accuracy: 0.5649\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7430 - accuracy: 0.5852\n",
      "Epoch 4: val_loss improved from 0.68514 to 0.68468, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.7366 - accuracy: 0.5960 - val_loss: 0.6847 - val_accuracy: 0.5649\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5835 - accuracy: 0.6903\n",
      "Epoch 5: val_loss did not improve from 0.68468\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6033 - accuracy: 0.6733 - val_loss: 0.6848 - val_accuracy: 0.5649\n",
      "Epoch 6/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.5816 - accuracy: 0.6812\n",
      "Epoch 6: val_loss did not improve from 0.68468\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5994 - accuracy: 0.6683 - val_loss: 0.6851 - val_accuracy: 0.5649\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5755 - accuracy: 0.7045\n",
      "Epoch 7: val_loss did not improve from 0.68468\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5832 - accuracy: 0.7007 - val_loss: 0.6858 - val_accuracy: 0.5649\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5047 - accuracy: 0.7472\n",
      "Epoch 8: val_loss did not improve from 0.68468\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5074 - accuracy: 0.7431 - val_loss: 0.6867 - val_accuracy: 0.5649\n",
      "Epoch 9/100\n",
      " 9/13 [===================>..........] - ETA: 0s - loss: 0.5139 - accuracy: 0.7604\n",
      "Epoch 9: val_loss did not improve from 0.68468\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5040 - accuracy: 0.7656 - val_loss: 0.6878 - val_accuracy: 0.5649\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4739 - accuracy: 0.7585\n",
      "Epoch 10: val_loss did not improve from 0.68468\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4804 - accuracy: 0.7606 - val_loss: 0.6895 - val_accuracy: 0.5649\n",
      "Epoch 11/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4712 - accuracy: 0.7784\n",
      "Epoch 11: val_loss did not improve from 0.68468\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4742 - accuracy: 0.7706 - val_loss: 0.6906 - val_accuracy: 0.5649\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4445 - accuracy: 0.7955\n",
      "Epoch 12: val_loss did not improve from 0.68468\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4399 - accuracy: 0.8005 - val_loss: 0.6916 - val_accuracy: 0.5649\n",
      "Epoch 13/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4135 - accuracy: 0.8068\n",
      "Epoch 13: val_loss did not improve from 0.68468\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4273 - accuracy: 0.7955 - val_loss: 0.6931 - val_accuracy: 0.5649\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3829 - accuracy: 0.8494\n",
      "Epoch 14: val_loss did not improve from 0.68468\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3767 - accuracy: 0.8529 - val_loss: 0.6948 - val_accuracy: 0.5649\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3832 - accuracy: 0.8494\n",
      "Epoch 15: val_loss did not improve from 0.68468\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3797 - accuracy: 0.8554 - val_loss: 0.6946 - val_accuracy: 0.5649\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3759 - accuracy: 0.8494\n",
      "Epoch 16: val_loss did not improve from 0.68468\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3701 - accuracy: 0.8529 - val_loss: 0.6964 - val_accuracy: 0.5649\n",
      "Epoch 17/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3343 - accuracy: 0.8580\n",
      "Epoch 17: val_loss did not improve from 0.68468\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3358 - accuracy: 0.8554 - val_loss: 0.6988 - val_accuracy: 0.5649\n",
      "Epoch 18/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3145 - accuracy: 0.9034\n",
      "Epoch 18: val_loss did not improve from 0.68468\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3183 - accuracy: 0.8953 - val_loss: 0.7013 - val_accuracy: 0.5649\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3501 - accuracy: 0.8580Restoring model weights from the end of the best epoch: 4.\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.68468\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3353 - accuracy: 0.8653 - val_loss: 0.7055 - val_accuracy: 0.5649\n",
      "Epoch 19: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.6090\n",
      "Test AUC for Layer 1: 0.6256\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_134\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_512 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_378 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_268 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_513 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_379 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_269 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_514 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_380 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_515 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.8682 - accuracy: 0.5146\n",
      "Epoch 1: val_loss improved from inf to 0.70986, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 26ms/step - loss: 0.8845 - accuracy: 0.5038 - val_loss: 0.7099 - val_accuracy: 0.3910\n",
      "Epoch 2/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.8028 - accuracy: 0.5558\n",
      "Epoch 2: val_loss did not improve from 0.70986\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.8030 - accuracy: 0.5583 - val_loss: 0.7310 - val_accuracy: 0.3910\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7675 - accuracy: 0.5898\n",
      "Epoch 3: val_loss did not improve from 0.70986\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7640 - accuracy: 0.5902 - val_loss: 0.7516 - val_accuracy: 0.3910\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6564 - accuracy: 0.6447\n",
      "Epoch 4: val_loss did not improve from 0.70986\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6564 - accuracy: 0.6447 - val_loss: 0.7758 - val_accuracy: 0.3910\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6682 - accuracy: 0.6309\n",
      "Epoch 5: val_loss did not improve from 0.70986\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6597 - accuracy: 0.6372 - val_loss: 0.7956 - val_accuracy: 0.3910\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6089 - accuracy: 0.6823\n",
      "Epoch 6: val_loss did not improve from 0.70986\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6089 - accuracy: 0.6823 - val_loss: 0.8133 - val_accuracy: 0.3910\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5825 - accuracy: 0.7031\n",
      "Epoch 7: val_loss did not improve from 0.70986\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5929 - accuracy: 0.6974 - val_loss: 0.8307 - val_accuracy: 0.3910\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5325 - accuracy: 0.7402\n",
      "Epoch 8: val_loss did not improve from 0.70986\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5380 - accuracy: 0.7350 - val_loss: 0.8452 - val_accuracy: 0.3910\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4879 - accuracy: 0.7656\n",
      "Epoch 9: val_loss did not improve from 0.70986\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4908 - accuracy: 0.7650 - val_loss: 0.8587 - val_accuracy: 0.3910\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4959 - accuracy: 0.7594\n",
      "Epoch 10: val_loss did not improve from 0.70986\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4959 - accuracy: 0.7594 - val_loss: 0.8643 - val_accuracy: 0.3910\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4723 - accuracy: 0.7852\n",
      "Epoch 11: val_loss did not improve from 0.70986\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4772 - accuracy: 0.7801 - val_loss: 0.8715 - val_accuracy: 0.3910\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4751 - accuracy: 0.7520\n",
      "Epoch 12: val_loss did not improve from 0.70986\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4742 - accuracy: 0.7538 - val_loss: 0.8744 - val_accuracy: 0.3910\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4096 - accuracy: 0.8184\n",
      "Epoch 13: val_loss did not improve from 0.70986\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4122 - accuracy: 0.8158 - val_loss: 0.8800 - val_accuracy: 0.3910\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3945 - accuracy: 0.8203\n",
      "Epoch 14: val_loss did not improve from 0.70986\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3991 - accuracy: 0.8158 - val_loss: 0.8824 - val_accuracy: 0.3910\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3864 - accuracy: 0.8477\n",
      "Epoch 15: val_loss did not improve from 0.70986\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3872 - accuracy: 0.8477 - val_loss: 0.8799 - val_accuracy: 0.3910\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3719 - accuracy: 0.8672Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70986\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3710 - accuracy: 0.8665 - val_loss: 0.8791 - val_accuracy: 0.3910\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4653\n",
      "Test AUC for Layer 2: 0.5418\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_135\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_516 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_381 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_270 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_517 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_382 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_271 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_518 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_383 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_519 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9146 - accuracy: 0.5150\n",
      "Epoch 1: val_loss improved from inf to 0.69542, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 23ms/step - loss: 0.9146 - accuracy: 0.5150 - val_loss: 0.6954 - val_accuracy: 0.4653\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7930 - accuracy: 0.5450\n",
      "Epoch 2: val_loss did not improve from 0.69542\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7930 - accuracy: 0.5450 - val_loss: 0.6986 - val_accuracy: 0.4653\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8021 - accuracy: 0.5465\n",
      "Epoch 3: val_loss did not improve from 0.69542\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.8021 - accuracy: 0.5465 - val_loss: 0.7020 - val_accuracy: 0.4653\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7735 - accuracy: 0.5556\n",
      "Epoch 4: val_loss did not improve from 0.69542\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7735 - accuracy: 0.5556 - val_loss: 0.7056 - val_accuracy: 0.4653\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6597 - accuracy: 0.6441\n",
      "Epoch 5: val_loss did not improve from 0.69542\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6597 - accuracy: 0.6441 - val_loss: 0.7104 - val_accuracy: 0.4653\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6471 - accuracy: 0.6622\n",
      "Epoch 6: val_loss did not improve from 0.69542\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6471 - accuracy: 0.6622 - val_loss: 0.7153 - val_accuracy: 0.4653\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6142 - accuracy: 0.6847\n",
      "Epoch 7: val_loss did not improve from 0.69542\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6142 - accuracy: 0.6847 - val_loss: 0.7185 - val_accuracy: 0.4653\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5523 - accuracy: 0.7282\n",
      "Epoch 8: val_loss did not improve from 0.69542\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5523 - accuracy: 0.7282 - val_loss: 0.7207 - val_accuracy: 0.4653\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5366 - accuracy: 0.7207\n",
      "Epoch 9: val_loss did not improve from 0.69542\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5366 - accuracy: 0.7207 - val_loss: 0.7233 - val_accuracy: 0.4653\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5167 - accuracy: 0.7342\n",
      "Epoch 10: val_loss did not improve from 0.69542\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5167 - accuracy: 0.7342 - val_loss: 0.7257 - val_accuracy: 0.4653\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4859 - accuracy: 0.7462\n",
      "Epoch 11: val_loss did not improve from 0.69542\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4859 - accuracy: 0.7462 - val_loss: 0.7280 - val_accuracy: 0.4653\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4741 - accuracy: 0.7763\n",
      "Epoch 12: val_loss did not improve from 0.69542\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4741 - accuracy: 0.7763 - val_loss: 0.7328 - val_accuracy: 0.4653\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4692 - accuracy: 0.7643\n",
      "Epoch 13: val_loss did not improve from 0.69542\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4692 - accuracy: 0.7643 - val_loss: 0.7334 - val_accuracy: 0.4653\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4118 - accuracy: 0.8168\n",
      "Epoch 14: val_loss did not improve from 0.69542\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4118 - accuracy: 0.8168 - val_loss: 0.7353 - val_accuracy: 0.4653\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4305 - accuracy: 0.8063\n",
      "Epoch 15: val_loss did not improve from 0.69542\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4305 - accuracy: 0.8063 - val_loss: 0.7341 - val_accuracy: 0.4653\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3839 - accuracy: 0.8363Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69542\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3839 - accuracy: 0.8363 - val_loss: 0.7323 - val_accuracy: 0.4752\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4857\n",
      "Test AUC for Layer 3: 0.4910\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5200\n",
      "Average Test AUC across all layers: 0.5528\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_136\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_520 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_384 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_272 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_521 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_385 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_273 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_522 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_386 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_523 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.9405 - accuracy: 0.5063\n",
      "Epoch 1: val_loss improved from inf to 0.69371, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 30ms/step - loss: 0.9179 - accuracy: 0.4938 - val_loss: 0.6937 - val_accuracy: 0.4733\n",
      "Epoch 2/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.8961 - accuracy: 0.5031\n",
      "Epoch 2: val_loss improved from 0.69371 to 0.68678, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.8994 - accuracy: 0.4988 - val_loss: 0.6868 - val_accuracy: 0.6794\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7886 - accuracy: 0.5682\n",
      "Epoch 3: val_loss improved from 0.68678 to 0.67956, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.7838 - accuracy: 0.5686 - val_loss: 0.6796 - val_accuracy: 0.6794\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7149 - accuracy: 0.5710\n",
      "Epoch 4: val_loss improved from 0.67956 to 0.67413, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.7040 - accuracy: 0.5860 - val_loss: 0.6741 - val_accuracy: 0.6794\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6503 - accuracy: 0.6193\n",
      "Epoch 5: val_loss improved from 0.67413 to 0.66736, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.6548 - accuracy: 0.6160 - val_loss: 0.6674 - val_accuracy: 0.6794\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6543 - accuracy: 0.6250\n",
      "Epoch 6: val_loss improved from 0.66736 to 0.66302, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.6402 - accuracy: 0.6309 - val_loss: 0.6630 - val_accuracy: 0.6794\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6497 - accuracy: 0.6676\n",
      "Epoch 7: val_loss improved from 0.66302 to 0.65798, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.6499 - accuracy: 0.6658 - val_loss: 0.6580 - val_accuracy: 0.6794\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5659 - accuracy: 0.7074\n",
      "Epoch 8: val_loss improved from 0.65798 to 0.65475, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.5649 - accuracy: 0.7057 - val_loss: 0.6548 - val_accuracy: 0.6794\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5252 - accuracy: 0.7585\n",
      "Epoch 9: val_loss improved from 0.65475 to 0.65113, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5395 - accuracy: 0.7382 - val_loss: 0.6511 - val_accuracy: 0.6794\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5652 - accuracy: 0.7074\n",
      "Epoch 10: val_loss improved from 0.65113 to 0.64769, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.5565 - accuracy: 0.7157 - val_loss: 0.6477 - val_accuracy: 0.6794\n",
      "Epoch 11/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.4861 - accuracy: 0.7594\n",
      "Epoch 11: val_loss improved from 0.64769 to 0.64497, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.4855 - accuracy: 0.7656 - val_loss: 0.6450 - val_accuracy: 0.6794\n",
      "Epoch 12/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.4589 - accuracy: 0.7906\n",
      "Epoch 12: val_loss improved from 0.64497 to 0.64121, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.4562 - accuracy: 0.7930 - val_loss: 0.6412 - val_accuracy: 0.6794\n",
      "Epoch 13/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4307 - accuracy: 0.8125\n",
      "Epoch 13: val_loss improved from 0.64121 to 0.63832, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.4271 - accuracy: 0.8105 - val_loss: 0.6383 - val_accuracy: 0.6794\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4146 - accuracy: 0.8097\n",
      "Epoch 14: val_loss improved from 0.63832 to 0.63683, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.4103 - accuracy: 0.8155 - val_loss: 0.6368 - val_accuracy: 0.6794\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4179 - accuracy: 0.8210\n",
      "Epoch 15: val_loss improved from 0.63683 to 0.63496, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.4215 - accuracy: 0.8204 - val_loss: 0.6350 - val_accuracy: 0.6794\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4021 - accuracy: 0.8239\n",
      "Epoch 16: val_loss improved from 0.63496 to 0.63416, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.3981 - accuracy: 0.8254 - val_loss: 0.6342 - val_accuracy: 0.6794\n",
      "Epoch 17/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3780 - accuracy: 0.8494\n",
      "Epoch 17: val_loss improved from 0.63416 to 0.63406, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.3749 - accuracy: 0.8529 - val_loss: 0.6341 - val_accuracy: 0.6794\n",
      "Epoch 18/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3754 - accuracy: 0.8381\n",
      "Epoch 18: val_loss improved from 0.63406 to 0.63404, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.3641 - accuracy: 0.8479 - val_loss: 0.6340 - val_accuracy: 0.6794\n",
      "Epoch 19/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3299 - accuracy: 0.8807\n",
      "Epoch 19: val_loss did not improve from 0.63404\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3314 - accuracy: 0.8828 - val_loss: 0.6342 - val_accuracy: 0.6794\n",
      "Epoch 20/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2963 - accuracy: 0.8949\n",
      "Epoch 20: val_loss did not improve from 0.63404\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3026 - accuracy: 0.8853 - val_loss: 0.6343 - val_accuracy: 0.6794\n",
      "Epoch 21/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3050 - accuracy: 0.9006\n",
      "Epoch 21: val_loss did not improve from 0.63404\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3114 - accuracy: 0.8928 - val_loss: 0.6346 - val_accuracy: 0.6794\n",
      "Epoch 22/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3042 - accuracy: 0.8750\n",
      "Epoch 22: val_loss did not improve from 0.63404\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.2977 - accuracy: 0.8853 - val_loss: 0.6350 - val_accuracy: 0.6794\n",
      "Epoch 23/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2834 - accuracy: 0.8892\n",
      "Epoch 23: val_loss did not improve from 0.63404\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.2793 - accuracy: 0.8953 - val_loss: 0.6356 - val_accuracy: 0.6794\n",
      "Epoch 24/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2765 - accuracy: 0.9148\n",
      "Epoch 24: val_loss did not improve from 0.63404\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.2764 - accuracy: 0.9152 - val_loss: 0.6374 - val_accuracy: 0.6794\n",
      "Epoch 25/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2640 - accuracy: 0.9148\n",
      "Epoch 25: val_loss did not improve from 0.63404\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.2616 - accuracy: 0.9152 - val_loss: 0.6394 - val_accuracy: 0.6794\n",
      "Epoch 26/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.2623 - accuracy: 0.9141\n",
      "Epoch 26: val_loss did not improve from 0.63404\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.2595 - accuracy: 0.9177 - val_loss: 0.6417 - val_accuracy: 0.6794\n",
      "Epoch 27/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2564 - accuracy: 0.9318\n",
      "Epoch 27: val_loss did not improve from 0.63404\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.2580 - accuracy: 0.9252 - val_loss: 0.6438 - val_accuracy: 0.6794\n",
      "Epoch 28/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2650 - accuracy: 0.9233\n",
      "Epoch 28: val_loss did not improve from 0.63404\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.2536 - accuracy: 0.9327 - val_loss: 0.6461 - val_accuracy: 0.6794\n",
      "Epoch 29/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2073 - accuracy: 0.9517\n",
      "Epoch 29: val_loss did not improve from 0.63404\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.2206 - accuracy: 0.9451 - val_loss: 0.6473 - val_accuracy: 0.6870\n",
      "Epoch 30/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2015 - accuracy: 0.9489\n",
      "Epoch 30: val_loss did not improve from 0.63404\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.2172 - accuracy: 0.9377 - val_loss: 0.6502 - val_accuracy: 0.6794\n",
      "Epoch 31/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.1843 - accuracy: 0.9631\n",
      "Epoch 31: val_loss did not improve from 0.63404\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.1810 - accuracy: 0.9651 - val_loss: 0.6526 - val_accuracy: 0.6794\n",
      "Epoch 32/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.1946 - accuracy: 0.9545\n",
      "Epoch 32: val_loss did not improve from 0.63404\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.1916 - accuracy: 0.9551 - val_loss: 0.6553 - val_accuracy: 0.6718\n",
      "Epoch 33/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2160 - accuracy: 0.9347Restoring model weights from the end of the best epoch: 18.\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.63404\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.2142 - accuracy: 0.9352 - val_loss: 0.6582 - val_accuracy: 0.6641\n",
      "Epoch 33: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.7368\n",
      "Test AUC for Layer 1: 0.4134\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_137\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_524 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_387 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_274 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_525 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_388 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_275 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_526 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_389 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_527 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.9654 - accuracy: 0.4771\n",
      "Epoch 1: val_loss improved from inf to 0.73995, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.9566 - accuracy: 0.4774 - val_loss: 0.7399 - val_accuracy: 0.2632\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8276 - accuracy: 0.5332\n",
      "Epoch 2: val_loss did not improve from 0.73995\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.8308 - accuracy: 0.5282 - val_loss: 0.7834 - val_accuracy: 0.2632\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7967 - accuracy: 0.5332\n",
      "Epoch 3: val_loss did not improve from 0.73995\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7880 - accuracy: 0.5414 - val_loss: 0.8266 - val_accuracy: 0.2632\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7313 - accuracy: 0.5918\n",
      "Epoch 4: val_loss did not improve from 0.73995\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7191 - accuracy: 0.5996 - val_loss: 0.8655 - val_accuracy: 0.2632\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6686 - accuracy: 0.6211\n",
      "Epoch 5: val_loss did not improve from 0.73995\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6697 - accuracy: 0.6203 - val_loss: 0.9008 - val_accuracy: 0.2632\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6610 - accuracy: 0.6152\n",
      "Epoch 6: val_loss did not improve from 0.73995\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6554 - accuracy: 0.6222 - val_loss: 0.9350 - val_accuracy: 0.2632\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5788 - accuracy: 0.7051\n",
      "Epoch 7: val_loss did not improve from 0.73995\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5798 - accuracy: 0.7049 - val_loss: 0.9679 - val_accuracy: 0.2632\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5538 - accuracy: 0.7227\n",
      "Epoch 8: val_loss did not improve from 0.73995\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5507 - accuracy: 0.7274 - val_loss: 0.9881 - val_accuracy: 0.2632\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5127 - accuracy: 0.7598\n",
      "Epoch 9: val_loss did not improve from 0.73995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5171 - accuracy: 0.7594 - val_loss: 1.0111 - val_accuracy: 0.2632\n",
      "Epoch 10/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5423 - accuracy: 0.7208\n",
      "Epoch 10: val_loss did not improve from 0.73995\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5393 - accuracy: 0.7218 - val_loss: 1.0268 - val_accuracy: 0.2632\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4658 - accuracy: 0.8008\n",
      "Epoch 11: val_loss did not improve from 0.73995\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4629 - accuracy: 0.8008 - val_loss: 1.0390 - val_accuracy: 0.2632\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4598 - accuracy: 0.7656\n",
      "Epoch 12: val_loss did not improve from 0.73995\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4648 - accuracy: 0.7594 - val_loss: 1.0522 - val_accuracy: 0.2632\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4403 - accuracy: 0.8145\n",
      "Epoch 13: val_loss did not improve from 0.73995\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4433 - accuracy: 0.8158 - val_loss: 1.0632 - val_accuracy: 0.2632\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4079 - accuracy: 0.8242\n",
      "Epoch 14: val_loss did not improve from 0.73995\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4123 - accuracy: 0.8214 - val_loss: 1.0663 - val_accuracy: 0.2632\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4022 - accuracy: 0.8125\n",
      "Epoch 15: val_loss did not improve from 0.73995\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4003 - accuracy: 0.8158 - val_loss: 1.0751 - val_accuracy: 0.2632\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3855 - accuracy: 0.8379Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.73995\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3912 - accuracy: 0.8346 - val_loss: 1.0830 - val_accuracy: 0.2632\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5842\n",
      "Test AUC for Layer 2: 0.5125\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_138\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_528 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_390 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_276 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_529 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_391 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_277 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_530 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_392 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_531 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.8944 - accuracy: 0.4859\n",
      "Epoch 1: val_loss improved from inf to 0.69138, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 23ms/step - loss: 0.8889 - accuracy: 0.4880 - val_loss: 0.6914 - val_accuracy: 0.5743\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7811 - accuracy: 0.5661\n",
      "Epoch 2: val_loss improved from 0.69138 to 0.69057, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7811 - accuracy: 0.5661 - val_loss: 0.6906 - val_accuracy: 0.5842\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7501 - accuracy: 0.5676\n",
      "Epoch 3: val_loss did not improve from 0.69057\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7501 - accuracy: 0.5676 - val_loss: 0.6906 - val_accuracy: 0.5842\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6629 - accuracy: 0.6562\n",
      "Epoch 4: val_loss did not improve from 0.69057\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6629 - accuracy: 0.6562 - val_loss: 0.6917 - val_accuracy: 0.5743\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6338 - accuracy: 0.6637\n",
      "Epoch 5: val_loss did not improve from 0.69057\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6338 - accuracy: 0.6637 - val_loss: 0.6932 - val_accuracy: 0.5644\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6081 - accuracy: 0.6652\n",
      "Epoch 6: val_loss did not improve from 0.69057\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6081 - accuracy: 0.6652 - val_loss: 0.6950 - val_accuracy: 0.4653\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5844 - accuracy: 0.7042\n",
      "Epoch 7: val_loss did not improve from 0.69057\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5844 - accuracy: 0.7042 - val_loss: 0.6968 - val_accuracy: 0.4356\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5159 - accuracy: 0.7402\n",
      "Epoch 8: val_loss did not improve from 0.69057\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5159 - accuracy: 0.7402 - val_loss: 0.6980 - val_accuracy: 0.4356\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5048 - accuracy: 0.7462\n",
      "Epoch 9: val_loss did not improve from 0.69057\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5048 - accuracy: 0.7462 - val_loss: 0.6990 - val_accuracy: 0.4653\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4870 - accuracy: 0.7733\n",
      "Epoch 10: val_loss did not improve from 0.69057\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4870 - accuracy: 0.7733 - val_loss: 0.7024 - val_accuracy: 0.4455\n",
      "Epoch 11/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.4765 - accuracy: 0.7852\n",
      "Epoch 11: val_loss did not improve from 0.69057\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4886 - accuracy: 0.7688 - val_loss: 0.7056 - val_accuracy: 0.3960\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4326 - accuracy: 0.8153\n",
      "Epoch 12: val_loss did not improve from 0.69057\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4326 - accuracy: 0.8153 - val_loss: 0.7095 - val_accuracy: 0.3861\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4324 - accuracy: 0.8093\n",
      "Epoch 13: val_loss did not improve from 0.69057\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4324 - accuracy: 0.8093 - val_loss: 0.7135 - val_accuracy: 0.3663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4014 - accuracy: 0.8258\n",
      "Epoch 14: val_loss did not improve from 0.69057\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4014 - accuracy: 0.8258 - val_loss: 0.7178 - val_accuracy: 0.3960\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4069 - accuracy: 0.8258\n",
      "Epoch 15: val_loss did not improve from 0.69057\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4069 - accuracy: 0.8258 - val_loss: 0.7222 - val_accuracy: 0.3861\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3732 - accuracy: 0.8544\n",
      "Epoch 16: val_loss did not improve from 0.69057\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3732 - accuracy: 0.8544 - val_loss: 0.7247 - val_accuracy: 0.4059\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3936 - accuracy: 0.8258Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.69057\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3936 - accuracy: 0.8258 - val_loss: 0.7338 - val_accuracy: 0.4356\n",
      "Epoch 17: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4429\n",
      "Test AUC for Layer 3: 0.5798\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5880\n",
      "Average Test AUC across all layers: 0.5019\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_139\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_532 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_393 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_278 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_533 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_394 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_279 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_534 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_395 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_535 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.9083 - accuracy: 0.4631\n",
      "Epoch 1: val_loss improved from inf to 0.69722, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 30ms/step - loss: 0.9085 - accuracy: 0.4589 - val_loss: 0.6972 - val_accuracy: 0.4351\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7544 - accuracy: 0.5710\n",
      "Epoch 2: val_loss did not improve from 0.69722\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7662 - accuracy: 0.5611 - val_loss: 0.7018 - val_accuracy: 0.4351\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7428 - accuracy: 0.5483\n",
      "Epoch 3: val_loss did not improve from 0.69722\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.7371 - accuracy: 0.5511 - val_loss: 0.7063 - val_accuracy: 0.4351\n",
      "Epoch 4/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.6986 - accuracy: 0.5906\n",
      "Epoch 4: val_loss did not improve from 0.69722\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7084 - accuracy: 0.5985 - val_loss: 0.7098 - val_accuracy: 0.4351\n",
      "Epoch 5/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.6708 - accuracy: 0.6094\n",
      "Epoch 5: val_loss did not improve from 0.69722\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6826 - accuracy: 0.6035 - val_loss: 0.7139 - val_accuracy: 0.4351\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6240 - accuracy: 0.6790\n",
      "Epoch 6: val_loss did not improve from 0.69722\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6181 - accuracy: 0.6833 - val_loss: 0.7173 - val_accuracy: 0.4351\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5982 - accuracy: 0.6591\n",
      "Epoch 7: val_loss did not improve from 0.69722\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5923 - accuracy: 0.6608 - val_loss: 0.7205 - val_accuracy: 0.4351\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5809 - accuracy: 0.6960\n",
      "Epoch 8: val_loss did not improve from 0.69722\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5698 - accuracy: 0.7057 - val_loss: 0.7234 - val_accuracy: 0.4351\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5272 - accuracy: 0.7273\n",
      "Epoch 9: val_loss did not improve from 0.69722\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5343 - accuracy: 0.7257 - val_loss: 0.7270 - val_accuracy: 0.4351\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4802 - accuracy: 0.7585\n",
      "Epoch 10: val_loss did not improve from 0.69722\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4896 - accuracy: 0.7531 - val_loss: 0.7292 - val_accuracy: 0.4351\n",
      "Epoch 11/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5094 - accuracy: 0.7656\n",
      "Epoch 11: val_loss did not improve from 0.69722\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5139 - accuracy: 0.7656 - val_loss: 0.7323 - val_accuracy: 0.4351\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4849 - accuracy: 0.7727\n",
      "Epoch 12: val_loss did not improve from 0.69722\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5037 - accuracy: 0.7631 - val_loss: 0.7332 - val_accuracy: 0.4351\n",
      "Epoch 13/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4518 - accuracy: 0.8073\n",
      "Epoch 13: val_loss did not improve from 0.69722\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4572 - accuracy: 0.8030 - val_loss: 0.7361 - val_accuracy: 0.4351\n",
      "Epoch 14/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4465 - accuracy: 0.7891\n",
      "Epoch 14: val_loss did not improve from 0.69722\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4460 - accuracy: 0.7905 - val_loss: 0.7387 - val_accuracy: 0.4351\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4619 - accuracy: 0.7756\n",
      "Epoch 15: val_loss did not improve from 0.69722\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4588 - accuracy: 0.7830 - val_loss: 0.7378 - val_accuracy: 0.4351\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4284 - accuracy: 0.8153Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69722\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4315 - accuracy: 0.8080 - val_loss: 0.7385 - val_accuracy: 0.4351\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.3910\n",
      "Test AUC for Layer 1: 0.5598\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_140\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_536 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_396 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_280 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_537 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_397 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_281 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_538 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_398 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_539 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.8686 - accuracy: 0.5312\n",
      "Epoch 1: val_loss improved from inf to 0.68173, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 26ms/step - loss: 0.8735 - accuracy: 0.5320 - val_loss: 0.6817 - val_accuracy: 0.6090\n",
      "Epoch 2/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.7905 - accuracy: 0.5792\n",
      "Epoch 2: val_loss improved from 0.68173 to 0.67679, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.7933 - accuracy: 0.5752 - val_loss: 0.6768 - val_accuracy: 0.6090\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7236 - accuracy: 0.5938\n",
      "Epoch 3: val_loss improved from 0.67679 to 0.67374, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7263 - accuracy: 0.5940 - val_loss: 0.6737 - val_accuracy: 0.6090\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6982 - accuracy: 0.5918\n",
      "Epoch 4: val_loss improved from 0.67374 to 0.67180, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6994 - accuracy: 0.5940 - val_loss: 0.6718 - val_accuracy: 0.6090\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6655 - accuracy: 0.6504\n",
      "Epoch 5: val_loss improved from 0.67180 to 0.67134, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6657 - accuracy: 0.6485 - val_loss: 0.6713 - val_accuracy: 0.6090\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6370 - accuracy: 0.6543\n",
      "Epoch 6: val_loss improved from 0.67134 to 0.67074, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6417 - accuracy: 0.6560 - val_loss: 0.6707 - val_accuracy: 0.6090\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6234 - accuracy: 0.6660\n",
      "Epoch 7: val_loss improved from 0.67074 to 0.67070, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6220 - accuracy: 0.6635 - val_loss: 0.6707 - val_accuracy: 0.6090\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5549 - accuracy: 0.7363\n",
      "Epoch 8: val_loss did not improve from 0.67070\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5573 - accuracy: 0.7331 - val_loss: 0.6710 - val_accuracy: 0.6090\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5503 - accuracy: 0.7049\n",
      "Epoch 9: val_loss did not improve from 0.67070\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5503 - accuracy: 0.7049 - val_loss: 0.6719 - val_accuracy: 0.6090\n",
      "Epoch 10/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5119 - accuracy: 0.7396\n",
      "Epoch 10: val_loss did not improve from 0.67070\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5229 - accuracy: 0.7368 - val_loss: 0.6733 - val_accuracy: 0.6090\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5078 - accuracy: 0.7637\n",
      "Epoch 11: val_loss did not improve from 0.67070\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5024 - accuracy: 0.7669 - val_loss: 0.6748 - val_accuracy: 0.6015\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4942 - accuracy: 0.7832\n",
      "Epoch 12: val_loss did not improve from 0.67070\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4943 - accuracy: 0.7801 - val_loss: 0.6757 - val_accuracy: 0.6015\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5083 - accuracy: 0.7676\n",
      "Epoch 13: val_loss did not improve from 0.67070\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5085 - accuracy: 0.7688 - val_loss: 0.6770 - val_accuracy: 0.5789\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4686 - accuracy: 0.7832\n",
      "Epoch 14: val_loss did not improve from 0.67070\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4630 - accuracy: 0.7857 - val_loss: 0.6783 - val_accuracy: 0.5639\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4605 - accuracy: 0.7773\n",
      "Epoch 15: val_loss did not improve from 0.67070\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4574 - accuracy: 0.7763 - val_loss: 0.6805 - val_accuracy: 0.5038\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4596 - accuracy: 0.7832\n",
      "Epoch 16: val_loss did not improve from 0.67070\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4587 - accuracy: 0.7801 - val_loss: 0.6825 - val_accuracy: 0.5038\n",
      "Epoch 17/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4101 - accuracy: 0.8066\n",
      "Epoch 17: val_loss did not improve from 0.67070\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4165 - accuracy: 0.8008 - val_loss: 0.6854 - val_accuracy: 0.5338\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4090 - accuracy: 0.8262\n",
      "Epoch 18: val_loss did not improve from 0.67070\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4032 - accuracy: 0.8289 - val_loss: 0.6894 - val_accuracy: 0.5564\n",
      "Epoch 19/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3641 - accuracy: 0.8574\n",
      "Epoch 19: val_loss did not improve from 0.67070\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3649 - accuracy: 0.8571 - val_loss: 0.6888 - val_accuracy: 0.5639\n",
      "Epoch 20/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3756 - accuracy: 0.8477\n",
      "Epoch 20: val_loss did not improve from 0.67070\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3729 - accuracy: 0.8515 - val_loss: 0.6890 - val_accuracy: 0.5564\n",
      "Epoch 21/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3339 - accuracy: 0.8672\n",
      "Epoch 21: val_loss did not improve from 0.67070\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3360 - accuracy: 0.8647 - val_loss: 0.6895 - val_accuracy: 0.5489\n",
      "Epoch 22/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3307 - accuracy: 0.8691Restoring model weights from the end of the best epoch: 7.\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.67070\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3365 - accuracy: 0.8647 - val_loss: 0.6912 - val_accuracy: 0.5564\n",
      "Epoch 22: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5347\n",
      "Test AUC for Layer 2: 0.5386\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_141\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_540 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_399 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_282 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_541 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_400 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_283 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_542 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_401 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_543 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8731 - accuracy: 0.4910\n",
      "Epoch 1: val_loss improved from inf to 0.69097, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 23ms/step - loss: 0.8731 - accuracy: 0.4910 - val_loss: 0.6910 - val_accuracy: 0.5347\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7496 - accuracy: 0.5661\n",
      "Epoch 2: val_loss improved from 0.69097 to 0.69081, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7496 - accuracy: 0.5661 - val_loss: 0.6908 - val_accuracy: 0.5347\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7072 - accuracy: 0.5871\n",
      "Epoch 3: val_loss did not improve from 0.69081\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7072 - accuracy: 0.5871 - val_loss: 0.6914 - val_accuracy: 0.5347\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6520 - accuracy: 0.6411\n",
      "Epoch 4: val_loss did not improve from 0.69081\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6520 - accuracy: 0.6411 - val_loss: 0.6923 - val_accuracy: 0.5347\n",
      "Epoch 5/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.6483 - accuracy: 0.6507\n",
      "Epoch 5: val_loss did not improve from 0.69081\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6338 - accuracy: 0.6577 - val_loss: 0.6924 - val_accuracy: 0.5347\n",
      "Epoch 6/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.5973 - accuracy: 0.6816\n",
      "Epoch 6: val_loss did not improve from 0.69081\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6052 - accuracy: 0.6832 - val_loss: 0.6920 - val_accuracy: 0.5347\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5887 - accuracy: 0.6967\n",
      "Epoch 7: val_loss did not improve from 0.69081\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5887 - accuracy: 0.6967 - val_loss: 0.6916 - val_accuracy: 0.5347\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5510 - accuracy: 0.7237\n",
      "Epoch 8: val_loss did not improve from 0.69081\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5510 - accuracy: 0.7237 - val_loss: 0.6917 - val_accuracy: 0.5347\n",
      "Epoch 9/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.5264 - accuracy: 0.7168\n",
      "Epoch 9: val_loss did not improve from 0.69081\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5266 - accuracy: 0.7177 - val_loss: 0.6929 - val_accuracy: 0.5347\n",
      "Epoch 10/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.4871 - accuracy: 0.7665\n",
      "Epoch 10: val_loss did not improve from 0.69081\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5051 - accuracy: 0.7417 - val_loss: 0.6930 - val_accuracy: 0.5347\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5214 - accuracy: 0.7207\n",
      "Epoch 11: val_loss did not improve from 0.69081\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5214 - accuracy: 0.7207 - val_loss: 0.6935 - val_accuracy: 0.5050\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4761 - accuracy: 0.7703\n",
      "Epoch 12: val_loss did not improve from 0.69081\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4761 - accuracy: 0.7703 - val_loss: 0.6940 - val_accuracy: 0.4950\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4970 - accuracy: 0.7598\n",
      "Epoch 13: val_loss did not improve from 0.69081\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4970 - accuracy: 0.7598 - val_loss: 0.6951 - val_accuracy: 0.5743\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4431 - accuracy: 0.7958\n",
      "Epoch 14: val_loss did not improve from 0.69081\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4431 - accuracy: 0.7958 - val_loss: 0.6959 - val_accuracy: 0.5644\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4283 - accuracy: 0.8048\n",
      "Epoch 15: val_loss did not improve from 0.69081\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4283 - accuracy: 0.8048 - val_loss: 0.6992 - val_accuracy: 0.5149\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4519 - accuracy: 0.7853\n",
      "Epoch 16: val_loss did not improve from 0.69081\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4519 - accuracy: 0.7853 - val_loss: 0.7020 - val_accuracy: 0.5248\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4060 - accuracy: 0.8288Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.69081\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4060 - accuracy: 0.8288 - val_loss: 0.7033 - val_accuracy: 0.5050\n",
      "Epoch 17: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5143\n",
      "Test AUC for Layer 3: 0.4551\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4800\n",
      "Average Test AUC across all layers: 0.5178\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_142\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_544 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_402 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_284 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_545 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_403 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_285 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_546 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_404 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_547 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8687 - accuracy: 0.4688\n",
      "Epoch 1: val_loss improved from inf to 0.68438, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 31ms/step - loss: 0.8712 - accuracy: 0.4663 - val_loss: 0.6844 - val_accuracy: 0.6794\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7270 - accuracy: 0.5994\n",
      "Epoch 2: val_loss improved from 0.68438 to 0.67411, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.7408 - accuracy: 0.5910 - val_loss: 0.6741 - val_accuracy: 0.6794\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7514 - accuracy: 0.5795\n",
      "Epoch 3: val_loss improved from 0.67411 to 0.66703, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.7380 - accuracy: 0.5860 - val_loss: 0.6670 - val_accuracy: 0.6794\n",
      "Epoch 4/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.6564 - accuracy: 0.6156\n",
      "Epoch 4: val_loss improved from 0.66703 to 0.66012, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.6403 - accuracy: 0.6309 - val_loss: 0.6601 - val_accuracy: 0.6794\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6708 - accuracy: 0.6364\n",
      "Epoch 5: val_loss improved from 0.66012 to 0.65533, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.6778 - accuracy: 0.6334 - val_loss: 0.6553 - val_accuracy: 0.6794\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6198 - accuracy: 0.6705\n",
      "Epoch 6: val_loss improved from 0.65533 to 0.65242, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.6250 - accuracy: 0.6683 - val_loss: 0.6524 - val_accuracy: 0.6794\n",
      "Epoch 7/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.5566 - accuracy: 0.7094\n",
      "Epoch 7: val_loss improved from 0.65242 to 0.64961, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5604 - accuracy: 0.7132 - val_loss: 0.6496 - val_accuracy: 0.6794\n",
      "Epoch 8/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.5628 - accuracy: 0.6969\n",
      "Epoch 8: val_loss improved from 0.64961 to 0.64829, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5453 - accuracy: 0.7182 - val_loss: 0.6483 - val_accuracy: 0.6794\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5166 - accuracy: 0.7443\n",
      "Epoch 9: val_loss improved from 0.64829 to 0.64523, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5088 - accuracy: 0.7581 - val_loss: 0.6452 - val_accuracy: 0.6794\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5021 - accuracy: 0.7472\n",
      "Epoch 10: val_loss improved from 0.64523 to 0.64249, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.5004 - accuracy: 0.7506 - val_loss: 0.6425 - val_accuracy: 0.6794\n",
      "Epoch 11/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4729 - accuracy: 0.7557\n",
      "Epoch 11: val_loss improved from 0.64249 to 0.64013, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.4758 - accuracy: 0.7581 - val_loss: 0.6401 - val_accuracy: 0.6794\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4827 - accuracy: 0.7727\n",
      "Epoch 12: val_loss improved from 0.64013 to 0.63781, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.4842 - accuracy: 0.7731 - val_loss: 0.6378 - val_accuracy: 0.6794\n",
      "Epoch 13/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4391 - accuracy: 0.8267\n",
      "Epoch 13: val_loss improved from 0.63781 to 0.63636, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.4336 - accuracy: 0.8329 - val_loss: 0.6364 - val_accuracy: 0.6794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4214 - accuracy: 0.8182\n",
      "Epoch 14: val_loss improved from 0.63636 to 0.63540, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.4199 - accuracy: 0.8155 - val_loss: 0.6354 - val_accuracy: 0.6794\n",
      "Epoch 15/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.3945 - accuracy: 0.8406\n",
      "Epoch 15: val_loss improved from 0.63540 to 0.63399, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.3918 - accuracy: 0.8404 - val_loss: 0.6340 - val_accuracy: 0.6794\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4274 - accuracy: 0.8011\n",
      "Epoch 16: val_loss improved from 0.63399 to 0.63370, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.4161 - accuracy: 0.8155 - val_loss: 0.6337 - val_accuracy: 0.6794\n",
      "Epoch 17/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4143 - accuracy: 0.8097\n",
      "Epoch 17: val_loss improved from 0.63370 to 0.63306, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.4259 - accuracy: 0.8055 - val_loss: 0.6331 - val_accuracy: 0.6794\n",
      "Epoch 18/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3420 - accuracy: 0.8523\n",
      "Epoch 18: val_loss did not improve from 0.63306\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3466 - accuracy: 0.8479 - val_loss: 0.6337 - val_accuracy: 0.6794\n",
      "Epoch 19/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3237 - accuracy: 0.8807\n",
      "Epoch 19: val_loss did not improve from 0.63306\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3340 - accuracy: 0.8728 - val_loss: 0.6331 - val_accuracy: 0.6794\n",
      "Epoch 20/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3526 - accuracy: 0.8466\n",
      "Epoch 20: val_loss did not improve from 0.63306\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3468 - accuracy: 0.8529 - val_loss: 0.6333 - val_accuracy: 0.6794\n",
      "Epoch 21/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3499 - accuracy: 0.8620\n",
      "Epoch 21: val_loss improved from 0.63306 to 0.63241, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.3517 - accuracy: 0.8628 - val_loss: 0.6324 - val_accuracy: 0.6794\n",
      "Epoch 22/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3090 - accuracy: 0.9034\n",
      "Epoch 22: val_loss did not improve from 0.63241\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3142 - accuracy: 0.8953 - val_loss: 0.6329 - val_accuracy: 0.6794\n",
      "Epoch 23/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3165 - accuracy: 0.8693\n",
      "Epoch 23: val_loss did not improve from 0.63241\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3188 - accuracy: 0.8703 - val_loss: 0.6347 - val_accuracy: 0.6718\n",
      "Epoch 24/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3026 - accuracy: 0.9091\n",
      "Epoch 24: val_loss did not improve from 0.63241\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.2986 - accuracy: 0.9127 - val_loss: 0.6361 - val_accuracy: 0.6641\n",
      "Epoch 25/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2855 - accuracy: 0.9148\n",
      "Epoch 25: val_loss did not improve from 0.63241\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.2880 - accuracy: 0.9127 - val_loss: 0.6363 - val_accuracy: 0.6718\n",
      "Epoch 26/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2702 - accuracy: 0.9205\n",
      "Epoch 26: val_loss did not improve from 0.63241\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.2740 - accuracy: 0.9177 - val_loss: 0.6383 - val_accuracy: 0.6794\n",
      "Epoch 27/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2650 - accuracy: 0.9119\n",
      "Epoch 27: val_loss did not improve from 0.63241\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.2668 - accuracy: 0.9102 - val_loss: 0.6402 - val_accuracy: 0.6794\n",
      "Epoch 28/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2909 - accuracy: 0.8949\n",
      "Epoch 28: val_loss did not improve from 0.63241\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.2840 - accuracy: 0.9027 - val_loss: 0.6399 - val_accuracy: 0.6718\n",
      "Epoch 29/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2619 - accuracy: 0.9205\n",
      "Epoch 29: val_loss did not improve from 0.63241\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.2571 - accuracy: 0.9177 - val_loss: 0.6414 - val_accuracy: 0.6870\n",
      "Epoch 30/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2321 - accuracy: 0.9261\n",
      "Epoch 30: val_loss did not improve from 0.63241\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.2290 - accuracy: 0.9302 - val_loss: 0.6445 - val_accuracy: 0.6870\n",
      "Epoch 31/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2572 - accuracy: 0.8977\n",
      "Epoch 31: val_loss did not improve from 0.63241\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.2548 - accuracy: 0.9102 - val_loss: 0.6500 - val_accuracy: 0.6794\n",
      "Epoch 32/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2287 - accuracy: 0.9290\n",
      "Epoch 32: val_loss did not improve from 0.63241\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.2382 - accuracy: 0.9202 - val_loss: 0.6531 - val_accuracy: 0.6718\n",
      "Epoch 33/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2058 - accuracy: 0.9460\n",
      "Epoch 33: val_loss did not improve from 0.63241\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.2183 - accuracy: 0.9401 - val_loss: 0.6550 - val_accuracy: 0.6794\n",
      "Epoch 34/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2007 - accuracy: 0.9574\n",
      "Epoch 34: val_loss did not improve from 0.63241\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.2070 - accuracy: 0.9526 - val_loss: 0.6607 - val_accuracy: 0.6718\n",
      "Epoch 35/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.1951 - accuracy: 0.9517\n",
      "Epoch 35: val_loss did not improve from 0.63241\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.1950 - accuracy: 0.9526 - val_loss: 0.6677 - val_accuracy: 0.6412\n",
      "Epoch 36/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.1940 - accuracy: 0.9517Restoring model weights from the end of the best epoch: 21.\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.63241\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.1947 - accuracy: 0.9551 - val_loss: 0.6730 - val_accuracy: 0.6260\n",
      "Epoch 36: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.7368\n",
      "Test AUC for Layer 1: 0.4332\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_143\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_548 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_405 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_286 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_549 (Dense)           (None, 256)               131328    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " batch_normalization_406 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_287 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_550 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_407 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_551 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.9211 - accuracy: 0.4746\n",
      "Epoch 1: val_loss improved from inf to 0.76876, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.9263 - accuracy: 0.4699 - val_loss: 0.7688 - val_accuracy: 0.2632\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8735 - accuracy: 0.4883\n",
      "Epoch 2: val_loss did not improve from 0.76876\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.8642 - accuracy: 0.4981 - val_loss: 0.8575 - val_accuracy: 0.2632\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7796 - accuracy: 0.5371\n",
      "Epoch 3: val_loss did not improve from 0.76876\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7772 - accuracy: 0.5376 - val_loss: 0.9447 - val_accuracy: 0.2632\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7147 - accuracy: 0.6055\n",
      "Epoch 4: val_loss did not improve from 0.76876\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7079 - accuracy: 0.6071 - val_loss: 1.0310 - val_accuracy: 0.2632\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6627 - accuracy: 0.6387\n",
      "Epoch 5: val_loss did not improve from 0.76876\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6610 - accuracy: 0.6391 - val_loss: 1.1123 - val_accuracy: 0.2632\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6530 - accuracy: 0.6309\n",
      "Epoch 6: val_loss did not improve from 0.76876\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6581 - accuracy: 0.6297 - val_loss: 1.1795 - val_accuracy: 0.2632\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5579 - accuracy: 0.6836\n",
      "Epoch 7: val_loss did not improve from 0.76876\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5591 - accuracy: 0.6842 - val_loss: 1.2519 - val_accuracy: 0.2632\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5783 - accuracy: 0.6855\n",
      "Epoch 8: val_loss did not improve from 0.76876\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5799 - accuracy: 0.6861 - val_loss: 1.3180 - val_accuracy: 0.2632\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5544 - accuracy: 0.6953\n",
      "Epoch 9: val_loss did not improve from 0.76876\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5511 - accuracy: 0.6992 - val_loss: 1.3763 - val_accuracy: 0.2632\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5342 - accuracy: 0.7305\n",
      "Epoch 10: val_loss did not improve from 0.76876\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5333 - accuracy: 0.7293 - val_loss: 1.4208 - val_accuracy: 0.2632\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5161 - accuracy: 0.7402\n",
      "Epoch 11: val_loss did not improve from 0.76876\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5176 - accuracy: 0.7425 - val_loss: 1.4612 - val_accuracy: 0.2632\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4643 - accuracy: 0.7812\n",
      "Epoch 12: val_loss did not improve from 0.76876\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4615 - accuracy: 0.7838 - val_loss: 1.5065 - val_accuracy: 0.2632\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4569 - accuracy: 0.7969\n",
      "Epoch 13: val_loss did not improve from 0.76876\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4567 - accuracy: 0.7951 - val_loss: 1.5351 - val_accuracy: 0.2632\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4783 - accuracy: 0.7441\n",
      "Epoch 14: val_loss did not improve from 0.76876\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4737 - accuracy: 0.7500 - val_loss: 1.5605 - val_accuracy: 0.2632\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4215 - accuracy: 0.8047\n",
      "Epoch 15: val_loss did not improve from 0.76876\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4193 - accuracy: 0.8064 - val_loss: 1.5884 - val_accuracy: 0.2632\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4068 - accuracy: 0.8262Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.76876\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4118 - accuracy: 0.8195 - val_loss: 1.6074 - val_accuracy: 0.2632\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5842\n",
      "Test AUC for Layer 2: 0.5666\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_144\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_552 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_408 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_288 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_553 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_409 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_289 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_554 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_410 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_555 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8622 - accuracy: 0.5390\n",
      "Epoch 1: val_loss improved from inf to 0.68477, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 22ms/step - loss: 0.8622 - accuracy: 0.5390 - val_loss: 0.6848 - val_accuracy: 0.5842\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8124 - accuracy: 0.5450\n",
      "Epoch 2: val_loss improved from 0.68477 to 0.68090, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.8124 - accuracy: 0.5450 - val_loss: 0.6809 - val_accuracy: 0.5842\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7143 - accuracy: 0.5961\n",
      "Epoch 3: val_loss improved from 0.68090 to 0.67925, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7143 - accuracy: 0.5961 - val_loss: 0.6792 - val_accuracy: 0.5842\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6563 - accuracy: 0.6231\n",
      "Epoch 4: val_loss improved from 0.67925 to 0.67882, saving model to OpenAI_MLP_XGB/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.6563 - accuracy: 0.6231 - val_loss: 0.6788 - val_accuracy: 0.5842\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6451 - accuracy: 0.6532\n",
      "Epoch 5: val_loss did not improve from 0.67882\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6451 - accuracy: 0.6532 - val_loss: 0.6794 - val_accuracy: 0.5842\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6045 - accuracy: 0.6727\n",
      "Epoch 6: val_loss did not improve from 0.67882\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6045 - accuracy: 0.6727 - val_loss: 0.6809 - val_accuracy: 0.5842\n",
      "Epoch 7/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.5750 - accuracy: 0.6992\n",
      "Epoch 7: val_loss did not improve from 0.67882\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5711 - accuracy: 0.7012 - val_loss: 0.6837 - val_accuracy: 0.6139\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5220 - accuracy: 0.7417\n",
      "Epoch 8: val_loss did not improve from 0.67882\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5220 - accuracy: 0.7417 - val_loss: 0.6866 - val_accuracy: 0.6040\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5212 - accuracy: 0.7192\n",
      "Epoch 9: val_loss did not improve from 0.67882\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5212 - accuracy: 0.7192 - val_loss: 0.6890 - val_accuracy: 0.5644\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5057 - accuracy: 0.7613\n",
      "Epoch 10: val_loss did not improve from 0.67882\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5057 - accuracy: 0.7613 - val_loss: 0.6933 - val_accuracy: 0.4257\n",
      "Epoch 11/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4565 - accuracy: 0.7891\n",
      "Epoch 11: val_loss did not improve from 0.67882\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4635 - accuracy: 0.7838 - val_loss: 0.6992 - val_accuracy: 0.4257\n",
      "Epoch 12/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4532 - accuracy: 0.7830\n",
      "Epoch 12: val_loss did not improve from 0.67882\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4552 - accuracy: 0.7823 - val_loss: 0.7082 - val_accuracy: 0.4455\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4453 - accuracy: 0.7898\n",
      "Epoch 13: val_loss did not improve from 0.67882\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4453 - accuracy: 0.7898 - val_loss: 0.7165 - val_accuracy: 0.4257\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4472 - accuracy: 0.7898\n",
      "Epoch 14: val_loss did not improve from 0.67882\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4472 - accuracy: 0.7898 - val_loss: 0.7253 - val_accuracy: 0.4158\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3876 - accuracy: 0.8273\n",
      "Epoch 15: val_loss did not improve from 0.67882\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3876 - accuracy: 0.8273 - val_loss: 0.7285 - val_accuracy: 0.4257\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3972 - accuracy: 0.8243\n",
      "Epoch 16: val_loss did not improve from 0.67882\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3972 - accuracy: 0.8243 - val_loss: 0.7360 - val_accuracy: 0.4158\n",
      "Epoch 17/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.3828 - accuracy: 0.8254\n",
      "Epoch 17: val_loss did not improve from 0.67882\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3916 - accuracy: 0.8213 - val_loss: 0.7436 - val_accuracy: 0.4158\n",
      "Epoch 18/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.3473 - accuracy: 0.8732\n",
      "Epoch 18: val_loss did not improve from 0.67882\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3619 - accuracy: 0.8619 - val_loss: 0.7459 - val_accuracy: 0.4257\n",
      "Epoch 19/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.3309 - accuracy: 0.8676Restoring model weights from the end of the best epoch: 4.\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.67882\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3362 - accuracy: 0.8664 - val_loss: 0.7536 - val_accuracy: 0.4455\n",
      "Epoch 19: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4429\n",
      "Test AUC for Layer 3: 0.3978\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5880\n",
      "Average Test AUC across all layers: 0.4659\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Title and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.44772\n",
      "[1]\tvalidation_0-auc:0.44855\n",
      "[2]\tvalidation_0-auc:0.41572\n",
      "[3]\tvalidation_0-auc:0.43528\n",
      "[4]\tvalidation_0-auc:0.42485\n",
      "[5]\tvalidation_0-auc:0.42046\n",
      "[6]\tvalidation_0-auc:0.39403\n",
      "[7]\tvalidation_0-auc:0.39308\n",
      "[8]\tvalidation_0-auc:0.39260\n",
      "[9]\tvalidation_0-auc:0.39960\n",
      "[10]\tvalidation_0-auc:0.40635\n",
      "[11]\tvalidation_0-auc:0.38431\n",
      "[12]\tvalidation_0-auc:0.37079\n",
      "[13]\tvalidation_0-auc:0.37459\n",
      "[14]\tvalidation_0-auc:0.37482\n",
      "[15]\tvalidation_0-auc:0.38276\n",
      "[16]\tvalidation_0-auc:0.40019\n",
      "[17]\tvalidation_0-auc:0.40232\n",
      "[18]\tvalidation_0-auc:0.40398\n",
      "[19]\tvalidation_0-auc:0.40351\n",
      "[20]\tvalidation_0-auc:0.41607\n",
      "[21]\tvalidation_0-auc:0.40896\n",
      "[22]\tvalidation_0-auc:0.41596\n",
      "[23]\tvalidation_0-auc:0.42093\n",
      "[24]\tvalidation_0-auc:0.42674\n",
      "[25]\tvalidation_0-auc:0.42745\n",
      "[26]\tvalidation_0-auc:0.42010\n",
      "[27]\tvalidation_0-auc:0.41584\n",
      "[28]\tvalidation_0-auc:0.39900\n",
      "[29]\tvalidation_0-auc:0.40659\n",
      "[30]\tvalidation_0-auc:0.40517\n",
      "[31]\tvalidation_0-auc:0.40730\n",
      "[32]\tvalidation_0-auc:0.39569\n",
      "[33]\tvalidation_0-auc:0.39853\n",
      "[34]\tvalidation_0-auc:0.39308\n",
      "[35]\tvalidation_0-auc:0.38644\n",
      "[36]\tvalidation_0-auc:0.38359\n",
      "[37]\tvalidation_0-auc:0.38193\n",
      "[38]\tvalidation_0-auc:0.38691\n",
      "[39]\tvalidation_0-auc:0.37956\n",
      "[40]\tvalidation_0-auc:0.37933\n",
      "[41]\tvalidation_0-auc:0.37909\n",
      "[42]\tvalidation_0-auc:0.37601\n",
      "[43]\tvalidation_0-auc:0.38004\n",
      "[44]\tvalidation_0-auc:0.38502\n",
      "[45]\tvalidation_0-auc:0.37719\n",
      "[46]\tvalidation_0-auc:0.37103\n",
      "[47]\tvalidation_0-auc:0.37411\n",
      "[48]\tvalidation_0-auc:0.37079\n",
      "[49]\tvalidation_0-auc:0.37885\n",
      "[50]\tvalidation_0-auc:0.37838\n",
      "[51]\tvalidation_0-auc:0.37553\n",
      "[52]\tvalidation_0-auc:0.38028\n",
      "[53]\tvalidation_0-auc:0.37862\n",
      "[54]\tvalidation_0-auc:0.37696\n",
      "[55]\tvalidation_0-auc:0.38454\n",
      "[56]\tvalidation_0-auc:0.38312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57]\tvalidation_0-auc:0.38454\n",
      "[58]\tvalidation_0-auc:0.37672\n",
      "[59]\tvalidation_0-auc:0.37340\n",
      "[60]\tvalidation_0-auc:0.38028\n",
      "[61]\tvalidation_0-auc:0.38383\n",
      "[62]\tvalidation_0-auc:0.37933\n",
      "[63]\tvalidation_0-auc:0.37767\n",
      "[64]\tvalidation_0-auc:0.38573\n",
      "[65]\tvalidation_0-auc:0.38146\n",
      "[66]\tvalidation_0-auc:0.37648\n",
      "[67]\tvalidation_0-auc:0.37482\n",
      "[68]\tvalidation_0-auc:0.37364\n",
      "[69]\tvalidation_0-auc:0.37150\n",
      "[70]\tvalidation_0-auc:0.37553\n",
      "[71]\tvalidation_0-auc:0.37790\n",
      "[72]\tvalidation_0-auc:0.38051\n",
      "[73]\tvalidation_0-auc:0.38407\n",
      "[74]\tvalidation_0-auc:0.38620\n",
      "[75]\tvalidation_0-auc:0.38407\n",
      "[76]\tvalidation_0-auc:0.38525\n",
      "[77]\tvalidation_0-auc:0.39142\n",
      "[78]\tvalidation_0-auc:0.40043\n",
      "[79]\tvalidation_0-auc:0.39545\n",
      "Test Accuracy for Layer 1: 0.6090\n",
      "Test AUC for Layer 1: 0.4734\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.46819\n",
      "[1]\tvalidation_0-auc:0.43661\n",
      "[2]\tvalidation_0-auc:0.44860\n",
      "[3]\tvalidation_0-auc:0.44729\n",
      "[4]\tvalidation_0-auc:0.42189\n",
      "[5]\tvalidation_0-auc:0.43269\n",
      "[6]\tvalidation_0-auc:0.41227\n",
      "[7]\tvalidation_0-auc:0.40349\n",
      "[8]\tvalidation_0-auc:0.37987\n",
      "[9]\tvalidation_0-auc:0.37037\n",
      "[10]\tvalidation_0-auc:0.39589\n",
      "[11]\tvalidation_0-auc:0.39672\n",
      "[12]\tvalidation_0-auc:0.39862\n",
      "[13]\tvalidation_0-auc:0.39672\n",
      "[14]\tvalidation_0-auc:0.38913\n",
      "[15]\tvalidation_0-auc:0.38699\n",
      "[16]\tvalidation_0-auc:0.36847\n",
      "[17]\tvalidation_0-auc:0.39150\n",
      "[18]\tvalidation_0-auc:0.37939\n",
      "[19]\tvalidation_0-auc:0.38794\n",
      "[20]\tvalidation_0-auc:0.41785\n",
      "[21]\tvalidation_0-auc:0.41809\n",
      "[22]\tvalidation_0-auc:0.42213\n",
      "[23]\tvalidation_0-auc:0.42925\n",
      "[24]\tvalidation_0-auc:0.44444\n",
      "[25]\tvalidation_0-auc:0.44017\n",
      "[26]\tvalidation_0-auc:0.43732\n",
      "[27]\tvalidation_0-auc:0.44587\n",
      "[28]\tvalidation_0-auc:0.43447\n",
      "[29]\tvalidation_0-auc:0.42937\n",
      "[30]\tvalidation_0-auc:0.42996\n",
      "[31]\tvalidation_0-auc:0.42854\n",
      "[32]\tvalidation_0-auc:0.42070\n",
      "[33]\tvalidation_0-auc:0.41975\n",
      "[34]\tvalidation_0-auc:0.41073\n",
      "[35]\tvalidation_0-auc:0.39886\n",
      "[36]\tvalidation_0-auc:0.41109\n",
      "[37]\tvalidation_0-auc:0.41928\n",
      "[38]\tvalidation_0-auc:0.41406\n",
      "[39]\tvalidation_0-auc:0.40978\n",
      "[40]\tvalidation_0-auc:0.41358\n",
      "[41]\tvalidation_0-auc:0.41500\n",
      "[42]\tvalidation_0-auc:0.41833\n",
      "[43]\tvalidation_0-auc:0.41690\n",
      "[44]\tvalidation_0-auc:0.41714\n",
      "[45]\tvalidation_0-auc:0.41548\n",
      "[46]\tvalidation_0-auc:0.42189\n",
      "[47]\tvalidation_0-auc:0.41833\n",
      "[48]\tvalidation_0-auc:0.42308\n",
      "[49]\tvalidation_0-auc:0.42379\n",
      "[50]\tvalidation_0-auc:0.42711\n",
      "[51]\tvalidation_0-auc:0.42783\n",
      "[52]\tvalidation_0-auc:0.43352\n",
      "[53]\tvalidation_0-auc:0.42925\n",
      "[54]\tvalidation_0-auc:0.42355\n",
      "[55]\tvalidation_0-auc:0.41714\n",
      "[56]\tvalidation_0-auc:0.41311\n",
      "[57]\tvalidation_0-auc:0.41453\n",
      "[58]\tvalidation_0-auc:0.41595\n",
      "[59]\tvalidation_0-auc:0.41928\n",
      "[60]\tvalidation_0-auc:0.42877\n",
      "[61]\tvalidation_0-auc:0.42972\n",
      "[62]\tvalidation_0-auc:0.43174\n",
      "[63]\tvalidation_0-auc:0.43685\n",
      "[64]\tvalidation_0-auc:0.43851\n",
      "[65]\tvalidation_0-auc:0.43993\n",
      "[66]\tvalidation_0-auc:0.44373\n",
      "[67]\tvalidation_0-auc:0.44801\n",
      "[68]\tvalidation_0-auc:0.45133\n",
      "[69]\tvalidation_0-auc:0.44682\n",
      "[70]\tvalidation_0-auc:0.44896\n",
      "[71]\tvalidation_0-auc:0.45323\n",
      "[72]\tvalidation_0-auc:0.45394\n",
      "[73]\tvalidation_0-auc:0.45703\n",
      "[74]\tvalidation_0-auc:0.45632\n",
      "[75]\tvalidation_0-auc:0.45418\n",
      "[76]\tvalidation_0-auc:0.44896\n",
      "[77]\tvalidation_0-auc:0.45370\n",
      "[78]\tvalidation_0-auc:0.45347\n",
      "[79]\tvalidation_0-auc:0.45774\n",
      "Test Accuracy for Layer 2: 0.5347\n",
      "Test AUC for Layer 2: 0.4413\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.48385\n",
      "[1]\tvalidation_0-auc:0.41962\n",
      "[2]\tvalidation_0-auc:0.48325\n",
      "[3]\tvalidation_0-auc:0.51024\n",
      "[4]\tvalidation_0-auc:0.51162\n",
      "[5]\tvalidation_0-auc:0.51458\n",
      "[6]\tvalidation_0-auc:0.51458\n",
      "[7]\tvalidation_0-auc:0.49133\n",
      "[8]\tvalidation_0-auc:0.49232\n",
      "[9]\tvalidation_0-auc:0.49409\n",
      "[10]\tvalidation_0-auc:0.50236\n",
      "[11]\tvalidation_0-auc:0.50709\n",
      "[12]\tvalidation_0-auc:0.49507\n",
      "[13]\tvalidation_0-auc:0.50906\n",
      "[14]\tvalidation_0-auc:0.50591\n",
      "[15]\tvalidation_0-auc:0.52522\n",
      "[16]\tvalidation_0-auc:0.52285\n",
      "[17]\tvalidation_0-auc:0.51340\n",
      "[18]\tvalidation_0-auc:0.52049\n",
      "[19]\tvalidation_0-auc:0.53901\n",
      "[20]\tvalidation_0-auc:0.52916\n",
      "[21]\tvalidation_0-auc:0.52797\n",
      "[22]\tvalidation_0-auc:0.52600\n",
      "[23]\tvalidation_0-auc:0.53743\n",
      "[24]\tvalidation_0-auc:0.52679\n",
      "[25]\tvalidation_0-auc:0.54413\n",
      "[26]\tvalidation_0-auc:0.53467\n",
      "[27]\tvalidation_0-auc:0.52876\n",
      "[28]\tvalidation_0-auc:0.53310\n",
      "[29]\tvalidation_0-auc:0.52994\n",
      "[30]\tvalidation_0-auc:0.51852\n",
      "[31]\tvalidation_0-auc:0.51852\n",
      "[32]\tvalidation_0-auc:0.50039\n",
      "[33]\tvalidation_0-auc:0.51143\n",
      "[34]\tvalidation_0-auc:0.51773\n",
      "[35]\tvalidation_0-auc:0.52522\n",
      "[36]\tvalidation_0-auc:0.51990\n",
      "[37]\tvalidation_0-auc:0.52167\n",
      "[38]\tvalidation_0-auc:0.52009\n",
      "[39]\tvalidation_0-auc:0.50552\n",
      "[40]\tvalidation_0-auc:0.49251\n",
      "[41]\tvalidation_0-auc:0.49251\n",
      "[42]\tvalidation_0-auc:0.48030\n",
      "[43]\tvalidation_0-auc:0.48306\n",
      "[44]\tvalidation_0-auc:0.48424\n",
      "[45]\tvalidation_0-auc:0.47439\n",
      "[46]\tvalidation_0-auc:0.48227\n",
      "[47]\tvalidation_0-auc:0.48148\n",
      "[48]\tvalidation_0-auc:0.48424\n",
      "[49]\tvalidation_0-auc:0.48109\n",
      "[50]\tvalidation_0-auc:0.46651\n",
      "[51]\tvalidation_0-auc:0.47321\n",
      "[52]\tvalidation_0-auc:0.46848\n",
      "[53]\tvalidation_0-auc:0.47281\n",
      "[54]\tvalidation_0-auc:0.48188\n",
      "[55]\tvalidation_0-auc:0.48030\n",
      "[56]\tvalidation_0-auc:0.48227\n",
      "[57]\tvalidation_0-auc:0.48936\n",
      "[58]\tvalidation_0-auc:0.49527\n",
      "[59]\tvalidation_0-auc:0.49212\n",
      "[60]\tvalidation_0-auc:0.49567\n",
      "[61]\tvalidation_0-auc:0.48779\n",
      "[62]\tvalidation_0-auc:0.47951\n",
      "[63]\tvalidation_0-auc:0.48069\n",
      "[64]\tvalidation_0-auc:0.48424\n",
      "[65]\tvalidation_0-auc:0.48148\n",
      "[66]\tvalidation_0-auc:0.49094\n",
      "[67]\tvalidation_0-auc:0.49980\n",
      "[68]\tvalidation_0-auc:0.49488\n",
      "[69]\tvalidation_0-auc:0.48779\n",
      "[70]\tvalidation_0-auc:0.49764\n",
      "[71]\tvalidation_0-auc:0.49212\n",
      "[72]\tvalidation_0-auc:0.48306\n",
      "[73]\tvalidation_0-auc:0.47794\n",
      "[74]\tvalidation_0-auc:0.48148\n",
      "[75]\tvalidation_0-auc:0.47597\n",
      "[76]\tvalidation_0-auc:0.47518\n",
      "[77]\tvalidation_0-auc:0.47321\n",
      "[78]\tvalidation_0-auc:0.47045\n",
      "[79]\tvalidation_0-auc:0.47400\n",
      "Test Accuracy for Layer 3: 0.5143\n",
      "Test AUC for Layer 3: 0.3154\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5527\n",
      "Average Test AUC across all layers: 0.4100\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Title and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.48676\n",
      "[1]\tvalidation_0-auc:0.55297\n",
      "[2]\tvalidation_0-auc:0.51766\n",
      "[3]\tvalidation_0-auc:0.54829\n",
      "[4]\tvalidation_0-auc:0.56367\n",
      "[5]\tvalidation_0-auc:0.52220\n",
      "[6]\tvalidation_0-auc:0.52555\n",
      "[7]\tvalidation_0-auc:0.51418\n",
      "[8]\tvalidation_0-auc:0.51980\n",
      "[9]\tvalidation_0-auc:0.52622\n",
      "[10]\tvalidation_0-auc:0.52167\n",
      "[11]\tvalidation_0-auc:0.51578\n",
      "[12]\tvalidation_0-auc:0.50963\n",
      "[13]\tvalidation_0-auc:0.50589\n",
      "[14]\tvalidation_0-auc:0.51578\n",
      "[15]\tvalidation_0-auc:0.51926\n",
      "[16]\tvalidation_0-auc:0.51659\n",
      "[17]\tvalidation_0-auc:0.52087\n",
      "[18]\tvalidation_0-auc:0.51659\n",
      "[19]\tvalidation_0-auc:0.52555\n",
      "[20]\tvalidation_0-auc:0.51244\n",
      "[21]\tvalidation_0-auc:0.51659\n",
      "[22]\tvalidation_0-auc:0.50482\n",
      "[23]\tvalidation_0-auc:0.49278\n",
      "[24]\tvalidation_0-auc:0.47967\n",
      "[25]\tvalidation_0-auc:0.48074\n",
      "[26]\tvalidation_0-auc:0.47860\n",
      "[27]\tvalidation_0-auc:0.47940\n",
      "[28]\tvalidation_0-auc:0.48395\n",
      "[29]\tvalidation_0-auc:0.48074\n",
      "[30]\tvalidation_0-auc:0.48261\n",
      "[31]\tvalidation_0-auc:0.48234\n",
      "[32]\tvalidation_0-auc:0.48127\n",
      "[33]\tvalidation_0-auc:0.47940\n",
      "[34]\tvalidation_0-auc:0.49117\n",
      "[35]\tvalidation_0-auc:0.48448\n",
      "[36]\tvalidation_0-auc:0.48529\n",
      "[37]\tvalidation_0-auc:0.48636\n",
      "[38]\tvalidation_0-auc:0.49625\n",
      "[39]\tvalidation_0-auc:0.50562\n",
      "[40]\tvalidation_0-auc:0.50241\n",
      "[41]\tvalidation_0-auc:0.50669\n",
      "[42]\tvalidation_0-auc:0.50562\n",
      "[43]\tvalidation_0-auc:0.50936\n",
      "[44]\tvalidation_0-auc:0.51017\n",
      "[45]\tvalidation_0-auc:0.50910\n",
      "[46]\tvalidation_0-auc:0.50776\n",
      "[47]\tvalidation_0-auc:0.50829\n",
      "[48]\tvalidation_0-auc:0.50589\n",
      "[49]\tvalidation_0-auc:0.51097\n",
      "[50]\tvalidation_0-auc:0.51659\n",
      "[51]\tvalidation_0-auc:0.51257\n",
      "[52]\tvalidation_0-auc:0.51873\n",
      "[53]\tvalidation_0-auc:0.51498\n",
      "[54]\tvalidation_0-auc:0.51418\n",
      "[55]\tvalidation_0-auc:0.51578\n",
      "[56]\tvalidation_0-auc:0.51846\n",
      "[57]\tvalidation_0-auc:0.51819\n",
      "[58]\tvalidation_0-auc:0.51605\n",
      "[59]\tvalidation_0-auc:0.52247\n",
      "[60]\tvalidation_0-auc:0.52461\n",
      "[61]\tvalidation_0-auc:0.52220\n",
      "[62]\tvalidation_0-auc:0.51899\n",
      "[63]\tvalidation_0-auc:0.51338\n",
      "[64]\tvalidation_0-auc:0.50749\n",
      "[65]\tvalidation_0-auc:0.50803\n",
      "[66]\tvalidation_0-auc:0.51097\n",
      "[67]\tvalidation_0-auc:0.51364\n",
      "[68]\tvalidation_0-auc:0.51525\n",
      "[69]\tvalidation_0-auc:0.51498\n",
      "[70]\tvalidation_0-auc:0.51231\n",
      "[71]\tvalidation_0-auc:0.51525\n",
      "[72]\tvalidation_0-auc:0.51525\n",
      "[73]\tvalidation_0-auc:0.51231\n",
      "[74]\tvalidation_0-auc:0.51445\n",
      "[75]\tvalidation_0-auc:0.51498\n",
      "[76]\tvalidation_0-auc:0.51364\n",
      "[77]\tvalidation_0-auc:0.51150\n",
      "[78]\tvalidation_0-auc:0.51284\n",
      "[79]\tvalidation_0-auc:0.51257\n",
      "Test Accuracy for Layer 1: 0.2632\n",
      "Test AUC for Layer 1: 0.4111\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.47668\n",
      "[1]\tvalidation_0-auc:0.47624\n",
      "[2]\tvalidation_0-auc:0.50918\n",
      "[3]\tvalidation_0-auc:0.49708\n",
      "[4]\tvalidation_0-auc:0.50729\n",
      "[5]\tvalidation_0-auc:0.48411\n",
      "[6]\tvalidation_0-auc:0.49534\n",
      "[7]\tvalidation_0-auc:0.46472\n",
      "[8]\tvalidation_0-auc:0.50350\n",
      "[9]\tvalidation_0-auc:0.48717\n",
      "[10]\tvalidation_0-auc:0.46181\n",
      "[11]\tvalidation_0-auc:0.45889\n",
      "[12]\tvalidation_0-auc:0.44373\n",
      "[13]\tvalidation_0-auc:0.44052\n",
      "[14]\tvalidation_0-auc:0.46239\n",
      "[15]\tvalidation_0-auc:0.46356\n",
      "[16]\tvalidation_0-auc:0.42653\n",
      "[17]\tvalidation_0-auc:0.42682\n",
      "[18]\tvalidation_0-auc:0.43382\n",
      "[19]\tvalidation_0-auc:0.42449\n",
      "[20]\tvalidation_0-auc:0.43528\n",
      "[21]\tvalidation_0-auc:0.44898\n",
      "[22]\tvalidation_0-auc:0.44490\n",
      "[23]\tvalidation_0-auc:0.44257\n",
      "[24]\tvalidation_0-auc:0.44315\n",
      "[25]\tvalidation_0-auc:0.43848\n",
      "[26]\tvalidation_0-auc:0.43499\n",
      "[27]\tvalidation_0-auc:0.43659\n",
      "[28]\tvalidation_0-auc:0.43805\n",
      "[29]\tvalidation_0-auc:0.44592\n",
      "[30]\tvalidation_0-auc:0.44490\n",
      "[31]\tvalidation_0-auc:0.43994\n",
      "[32]\tvalidation_0-auc:0.44927\n",
      "[33]\tvalidation_0-auc:0.44402\n",
      "[34]\tvalidation_0-auc:0.43848\n",
      "[35]\tvalidation_0-auc:0.43878\n",
      "[36]\tvalidation_0-auc:0.43265\n",
      "[37]\tvalidation_0-auc:0.43090\n",
      "[38]\tvalidation_0-auc:0.43411\n",
      "[39]\tvalidation_0-auc:0.41633\n",
      "[40]\tvalidation_0-auc:0.41895\n",
      "[41]\tvalidation_0-auc:0.41545\n",
      "[42]\tvalidation_0-auc:0.40262\n",
      "[43]\tvalidation_0-auc:0.40612\n",
      "[44]\tvalidation_0-auc:0.40204\n",
      "[45]\tvalidation_0-auc:0.40525\n",
      "[46]\tvalidation_0-auc:0.40612\n",
      "[47]\tvalidation_0-auc:0.40816\n",
      "[48]\tvalidation_0-auc:0.40466\n",
      "[49]\tvalidation_0-auc:0.40466\n",
      "[50]\tvalidation_0-auc:0.40437\n",
      "[51]\tvalidation_0-auc:0.40641\n",
      "[52]\tvalidation_0-auc:0.41312\n",
      "[53]\tvalidation_0-auc:0.40583\n",
      "[54]\tvalidation_0-auc:0.40991\n",
      "[55]\tvalidation_0-auc:0.40787\n",
      "[56]\tvalidation_0-auc:0.40962\n",
      "[57]\tvalidation_0-auc:0.40612\n",
      "[58]\tvalidation_0-auc:0.40845\n",
      "[59]\tvalidation_0-auc:0.40787\n",
      "[60]\tvalidation_0-auc:0.40904\n",
      "[61]\tvalidation_0-auc:0.40700\n",
      "[62]\tvalidation_0-auc:0.40525\n",
      "[63]\tvalidation_0-auc:0.41020\n",
      "[64]\tvalidation_0-auc:0.40496\n",
      "[65]\tvalidation_0-auc:0.40583\n",
      "[66]\tvalidation_0-auc:0.41050\n",
      "[67]\tvalidation_0-auc:0.40962\n",
      "[68]\tvalidation_0-auc:0.40437\n",
      "[69]\tvalidation_0-auc:0.41778\n",
      "[70]\tvalidation_0-auc:0.41691\n",
      "[71]\tvalidation_0-auc:0.41487\n",
      "[72]\tvalidation_0-auc:0.41224\n",
      "[73]\tvalidation_0-auc:0.40729\n",
      "[74]\tvalidation_0-auc:0.40787\n",
      "[75]\tvalidation_0-auc:0.41487\n",
      "[76]\tvalidation_0-auc:0.41691\n",
      "[77]\tvalidation_0-auc:0.41720\n",
      "[78]\tvalidation_0-auc:0.42128\n",
      "[79]\tvalidation_0-auc:0.41778\n",
      "Test Accuracy for Layer 2: 0.4158\n",
      "Test AUC for Layer 2: 0.3656\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.53390\n",
      "[1]\tvalidation_0-auc:0.52885\n",
      "[2]\tvalidation_0-auc:0.51069\n",
      "[3]\tvalidation_0-auc:0.51029\n",
      "[4]\tvalidation_0-auc:0.49052\n",
      "[5]\tvalidation_0-auc:0.46065\n",
      "[6]\tvalidation_0-auc:0.42958\n",
      "[7]\tvalidation_0-auc:0.43947\n",
      "[8]\tvalidation_0-auc:0.43705\n",
      "[9]\tvalidation_0-auc:0.47579\n",
      "[10]\tvalidation_0-auc:0.48668\n",
      "[11]\tvalidation_0-auc:0.48910\n",
      "[12]\tvalidation_0-auc:0.49233\n",
      "[13]\tvalidation_0-auc:0.51211\n",
      "[14]\tvalidation_0-auc:0.48507\n",
      "[15]\tvalidation_0-auc:0.47458\n",
      "[16]\tvalidation_0-auc:0.46610\n",
      "[17]\tvalidation_0-auc:0.47417\n",
      "[18]\tvalidation_0-auc:0.48265\n",
      "[19]\tvalidation_0-auc:0.48103\n",
      "[20]\tvalidation_0-auc:0.49475\n",
      "[21]\tvalidation_0-auc:0.48951\n",
      "[22]\tvalidation_0-auc:0.50404\n",
      "[23]\tvalidation_0-auc:0.51251\n",
      "[24]\tvalidation_0-auc:0.50726\n",
      "[25]\tvalidation_0-auc:0.49919\n",
      "[26]\tvalidation_0-auc:0.48426\n",
      "[27]\tvalidation_0-auc:0.47337\n",
      "[28]\tvalidation_0-auc:0.47780\n",
      "[29]\tvalidation_0-auc:0.47902\n",
      "[30]\tvalidation_0-auc:0.47377\n",
      "[31]\tvalidation_0-auc:0.46852\n",
      "[32]\tvalidation_0-auc:0.47902\n",
      "[33]\tvalidation_0-auc:0.47538\n",
      "[34]\tvalidation_0-auc:0.47337\n",
      "[35]\tvalidation_0-auc:0.48345\n",
      "[36]\tvalidation_0-auc:0.47296\n",
      "[37]\tvalidation_0-auc:0.47579\n",
      "[38]\tvalidation_0-auc:0.47700\n",
      "[39]\tvalidation_0-auc:0.48709\n",
      "[40]\tvalidation_0-auc:0.48023\n",
      "[41]\tvalidation_0-auc:0.47579\n",
      "[42]\tvalidation_0-auc:0.48507\n",
      "[43]\tvalidation_0-auc:0.48345\n",
      "[44]\tvalidation_0-auc:0.48345\n",
      "[45]\tvalidation_0-auc:0.48103\n",
      "[46]\tvalidation_0-auc:0.48789\n",
      "[47]\tvalidation_0-auc:0.49072\n",
      "[48]\tvalidation_0-auc:0.48870\n",
      "[49]\tvalidation_0-auc:0.48144\n",
      "[50]\tvalidation_0-auc:0.46852\n",
      "[51]\tvalidation_0-auc:0.47175\n",
      "[52]\tvalidation_0-auc:0.46529\n",
      "[53]\tvalidation_0-auc:0.46651\n",
      "[54]\tvalidation_0-auc:0.46489\n",
      "[55]\tvalidation_0-auc:0.46207\n",
      "[56]\tvalidation_0-auc:0.45843\n",
      "[57]\tvalidation_0-auc:0.45238\n",
      "[58]\tvalidation_0-auc:0.44633\n",
      "[59]\tvalidation_0-auc:0.45117\n",
      "[60]\tvalidation_0-auc:0.44875\n",
      "[61]\tvalidation_0-auc:0.45843\n",
      "[62]\tvalidation_0-auc:0.46166\n",
      "[63]\tvalidation_0-auc:0.45359\n",
      "[64]\tvalidation_0-auc:0.46207\n",
      "[65]\tvalidation_0-auc:0.46731\n",
      "[66]\tvalidation_0-auc:0.46287\n",
      "[67]\tvalidation_0-auc:0.45964\n",
      "[68]\tvalidation_0-auc:0.46166\n",
      "[69]\tvalidation_0-auc:0.46287\n",
      "[70]\tvalidation_0-auc:0.46126\n",
      "[71]\tvalidation_0-auc:0.45642\n",
      "[72]\tvalidation_0-auc:0.45964\n",
      "[73]\tvalidation_0-auc:0.46287\n",
      "[74]\tvalidation_0-auc:0.46005\n",
      "[75]\tvalidation_0-auc:0.45924\n",
      "[76]\tvalidation_0-auc:0.45803\n",
      "[77]\tvalidation_0-auc:0.46328\n",
      "[78]\tvalidation_0-auc:0.46812\n",
      "[79]\tvalidation_0-auc:0.46529\n",
      "Test Accuracy for Layer 3: 0.5571\n",
      "Test AUC for Layer 3: 0.4806\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4120\n",
      "Average Test AUC across all layers: 0.4191\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Full text and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.54682\n",
      "[1]\tvalidation_0-auc:0.56235\n",
      "[2]\tvalidation_0-auc:0.53046\n",
      "[3]\tvalidation_0-auc:0.52738\n",
      "[4]\tvalidation_0-auc:0.51517\n",
      "[5]\tvalidation_0-auc:0.50403\n",
      "[6]\tvalidation_0-auc:0.52774\n",
      "[7]\tvalidation_0-auc:0.53687\n",
      "[8]\tvalidation_0-auc:0.51826\n",
      "[9]\tvalidation_0-auc:0.49004\n",
      "[10]\tvalidation_0-auc:0.47297\n",
      "[11]\tvalidation_0-auc:0.44666\n",
      "[12]\tvalidation_0-auc:0.44488\n",
      "[13]\tvalidation_0-auc:0.43433\n",
      "[14]\tvalidation_0-auc:0.42793\n",
      "[15]\tvalidation_0-auc:0.42532\n",
      "[16]\tvalidation_0-auc:0.42615\n",
      "[17]\tvalidation_0-auc:0.44061\n",
      "[18]\tvalidation_0-auc:0.44950\n",
      "[19]\tvalidation_0-auc:0.45661\n",
      "[20]\tvalidation_0-auc:0.44879\n",
      "[21]\tvalidation_0-auc:0.43220\n",
      "[22]\tvalidation_0-auc:0.42532\n",
      "[23]\tvalidation_0-auc:0.43065\n",
      "[24]\tvalidation_0-auc:0.42556\n",
      "[25]\tvalidation_0-auc:0.43220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26]\tvalidation_0-auc:0.42769\n",
      "[27]\tvalidation_0-auc:0.42390\n",
      "[28]\tvalidation_0-auc:0.42366\n",
      "[29]\tvalidation_0-auc:0.42627\n",
      "[30]\tvalidation_0-auc:0.43030\n",
      "[31]\tvalidation_0-auc:0.43575\n",
      "[32]\tvalidation_0-auc:0.44168\n",
      "[33]\tvalidation_0-auc:0.42722\n",
      "[34]\tvalidation_0-auc:0.42769\n",
      "[35]\tvalidation_0-auc:0.42437\n",
      "[36]\tvalidation_0-auc:0.42485\n",
      "[37]\tvalidation_0-auc:0.42461\n",
      "[38]\tvalidation_0-auc:0.43480\n",
      "[39]\tvalidation_0-auc:0.43125\n",
      "[40]\tvalidation_0-auc:0.43101\n",
      "[41]\tvalidation_0-auc:0.43528\n",
      "[42]\tvalidation_0-auc:0.42982\n",
      "[43]\tvalidation_0-auc:0.43860\n",
      "[44]\tvalidation_0-auc:0.44476\n",
      "[45]\tvalidation_0-auc:0.44618\n",
      "[46]\tvalidation_0-auc:0.45211\n",
      "[47]\tvalidation_0-auc:0.45116\n",
      "[48]\tvalidation_0-auc:0.44855\n",
      "[49]\tvalidation_0-auc:0.45330\n",
      "[50]\tvalidation_0-auc:0.45519\n",
      "[51]\tvalidation_0-auc:0.45092\n",
      "[52]\tvalidation_0-auc:0.45567\n",
      "[53]\tvalidation_0-auc:0.45282\n",
      "[54]\tvalidation_0-auc:0.44239\n",
      "[55]\tvalidation_0-auc:0.44429\n",
      "[56]\tvalidation_0-auc:0.43646\n",
      "[57]\tvalidation_0-auc:0.44026\n",
      "[58]\tvalidation_0-auc:0.44286\n",
      "[59]\tvalidation_0-auc:0.44571\n",
      "[60]\tvalidation_0-auc:0.44429\n",
      "[61]\tvalidation_0-auc:0.44452\n",
      "[62]\tvalidation_0-auc:0.44002\n",
      "[63]\tvalidation_0-auc:0.44405\n",
      "[64]\tvalidation_0-auc:0.44073\n",
      "[65]\tvalidation_0-auc:0.44286\n",
      "[66]\tvalidation_0-auc:0.43362\n",
      "[67]\tvalidation_0-auc:0.43528\n",
      "[68]\tvalidation_0-auc:0.43812\n",
      "[69]\tvalidation_0-auc:0.43907\n",
      "[70]\tvalidation_0-auc:0.44334\n",
      "[71]\tvalidation_0-auc:0.44334\n",
      "[72]\tvalidation_0-auc:0.44761\n",
      "[73]\tvalidation_0-auc:0.43978\n",
      "[74]\tvalidation_0-auc:0.43895\n",
      "[75]\tvalidation_0-auc:0.43125\n",
      "[76]\tvalidation_0-auc:0.43338\n",
      "[77]\tvalidation_0-auc:0.43504\n",
      "[78]\tvalidation_0-auc:0.43148\n",
      "[79]\tvalidation_0-auc:0.43457\n",
      "Test Accuracy for Layer 1: 0.6090\n",
      "Test AUC for Layer 1: 0.4570\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.44872\n",
      "[1]\tvalidation_0-auc:0.47436\n",
      "[2]\tvalidation_0-auc:0.51638\n",
      "[3]\tvalidation_0-auc:0.51555\n",
      "[4]\tvalidation_0-auc:0.51709\n",
      "[5]\tvalidation_0-auc:0.52445\n",
      "[6]\tvalidation_0-auc:0.49264\n",
      "[7]\tvalidation_0-auc:0.48433\n",
      "[8]\tvalidation_0-auc:0.48825\n",
      "[9]\tvalidation_0-auc:0.46652\n",
      "[10]\tvalidation_0-auc:0.45750\n",
      "[11]\tvalidation_0-auc:0.47104\n",
      "[12]\tvalidation_0-auc:0.46296\n",
      "[13]\tvalidation_0-auc:0.45798\n",
      "[14]\tvalidation_0-auc:0.45940\n",
      "[15]\tvalidation_0-auc:0.42664\n",
      "[16]\tvalidation_0-auc:0.41690\n",
      "[17]\tvalidation_0-auc:0.42593\n",
      "[18]\tvalidation_0-auc:0.44765\n",
      "[19]\tvalidation_0-auc:0.44231\n",
      "[20]\tvalidation_0-auc:0.45394\n",
      "[21]\tvalidation_0-auc:0.46344\n",
      "[22]\tvalidation_0-auc:0.46368\n",
      "[23]\tvalidation_0-auc:0.46652\n",
      "[24]\tvalidation_0-auc:0.48267\n",
      "[25]\tvalidation_0-auc:0.47032\n",
      "[26]\tvalidation_0-auc:0.46985\n",
      "[27]\tvalidation_0-auc:0.47032\n",
      "[28]\tvalidation_0-auc:0.46581\n",
      "[29]\tvalidation_0-auc:0.46629\n",
      "[30]\tvalidation_0-auc:0.46201\n",
      "[31]\tvalidation_0-auc:0.46985\n",
      "[32]\tvalidation_0-auc:0.47365\n",
      "[33]\tvalidation_0-auc:0.47317\n",
      "[34]\tvalidation_0-auc:0.48362\n",
      "[35]\tvalidation_0-auc:0.47934\n",
      "[36]\tvalidation_0-auc:0.48670\n",
      "[37]\tvalidation_0-auc:0.47626\n",
      "[38]\tvalidation_0-auc:0.48338\n",
      "[39]\tvalidation_0-auc:0.48077\n",
      "[40]\tvalidation_0-auc:0.48362\n",
      "[41]\tvalidation_0-auc:0.49478\n",
      "[42]\tvalidation_0-auc:0.49027\n",
      "[43]\tvalidation_0-auc:0.49003\n",
      "[44]\tvalidation_0-auc:0.49193\n",
      "[45]\tvalidation_0-auc:0.48029\n",
      "[46]\tvalidation_0-auc:0.47365\n",
      "[47]\tvalidation_0-auc:0.47246\n",
      "[48]\tvalidation_0-auc:0.47293\n",
      "[49]\tvalidation_0-auc:0.47483\n",
      "[50]\tvalidation_0-auc:0.46985\n",
      "[51]\tvalidation_0-auc:0.47578\n",
      "[52]\tvalidation_0-auc:0.47816\n",
      "[53]\tvalidation_0-auc:0.47578\n",
      "[54]\tvalidation_0-auc:0.47246\n",
      "[55]\tvalidation_0-auc:0.47293\n",
      "[56]\tvalidation_0-auc:0.47151\n",
      "[57]\tvalidation_0-auc:0.46819\n",
      "[58]\tvalidation_0-auc:0.46842\n",
      "[59]\tvalidation_0-auc:0.47745\n",
      "[60]\tvalidation_0-auc:0.48053\n",
      "[61]\tvalidation_0-auc:0.48979\n",
      "[62]\tvalidation_0-auc:0.49050\n",
      "[63]\tvalidation_0-auc:0.48101\n",
      "[64]\tvalidation_0-auc:0.48694\n",
      "[65]\tvalidation_0-auc:0.47650\n",
      "[66]\tvalidation_0-auc:0.48314\n",
      "[67]\tvalidation_0-auc:0.48409\n",
      "[68]\tvalidation_0-auc:0.47507\n",
      "[69]\tvalidation_0-auc:0.47151\n",
      "[70]\tvalidation_0-auc:0.47460\n",
      "[71]\tvalidation_0-auc:0.47317\n",
      "[72]\tvalidation_0-auc:0.46178\n",
      "[73]\tvalidation_0-auc:0.46486\n",
      "[74]\tvalidation_0-auc:0.46724\n",
      "[75]\tvalidation_0-auc:0.46937\n",
      "[76]\tvalidation_0-auc:0.46985\n",
      "[77]\tvalidation_0-auc:0.47056\n",
      "[78]\tvalidation_0-auc:0.47080\n",
      "[79]\tvalidation_0-auc:0.47080\n",
      "Test Accuracy for Layer 2: 0.5347\n",
      "Test AUC for Layer 2: 0.4283\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.51990\n",
      "[1]\tvalidation_0-auc:0.50197\n",
      "[2]\tvalidation_0-auc:0.51221\n",
      "[3]\tvalidation_0-auc:0.50236\n",
      "[4]\tvalidation_0-auc:0.49606\n",
      "[5]\tvalidation_0-auc:0.46769\n",
      "[6]\tvalidation_0-auc:0.44523\n",
      "[7]\tvalidation_0-auc:0.44011\n",
      "[8]\tvalidation_0-auc:0.39992\n",
      "[9]\tvalidation_0-auc:0.40721\n",
      "[10]\tvalidation_0-auc:0.39638\n",
      "[11]\tvalidation_0-auc:0.40898\n",
      "[12]\tvalidation_0-auc:0.39835\n",
      "[13]\tvalidation_0-auc:0.38731\n",
      "[14]\tvalidation_0-auc:0.39204\n",
      "[15]\tvalidation_0-auc:0.36485\n",
      "[16]\tvalidation_0-auc:0.36682\n",
      "[17]\tvalidation_0-auc:0.37746\n",
      "[18]\tvalidation_0-auc:0.38101\n",
      "[19]\tvalidation_0-auc:0.36091\n",
      "[20]\tvalidation_0-auc:0.36052\n",
      "[21]\tvalidation_0-auc:0.34161\n",
      "[22]\tvalidation_0-auc:0.33274\n",
      "[23]\tvalidation_0-auc:0.32861\n",
      "[24]\tvalidation_0-auc:0.33058\n",
      "[25]\tvalidation_0-auc:0.34909\n",
      "[26]\tvalidation_0-auc:0.34437\n",
      "[27]\tvalidation_0-auc:0.34161\n",
      "[28]\tvalidation_0-auc:0.34121\n",
      "[29]\tvalidation_0-auc:0.35106\n",
      "[30]\tvalidation_0-auc:0.35776\n",
      "[31]\tvalidation_0-auc:0.35973\n",
      "[32]\tvalidation_0-auc:0.34673\n",
      "[33]\tvalidation_0-auc:0.34082\n",
      "[34]\tvalidation_0-auc:0.35697\n",
      "[35]\tvalidation_0-auc:0.34397\n",
      "[36]\tvalidation_0-auc:0.34712\n",
      "[37]\tvalidation_0-auc:0.33530\n",
      "[38]\tvalidation_0-auc:0.34240\n",
      "[39]\tvalidation_0-auc:0.34712\n",
      "[40]\tvalidation_0-auc:0.34240\n",
      "[41]\tvalidation_0-auc:0.35067\n",
      "[42]\tvalidation_0-auc:0.33964\n",
      "[43]\tvalidation_0-auc:0.34240\n",
      "[44]\tvalidation_0-auc:0.33570\n",
      "[45]\tvalidation_0-auc:0.34121\n",
      "[46]\tvalidation_0-auc:0.34634\n",
      "[47]\tvalidation_0-auc:0.35816\n",
      "[48]\tvalidation_0-auc:0.34831\n",
      "[49]\tvalidation_0-auc:0.35500\n",
      "[50]\tvalidation_0-auc:0.35461\n",
      "[51]\tvalidation_0-auc:0.35894\n",
      "[52]\tvalidation_0-auc:0.35579\n",
      "[53]\tvalidation_0-auc:0.35737\n",
      "[54]\tvalidation_0-auc:0.36170\n",
      "[55]\tvalidation_0-auc:0.36210\n",
      "[56]\tvalidation_0-auc:0.36801\n",
      "[57]\tvalidation_0-auc:0.37392\n",
      "[58]\tvalidation_0-auc:0.37667\n",
      "[59]\tvalidation_0-auc:0.37943\n",
      "[60]\tvalidation_0-auc:0.37549\n",
      "[61]\tvalidation_0-auc:0.39362\n",
      "[62]\tvalidation_0-auc:0.39716\n",
      "[63]\tvalidation_0-auc:0.39756\n",
      "[64]\tvalidation_0-auc:0.39046\n",
      "[65]\tvalidation_0-auc:0.38337\n",
      "[66]\tvalidation_0-auc:0.38219\n",
      "[67]\tvalidation_0-auc:0.38022\n",
      "[68]\tvalidation_0-auc:0.38534\n",
      "[69]\tvalidation_0-auc:0.37352\n",
      "[70]\tvalidation_0-auc:0.38652\n",
      "[71]\tvalidation_0-auc:0.38455\n",
      "[72]\tvalidation_0-auc:0.37510\n",
      "[73]\tvalidation_0-auc:0.37195\n",
      "[74]\tvalidation_0-auc:0.37392\n",
      "[75]\tvalidation_0-auc:0.37431\n",
      "[76]\tvalidation_0-auc:0.38534\n",
      "[77]\tvalidation_0-auc:0.38692\n",
      "[78]\tvalidation_0-auc:0.38613\n",
      "[79]\tvalidation_0-auc:0.37943\n",
      "Test Accuracy for Layer 3: 0.5143\n",
      "Test AUC for Layer 3: 0.4812\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5527\n",
      "Average Test AUC across all layers: 0.4555\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Full text and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.52943\n",
      "[1]\tvalidation_0-auc:0.52033\n",
      "[2]\tvalidation_0-auc:0.40516\n",
      "[3]\tvalidation_0-auc:0.35072\n",
      "[4]\tvalidation_0-auc:0.39192\n",
      "[5]\tvalidation_0-auc:0.39861\n",
      "[6]\tvalidation_0-auc:0.41800\n",
      "[7]\tvalidation_0-auc:0.43365\n",
      "[8]\tvalidation_0-auc:0.46362\n",
      "[9]\tvalidation_0-auc:0.44462\n",
      "[10]\tvalidation_0-auc:0.44275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11]\tvalidation_0-auc:0.44462\n",
      "[12]\tvalidation_0-auc:0.45185\n",
      "[13]\tvalidation_0-auc:0.45613\n",
      "[14]\tvalidation_0-auc:0.45452\n",
      "[15]\tvalidation_0-auc:0.46977\n",
      "[16]\tvalidation_0-auc:0.49465\n",
      "[17]\tvalidation_0-auc:0.49973\n",
      "[18]\tvalidation_0-auc:0.50963\n",
      "[19]\tvalidation_0-auc:0.49572\n",
      "[20]\tvalidation_0-auc:0.49759\n",
      "[21]\tvalidation_0-auc:0.49572\n",
      "[22]\tvalidation_0-auc:0.49117\n",
      "[23]\tvalidation_0-auc:0.50027\n",
      "[24]\tvalidation_0-auc:0.50147\n",
      "[25]\tvalidation_0-auc:0.50107\n",
      "[26]\tvalidation_0-auc:0.50696\n",
      "[27]\tvalidation_0-auc:0.51632\n",
      "[28]\tvalidation_0-auc:0.51445\n",
      "[29]\tvalidation_0-auc:0.51364\n",
      "[30]\tvalidation_0-auc:0.52622\n",
      "[31]\tvalidation_0-auc:0.52996\n",
      "[32]\tvalidation_0-auc:0.53237\n",
      "[33]\tvalidation_0-auc:0.52782\n",
      "[34]\tvalidation_0-auc:0.51097\n",
      "[35]\tvalidation_0-auc:0.50776\n",
      "[36]\tvalidation_0-auc:0.50562\n",
      "[37]\tvalidation_0-auc:0.49906\n",
      "[38]\tvalidation_0-auc:0.49050\n",
      "[39]\tvalidation_0-auc:0.49973\n",
      "[40]\tvalidation_0-auc:0.49973\n",
      "[41]\tvalidation_0-auc:0.50669\n",
      "[42]\tvalidation_0-auc:0.50428\n",
      "[43]\tvalidation_0-auc:0.50803\n",
      "[44]\tvalidation_0-auc:0.50375\n",
      "[45]\tvalidation_0-auc:0.49973\n",
      "[46]\tvalidation_0-auc:0.49465\n",
      "[47]\tvalidation_0-auc:0.49144\n",
      "[48]\tvalidation_0-auc:0.48689\n",
      "[49]\tvalidation_0-auc:0.49064\n",
      "[50]\tvalidation_0-auc:0.50080\n",
      "[51]\tvalidation_0-auc:0.49946\n",
      "[52]\tvalidation_0-auc:0.49518\n",
      "[53]\tvalidation_0-auc:0.49706\n",
      "[54]\tvalidation_0-auc:0.49946\n",
      "[55]\tvalidation_0-auc:0.49652\n",
      "[56]\tvalidation_0-auc:0.49652\n",
      "[57]\tvalidation_0-auc:0.49920\n",
      "[58]\tvalidation_0-auc:0.50615\n",
      "[59]\tvalidation_0-auc:0.50334\n",
      "[60]\tvalidation_0-auc:0.50722\n",
      "[61]\tvalidation_0-auc:0.49652\n",
      "[62]\tvalidation_0-auc:0.49358\n",
      "[63]\tvalidation_0-auc:0.49064\n",
      "[64]\tvalidation_0-auc:0.49117\n",
      "[65]\tvalidation_0-auc:0.49385\n",
      "[66]\tvalidation_0-auc:0.49197\n",
      "[67]\tvalidation_0-auc:0.49518\n",
      "[68]\tvalidation_0-auc:0.48475\n",
      "[69]\tvalidation_0-auc:0.47512\n",
      "[70]\tvalidation_0-auc:0.47405\n",
      "[71]\tvalidation_0-auc:0.47994\n",
      "[72]\tvalidation_0-auc:0.48609\n",
      "[73]\tvalidation_0-auc:0.48422\n",
      "[74]\tvalidation_0-auc:0.48475\n",
      "[75]\tvalidation_0-auc:0.48609\n",
      "[76]\tvalidation_0-auc:0.48903\n",
      "[77]\tvalidation_0-auc:0.48636\n",
      "[78]\tvalidation_0-auc:0.48850\n",
      "[79]\tvalidation_0-auc:0.48609\n",
      "Test Accuracy for Layer 1: 0.2632\n",
      "Test AUC for Layer 1: 0.4475\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.46837\n",
      "[1]\tvalidation_0-auc:0.50466\n",
      "[2]\tvalidation_0-auc:0.51589\n",
      "[3]\tvalidation_0-auc:0.49694\n",
      "[4]\tvalidation_0-auc:0.49388\n",
      "[5]\tvalidation_0-auc:0.50306\n",
      "[6]\tvalidation_0-auc:0.51910\n",
      "[7]\tvalidation_0-auc:0.50889\n",
      "[8]\tvalidation_0-auc:0.51443\n",
      "[9]\tvalidation_0-auc:0.51283\n",
      "[10]\tvalidation_0-auc:0.53528\n",
      "[11]\tvalidation_0-auc:0.48921\n",
      "[12]\tvalidation_0-auc:0.50773\n",
      "[13]\tvalidation_0-auc:0.50408\n",
      "[14]\tvalidation_0-auc:0.52507\n",
      "[15]\tvalidation_0-auc:0.53353\n",
      "[16]\tvalidation_0-auc:0.54577\n",
      "[17]\tvalidation_0-auc:0.53907\n",
      "[18]\tvalidation_0-auc:0.52536\n",
      "[19]\tvalidation_0-auc:0.51603\n",
      "[20]\tvalidation_0-auc:0.50496\n",
      "[21]\tvalidation_0-auc:0.51079\n",
      "[22]\tvalidation_0-auc:0.51662\n",
      "[23]\tvalidation_0-auc:0.50510\n",
      "[24]\tvalidation_0-auc:0.50408\n",
      "[25]\tvalidation_0-auc:0.48950\n",
      "[26]\tvalidation_0-auc:0.47988\n",
      "[27]\tvalidation_0-auc:0.48047\n",
      "[28]\tvalidation_0-auc:0.46764\n",
      "[29]\tvalidation_0-auc:0.47959\n",
      "[30]\tvalidation_0-auc:0.47901\n",
      "[31]\tvalidation_0-auc:0.47085\n",
      "[32]\tvalidation_0-auc:0.47493\n",
      "[33]\tvalidation_0-auc:0.48309\n",
      "[34]\tvalidation_0-auc:0.48426\n",
      "[35]\tvalidation_0-auc:0.48863\n",
      "[36]\tvalidation_0-auc:0.49242\n",
      "[37]\tvalidation_0-auc:0.50787\n",
      "[38]\tvalidation_0-auc:0.50875\n",
      "[39]\tvalidation_0-auc:0.50962\n",
      "[40]\tvalidation_0-auc:0.51589\n",
      "[41]\tvalidation_0-auc:0.51006\n",
      "[42]\tvalidation_0-auc:0.50991\n",
      "[43]\tvalidation_0-auc:0.51312\n",
      "[44]\tvalidation_0-auc:0.51618\n",
      "[45]\tvalidation_0-auc:0.50408\n",
      "[46]\tvalidation_0-auc:0.50408\n",
      "[47]\tvalidation_0-auc:0.51166\n",
      "[48]\tvalidation_0-auc:0.51050\n",
      "[49]\tvalidation_0-auc:0.50816\n",
      "[50]\tvalidation_0-auc:0.51603\n",
      "[51]\tvalidation_0-auc:0.52216\n",
      "[52]\tvalidation_0-auc:0.53090\n",
      "[53]\tvalidation_0-auc:0.53032\n",
      "[54]\tvalidation_0-auc:0.52945\n",
      "[55]\tvalidation_0-auc:0.53003\n",
      "[56]\tvalidation_0-auc:0.53120\n",
      "[57]\tvalidation_0-auc:0.53149\n",
      "[58]\tvalidation_0-auc:0.52741\n",
      "[59]\tvalidation_0-auc:0.52799\n",
      "[60]\tvalidation_0-auc:0.52945\n",
      "[61]\tvalidation_0-auc:0.52886\n",
      "[62]\tvalidation_0-auc:0.53440\n",
      "[63]\tvalidation_0-auc:0.53090\n",
      "[64]\tvalidation_0-auc:0.53761\n",
      "[65]\tvalidation_0-auc:0.53003\n",
      "[66]\tvalidation_0-auc:0.53586\n",
      "[67]\tvalidation_0-auc:0.53557\n",
      "[68]\tvalidation_0-auc:0.53120\n",
      "[69]\tvalidation_0-auc:0.53790\n",
      "[70]\tvalidation_0-auc:0.53994\n",
      "[71]\tvalidation_0-auc:0.53178\n",
      "[72]\tvalidation_0-auc:0.53353\n",
      "[73]\tvalidation_0-auc:0.53703\n",
      "[74]\tvalidation_0-auc:0.54519\n",
      "[75]\tvalidation_0-auc:0.54723\n",
      "[76]\tvalidation_0-auc:0.53936\n",
      "[77]\tvalidation_0-auc:0.53819\n",
      "[78]\tvalidation_0-auc:0.53878\n",
      "[79]\tvalidation_0-auc:0.54577\n",
      "Test Accuracy for Layer 2: 0.4158\n",
      "Test AUC for Layer 2: 0.4492\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.50747\n",
      "[1]\tvalidation_0-auc:0.47740\n",
      "[2]\tvalidation_0-auc:0.46973\n",
      "[3]\tvalidation_0-auc:0.50626\n",
      "[4]\tvalidation_0-auc:0.52058\n",
      "[5]\tvalidation_0-auc:0.49516\n",
      "[6]\tvalidation_0-auc:0.47135\n",
      "[7]\tvalidation_0-auc:0.46590\n",
      "[8]\tvalidation_0-auc:0.47700\n",
      "[9]\tvalidation_0-auc:0.47094\n",
      "[10]\tvalidation_0-auc:0.49233\n",
      "[11]\tvalidation_0-auc:0.49435\n",
      "[12]\tvalidation_0-auc:0.49354\n",
      "[13]\tvalidation_0-auc:0.50726\n",
      "[14]\tvalidation_0-auc:0.49798\n",
      "[15]\tvalidation_0-auc:0.49798\n",
      "[16]\tvalidation_0-auc:0.51695\n",
      "[17]\tvalidation_0-auc:0.51049\n",
      "[18]\tvalidation_0-auc:0.51897\n",
      "[19]\tvalidation_0-auc:0.51170\n",
      "[20]\tvalidation_0-auc:0.50484\n",
      "[21]\tvalidation_0-auc:0.51412\n",
      "[22]\tvalidation_0-auc:0.50888\n",
      "[23]\tvalidation_0-auc:0.49960\n",
      "[24]\tvalidation_0-auc:0.49677\n",
      "[25]\tvalidation_0-auc:0.49798\n",
      "[26]\tvalidation_0-auc:0.48789\n",
      "[27]\tvalidation_0-auc:0.49516\n",
      "[28]\tvalidation_0-auc:0.49395\n",
      "[29]\tvalidation_0-auc:0.49960\n",
      "[30]\tvalidation_0-auc:0.49919\n",
      "[31]\tvalidation_0-auc:0.48910\n",
      "[32]\tvalidation_0-auc:0.48345\n",
      "[33]\tvalidation_0-auc:0.47821\n",
      "[34]\tvalidation_0-auc:0.47296\n",
      "[35]\tvalidation_0-auc:0.47175\n",
      "[36]\tvalidation_0-auc:0.47296\n",
      "[37]\tvalidation_0-auc:0.47861\n",
      "[38]\tvalidation_0-auc:0.48467\n",
      "[39]\tvalidation_0-auc:0.48870\n",
      "[40]\tvalidation_0-auc:0.49274\n",
      "[41]\tvalidation_0-auc:0.49274\n",
      "[42]\tvalidation_0-auc:0.49798\n",
      "[43]\tvalidation_0-auc:0.50847\n",
      "[44]\tvalidation_0-auc:0.49395\n",
      "[45]\tvalidation_0-auc:0.49435\n",
      "[46]\tvalidation_0-auc:0.48830\n",
      "[47]\tvalidation_0-auc:0.48789\n",
      "[48]\tvalidation_0-auc:0.49233\n",
      "[49]\tvalidation_0-auc:0.48507\n",
      "[50]\tvalidation_0-auc:0.48345\n",
      "[51]\tvalidation_0-auc:0.48063\n",
      "[52]\tvalidation_0-auc:0.47700\n",
      "[53]\tvalidation_0-auc:0.47498\n",
      "[54]\tvalidation_0-auc:0.48144\n",
      "[55]\tvalidation_0-auc:0.48023\n",
      "[56]\tvalidation_0-auc:0.48023\n",
      "[57]\tvalidation_0-auc:0.48103\n",
      "[58]\tvalidation_0-auc:0.48023\n",
      "[59]\tvalidation_0-auc:0.48386\n",
      "[60]\tvalidation_0-auc:0.47659\n",
      "[61]\tvalidation_0-auc:0.48023\n",
      "[62]\tvalidation_0-auc:0.48305\n",
      "[63]\tvalidation_0-auc:0.47659\n",
      "[64]\tvalidation_0-auc:0.47942\n",
      "[65]\tvalidation_0-auc:0.48386\n",
      "[66]\tvalidation_0-auc:0.48749\n",
      "[67]\tvalidation_0-auc:0.48628\n",
      "[68]\tvalidation_0-auc:0.48305\n",
      "[69]\tvalidation_0-auc:0.48023\n",
      "[70]\tvalidation_0-auc:0.47659\n",
      "[71]\tvalidation_0-auc:0.47700\n",
      "[72]\tvalidation_0-auc:0.48023\n",
      "[73]\tvalidation_0-auc:0.49516\n",
      "[74]\tvalidation_0-auc:0.50161\n",
      "[75]\tvalidation_0-auc:0.50202\n",
      "[76]\tvalidation_0-auc:0.50202\n",
      "[77]\tvalidation_0-auc:0.49596\n",
      "[78]\tvalidation_0-auc:0.49637\n",
      "[79]\tvalidation_0-auc:0.49596\n",
      "Test Accuracy for Layer 3: 0.5571\n",
      "Test AUC for Layer 3: 0.4996\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4120\n",
      "Average Test AUC across all layers: 0.4654\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Title + S_label\n",
      "Average Accuracy: 0.5200\n",
      "Average AUC: 0.5528\n",
      "  Layer 1 - Accuracy: 0.6090, AUC: 0.6256\n",
      "  Layer 2 - Accuracy: 0.4653, AUC: 0.5418\n",
      "  Layer 3 - Accuracy: 0.4857, AUC: 0.4910\n",
      "\n",
      "Combination: MLP with Title + L_label\n",
      "Average Accuracy: 0.5880\n",
      "Average AUC: 0.5019\n",
      "  Layer 1 - Accuracy: 0.7368, AUC: 0.4134\n",
      "  Layer 2 - Accuracy: 0.5842, AUC: 0.5125\n",
      "  Layer 3 - Accuracy: 0.4429, AUC: 0.5798\n",
      "\n",
      "Combination: MLP with Full text + S_label\n",
      "Average Accuracy: 0.4800\n",
      "Average AUC: 0.5178\n",
      "  Layer 1 - Accuracy: 0.3910, AUC: 0.5598\n",
      "  Layer 2 - Accuracy: 0.5347, AUC: 0.5386\n",
      "  Layer 3 - Accuracy: 0.5143, AUC: 0.4551\n",
      "\n",
      "Combination: MLP with Full text + L_label\n",
      "Average Accuracy: 0.5880\n",
      "Average AUC: 0.4659\n",
      "  Layer 1 - Accuracy: 0.7368, AUC: 0.4332\n",
      "  Layer 2 - Accuracy: 0.5842, AUC: 0.5666\n",
      "  Layer 3 - Accuracy: 0.4429, AUC: 0.3978\n",
      "\n",
      "Combination: XGBoost with Title + S_label\n",
      "Average Accuracy: 0.5527\n",
      "Average AUC: 0.4100\n",
      "  Layer 1 - Accuracy: 0.6090, AUC: 0.4734\n",
      "  Layer 2 - Accuracy: 0.5347, AUC: 0.4413\n",
      "  Layer 3 - Accuracy: 0.5143, AUC: 0.3154\n",
      "\n",
      "Combination: XGBoost with Title + L_label\n",
      "Average Accuracy: 0.4120\n",
      "Average AUC: 0.4191\n",
      "  Layer 1 - Accuracy: 0.2632, AUC: 0.4111\n",
      "  Layer 2 - Accuracy: 0.4158, AUC: 0.3656\n",
      "  Layer 3 - Accuracy: 0.5571, AUC: 0.4806\n",
      "\n",
      "Combination: XGBoost with Full text + S_label\n",
      "Average Accuracy: 0.5527\n",
      "Average AUC: 0.4555\n",
      "  Layer 1 - Accuracy: 0.6090, AUC: 0.4570\n",
      "  Layer 2 - Accuracy: 0.5347, AUC: 0.4283\n",
      "  Layer 3 - Accuracy: 0.5143, AUC: 0.4812\n",
      "\n",
      "Combination: XGBoost with Full text + L_label\n",
      "Average Accuracy: 0.4120\n",
      "Average AUC: 0.4654\n",
      "  Layer 1 - Accuracy: 0.2632, AUC: 0.4475\n",
      "  Layer 2 - Accuracy: 0.4158, AUC: 0.4492\n",
      "  Layer 3 - Accuracy: 0.5571, AUC: 0.4996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP summary comparison visualization saved as: OpenAI_MLP_XGB/visualizations_summary/CVX\\mlp_performance_comparison.png\n",
      "XGBoost summary comparison visualization saved as: OpenAI_MLP_XGB/visualizations_summary/CVX\\xgb_performance_comparison.png\n",
      "MLP layer performance visualization saved as: OpenAI_MLP_XGB/visualizations_summary/CVX\\mlp_layer_performance.png\n",
      "XGBoost layer performance visualization saved as: OpenAI_MLP_XGB/visualizations_summary/CVX\\xgboost_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'OpenAI_MLP_XGB' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "from xgboost import callback\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, model_type, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class ClimateNewsOpenAIPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'OpenAI_MLP_XGB/visualizations_mlp/CVX'\n",
    "        self.xgb_viz_dir = 'OpenAI_MLP_XGB/visualizations_xgb/CVX'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs(self.xgb_viz_dir, exist_ok=True)\n",
    "        os.makedirs('OpenAI_MLP_XGB/visualizations_summary/CVX', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert embeddings to numpy arrays\n",
    "        self.data['Title_embedding'] = self.data['Title_embedding_vector'].apply(parse_embedding)\n",
    "        self.data['Fulltext_embedding'] = self.data['Full_text_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_title_embedding = self.data['Title_embedding'].iloc[0]\n",
    "        sample_fulltext_embedding = self.data['Fulltext_embedding'].iloc[0]\n",
    "        \n",
    "        print(f\"Sample Title embedding shape: {sample_title_embedding.shape}\")\n",
    "        print(f\"Sample Fulltext embedding shape: {sample_fulltext_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2014-2020), validation (2020-2021), test (2021-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2020-10-31'),\n",
    "            'val_start': pd.Timestamp('2020-11-01'),\n",
    "            'val_end': pd.Timestamp('2021-10-31'),\n",
    "            'test_start': pd.Timestamp('2021-11-01'),\n",
    "            'test_end': pd.Timestamp('2022-10-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2014-2021), validation (2021-2022), test (2022-2023)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2021-10-31'),\n",
    "            'val_start': pd.Timestamp('2021-11-01'),\n",
    "            'val_end': pd.Timestamp('2022-10-31'),\n",
    "            'test_start': pd.Timestamp('2022-11-01'),\n",
    "            'test_end': pd.Timestamp('2023-10-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2014-2022), validation (2022-2023), test (2023-2024)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2022-10-31'),\n",
    "            'val_start': pd.Timestamp('2022-11-01'),\n",
    "            'val_end': pd.Timestamp('2023-10-31'),\n",
    "            'test_start': pd.Timestamp('2023-11-01'),\n",
    "            'test_end': pd.Timestamp('2024-11-01')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, text_col, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            text_col: The column containing the embeddings ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data[text_col].values)\n",
    "        X_val = np.stack(val_data[text_col].values)\n",
    "        X_test = np.stack(test_data[text_col].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def get_xgb_parameters(self):\n",
    "        \"\"\"\n",
    "        Get XGBoost parameters optimized for high-dimensional embeddings.\n",
    "        Uses a single parameter set for both Title and Full text embeddings since they have the same dimension (1536).\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of base parameters\n",
    "        \"\"\"\n",
    "        # Setup XGBoost base parameters optimized for high-dimensional embeddings\n",
    "        base_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'n_estimators': 80,\n",
    "            'max_depth': 3,\n",
    "            'learning_rate': 0.001,\n",
    "            'subsample': 0.7,          # Row subsampling to prevent overfitting\n",
    "            'colsample_bytree': 0.5,   # Column subsampling to handle high dimensionality\n",
    "            'min_child_weight': 3,     # Prevents overfitting on high-dimensional embeddings\n",
    "            'reg_alpha': 1.0,          # L1 regularization\n",
    "            'reg_lambda': 2.0,         # L2 regularization\n",
    "            'random_state': 42,\n",
    "            'use_label_encoder': False # Avoid deprecation warning\n",
    "        }\n",
    "        \n",
    "        return base_params\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, model_type, text_col, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            model_type: Model type (MLP)\n",
    "            text_col: Text column used\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_{model_type.lower()}_{display_text.replace(' ', '_')}_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, text_col, label_col, model_type):\n",
    "        \"\"\"\n",
    "        Train and evaluate a model for a specific text column and label column.\n",
    "        \n",
    "        Args:\n",
    "            text_col: The embedding column to use ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "            model_type: The model type to use ('MLP' or 'XGBoost')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        combination_key = f\"{model_type}|{display_text}|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training {model_type} model for {display_text} and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        visualization_dir = self.mlp_viz_dir if model_type == 'MLP' else self.xgb_viz_dir\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, text_col, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            if model_type == 'MLP':\n",
    "                # MLP model training\n",
    "                model = self.create_mlp_model((1536,))\n",
    "                print(f\"Created MLP model for {display_text} ({label_col})\")\n",
    "                model.summary()\n",
    "                \n",
    "                # Setup callbacks\n",
    "                plot_callback = PlotLearningCallback(model_type, display_text, label_col, i+1, visualization_dir)\n",
    "                early_stopping = EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=15,\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                model_checkpoint = ModelCheckpoint(\n",
    "                    filepath=f\"{visualization_dir}/best_{model_type.lower()}_{display_text.lower().replace(' ', '_')}_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                    monitor='val_loss',\n",
    "                    save_weights_only=True,\n",
    "                    save_best_only=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Train model\n",
    "                print(f\"Training MLP model...\")\n",
    "                batch_size = 32  # Larger batch size for embeddings\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                y_pred_proba = model.predict(X_test)\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                # Create final learning curve visualization\n",
    "                self.plot_final_learning_curves(history, model_type, text_col, label_col, i+1, visualization_dir)\n",
    "                \n",
    "                # Store training history\n",
    "                training_history = history.history\n",
    "                \n",
    "            else:  # XGBoost - simplified approach to fix API issues\n",
    "                # Get XGBoost parameters\n",
    "                base_params = self.get_xgb_parameters()\n",
    "                \n",
    "                # Create and train XGBoost model\n",
    "                print(f\"Creating and training XGBoost model...\")\n",
    "                model = xgb.XGBClassifier(**base_params)\n",
    "                \n",
    "                # Only use validation set for evaluation (not training set)\n",
    "                eval_set = [(X_val, y_val)]\n",
    "                \n",
    "                # Train the model\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    verbose=True\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                # Create a simple summary for XGBoost (no detailed learning curves available)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.text(0.5, 0.5, f'XGBoost Model Trained Successfully\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\\nAUC: {roc_auc_score(y_test, y_pred_proba):.4f}',\n",
    "                         ha='center', va='center', size=14, fontweight='bold')\n",
    "                plt.title(f'XGBoost Results ({display_text}, {label_col}, Layer {i+1})')\n",
    "                plt.axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{visualization_dir}/xgb_{display_text.replace(' ', '_')}_{label_col}_layer_{i+1}.png\")\n",
    "                plt.close()\n",
    "                \n",
    "                # Use None for training history since detailed learning curves aren't available\n",
    "                training_history = None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'model_type': model_type,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': training_history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for all combinations of text inputs, label columns, and model types.\"\"\"\n",
    "        # Define all combinations\n",
    "        embedding_cols = ['Title_embedding', 'Fulltext_embedding']\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        model_types = ['MLP', 'XGBoost']\n",
    "        \n",
    "        # Run analysis for each combination\n",
    "        for model_type in model_types:\n",
    "            for embedding_col in embedding_cols:\n",
    "                for label_col in label_cols:\n",
    "                    self.train_and_evaluate_model(embedding_col, label_col, model_type)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Model': model_type,\n",
    "                'Text': text_col,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing all model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Split by model type\n",
    "        mlp_data = df[df['Model'] == 'MLP']\n",
    "        xgb_data = df[df['Model'] == 'XGBoost']\n",
    "        \n",
    "        # 1. Performance comparison for MLP\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot accuracy and AUC bars for MLP\n",
    "        x = np.arange(len(mlp_data))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, mlp_data['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, mlp_data['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('MLP Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with OpenAI Embeddings')\n",
    "        labels = [f\"{row['Text']} + {row['Label']}\" for _, row in mlp_data.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(mlp_data['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(mlp_data['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(mlp_data['Avg Accuracy'].max(), mlp_data['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/CVX', \"mlp_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Performance comparison for XGBoost\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot accuracy and AUC bars for XGBoost\n",
    "        x = np.arange(len(xgb_data))\n",
    "        \n",
    "        plt.bar(x - width/2, xgb_data['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, xgb_data['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('XGBoost Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of XGBoost Models with OpenAI Embeddings')\n",
    "        labels = [f\"{row['Text']} + {row['Label']}\" for _, row in xgb_data.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(xgb_data['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(xgb_data['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(xgb_data['Avg Accuracy'].max(), xgb_data['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/CVX', \"xgb_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"XGBoost summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP and XGBoost models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        mlp_layer_data = []\n",
    "        xgb_layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Model': model_type,\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                if model_type == 'MLP':\n",
    "                    mlp_layer_data.append(layer_info)\n",
    "                else:  # XGBoost\n",
    "                    xgb_layer_data.append(layer_info)\n",
    "        \n",
    "        # Create visualizations for each model type\n",
    "        self._create_model_layer_visualization(mlp_layer_data, 'MLP')\n",
    "        self._create_model_layer_visualization(xgb_layer_data, 'XGBoost')\n",
    "    \n",
    "    def _create_model_layer_visualization(self, layer_data, model_type):\n",
    "        \"\"\"Create layer-specific visualizations for a given model type.\"\"\"\n",
    "        if not layer_data:\n",
    "            print(f\"No layer data available for {model_type}\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} Accuracy by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} AUC by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/CVX', f\"{model_type.lower()}_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"{model_type} layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['OpenAI_MLP_XGB/visualizations_mlp/CVX', 'OpenAI_MLP_XGB/visualizations_xgb/CVX', \n",
    "                      'OpenAI_MLP_XGB/visualizations_summary/CVX']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/wall_street_news_semantics_SP500_database/wall_street_news_semantics_CVX_completed_openai.csv'\n",
    "    \n",
    "    # Initialize the predictor with OpenAI embeddings\n",
    "    predictor = ClimateNewsOpenAIPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'OpenAI_MLP_XGB' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing OpenAI embedding vectors from string format...\n",
      "Sample Title embedding shape: (1536,)\n",
      "Sample Fulltext embedding shape: (1536,)\n",
      "Loaded 838 climate change news articles spanning from 07/11/2014 to 24/09/2024\n",
      "Class distribution for short-term prediction: {1: 460, 0: 378}\n",
      "Class distribution for long-term prediction: {1: 503, 0: 335}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_145\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_556 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_411 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_290 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_557 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_412 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_291 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_558 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_413 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_559 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      " 9/13 [===================>..........] - ETA: 0s - loss: 0.9135 - accuracy: 0.4826\n",
      "Epoch 1: val_loss improved from inf to 0.69716, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 32ms/step - loss: 0.9289 - accuracy: 0.4763 - val_loss: 0.6972 - val_accuracy: 0.4198\n",
      "Epoch 2/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.7602 - accuracy: 0.5688\n",
      "Epoch 2: val_loss did not improve from 0.69716\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7768 - accuracy: 0.5586 - val_loss: 0.7019 - val_accuracy: 0.4046\n",
      "Epoch 3/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.7778 - accuracy: 0.6031\n",
      "Epoch 3: val_loss did not improve from 0.69716\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7530 - accuracy: 0.6110 - val_loss: 0.7071 - val_accuracy: 0.4046\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7032 - accuracy: 0.6023\n",
      "Epoch 4: val_loss did not improve from 0.69716\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6911 - accuracy: 0.6135 - val_loss: 0.7148 - val_accuracy: 0.4046\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6575 - accuracy: 0.6335\n",
      "Epoch 5: val_loss did not improve from 0.69716\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6509 - accuracy: 0.6409 - val_loss: 0.7202 - val_accuracy: 0.4046\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5878 - accuracy: 0.6676\n",
      "Epoch 6: val_loss did not improve from 0.69716\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5842 - accuracy: 0.6758 - val_loss: 0.7269 - val_accuracy: 0.4046\n",
      "Epoch 7/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.5727 - accuracy: 0.6938\n",
      "Epoch 7: val_loss did not improve from 0.69716\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5759 - accuracy: 0.7007 - val_loss: 0.7349 - val_accuracy: 0.4046\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5284 - accuracy: 0.7273\n",
      "Epoch 8: val_loss did not improve from 0.69716\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5424 - accuracy: 0.7282 - val_loss: 0.7415 - val_accuracy: 0.4046\n",
      "Epoch 9/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.4723 - accuracy: 0.7906\n",
      "Epoch 9: val_loss did not improve from 0.69716\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4920 - accuracy: 0.7756 - val_loss: 0.7490 - val_accuracy: 0.4046\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4771 - accuracy: 0.7955\n",
      "Epoch 10: val_loss did not improve from 0.69716\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4663 - accuracy: 0.8030 - val_loss: 0.7568 - val_accuracy: 0.4046\n",
      "Epoch 11/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4266 - accuracy: 0.8073\n",
      "Epoch 11: val_loss did not improve from 0.69716\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4301 - accuracy: 0.8030 - val_loss: 0.7651 - val_accuracy: 0.4046\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4512 - accuracy: 0.7869\n",
      "Epoch 12: val_loss did not improve from 0.69716\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4517 - accuracy: 0.7880 - val_loss: 0.7754 - val_accuracy: 0.4046\n",
      "Epoch 13/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4124 - accuracy: 0.8125\n",
      "Epoch 13: val_loss did not improve from 0.69716\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4128 - accuracy: 0.8105 - val_loss: 0.7866 - val_accuracy: 0.4046\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4534 - accuracy: 0.7727\n",
      "Epoch 14: val_loss did not improve from 0.69716\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4431 - accuracy: 0.7830 - val_loss: 0.7936 - val_accuracy: 0.4046\n",
      "Epoch 15/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.4156 - accuracy: 0.8031\n",
      "Epoch 15: val_loss did not improve from 0.69716\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4036 - accuracy: 0.8130 - val_loss: 0.8041 - val_accuracy: 0.4046\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3720 - accuracy: 0.8466Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69716\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3630 - accuracy: 0.8529 - val_loss: 0.8144 - val_accuracy: 0.4046\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.4511\n",
      "Test AUC for Layer 1: 0.5016\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_146\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_560 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_414 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_292 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_561 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_415 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_293 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_562 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_416 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_563 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8801 - accuracy: 0.5098\n",
      "Epoch 1: val_loss improved from inf to 0.69308, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8752 - accuracy: 0.5169 - val_loss: 0.6931 - val_accuracy: 0.4962\n",
      "Epoch 2/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.8042 - accuracy: 0.5646\n",
      "Epoch 2: val_loss improved from 0.69308 to 0.69190, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.8087 - accuracy: 0.5583 - val_loss: 0.6919 - val_accuracy: 0.5414\n",
      "Epoch 3/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.7392 - accuracy: 0.5871\n",
      "Epoch 3: val_loss improved from 0.69190 to 0.69127, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.7568 - accuracy: 0.5771 - val_loss: 0.6913 - val_accuracy: 0.5414\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7319 - accuracy: 0.5918\n",
      "Epoch 4: val_loss improved from 0.69127 to 0.68995, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7327 - accuracy: 0.5921 - val_loss: 0.6900 - val_accuracy: 0.5414\n",
      "Epoch 5/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6543 - accuracy: 0.6458\n",
      "Epoch 5: val_loss improved from 0.68995 to 0.68909, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.6670 - accuracy: 0.6353 - val_loss: 0.6891 - val_accuracy: 0.5489\n",
      "Epoch 6/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.6187 - accuracy: 0.6741\n",
      "Epoch 6: val_loss improved from 0.68909 to 0.68842, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6025 - accuracy: 0.6842 - val_loss: 0.6884 - val_accuracy: 0.5489\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6019 - accuracy: 0.6797\n",
      "Epoch 7: val_loss improved from 0.68842 to 0.68787, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5965 - accuracy: 0.6880 - val_loss: 0.6879 - val_accuracy: 0.5489\n",
      "Epoch 8/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5254 - accuracy: 0.7396\n",
      "Epoch 8: val_loss did not improve from 0.68787\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5300 - accuracy: 0.7350 - val_loss: 0.6882 - val_accuracy: 0.5489\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4980 - accuracy: 0.7695\n",
      "Epoch 9: val_loss did not improve from 0.68787\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4976 - accuracy: 0.7688 - val_loss: 0.6888 - val_accuracy: 0.5489\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5215 - accuracy: 0.7285\n",
      "Epoch 10: val_loss did not improve from 0.68787\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5170 - accuracy: 0.7331 - val_loss: 0.6893 - val_accuracy: 0.5489\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4674 - accuracy: 0.7617\n",
      "Epoch 11: val_loss did not improve from 0.68787\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4660 - accuracy: 0.7613 - val_loss: 0.6901 - val_accuracy: 0.5489\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4461 - accuracy: 0.7969\n",
      "Epoch 12: val_loss did not improve from 0.68787\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4530 - accuracy: 0.7914 - val_loss: 0.6912 - val_accuracy: 0.5489\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4438 - accuracy: 0.8105\n",
      "Epoch 13: val_loss did not improve from 0.68787\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4415 - accuracy: 0.8120 - val_loss: 0.6920 - val_accuracy: 0.5489\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4103 - accuracy: 0.8281\n",
      "Epoch 14: val_loss did not improve from 0.68787\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4076 - accuracy: 0.8289 - val_loss: 0.6936 - val_accuracy: 0.5489\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3940 - accuracy: 0.8320\n",
      "Epoch 15: val_loss did not improve from 0.68787\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3927 - accuracy: 0.8365 - val_loss: 0.6961 - val_accuracy: 0.5489\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4001 - accuracy: 0.8301\n",
      "Epoch 16: val_loss did not improve from 0.68787\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3952 - accuracy: 0.8346 - val_loss: 0.6973 - val_accuracy: 0.5489\n",
      "Epoch 17/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3933 - accuracy: 0.8477\n",
      "Epoch 17: val_loss did not improve from 0.68787\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4054 - accuracy: 0.8383 - val_loss: 0.6992 - val_accuracy: 0.5489\n",
      "Epoch 18/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3545 - accuracy: 0.8594\n",
      "Epoch 18: val_loss did not improve from 0.68787\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3566 - accuracy: 0.8571 - val_loss: 0.7018 - val_accuracy: 0.5489\n",
      "Epoch 19/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3500 - accuracy: 0.8594\n",
      "Epoch 19: val_loss did not improve from 0.68787\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3481 - accuracy: 0.8609 - val_loss: 0.7059 - val_accuracy: 0.5489\n",
      "Epoch 20/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3314 - accuracy: 0.8633\n",
      "Epoch 20: val_loss did not improve from 0.68787\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3302 - accuracy: 0.8647 - val_loss: 0.7084 - val_accuracy: 0.5489\n",
      "Epoch 21/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3033 - accuracy: 0.8926\n",
      "Epoch 21: val_loss did not improve from 0.68787\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3026 - accuracy: 0.8929 - val_loss: 0.7124 - val_accuracy: 0.5414\n",
      "Epoch 22/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2881 - accuracy: 0.9082Restoring model weights from the end of the best epoch: 7.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: val_loss did not improve from 0.68787\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.2893 - accuracy: 0.9041 - val_loss: 0.7155 - val_accuracy: 0.5414\n",
      "Epoch 22: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5347\n",
      "Test AUC for Layer 2: 0.4350\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_147\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_564 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_417 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_294 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_565 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_418 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_295 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_566 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_419 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_567 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9680 - accuracy: 0.4985\n",
      "Epoch 1: val_loss improved from inf to 0.69459, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 23ms/step - loss: 0.9680 - accuracy: 0.4985 - val_loss: 0.6946 - val_accuracy: 0.4653\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8564 - accuracy: 0.5375\n",
      "Epoch 2: val_loss did not improve from 0.69459\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.8564 - accuracy: 0.5375 - val_loss: 0.6951 - val_accuracy: 0.4653\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7716 - accuracy: 0.5766\n",
      "Epoch 3: val_loss improved from 0.69459 to 0.69397, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7716 - accuracy: 0.5766 - val_loss: 0.6940 - val_accuracy: 0.4554\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7502 - accuracy: 0.5601\n",
      "Epoch 4: val_loss improved from 0.69397 to 0.69202, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7502 - accuracy: 0.5601 - val_loss: 0.6920 - val_accuracy: 0.4752\n",
      "Epoch 5/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6688 - accuracy: 0.6438\n",
      "Epoch 5: val_loss improved from 0.69202 to 0.69025, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.6687 - accuracy: 0.6486 - val_loss: 0.6902 - val_accuracy: 0.5842\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6290 - accuracy: 0.6697\n",
      "Epoch 6: val_loss improved from 0.69025 to 0.68903, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.6290 - accuracy: 0.6697 - val_loss: 0.6890 - val_accuracy: 0.5545\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6032 - accuracy: 0.6922\n",
      "Epoch 7: val_loss improved from 0.68903 to 0.68825, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.6032 - accuracy: 0.6922 - val_loss: 0.6883 - val_accuracy: 0.5347\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5762 - accuracy: 0.7192\n",
      "Epoch 8: val_loss did not improve from 0.68825\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5762 - accuracy: 0.7192 - val_loss: 0.6886 - val_accuracy: 0.5347\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5403 - accuracy: 0.7432\n",
      "Epoch 9: val_loss did not improve from 0.68825\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5403 - accuracy: 0.7432 - val_loss: 0.6894 - val_accuracy: 0.5347\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5206 - accuracy: 0.7402\n",
      "Epoch 10: val_loss did not improve from 0.68825\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5206 - accuracy: 0.7402 - val_loss: 0.6913 - val_accuracy: 0.5347\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5075 - accuracy: 0.7477\n",
      "Epoch 11: val_loss did not improve from 0.68825\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5075 - accuracy: 0.7477 - val_loss: 0.6957 - val_accuracy: 0.5347\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4628 - accuracy: 0.7808\n",
      "Epoch 12: val_loss did not improve from 0.68825\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4628 - accuracy: 0.7808 - val_loss: 0.6991 - val_accuracy: 0.5347\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4774 - accuracy: 0.7778\n",
      "Epoch 13: val_loss did not improve from 0.68825\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4774 - accuracy: 0.7778 - val_loss: 0.7015 - val_accuracy: 0.5347\n",
      "Epoch 14/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.4616 - accuracy: 0.7812\n",
      "Epoch 14: val_loss did not improve from 0.68825\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4562 - accuracy: 0.7748 - val_loss: 0.7057 - val_accuracy: 0.5347\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4111 - accuracy: 0.8138\n",
      "Epoch 15: val_loss did not improve from 0.68825\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4111 - accuracy: 0.8138 - val_loss: 0.7076 - val_accuracy: 0.5347\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4003 - accuracy: 0.8333\n",
      "Epoch 16: val_loss did not improve from 0.68825\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4003 - accuracy: 0.8333 - val_loss: 0.7103 - val_accuracy: 0.5347\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3726 - accuracy: 0.8408\n",
      "Epoch 17: val_loss did not improve from 0.68825\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3726 - accuracy: 0.8408 - val_loss: 0.7139 - val_accuracy: 0.5347\n",
      "Epoch 18/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.3501 - accuracy: 0.8566\n",
      "Epoch 18: val_loss did not improve from 0.68825\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3506 - accuracy: 0.8514 - val_loss: 0.7176 - val_accuracy: 0.5347\n",
      "Epoch 19/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3448 - accuracy: 0.8589\n",
      "Epoch 19: val_loss did not improve from 0.68825\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3448 - accuracy: 0.8589 - val_loss: 0.7205 - val_accuracy: 0.5347\n",
      "Epoch 20/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.3278 - accuracy: 0.8640\n",
      "Epoch 20: val_loss did not improve from 0.68825\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3250 - accuracy: 0.8679 - val_loss: 0.7202 - val_accuracy: 0.5347\n",
      "Epoch 21/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.2881 - accuracy: 0.9099\n",
      "Epoch 21: val_loss did not improve from 0.68825\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2956 - accuracy: 0.9054 - val_loss: 0.7187 - val_accuracy: 0.5248\n",
      "Epoch 22/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3010 - accuracy: 0.8934Restoring model weights from the end of the best epoch: 7.\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.68825\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3010 - accuracy: 0.8934 - val_loss: 0.7208 - val_accuracy: 0.5545\n",
      "Epoch 22: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5857\n",
      "Test AUC for Layer 3: 0.5692\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5238\n",
      "Average Test AUC across all layers: 0.5019\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_148\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_568 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_420 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_296 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_569 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_421 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_297 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_570 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_422 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_571 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.8139 - accuracy: 0.5406\n",
      "Epoch 1: val_loss improved from inf to 0.70761, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 31ms/step - loss: 0.8003 - accuracy: 0.5461 - val_loss: 0.7076 - val_accuracy: 0.3053\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8078 - accuracy: 0.5256\n",
      "Epoch 2: val_loss did not improve from 0.70761\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.7980 - accuracy: 0.5337 - val_loss: 0.7188 - val_accuracy: 0.3053\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7342 - accuracy: 0.5767\n",
      "Epoch 3: val_loss did not improve from 0.70761\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7346 - accuracy: 0.5711 - val_loss: 0.7290 - val_accuracy: 0.3053\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6596 - accuracy: 0.6335\n",
      "Epoch 4: val_loss did not improve from 0.70761\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6577 - accuracy: 0.6334 - val_loss: 0.7386 - val_accuracy: 0.3053\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6533 - accuracy: 0.6364\n",
      "Epoch 5: val_loss did not improve from 0.70761\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6466 - accuracy: 0.6434 - val_loss: 0.7474 - val_accuracy: 0.3053\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6081 - accuracy: 0.6676\n",
      "Epoch 6: val_loss did not improve from 0.70761\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6079 - accuracy: 0.6708 - val_loss: 0.7533 - val_accuracy: 0.3053\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5944 - accuracy: 0.7074\n",
      "Epoch 7: val_loss did not improve from 0.70761\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6053 - accuracy: 0.6983 - val_loss: 0.7571 - val_accuracy: 0.3053\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5078 - accuracy: 0.7699\n",
      "Epoch 8: val_loss did not improve from 0.70761\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5245 - accuracy: 0.7556 - val_loss: 0.7594 - val_accuracy: 0.3053\n",
      "Epoch 9/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5121 - accuracy: 0.7682\n",
      "Epoch 9: val_loss did not improve from 0.70761\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5159 - accuracy: 0.7606 - val_loss: 0.7603 - val_accuracy: 0.3053\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4775 - accuracy: 0.7642\n",
      "Epoch 10: val_loss did not improve from 0.70761\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4776 - accuracy: 0.7631 - val_loss: 0.7621 - val_accuracy: 0.3053\n",
      "Epoch 11/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4506 - accuracy: 0.8040\n",
      "Epoch 11: val_loss did not improve from 0.70761\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4421 - accuracy: 0.8055 - val_loss: 0.7629 - val_accuracy: 0.3053\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4150 - accuracy: 0.8068\n",
      "Epoch 12: val_loss did not improve from 0.70761\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4093 - accuracy: 0.8204 - val_loss: 0.7660 - val_accuracy: 0.3053\n",
      "Epoch 13/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4134 - accuracy: 0.8464\n",
      "Epoch 13: val_loss did not improve from 0.70761\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4127 - accuracy: 0.8454 - val_loss: 0.7684 - val_accuracy: 0.3053\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3870 - accuracy: 0.8324\n",
      "Epoch 14: val_loss did not improve from 0.70761\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3860 - accuracy: 0.8329 - val_loss: 0.7686 - val_accuracy: 0.3053\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3961 - accuracy: 0.8409\n",
      "Epoch 15: val_loss did not improve from 0.70761\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3919 - accuracy: 0.8429 - val_loss: 0.7685 - val_accuracy: 0.3053\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3913 - accuracy: 0.8409Restoring model weights from the end of the best epoch: 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: val_loss did not improve from 0.70761\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3864 - accuracy: 0.8454 - val_loss: 0.7650 - val_accuracy: 0.3053\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.2632\n",
      "Test AUC for Layer 1: 0.5032\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_149\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_572 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_423 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_298 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_573 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_424 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_299 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_574 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_425 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_575 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.9424 - accuracy: 0.4902\n",
      "Epoch 1: val_loss improved from inf to 0.64734, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.9436 - accuracy: 0.4887 - val_loss: 0.6473 - val_accuracy: 0.7368\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8142 - accuracy: 0.5391\n",
      "Epoch 2: val_loss improved from 0.64734 to 0.61489, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.8102 - accuracy: 0.5414 - val_loss: 0.6149 - val_accuracy: 0.7368\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7559 - accuracy: 0.5742\n",
      "Epoch 3: val_loss improved from 0.61489 to 0.59413, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7698 - accuracy: 0.5695 - val_loss: 0.5941 - val_accuracy: 0.7368\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7149 - accuracy: 0.5879\n",
      "Epoch 4: val_loss improved from 0.59413 to 0.58391, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7152 - accuracy: 0.5846 - val_loss: 0.5839 - val_accuracy: 0.7368\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6831 - accuracy: 0.6328\n",
      "Epoch 5: val_loss improved from 0.58391 to 0.57883, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6817 - accuracy: 0.6353 - val_loss: 0.5788 - val_accuracy: 0.7368\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6069 - accuracy: 0.6680\n",
      "Epoch 6: val_loss improved from 0.57883 to 0.57743, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6013 - accuracy: 0.6711 - val_loss: 0.5774 - val_accuracy: 0.7368\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5447 - accuracy: 0.7246\n",
      "Epoch 7: val_loss did not improve from 0.57743\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5558 - accuracy: 0.7180 - val_loss: 0.5788 - val_accuracy: 0.7368\n",
      "Epoch 8/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5434 - accuracy: 0.7163\n",
      "Epoch 8: val_loss did not improve from 0.57743\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5467 - accuracy: 0.7256 - val_loss: 0.5818 - val_accuracy: 0.7368\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5118 - accuracy: 0.7368\n",
      "Epoch 9: val_loss did not improve from 0.57743\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5118 - accuracy: 0.7368 - val_loss: 0.5866 - val_accuracy: 0.7368\n",
      "Epoch 10/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4697 - accuracy: 0.7812\n",
      "Epoch 10: val_loss did not improve from 0.57743\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4743 - accuracy: 0.7895 - val_loss: 0.5923 - val_accuracy: 0.7368\n",
      "Epoch 11/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4830 - accuracy: 0.7552\n",
      "Epoch 11: val_loss did not improve from 0.57743\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4700 - accuracy: 0.7744 - val_loss: 0.5970 - val_accuracy: 0.7368\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4260 - accuracy: 0.8027\n",
      "Epoch 12: val_loss did not improve from 0.57743\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4263 - accuracy: 0.8026 - val_loss: 0.6027 - val_accuracy: 0.7368\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4012 - accuracy: 0.8346\n",
      "Epoch 13: val_loss did not improve from 0.57743\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4012 - accuracy: 0.8346 - val_loss: 0.6076 - val_accuracy: 0.7368\n",
      "Epoch 14/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4284 - accuracy: 0.7969\n",
      "Epoch 14: val_loss did not improve from 0.57743\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4239 - accuracy: 0.7989 - val_loss: 0.6124 - val_accuracy: 0.7368\n",
      "Epoch 15/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3663 - accuracy: 0.8516\n",
      "Epoch 15: val_loss did not improve from 0.57743\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3487 - accuracy: 0.8684 - val_loss: 0.6187 - val_accuracy: 0.7368\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3590 - accuracy: 0.8574\n",
      "Epoch 16: val_loss did not improve from 0.57743\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3592 - accuracy: 0.8590 - val_loss: 0.6226 - val_accuracy: 0.7368\n",
      "Epoch 17/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3908 - accuracy: 0.8359\n",
      "Epoch 17: val_loss did not improve from 0.57743\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3834 - accuracy: 0.8327 - val_loss: 0.6277 - val_accuracy: 0.7368\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3429 - accuracy: 0.8684\n",
      "Epoch 18: val_loss did not improve from 0.57743\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3429 - accuracy: 0.8684 - val_loss: 0.6295 - val_accuracy: 0.7368\n",
      "Epoch 19/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3102 - accuracy: 0.8698\n",
      "Epoch 19: val_loss did not improve from 0.57743\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3084 - accuracy: 0.8778 - val_loss: 0.6308 - val_accuracy: 0.7368\n",
      "Epoch 20/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3146 - accuracy: 0.8809\n",
      "Epoch 20: val_loss did not improve from 0.57743\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3154 - accuracy: 0.8816 - val_loss: 0.6351 - val_accuracy: 0.7368\n",
      "Epoch 21/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2894 - accuracy: 0.9062Restoring model weights from the end of the best epoch: 6.\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.57743\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2878 - accuracy: 0.9060 - val_loss: 0.6438 - val_accuracy: 0.7368\n",
      "Epoch 21: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.6139\n",
      "Test AUC for Layer 2: 0.5182\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_150\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_576 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_426 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_300 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_577 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_427 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_301 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_578 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_428 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_579 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.9116 - accuracy: 0.5164\n",
      "Epoch 1: val_loss improved from inf to 0.68351, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 23ms/step - loss: 0.9086 - accuracy: 0.5150 - val_loss: 0.6835 - val_accuracy: 0.6139\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8385 - accuracy: 0.5195\n",
      "Epoch 2: val_loss improved from 0.68351 to 0.67525, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.8385 - accuracy: 0.5195 - val_loss: 0.6753 - val_accuracy: 0.6139\n",
      "Epoch 3/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.7309 - accuracy: 0.5984\n",
      "Epoch 3: val_loss improved from 0.67525 to 0.66999, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7270 - accuracy: 0.5961 - val_loss: 0.6700 - val_accuracy: 0.6139\n",
      "Epoch 4/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.7133 - accuracy: 0.5984\n",
      "Epoch 4: val_loss improved from 0.66999 to 0.66618, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7104 - accuracy: 0.6006 - val_loss: 0.6662 - val_accuracy: 0.6139\n",
      "Epoch 5/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6522 - accuracy: 0.6422\n",
      "Epoch 5: val_loss improved from 0.66618 to 0.66366, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6530 - accuracy: 0.6411 - val_loss: 0.6637 - val_accuracy: 0.6139\n",
      "Epoch 6/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6378 - accuracy: 0.6531\n",
      "Epoch 6: val_loss improved from 0.66366 to 0.66228, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.6361 - accuracy: 0.6532 - val_loss: 0.6623 - val_accuracy: 0.6139\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5990 - accuracy: 0.7027\n",
      "Epoch 7: val_loss improved from 0.66228 to 0.66147, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 0.5990 - accuracy: 0.7027 - val_loss: 0.6615 - val_accuracy: 0.6139\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5384 - accuracy: 0.7312\n",
      "Epoch 8: val_loss did not improve from 0.66147\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5384 - accuracy: 0.7312 - val_loss: 0.6615 - val_accuracy: 0.6139\n",
      "Epoch 9/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.5248 - accuracy: 0.7353\n",
      "Epoch 9: val_loss improved from 0.66147 to 0.66135, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5338 - accuracy: 0.7297 - val_loss: 0.6614 - val_accuracy: 0.6139\n",
      "Epoch 10/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5097 - accuracy: 0.7453\n",
      "Epoch 10: val_loss improved from 0.66135 to 0.66006, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5099 - accuracy: 0.7447 - val_loss: 0.6601 - val_accuracy: 0.6139\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4836 - accuracy: 0.7748\n",
      "Epoch 11: val_loss did not improve from 0.66006\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4836 - accuracy: 0.7748 - val_loss: 0.6605 - val_accuracy: 0.6139\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4296 - accuracy: 0.8093\n",
      "Epoch 12: val_loss did not improve from 0.66006\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4296 - accuracy: 0.8093 - val_loss: 0.6612 - val_accuracy: 0.6139\n",
      "Epoch 13/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.4169 - accuracy: 0.8180\n",
      "Epoch 13: val_loss improved from 0.66006 to 0.65941, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.4208 - accuracy: 0.8108 - val_loss: 0.6594 - val_accuracy: 0.6139\n",
      "Epoch 14/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.4240 - accuracy: 0.8309\n",
      "Epoch 14: val_loss improved from 0.65941 to 0.65775, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.4093 - accuracy: 0.8378 - val_loss: 0.6577 - val_accuracy: 0.6139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4026 - accuracy: 0.8328\n",
      "Epoch 15: val_loss did not improve from 0.65775\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4033 - accuracy: 0.8318 - val_loss: 0.6579 - val_accuracy: 0.6139\n",
      "Epoch 16/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3810 - accuracy: 0.8487\n",
      "Epoch 16: val_loss improved from 0.65775 to 0.65757, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3875 - accuracy: 0.8423 - val_loss: 0.6576 - val_accuracy: 0.6139\n",
      "Epoch 17/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3880 - accuracy: 0.8250\n",
      "Epoch 17: val_loss did not improve from 0.65757\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.3844 - accuracy: 0.8258 - val_loss: 0.6591 - val_accuracy: 0.6139\n",
      "Epoch 18/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3648 - accuracy: 0.8318\n",
      "Epoch 18: val_loss improved from 0.65757 to 0.65694, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.3648 - accuracy: 0.8318 - val_loss: 0.6569 - val_accuracy: 0.6139\n",
      "Epoch 19/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3454 - accuracy: 0.8609\n",
      "Epoch 19: val_loss improved from 0.65694 to 0.65368, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 0.3391 - accuracy: 0.8664 - val_loss: 0.6537 - val_accuracy: 0.6139\n",
      "Epoch 20/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3302 - accuracy: 0.8750\n",
      "Epoch 20: val_loss improved from 0.65368 to 0.65265, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.3364 - accuracy: 0.8679 - val_loss: 0.6527 - val_accuracy: 0.6139\n",
      "Epoch 21/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3113 - accuracy: 0.8904\n",
      "Epoch 21: val_loss did not improve from 0.65265\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3113 - accuracy: 0.8904 - val_loss: 0.6529 - val_accuracy: 0.6139\n",
      "Epoch 22/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2938 - accuracy: 0.8964\n",
      "Epoch 22: val_loss did not improve from 0.65265\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2938 - accuracy: 0.8964 - val_loss: 0.6574 - val_accuracy: 0.6139\n",
      "Epoch 23/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3024 - accuracy: 0.8859\n",
      "Epoch 23: val_loss did not improve from 0.65265\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3024 - accuracy: 0.8859 - val_loss: 0.6574 - val_accuracy: 0.6139\n",
      "Epoch 24/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2720 - accuracy: 0.8984\n",
      "Epoch 24: val_loss did not improve from 0.65265\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2627 - accuracy: 0.9084 - val_loss: 0.6534 - val_accuracy: 0.6436\n",
      "Epoch 25/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2549 - accuracy: 0.9167\n",
      "Epoch 25: val_loss did not improve from 0.65265\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2598 - accuracy: 0.9129 - val_loss: 0.6547 - val_accuracy: 0.6436\n",
      "Epoch 26/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2435 - accuracy: 0.9219\n",
      "Epoch 26: val_loss did not improve from 0.65265\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2510 - accuracy: 0.9189 - val_loss: 0.6551 - val_accuracy: 0.6535\n",
      "Epoch 27/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2584 - accuracy: 0.9115\n",
      "Epoch 27: val_loss did not improve from 0.65265\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2671 - accuracy: 0.9054 - val_loss: 0.6609 - val_accuracy: 0.6040\n",
      "Epoch 28/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2318 - accuracy: 0.9277\n",
      "Epoch 28: val_loss did not improve from 0.65265\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2337 - accuracy: 0.9279 - val_loss: 0.6667 - val_accuracy: 0.6139\n",
      "Epoch 29/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2309 - accuracy: 0.9324\n",
      "Epoch 29: val_loss did not improve from 0.65265\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2309 - accuracy: 0.9324 - val_loss: 0.6782 - val_accuracy: 0.5941\n",
      "Epoch 30/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2151 - accuracy: 0.9249\n",
      "Epoch 30: val_loss did not improve from 0.65265\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2151 - accuracy: 0.9249 - val_loss: 0.6882 - val_accuracy: 0.5941\n",
      "Epoch 31/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2047 - accuracy: 0.9354\n",
      "Epoch 31: val_loss did not improve from 0.65265\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2047 - accuracy: 0.9354 - val_loss: 0.6926 - val_accuracy: 0.6139\n",
      "Epoch 32/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1840 - accuracy: 0.9444\n",
      "Epoch 32: val_loss did not improve from 0.65265\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.1840 - accuracy: 0.9444 - val_loss: 0.7014 - val_accuracy: 0.6040\n",
      "Epoch 33/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1975 - accuracy: 0.9399\n",
      "Epoch 33: val_loss did not improve from 0.65265\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.1975 - accuracy: 0.9399 - val_loss: 0.7089 - val_accuracy: 0.6337\n",
      "Epoch 34/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1683 - accuracy: 0.9655\n",
      "Epoch 34: val_loss did not improve from 0.65265\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.1683 - accuracy: 0.9655 - val_loss: 0.7194 - val_accuracy: 0.5842\n",
      "Epoch 35/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 0.9535Restoring model weights from the end of the best epoch: 20.\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.65265\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.1731 - accuracy: 0.9535 - val_loss: 0.7415 - val_accuracy: 0.5347\n",
      "Epoch 35: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4857\n",
      "Test AUC for Layer 3: 0.4371\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4542\n",
      "Average Test AUC across all layers: 0.4862\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_151\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_580 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_429 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_302 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_581 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_430 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_303 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_582 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_431 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_583 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8541 - accuracy: 0.5256\n",
      "Epoch 1: val_loss improved from inf to 0.69252, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 30ms/step - loss: 0.8647 - accuracy: 0.5137 - val_loss: 0.6925 - val_accuracy: 0.5878\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8273 - accuracy: 0.5284\n",
      "Epoch 2: val_loss improved from 0.69252 to 0.68964, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.8019 - accuracy: 0.5411 - val_loss: 0.6896 - val_accuracy: 0.5954\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6851 - accuracy: 0.6278\n",
      "Epoch 3: val_loss improved from 0.68964 to 0.68701, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.6887 - accuracy: 0.6334 - val_loss: 0.6870 - val_accuracy: 0.5954\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6642 - accuracy: 0.6193\n",
      "Epoch 4: val_loss improved from 0.68701 to 0.68434, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.6842 - accuracy: 0.6135 - val_loss: 0.6843 - val_accuracy: 0.5954\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6117 - accuracy: 0.6648\n",
      "Epoch 5: val_loss improved from 0.68434 to 0.68253, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.6088 - accuracy: 0.6708 - val_loss: 0.6825 - val_accuracy: 0.5954\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5903 - accuracy: 0.6875\n",
      "Epoch 6: val_loss improved from 0.68253 to 0.68098, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.5962 - accuracy: 0.6758 - val_loss: 0.6810 - val_accuracy: 0.5954\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5847 - accuracy: 0.6903\n",
      "Epoch 7: val_loss improved from 0.68098 to 0.67995, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5735 - accuracy: 0.7032 - val_loss: 0.6800 - val_accuracy: 0.5954\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5084 - accuracy: 0.7614\n",
      "Epoch 8: val_loss improved from 0.67995 to 0.67944, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5175 - accuracy: 0.7481 - val_loss: 0.6794 - val_accuracy: 0.5954\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5101 - accuracy: 0.7443\n",
      "Epoch 9: val_loss improved from 0.67944 to 0.67865, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5106 - accuracy: 0.7456 - val_loss: 0.6786 - val_accuracy: 0.5954\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4690 - accuracy: 0.8040\n",
      "Epoch 10: val_loss improved from 0.67865 to 0.67816, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.4733 - accuracy: 0.8005 - val_loss: 0.6782 - val_accuracy: 0.5954\n",
      "Epoch 11/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4869 - accuracy: 0.7443\n",
      "Epoch 11: val_loss improved from 0.67816 to 0.67779, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.4754 - accuracy: 0.7581 - val_loss: 0.6778 - val_accuracy: 0.5954\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4907 - accuracy: 0.7557\n",
      "Epoch 12: val_loss improved from 0.67779 to 0.67754, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.4941 - accuracy: 0.7581 - val_loss: 0.6775 - val_accuracy: 0.5954\n",
      "Epoch 13/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4425 - accuracy: 0.8352\n",
      "Epoch 13: val_loss did not improve from 0.67754\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4306 - accuracy: 0.8379 - val_loss: 0.6776 - val_accuracy: 0.5954\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4191 - accuracy: 0.8324\n",
      "Epoch 14: val_loss did not improve from 0.67754\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4238 - accuracy: 0.8304 - val_loss: 0.6779 - val_accuracy: 0.5954\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4199 - accuracy: 0.8125\n",
      "Epoch 15: val_loss did not improve from 0.67754\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4059 - accuracy: 0.8180 - val_loss: 0.6784 - val_accuracy: 0.5954\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3608 - accuracy: 0.8551\n",
      "Epoch 16: val_loss did not improve from 0.67754\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3669 - accuracy: 0.8504 - val_loss: 0.6791 - val_accuracy: 0.5954\n",
      "Epoch 17/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3723 - accuracy: 0.8523\n",
      "Epoch 17: val_loss did not improve from 0.67754\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3619 - accuracy: 0.8653 - val_loss: 0.6802 - val_accuracy: 0.5954\n",
      "Epoch 18/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.3350 - accuracy: 0.8719\n",
      "Epoch 18: val_loss did not improve from 0.67754\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3302 - accuracy: 0.8828 - val_loss: 0.6815 - val_accuracy: 0.5954\n",
      "Epoch 19/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3397 - accuracy: 0.8828\n",
      "Epoch 19: val_loss did not improve from 0.67754\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3381 - accuracy: 0.8853 - val_loss: 0.6827 - val_accuracy: 0.5954\n",
      "Epoch 20/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3643 - accuracy: 0.8490\n",
      "Epoch 20: val_loss did not improve from 0.67754\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3599 - accuracy: 0.8529 - val_loss: 0.6836 - val_accuracy: 0.5954\n",
      "Epoch 21/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3158 - accuracy: 0.8693\n",
      "Epoch 21: val_loss did not improve from 0.67754\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3110 - accuracy: 0.8753 - val_loss: 0.6847 - val_accuracy: 0.5954\n",
      "Epoch 22/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3119 - accuracy: 0.8892\n",
      "Epoch 22: val_loss did not improve from 0.67754\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3115 - accuracy: 0.8878 - val_loss: 0.6860 - val_accuracy: 0.5954\n",
      "Epoch 23/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3073 - accuracy: 0.8854\n",
      "Epoch 23: val_loss did not improve from 0.67754\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3040 - accuracy: 0.8903 - val_loss: 0.6875 - val_accuracy: 0.5954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2828 - accuracy: 0.8920\n",
      "Epoch 24: val_loss did not improve from 0.67754\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.2789 - accuracy: 0.8953 - val_loss: 0.6890 - val_accuracy: 0.5954\n",
      "Epoch 25/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2930 - accuracy: 0.8693\n",
      "Epoch 25: val_loss did not improve from 0.67754\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.2907 - accuracy: 0.8678 - val_loss: 0.6910 - val_accuracy: 0.5954\n",
      "Epoch 26/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3011 - accuracy: 0.8864\n",
      "Epoch 26: val_loss did not improve from 0.67754\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3019 - accuracy: 0.8878 - val_loss: 0.6939 - val_accuracy: 0.5954\n",
      "Epoch 27/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2407 - accuracy: 0.9205Restoring model weights from the end of the best epoch: 12.\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.67754\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.2380 - accuracy: 0.9252 - val_loss: 0.6974 - val_accuracy: 0.5954\n",
      "Epoch 27: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.5489\n",
      "Test AUC for Layer 1: 0.5619\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_152\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_584 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_432 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_304 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_585 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_433 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_305 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_586 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_434 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_587 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8368 - accuracy: 0.5332\n",
      "Epoch 1: val_loss improved from inf to 0.69520, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8367 - accuracy: 0.5320 - val_loss: 0.6952 - val_accuracy: 0.4361\n",
      "Epoch 2/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.7657 - accuracy: 0.5542\n",
      "Epoch 2: val_loss did not improve from 0.69520\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7732 - accuracy: 0.5620 - val_loss: 0.6971 - val_accuracy: 0.4511\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7041 - accuracy: 0.5977\n",
      "Epoch 3: val_loss did not improve from 0.69520\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6998 - accuracy: 0.6015 - val_loss: 0.6992 - val_accuracy: 0.4511\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6889 - accuracy: 0.6094\n",
      "Epoch 4: val_loss did not improve from 0.69520\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6879 - accuracy: 0.6147 - val_loss: 0.7015 - val_accuracy: 0.4511\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6287 - accuracy: 0.6602\n",
      "Epoch 5: val_loss did not improve from 0.69520\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6361 - accuracy: 0.6560 - val_loss: 0.7051 - val_accuracy: 0.4511\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6257 - accuracy: 0.6562\n",
      "Epoch 6: val_loss did not improve from 0.69520\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6361 - accuracy: 0.6523 - val_loss: 0.7079 - val_accuracy: 0.4511\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5692 - accuracy: 0.7070\n",
      "Epoch 7: val_loss did not improve from 0.69520\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5674 - accuracy: 0.7105 - val_loss: 0.7089 - val_accuracy: 0.4511\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5856 - accuracy: 0.6992\n",
      "Epoch 8: val_loss did not improve from 0.69520\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5792 - accuracy: 0.7049 - val_loss: 0.7104 - val_accuracy: 0.4511\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5443 - accuracy: 0.7363\n",
      "Epoch 9: val_loss did not improve from 0.69520\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5369 - accuracy: 0.7406 - val_loss: 0.7129 - val_accuracy: 0.4511\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4895 - accuracy: 0.7520\n",
      "Epoch 10: val_loss did not improve from 0.69520\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4930 - accuracy: 0.7462 - val_loss: 0.7167 - val_accuracy: 0.4511\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5187 - accuracy: 0.7519\n",
      "Epoch 11: val_loss did not improve from 0.69520\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5187 - accuracy: 0.7519 - val_loss: 0.7196 - val_accuracy: 0.4511\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4940 - accuracy: 0.7500\n",
      "Epoch 12: val_loss did not improve from 0.69520\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4957 - accuracy: 0.7462 - val_loss: 0.7219 - val_accuracy: 0.4511\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4373 - accuracy: 0.8164\n",
      "Epoch 13: val_loss did not improve from 0.69520\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4440 - accuracy: 0.8083 - val_loss: 0.7256 - val_accuracy: 0.4511\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4487 - accuracy: 0.8047\n",
      "Epoch 14: val_loss did not improve from 0.69520\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4456 - accuracy: 0.8102 - val_loss: 0.7275 - val_accuracy: 0.4511\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4132 - accuracy: 0.8184\n",
      "Epoch 15: val_loss did not improve from 0.69520\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4160 - accuracy: 0.8158 - val_loss: 0.7322 - val_accuracy: 0.4511\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3923 - accuracy: 0.8164Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69520\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3877 - accuracy: 0.8214 - val_loss: 0.7343 - val_accuracy: 0.4511\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4752\n",
      "Test AUC for Layer 2: 0.4866\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_153\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_588 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_435 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_306 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_589 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_436 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_307 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_590 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_437 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_591 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8379 - accuracy: 0.5045\n",
      "Epoch 1: val_loss improved from inf to 0.68978, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 22ms/step - loss: 0.8379 - accuracy: 0.5045 - val_loss: 0.6898 - val_accuracy: 0.5347\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7873 - accuracy: 0.5240\n",
      "Epoch 2: val_loss did not improve from 0.68978\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7873 - accuracy: 0.5240 - val_loss: 0.6908 - val_accuracy: 0.5347\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7253 - accuracy: 0.6006\n",
      "Epoch 3: val_loss did not improve from 0.68978\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7253 - accuracy: 0.6006 - val_loss: 0.6939 - val_accuracy: 0.5347\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7059 - accuracy: 0.6111\n",
      "Epoch 4: val_loss did not improve from 0.68978\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7059 - accuracy: 0.6111 - val_loss: 0.6985 - val_accuracy: 0.5347\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6538 - accuracy: 0.6171\n",
      "Epoch 5: val_loss did not improve from 0.68978\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6538 - accuracy: 0.6171 - val_loss: 0.7029 - val_accuracy: 0.5347\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6210 - accuracy: 0.6712\n",
      "Epoch 6: val_loss did not improve from 0.68978\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6210 - accuracy: 0.6712 - val_loss: 0.7072 - val_accuracy: 0.5347\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5955 - accuracy: 0.6577\n",
      "Epoch 7: val_loss did not improve from 0.68978\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5955 - accuracy: 0.6577 - val_loss: 0.7116 - val_accuracy: 0.5347\n",
      "Epoch 8/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.5934 - accuracy: 0.6777\n",
      "Epoch 8: val_loss did not improve from 0.68978\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5918 - accuracy: 0.6787 - val_loss: 0.7137 - val_accuracy: 0.5347\n",
      "Epoch 9/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.5187 - accuracy: 0.7413\n",
      "Epoch 9: val_loss did not improve from 0.68978\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5235 - accuracy: 0.7462 - val_loss: 0.7170 - val_accuracy: 0.5347\n",
      "Epoch 10/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.5164 - accuracy: 0.7227\n",
      "Epoch 10: val_loss did not improve from 0.68978\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5263 - accuracy: 0.7192 - val_loss: 0.7187 - val_accuracy: 0.5347\n",
      "Epoch 11/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.5399 - accuracy: 0.7361\n",
      "Epoch 11: val_loss did not improve from 0.68978\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5330 - accuracy: 0.7402 - val_loss: 0.7204 - val_accuracy: 0.5347\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4580 - accuracy: 0.7793\n",
      "Epoch 12: val_loss did not improve from 0.68978\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4580 - accuracy: 0.7793 - val_loss: 0.7218 - val_accuracy: 0.5347\n",
      "Epoch 13/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.5112 - accuracy: 0.7656\n",
      "Epoch 13: val_loss did not improve from 0.68978\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5074 - accuracy: 0.7658 - val_loss: 0.7235 - val_accuracy: 0.5347\n",
      "Epoch 14/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4646 - accuracy: 0.7743\n",
      "Epoch 14: val_loss did not improve from 0.68978\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4649 - accuracy: 0.7703 - val_loss: 0.7281 - val_accuracy: 0.5347\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4275 - accuracy: 0.8318\n",
      "Epoch 15: val_loss did not improve from 0.68978\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4275 - accuracy: 0.8318 - val_loss: 0.7277 - val_accuracy: 0.5347\n",
      "Epoch 16/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4278 - accuracy: 0.8125Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.68978\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4234 - accuracy: 0.8078 - val_loss: 0.7269 - val_accuracy: 0.5347\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5714\n",
      "Test AUC for Layer 3: 0.4117\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5318\n",
      "Average Test AUC across all layers: 0.4867\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_154\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_592 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_438 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_308 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_593 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_439 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_309 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_594 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_440 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_595 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      " 9/13 [===================>..........] - ETA: 0s - loss: 0.9111 - accuracy: 0.5000\n",
      "Epoch 1: val_loss improved from inf to 0.71054, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 31ms/step - loss: 0.8692 - accuracy: 0.5262 - val_loss: 0.7105 - val_accuracy: 0.3053\n",
      "Epoch 2/100\n",
      " 9/13 [===================>..........] - ETA: 0s - loss: 0.7574 - accuracy: 0.6042\n",
      "Epoch 2: val_loss did not improve from 0.71054\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7571 - accuracy: 0.5885 - val_loss: 0.7264 - val_accuracy: 0.3053\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7110 - accuracy: 0.6278\n",
      "Epoch 3: val_loss did not improve from 0.71054\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.7014 - accuracy: 0.6234 - val_loss: 0.7389 - val_accuracy: 0.3053\n",
      "Epoch 4/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.6773 - accuracy: 0.6146\n",
      "Epoch 4: val_loss did not improve from 0.71054\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6743 - accuracy: 0.6185 - val_loss: 0.7496 - val_accuracy: 0.3053\n",
      "Epoch 5/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.6139 - accuracy: 0.6693\n",
      "Epoch 5: val_loss did not improve from 0.71054\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6104 - accuracy: 0.6733 - val_loss: 0.7594 - val_accuracy: 0.3053\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5842 - accuracy: 0.6534\n",
      "Epoch 6: val_loss did not improve from 0.71054\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5917 - accuracy: 0.6484 - val_loss: 0.7653 - val_accuracy: 0.3053\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5643 - accuracy: 0.6989\n",
      "Epoch 7: val_loss did not improve from 0.71054\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5660 - accuracy: 0.6933 - val_loss: 0.7712 - val_accuracy: 0.3053\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5578 - accuracy: 0.7273\n",
      "Epoch 8: val_loss did not improve from 0.71054\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5765 - accuracy: 0.7107 - val_loss: 0.7796 - val_accuracy: 0.3053\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5412 - accuracy: 0.7307\n",
      "Epoch 9: val_loss did not improve from 0.71054\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5412 - accuracy: 0.7307 - val_loss: 0.7838 - val_accuracy: 0.3053\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4776 - accuracy: 0.7869\n",
      "Epoch 10: val_loss did not improve from 0.71054\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4815 - accuracy: 0.7805 - val_loss: 0.7904 - val_accuracy: 0.3053\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4820 - accuracy: 0.7731\n",
      "Epoch 11: val_loss did not improve from 0.71054\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4820 - accuracy: 0.7731 - val_loss: 0.7946 - val_accuracy: 0.3053\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4397 - accuracy: 0.8210\n",
      "Epoch 12: val_loss did not improve from 0.71054\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4403 - accuracy: 0.8204 - val_loss: 0.7952 - val_accuracy: 0.3053\n",
      "Epoch 13/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4515 - accuracy: 0.7969\n",
      "Epoch 13: val_loss did not improve from 0.71054\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4561 - accuracy: 0.7955 - val_loss: 0.7965 - val_accuracy: 0.3053\n",
      "Epoch 14/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4472 - accuracy: 0.7943\n",
      "Epoch 14: val_loss did not improve from 0.71054\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4507 - accuracy: 0.7905 - val_loss: 0.7998 - val_accuracy: 0.3053\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3872 - accuracy: 0.8454\n",
      "Epoch 15: val_loss did not improve from 0.71054\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3872 - accuracy: 0.8454 - val_loss: 0.7987 - val_accuracy: 0.3053\n",
      "Epoch 16/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3726 - accuracy: 0.8490Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.71054\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3767 - accuracy: 0.8429 - val_loss: 0.8003 - val_accuracy: 0.3053\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.2632\n",
      "Test AUC for Layer 1: 0.4834\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_155\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_596 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_441 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_310 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_597 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_442 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_311 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_598 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_443 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_599 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.9258 - accuracy: 0.5020\n",
      "Epoch 1: val_loss improved from inf to 0.71755, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.9217 - accuracy: 0.5056 - val_loss: 0.7175 - val_accuracy: 0.2632\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8382 - accuracy: 0.5488\n",
      "Epoch 2: val_loss did not improve from 0.71755\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.8358 - accuracy: 0.5489 - val_loss: 0.7348 - val_accuracy: 0.2632\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7411 - accuracy: 0.5938\n",
      "Epoch 3: val_loss did not improve from 0.71755\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7430 - accuracy: 0.5883 - val_loss: 0.7514 - val_accuracy: 0.2632\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6831 - accuracy: 0.6426\n",
      "Epoch 4: val_loss did not improve from 0.71755\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6780 - accuracy: 0.6410 - val_loss: 0.7652 - val_accuracy: 0.2632\n",
      "Epoch 5/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6879 - accuracy: 0.6292\n",
      "Epoch 5: val_loss did not improve from 0.71755\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6769 - accuracy: 0.6391 - val_loss: 0.7787 - val_accuracy: 0.2632\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6367 - accuracy: 0.6406\n",
      "Epoch 6: val_loss did not improve from 0.71755\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6332 - accuracy: 0.6429 - val_loss: 0.7899 - val_accuracy: 0.2632\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5889 - accuracy: 0.6914\n",
      "Epoch 7: val_loss did not improve from 0.71755\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5818 - accuracy: 0.6992 - val_loss: 0.8001 - val_accuracy: 0.2632\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5566 - accuracy: 0.6953\n",
      "Epoch 8: val_loss did not improve from 0.71755\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5531 - accuracy: 0.6936 - val_loss: 0.8082 - val_accuracy: 0.2632\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5347 - accuracy: 0.7559\n",
      "Epoch 9: val_loss did not improve from 0.71755\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5306 - accuracy: 0.7556 - val_loss: 0.8111 - val_accuracy: 0.2632\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4878 - accuracy: 0.7539\n",
      "Epoch 10: val_loss did not improve from 0.71755\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4855 - accuracy: 0.7556 - val_loss: 0.8198 - val_accuracy: 0.2632\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5041 - accuracy: 0.7344\n",
      "Epoch 11: val_loss did not improve from 0.71755\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5053 - accuracy: 0.7350 - val_loss: 0.8277 - val_accuracy: 0.2632\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4747 - accuracy: 0.7793\n",
      "Epoch 12: val_loss did not improve from 0.71755\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4765 - accuracy: 0.7801 - val_loss: 0.8237 - val_accuracy: 0.2632\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4374 - accuracy: 0.8027\n",
      "Epoch 13: val_loss did not improve from 0.71755\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4374 - accuracy: 0.8064 - val_loss: 0.8271 - val_accuracy: 0.2632\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4331 - accuracy: 0.8008\n",
      "Epoch 14: val_loss did not improve from 0.71755\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4433 - accuracy: 0.7914 - val_loss: 0.8252 - val_accuracy: 0.2632\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4039 - accuracy: 0.8125\n",
      "Epoch 15: val_loss did not improve from 0.71755\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4032 - accuracy: 0.8139 - val_loss: 0.8254 - val_accuracy: 0.2632\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4219 - accuracy: 0.8164Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.71755\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4265 - accuracy: 0.8139 - val_loss: 0.8200 - val_accuracy: 0.2632\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.3861\n",
      "Test AUC for Layer 2: 0.5926\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_156\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_600 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_444 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_312 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_601 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_445 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_313 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_602 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_446 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_603 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8663 - accuracy: 0.5240\n",
      "Epoch 1: val_loss improved from inf to 0.69769, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 22ms/step - loss: 0.8663 - accuracy: 0.5240 - val_loss: 0.6977 - val_accuracy: 0.3861\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7992 - accuracy: 0.5586\n",
      "Epoch 2: val_loss did not improve from 0.69769\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7992 - accuracy: 0.5586 - val_loss: 0.6984 - val_accuracy: 0.3861\n",
      "Epoch 3/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.7179 - accuracy: 0.5977\n",
      "Epoch 3: val_loss did not improve from 0.69769\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6944 - accuracy: 0.6066 - val_loss: 0.6977 - val_accuracy: 0.3663\n",
      "Epoch 4/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.6931 - accuracy: 0.6285\n",
      "Epoch 4: val_loss improved from 0.69769 to 0.69448, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6849 - accuracy: 0.6351 - val_loss: 0.6945 - val_accuracy: 0.4059\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6377 - accuracy: 0.6562\n",
      "Epoch 5: val_loss improved from 0.69448 to 0.69090, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.6377 - accuracy: 0.6562 - val_loss: 0.6909 - val_accuracy: 0.5842\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5872 - accuracy: 0.7027\n",
      "Epoch 6: val_loss improved from 0.69090 to 0.68704, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5872 - accuracy: 0.7027 - val_loss: 0.6870 - val_accuracy: 0.5941\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5642 - accuracy: 0.7162\n",
      "Epoch 7: val_loss improved from 0.68704 to 0.68142, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5642 - accuracy: 0.7162 - val_loss: 0.6814 - val_accuracy: 0.6139\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5554 - accuracy: 0.7252\n",
      "Epoch 8: val_loss improved from 0.68142 to 0.67507, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5554 - accuracy: 0.7252 - val_loss: 0.6751 - val_accuracy: 0.6139\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5422 - accuracy: 0.7342\n",
      "Epoch 9: val_loss improved from 0.67507 to 0.67110, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5422 - accuracy: 0.7342 - val_loss: 0.6711 - val_accuracy: 0.6139\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4889 - accuracy: 0.7492\n",
      "Epoch 10: val_loss improved from 0.67110 to 0.66833, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4889 - accuracy: 0.7492 - val_loss: 0.6683 - val_accuracy: 0.6139\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4826 - accuracy: 0.7628\n",
      "Epoch 11: val_loss improved from 0.66833 to 0.66483, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4826 - accuracy: 0.7628 - val_loss: 0.6648 - val_accuracy: 0.6139\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4411 - accuracy: 0.8003\n",
      "Epoch 12: val_loss improved from 0.66483 to 0.66236, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4411 - accuracy: 0.8003 - val_loss: 0.6624 - val_accuracy: 0.6139\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4349 - accuracy: 0.7883\n",
      "Epoch 13: val_loss improved from 0.66236 to 0.65980, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4349 - accuracy: 0.7883 - val_loss: 0.6598 - val_accuracy: 0.6139\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4191 - accuracy: 0.8123\n",
      "Epoch 14: val_loss improved from 0.65980 to 0.65740, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4191 - accuracy: 0.8123 - val_loss: 0.6574 - val_accuracy: 0.6139\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4308 - accuracy: 0.7958\n",
      "Epoch 15: val_loss improved from 0.65740 to 0.65706, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4308 - accuracy: 0.7958 - val_loss: 0.6571 - val_accuracy: 0.6139\n",
      "Epoch 16/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.3899 - accuracy: 0.8254\n",
      "Epoch 16: val_loss improved from 0.65706 to 0.65535, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.3885 - accuracy: 0.8228 - val_loss: 0.6554 - val_accuracy: 0.6040\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3610 - accuracy: 0.8559\n",
      "Epoch 17: val_loss improved from 0.65535 to 0.65441, saving model to OpenAI_MLP_XGB/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.3610 - accuracy: 0.8559 - val_loss: 0.6544 - val_accuracy: 0.6139\n",
      "Epoch 18/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3551 - accuracy: 0.8529\n",
      "Epoch 18: val_loss did not improve from 0.65441\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3551 - accuracy: 0.8529 - val_loss: 0.6550 - val_accuracy: 0.5941\n",
      "Epoch 19/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.3338 - accuracy: 0.8576\n",
      "Epoch 19: val_loss did not improve from 0.65441\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3337 - accuracy: 0.8589 - val_loss: 0.6571 - val_accuracy: 0.5941\n",
      "Epoch 20/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3397 - accuracy: 0.8604\n",
      "Epoch 20: val_loss did not improve from 0.65441\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3397 - accuracy: 0.8604 - val_loss: 0.6575 - val_accuracy: 0.5743\n",
      "Epoch 21/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3213 - accuracy: 0.8679\n",
      "Epoch 21: val_loss did not improve from 0.65441\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3213 - accuracy: 0.8679 - val_loss: 0.6587 - val_accuracy: 0.6040\n",
      "Epoch 22/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.3124 - accuracy: 0.8768\n",
      "Epoch 22: val_loss did not improve from 0.65441\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3098 - accuracy: 0.8859 - val_loss: 0.6583 - val_accuracy: 0.5941\n",
      "Epoch 23/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2832 - accuracy: 0.8958\n",
      "Epoch 23: val_loss did not improve from 0.65441\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2972 - accuracy: 0.8889 - val_loss: 0.6554 - val_accuracy: 0.5842\n",
      "Epoch 24/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2960 - accuracy: 0.8904\n",
      "Epoch 24: val_loss did not improve from 0.65441\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2960 - accuracy: 0.8904 - val_loss: 0.6589 - val_accuracy: 0.5941\n",
      "Epoch 25/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2694 - accuracy: 0.8919\n",
      "Epoch 25: val_loss did not improve from 0.65441\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2694 - accuracy: 0.8919 - val_loss: 0.6587 - val_accuracy: 0.6040\n",
      "Epoch 26/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.2742 - accuracy: 0.9026\n",
      "Epoch 26: val_loss did not improve from 0.65441\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2779 - accuracy: 0.9009 - val_loss: 0.6576 - val_accuracy: 0.6238\n",
      "Epoch 27/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2544 - accuracy: 0.9062\n",
      "Epoch 27: val_loss did not improve from 0.65441\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2533 - accuracy: 0.9084 - val_loss: 0.6572 - val_accuracy: 0.6337\n",
      "Epoch 28/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2443 - accuracy: 0.9243\n",
      "Epoch 28: val_loss did not improve from 0.65441\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2434 - accuracy: 0.9234 - val_loss: 0.6670 - val_accuracy: 0.5941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.9204\n",
      "Epoch 29: val_loss did not improve from 0.65441\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2267 - accuracy: 0.9204 - val_loss: 0.6759 - val_accuracy: 0.6040\n",
      "Epoch 30/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2325 - accuracy: 0.9219\n",
      "Epoch 30: val_loss did not improve from 0.65441\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2351 - accuracy: 0.9189 - val_loss: 0.6844 - val_accuracy: 0.6139\n",
      "Epoch 31/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2163 - accuracy: 0.9234\n",
      "Epoch 31: val_loss did not improve from 0.65441\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2163 - accuracy: 0.9234 - val_loss: 0.6845 - val_accuracy: 0.6040\n",
      "Epoch 32/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2211 - accuracy: 0.9236Restoring model weights from the end of the best epoch: 17.\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.65441\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2153 - accuracy: 0.9279 - val_loss: 0.6916 - val_accuracy: 0.6238\n",
      "Epoch 32: early stopping\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5000\n",
      "Test AUC for Layer 3: 0.4485\n",
      "\n",
      "Average Test Accuracy across all layers: 0.3831\n",
      "Average Test AUC across all layers: 0.5082\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Title and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.62034\n",
      "[1]\tvalidation_0-auc:0.55285\n",
      "[2]\tvalidation_0-auc:0.54572\n",
      "[3]\tvalidation_0-auc:0.52516\n",
      "[4]\tvalidation_0-auc:0.53822\n",
      "[5]\tvalidation_0-auc:0.56338\n",
      "[6]\tvalidation_0-auc:0.58853\n",
      "[7]\tvalidation_0-auc:0.57317\n",
      "[8]\tvalidation_0-auc:0.57136\n",
      "[9]\tvalidation_0-auc:0.55902\n",
      "[10]\tvalidation_0-auc:0.55370\n",
      "[11]\tvalidation_0-auc:0.55104\n",
      "[12]\tvalidation_0-auc:0.53955\n",
      "[13]\tvalidation_0-auc:0.51064\n",
      "[14]\tvalidation_0-auc:0.52238\n",
      "[15]\tvalidation_0-auc:0.53495\n",
      "[16]\tvalidation_0-auc:0.52564\n",
      "[17]\tvalidation_0-auc:0.51330\n",
      "[18]\tvalidation_0-auc:0.50000\n",
      "[19]\tvalidation_0-auc:0.51802\n",
      "[20]\tvalidation_0-auc:0.51355\n",
      "[21]\tvalidation_0-auc:0.49843\n",
      "[22]\tvalidation_0-auc:0.50581\n",
      "[23]\tvalidation_0-auc:0.50363\n",
      "[24]\tvalidation_0-auc:0.52298\n",
      "[25]\tvalidation_0-auc:0.52927\n",
      "[26]\tvalidation_0-auc:0.52443\n",
      "[27]\tvalidation_0-auc:0.52455\n",
      "[28]\tvalidation_0-auc:0.51536\n",
      "[29]\tvalidation_0-auc:0.50677\n",
      "[30]\tvalidation_0-auc:0.51560\n",
      "[31]\tvalidation_0-auc:0.50629\n",
      "[32]\tvalidation_0-auc:0.49686\n",
      "[33]\tvalidation_0-auc:0.49661\n",
      "[34]\tvalidation_0-auc:0.49323\n",
      "[35]\tvalidation_0-auc:0.48428\n",
      "[36]\tvalidation_0-auc:0.48500\n",
      "[37]\tvalidation_0-auc:0.48428\n",
      "[38]\tvalidation_0-auc:0.48512\n",
      "[39]\tvalidation_0-auc:0.48573\n",
      "[40]\tvalidation_0-auc:0.49299\n",
      "[41]\tvalidation_0-auc:0.48887\n",
      "[42]\tvalidation_0-auc:0.50218\n",
      "[43]\tvalidation_0-auc:0.50169\n",
      "[44]\tvalidation_0-auc:0.50677\n",
      "[45]\tvalidation_0-auc:0.50677\n",
      "[46]\tvalidation_0-auc:0.51113\n",
      "[47]\tvalidation_0-auc:0.51113\n",
      "[48]\tvalidation_0-auc:0.50460\n",
      "[49]\tvalidation_0-auc:0.49686\n",
      "[50]\tvalidation_0-auc:0.50218\n",
      "[51]\tvalidation_0-auc:0.51209\n",
      "[52]\tvalidation_0-auc:0.51016\n",
      "[53]\tvalidation_0-auc:0.51234\n",
      "[54]\tvalidation_0-auc:0.51149\n",
      "[55]\tvalidation_0-auc:0.51040\n",
      "[56]\tvalidation_0-auc:0.51355\n",
      "[57]\tvalidation_0-auc:0.51089\n",
      "[58]\tvalidation_0-auc:0.51984\n",
      "[59]\tvalidation_0-auc:0.51766\n",
      "[60]\tvalidation_0-auc:0.52177\n",
      "[61]\tvalidation_0-auc:0.52129\n",
      "[62]\tvalidation_0-auc:0.51959\n",
      "[63]\tvalidation_0-auc:0.52806\n",
      "[64]\tvalidation_0-auc:0.52080\n",
      "[65]\tvalidation_0-auc:0.51947\n",
      "[66]\tvalidation_0-auc:0.52165\n",
      "[67]\tvalidation_0-auc:0.51524\n",
      "[68]\tvalidation_0-auc:0.51379\n",
      "[69]\tvalidation_0-auc:0.50847\n",
      "[70]\tvalidation_0-auc:0.50290\n",
      "[71]\tvalidation_0-auc:0.51016\n",
      "[72]\tvalidation_0-auc:0.50943\n",
      "[73]\tvalidation_0-auc:0.50919\n",
      "[74]\tvalidation_0-auc:0.51149\n",
      "[75]\tvalidation_0-auc:0.51681\n",
      "[76]\tvalidation_0-auc:0.51766\n",
      "[77]\tvalidation_0-auc:0.51451\n",
      "[78]\tvalidation_0-auc:0.51500\n",
      "[79]\tvalidation_0-auc:0.52104\n",
      "Test Accuracy for Layer 1: 0.5489\n",
      "Test AUC for Layer 1: 0.5432\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.53402\n",
      "[1]\tvalidation_0-auc:0.54783\n",
      "[2]\tvalidation_0-auc:0.56393\n",
      "[3]\tvalidation_0-auc:0.58927\n",
      "[4]\tvalidation_0-auc:0.56963\n",
      "[5]\tvalidation_0-auc:0.58162\n",
      "[6]\tvalidation_0-auc:0.55868\n",
      "[7]\tvalidation_0-auc:0.54509\n",
      "[8]\tvalidation_0-auc:0.53276\n",
      "[9]\tvalidation_0-auc:0.50594\n",
      "[10]\tvalidation_0-auc:0.52203\n",
      "[11]\tvalidation_0-auc:0.51861\n",
      "[12]\tvalidation_0-auc:0.50753\n",
      "[13]\tvalidation_0-auc:0.48813\n",
      "[14]\tvalidation_0-auc:0.50559\n",
      "[15]\tvalidation_0-auc:0.51884\n",
      "[16]\tvalidation_0-auc:0.51495\n",
      "[17]\tvalidation_0-auc:0.52911\n",
      "[18]\tvalidation_0-auc:0.52329\n",
      "[19]\tvalidation_0-auc:0.51370\n",
      "[20]\tvalidation_0-auc:0.52865\n",
      "[21]\tvalidation_0-auc:0.52637\n",
      "[22]\tvalidation_0-auc:0.52934\n",
      "[23]\tvalidation_0-auc:0.52957\n",
      "[24]\tvalidation_0-auc:0.52717\n",
      "[25]\tvalidation_0-auc:0.51895\n",
      "[26]\tvalidation_0-auc:0.53025\n",
      "[27]\tvalidation_0-auc:0.53904\n",
      "[28]\tvalidation_0-auc:0.53390\n",
      "[29]\tvalidation_0-auc:0.53059\n",
      "[30]\tvalidation_0-auc:0.52466\n",
      "[31]\tvalidation_0-auc:0.51724\n",
      "[32]\tvalidation_0-auc:0.52123\n",
      "[33]\tvalidation_0-auc:0.51347\n",
      "[34]\tvalidation_0-auc:0.50788\n",
      "[35]\tvalidation_0-auc:0.50468\n",
      "[36]\tvalidation_0-auc:0.51610\n",
      "[37]\tvalidation_0-auc:0.52146\n",
      "[38]\tvalidation_0-auc:0.51438\n",
      "[39]\tvalidation_0-auc:0.50959\n",
      "[40]\tvalidation_0-auc:0.50285\n",
      "[41]\tvalidation_0-auc:0.50776\n",
      "[42]\tvalidation_0-auc:0.49658\n",
      "[43]\tvalidation_0-auc:0.49384\n",
      "[44]\tvalidation_0-auc:0.48676\n",
      "[45]\tvalidation_0-auc:0.49680\n",
      "[46]\tvalidation_0-auc:0.49543\n",
      "[47]\tvalidation_0-auc:0.49749\n",
      "[48]\tvalidation_0-auc:0.48721\n",
      "[49]\tvalidation_0-auc:0.49509\n",
      "[50]\tvalidation_0-auc:0.48790\n",
      "[51]\tvalidation_0-auc:0.49429\n",
      "[52]\tvalidation_0-auc:0.49361\n",
      "[53]\tvalidation_0-auc:0.49486\n",
      "[54]\tvalidation_0-auc:0.49452\n",
      "[55]\tvalidation_0-auc:0.49566\n",
      "[56]\tvalidation_0-auc:0.49817\n",
      "[57]\tvalidation_0-auc:0.50103\n",
      "[58]\tvalidation_0-auc:0.50354\n",
      "[59]\tvalidation_0-auc:0.50571\n",
      "[60]\tvalidation_0-auc:0.50228\n",
      "[61]\tvalidation_0-auc:0.50126\n",
      "[62]\tvalidation_0-auc:0.50274\n",
      "[63]\tvalidation_0-auc:0.49589\n",
      "[64]\tvalidation_0-auc:0.49475\n",
      "[65]\tvalidation_0-auc:0.49521\n",
      "[66]\tvalidation_0-auc:0.49384\n",
      "[67]\tvalidation_0-auc:0.49749\n",
      "[68]\tvalidation_0-auc:0.49064\n",
      "[69]\tvalidation_0-auc:0.49098\n",
      "[70]\tvalidation_0-auc:0.49646\n",
      "[71]\tvalidation_0-auc:0.49132\n",
      "[72]\tvalidation_0-auc:0.49224\n",
      "[73]\tvalidation_0-auc:0.49498\n",
      "[74]\tvalidation_0-auc:0.49269\n",
      "[75]\tvalidation_0-auc:0.49155\n",
      "[76]\tvalidation_0-auc:0.48584\n",
      "[77]\tvalidation_0-auc:0.49041\n",
      "[78]\tvalidation_0-auc:0.49372\n",
      "[79]\tvalidation_0-auc:0.49201\n",
      "Test Accuracy for Layer 2: 0.5347\n",
      "Test AUC for Layer 2: 0.5508\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.39441\n",
      "[1]\tvalidation_0-auc:0.41214\n",
      "[2]\tvalidation_0-auc:0.43499\n",
      "[3]\tvalidation_0-auc:0.42100\n",
      "[4]\tvalidation_0-auc:0.41568\n",
      "[5]\tvalidation_0-auc:0.42474\n",
      "[6]\tvalidation_0-auc:0.42770\n",
      "[7]\tvalidation_0-auc:0.41371\n",
      "[8]\tvalidation_0-auc:0.46355\n",
      "[9]\tvalidation_0-auc:0.48306\n",
      "[10]\tvalidation_0-auc:0.53290\n",
      "[11]\tvalidation_0-auc:0.50749\n",
      "[12]\tvalidation_0-auc:0.49744\n",
      "[13]\tvalidation_0-auc:0.54452\n",
      "[14]\tvalidation_0-auc:0.53704\n",
      "[15]\tvalidation_0-auc:0.52994\n",
      "[16]\tvalidation_0-auc:0.50729\n",
      "[17]\tvalidation_0-auc:0.51576\n",
      "[18]\tvalidation_0-auc:0.51458\n",
      "[19]\tvalidation_0-auc:0.50867\n",
      "[20]\tvalidation_0-auc:0.51261\n",
      "[21]\tvalidation_0-auc:0.52837\n",
      "[22]\tvalidation_0-auc:0.52226\n",
      "[23]\tvalidation_0-auc:0.50749\n",
      "[24]\tvalidation_0-auc:0.52088\n",
      "[25]\tvalidation_0-auc:0.52463\n",
      "[26]\tvalidation_0-auc:0.51635\n",
      "[27]\tvalidation_0-auc:0.53073\n",
      "[28]\tvalidation_0-auc:0.52620\n",
      "[29]\tvalidation_0-auc:0.51202\n",
      "[30]\tvalidation_0-auc:0.52344\n",
      "[31]\tvalidation_0-auc:0.51340\n",
      "[32]\tvalidation_0-auc:0.51537\n",
      "[33]\tvalidation_0-auc:0.51970\n",
      "[34]\tvalidation_0-auc:0.52600\n",
      "[35]\tvalidation_0-auc:0.52325\n",
      "[36]\tvalidation_0-auc:0.52246\n",
      "[37]\tvalidation_0-auc:0.51458\n",
      "[38]\tvalidation_0-auc:0.48818\n",
      "[39]\tvalidation_0-auc:0.49133\n",
      "[40]\tvalidation_0-auc:0.47715\n",
      "[41]\tvalidation_0-auc:0.47557\n",
      "[42]\tvalidation_0-auc:0.46533\n",
      "[43]\tvalidation_0-auc:0.46493\n",
      "[44]\tvalidation_0-auc:0.45311\n",
      "[45]\tvalidation_0-auc:0.45055\n",
      "[46]\tvalidation_0-auc:0.45725\n",
      "[47]\tvalidation_0-auc:0.46395\n",
      "[48]\tvalidation_0-auc:0.46217\n",
      "[49]\tvalidation_0-auc:0.46927\n",
      "[50]\tvalidation_0-auc:0.45981\n",
      "[51]\tvalidation_0-auc:0.45469\n",
      "[52]\tvalidation_0-auc:0.45213\n",
      "[53]\tvalidation_0-auc:0.43775\n",
      "[54]\tvalidation_0-auc:0.43696\n",
      "[55]\tvalidation_0-auc:0.42809\n",
      "[56]\tvalidation_0-auc:0.42691\n",
      "[57]\tvalidation_0-auc:0.43105\n",
      "[58]\tvalidation_0-auc:0.42750\n",
      "[59]\tvalidation_0-auc:0.42987\n",
      "[60]\tvalidation_0-auc:0.43538\n",
      "[61]\tvalidation_0-auc:0.42987\n",
      "[62]\tvalidation_0-auc:0.43735\n",
      "[63]\tvalidation_0-auc:0.43381\n",
      "[64]\tvalidation_0-auc:0.42593\n",
      "[65]\tvalidation_0-auc:0.42277\n",
      "[66]\tvalidation_0-auc:0.42730\n",
      "[67]\tvalidation_0-auc:0.42356\n",
      "[68]\tvalidation_0-auc:0.43144\n",
      "[69]\tvalidation_0-auc:0.43026\n",
      "[70]\tvalidation_0-auc:0.42868\n",
      "[71]\tvalidation_0-auc:0.42632\n",
      "[72]\tvalidation_0-auc:0.43262\n",
      "[73]\tvalidation_0-auc:0.43262\n",
      "[74]\tvalidation_0-auc:0.43223\n",
      "[75]\tvalidation_0-auc:0.43656\n",
      "[76]\tvalidation_0-auc:0.43893\n",
      "[77]\tvalidation_0-auc:0.43952\n",
      "[78]\tvalidation_0-auc:0.45035\n",
      "[79]\tvalidation_0-auc:0.44878\n",
      "Test Accuracy for Layer 3: 0.5714\n",
      "Test AUC for Layer 3: 0.4892\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5517\n",
      "Average Test AUC across all layers: 0.5277\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Title and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.47225\n",
      "[1]\tvalidation_0-auc:0.46868\n",
      "[2]\tvalidation_0-auc:0.48997\n",
      "[3]\tvalidation_0-auc:0.48187\n",
      "[4]\tvalidation_0-auc:0.49945\n",
      "[5]\tvalidation_0-auc:0.51470\n",
      "[6]\tvalidation_0-auc:0.51415\n",
      "[7]\tvalidation_0-auc:0.49547\n",
      "[8]\tvalidation_0-auc:0.51154\n",
      "[9]\tvalidation_0-auc:0.50701\n",
      "[10]\tvalidation_0-auc:0.51181\n",
      "[11]\tvalidation_0-auc:0.51044\n",
      "[12]\tvalidation_0-auc:0.50508\n",
      "[13]\tvalidation_0-auc:0.48530\n",
      "[14]\tvalidation_0-auc:0.48201\n",
      "[15]\tvalidation_0-auc:0.47473\n",
      "[16]\tvalidation_0-auc:0.46580\n",
      "[17]\tvalidation_0-auc:0.46456\n",
      "[18]\tvalidation_0-auc:0.43997\n",
      "[19]\tvalidation_0-auc:0.44588\n",
      "[20]\tvalidation_0-auc:0.45852\n",
      "[21]\tvalidation_0-auc:0.45508\n",
      "[22]\tvalidation_0-auc:0.46168\n",
      "[23]\tvalidation_0-auc:0.47527\n",
      "[24]\tvalidation_0-auc:0.47582\n",
      "[25]\tvalidation_0-auc:0.46607\n",
      "[26]\tvalidation_0-auc:0.45742\n",
      "[27]\tvalidation_0-auc:0.47363\n",
      "[28]\tvalidation_0-auc:0.48352\n",
      "[29]\tvalidation_0-auc:0.48970\n",
      "[30]\tvalidation_0-auc:0.48324\n",
      "[31]\tvalidation_0-auc:0.48159\n",
      "[32]\tvalidation_0-auc:0.48709\n",
      "[33]\tvalidation_0-auc:0.48777\n",
      "[34]\tvalidation_0-auc:0.49396\n",
      "[35]\tvalidation_0-auc:0.48791\n",
      "[36]\tvalidation_0-auc:0.49231\n",
      "[37]\tvalidation_0-auc:0.48352\n",
      "[38]\tvalidation_0-auc:0.48681\n",
      "[39]\tvalidation_0-auc:0.49615\n",
      "[40]\tvalidation_0-auc:0.49093\n",
      "[41]\tvalidation_0-auc:0.49808\n",
      "[42]\tvalidation_0-auc:0.49890\n",
      "[43]\tvalidation_0-auc:0.50522\n",
      "[44]\tvalidation_0-auc:0.50879\n",
      "[45]\tvalidation_0-auc:0.51016\n",
      "[46]\tvalidation_0-auc:0.51209\n",
      "[47]\tvalidation_0-auc:0.50742\n",
      "[48]\tvalidation_0-auc:0.50549\n",
      "[49]\tvalidation_0-auc:0.50275\n",
      "[50]\tvalidation_0-auc:0.50398\n",
      "[51]\tvalidation_0-auc:0.51071\n",
      "[52]\tvalidation_0-auc:0.50604\n",
      "[53]\tvalidation_0-auc:0.50687\n",
      "[54]\tvalidation_0-auc:0.49753\n",
      "[55]\tvalidation_0-auc:0.50247\n",
      "[56]\tvalidation_0-auc:0.50508\n",
      "[57]\tvalidation_0-auc:0.50082\n",
      "[58]\tvalidation_0-auc:0.49643\n",
      "[59]\tvalidation_0-auc:0.49176\n",
      "[60]\tvalidation_0-auc:0.49396\n",
      "[61]\tvalidation_0-auc:0.48819\n",
      "[62]\tvalidation_0-auc:0.48695\n",
      "[63]\tvalidation_0-auc:0.48571\n",
      "[64]\tvalidation_0-auc:0.48901\n",
      "[65]\tvalidation_0-auc:0.48681\n",
      "[66]\tvalidation_0-auc:0.48846\n",
      "[67]\tvalidation_0-auc:0.48104\n",
      "[68]\tvalidation_0-auc:0.48269\n",
      "[69]\tvalidation_0-auc:0.48297\n",
      "[70]\tvalidation_0-auc:0.48159\n",
      "[71]\tvalidation_0-auc:0.48654\n",
      "[72]\tvalidation_0-auc:0.48571\n",
      "[73]\tvalidation_0-auc:0.48159\n",
      "[74]\tvalidation_0-auc:0.48077\n",
      "[75]\tvalidation_0-auc:0.48297\n",
      "[76]\tvalidation_0-auc:0.48242\n",
      "[77]\tvalidation_0-auc:0.48077\n",
      "[78]\tvalidation_0-auc:0.48242\n",
      "[79]\tvalidation_0-auc:0.47940\n",
      "Test Accuracy for Layer 1: 0.7368\n",
      "Test AUC for Layer 1: 0.5685\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.49242\n",
      "[1]\tvalidation_0-auc:0.57332\n",
      "[2]\tvalidation_0-auc:0.56443\n",
      "[3]\tvalidation_0-auc:0.53455\n",
      "[4]\tvalidation_0-auc:0.51851\n",
      "[5]\tvalidation_0-auc:0.53615\n",
      "[6]\tvalidation_0-auc:0.54636\n",
      "[7]\tvalidation_0-auc:0.54679\n",
      "[8]\tvalidation_0-auc:0.54781\n",
      "[9]\tvalidation_0-auc:0.57274\n",
      "[10]\tvalidation_0-auc:0.59373\n",
      "[11]\tvalidation_0-auc:0.60583\n",
      "[12]\tvalidation_0-auc:0.57741\n",
      "[13]\tvalidation_0-auc:0.56633\n",
      "[14]\tvalidation_0-auc:0.57915\n",
      "[15]\tvalidation_0-auc:0.56414\n",
      "[16]\tvalidation_0-auc:0.56181\n",
      "[17]\tvalidation_0-auc:0.55802\n",
      "[18]\tvalidation_0-auc:0.52872\n",
      "[19]\tvalidation_0-auc:0.55991\n",
      "[20]\tvalidation_0-auc:0.56501\n",
      "[21]\tvalidation_0-auc:0.56822\n",
      "[22]\tvalidation_0-auc:0.56356\n",
      "[23]\tvalidation_0-auc:0.56064\n",
      "[24]\tvalidation_0-auc:0.56414\n",
      "[25]\tvalidation_0-auc:0.57085\n",
      "[26]\tvalidation_0-auc:0.57653\n",
      "[27]\tvalidation_0-auc:0.55802\n",
      "[28]\tvalidation_0-auc:0.56968\n",
      "[29]\tvalidation_0-auc:0.55656\n",
      "[30]\tvalidation_0-auc:0.55175\n",
      "[31]\tvalidation_0-auc:0.55816\n",
      "[32]\tvalidation_0-auc:0.55248\n",
      "[33]\tvalidation_0-auc:0.56706\n",
      "[34]\tvalidation_0-auc:0.57026\n",
      "[35]\tvalidation_0-auc:0.56647\n",
      "[36]\tvalidation_0-auc:0.56749\n",
      "[37]\tvalidation_0-auc:0.57259\n",
      "[38]\tvalidation_0-auc:0.57114\n",
      "[39]\tvalidation_0-auc:0.56822\n",
      "[40]\tvalidation_0-auc:0.57085\n",
      "[41]\tvalidation_0-auc:0.57726\n",
      "[42]\tvalidation_0-auc:0.57434\n",
      "[43]\tvalidation_0-auc:0.58397\n",
      "[44]\tvalidation_0-auc:0.59111\n",
      "[45]\tvalidation_0-auc:0.60292\n",
      "[46]\tvalidation_0-auc:0.60962\n",
      "[47]\tvalidation_0-auc:0.60787\n",
      "[48]\tvalidation_0-auc:0.61720\n",
      "[49]\tvalidation_0-auc:0.61778\n",
      "[50]\tvalidation_0-auc:0.61341\n",
      "[51]\tvalidation_0-auc:0.61108\n",
      "[52]\tvalidation_0-auc:0.61953\n",
      "[53]\tvalidation_0-auc:0.61837\n",
      "[54]\tvalidation_0-auc:0.61283\n",
      "[55]\tvalidation_0-auc:0.61152\n",
      "[56]\tvalidation_0-auc:0.61385\n",
      "[57]\tvalidation_0-auc:0.61122\n",
      "[58]\tvalidation_0-auc:0.61618\n",
      "[59]\tvalidation_0-auc:0.61895\n",
      "[60]\tvalidation_0-auc:0.60991\n",
      "[61]\tvalidation_0-auc:0.61254\n",
      "[62]\tvalidation_0-auc:0.60904\n",
      "[63]\tvalidation_0-auc:0.60612\n",
      "[64]\tvalidation_0-auc:0.61341\n",
      "[65]\tvalidation_0-auc:0.61662\n",
      "[66]\tvalidation_0-auc:0.61808\n",
      "[67]\tvalidation_0-auc:0.62245\n",
      "[68]\tvalidation_0-auc:0.62303\n",
      "[69]\tvalidation_0-auc:0.62449\n",
      "[70]\tvalidation_0-auc:0.62624\n",
      "[71]\tvalidation_0-auc:0.62391\n",
      "[72]\tvalidation_0-auc:0.62668\n",
      "[73]\tvalidation_0-auc:0.62624\n",
      "[74]\tvalidation_0-auc:0.62245\n",
      "[75]\tvalidation_0-auc:0.62070\n",
      "[76]\tvalidation_0-auc:0.62449\n",
      "[77]\tvalidation_0-auc:0.62128\n",
      "[78]\tvalidation_0-auc:0.62128\n",
      "[79]\tvalidation_0-auc:0.61633\n",
      "Test Accuracy for Layer 2: 0.6139\n",
      "Test AUC for Layer 2: 0.5554\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.52833\n",
      "[1]\tvalidation_0-auc:0.42122\n",
      "[2]\tvalidation_0-auc:0.47312\n",
      "[3]\tvalidation_0-auc:0.47932\n",
      "[4]\tvalidation_0-auc:0.54880\n",
      "[5]\tvalidation_0-auc:0.52523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\tvalidation_0-auc:0.56266\n",
      "[7]\tvalidation_0-auc:0.59181\n",
      "[8]\tvalidation_0-auc:0.59553\n",
      "[9]\tvalidation_0-auc:0.61993\n",
      "[10]\tvalidation_0-auc:0.58644\n",
      "[11]\tvalidation_0-auc:0.56121\n",
      "[12]\tvalidation_0-auc:0.56307\n",
      "[13]\tvalidation_0-auc:0.55252\n",
      "[14]\tvalidation_0-auc:0.58065\n",
      "[15]\tvalidation_0-auc:0.54363\n",
      "[16]\tvalidation_0-auc:0.53929\n",
      "[17]\tvalidation_0-auc:0.52192\n",
      "[18]\tvalidation_0-auc:0.52316\n",
      "[19]\tvalidation_0-auc:0.53991\n",
      "[20]\tvalidation_0-auc:0.52771\n",
      "[21]\tvalidation_0-auc:0.52192\n",
      "[22]\tvalidation_0-auc:0.51261\n",
      "[23]\tvalidation_0-auc:0.51489\n",
      "[24]\tvalidation_0-auc:0.51406\n",
      "[25]\tvalidation_0-auc:0.50124\n",
      "[26]\tvalidation_0-auc:0.51427\n",
      "[27]\tvalidation_0-auc:0.51572\n",
      "[28]\tvalidation_0-auc:0.51799\n",
      "[29]\tvalidation_0-auc:0.52523\n",
      "[30]\tvalidation_0-auc:0.53102\n",
      "[31]\tvalidation_0-auc:0.52833\n",
      "[32]\tvalidation_0-auc:0.52978\n",
      "[33]\tvalidation_0-auc:0.52667\n",
      "[34]\tvalidation_0-auc:0.52502\n",
      "[35]\tvalidation_0-auc:0.52068\n",
      "[36]\tvalidation_0-auc:0.51861\n",
      "[37]\tvalidation_0-auc:0.52357\n",
      "[38]\tvalidation_0-auc:0.52026\n",
      "[39]\tvalidation_0-auc:0.51716\n",
      "[40]\tvalidation_0-auc:0.53226\n",
      "[41]\tvalidation_0-auc:0.52936\n",
      "[42]\tvalidation_0-auc:0.52109\n",
      "[43]\tvalidation_0-auc:0.51551\n",
      "[44]\tvalidation_0-auc:0.52233\n",
      "[45]\tvalidation_0-auc:0.52523\n",
      "[46]\tvalidation_0-auc:0.53474\n",
      "[47]\tvalidation_0-auc:0.53309\n",
      "[48]\tvalidation_0-auc:0.53453\n",
      "[49]\tvalidation_0-auc:0.53681\n",
      "[50]\tvalidation_0-auc:0.53433\n",
      "[51]\tvalidation_0-auc:0.53598\n",
      "[52]\tvalidation_0-auc:0.54508\n",
      "[53]\tvalidation_0-auc:0.53681\n",
      "[54]\tvalidation_0-auc:0.53350\n",
      "[55]\tvalidation_0-auc:0.53060\n",
      "[56]\tvalidation_0-auc:0.53846\n",
      "[57]\tvalidation_0-auc:0.54177\n",
      "[58]\tvalidation_0-auc:0.53846\n",
      "[59]\tvalidation_0-auc:0.54529\n",
      "[60]\tvalidation_0-auc:0.55397\n",
      "[61]\tvalidation_0-auc:0.54673\n",
      "[62]\tvalidation_0-auc:0.55624\n",
      "[63]\tvalidation_0-auc:0.55521\n",
      "[64]\tvalidation_0-auc:0.55438\n",
      "[65]\tvalidation_0-auc:0.55045\n",
      "[66]\tvalidation_0-auc:0.55831\n",
      "[67]\tvalidation_0-auc:0.54715\n",
      "[68]\tvalidation_0-auc:0.55252\n",
      "[69]\tvalidation_0-auc:0.55211\n",
      "[70]\tvalidation_0-auc:0.55376\n",
      "[71]\tvalidation_0-auc:0.55749\n",
      "[72]\tvalidation_0-auc:0.55170\n",
      "[73]\tvalidation_0-auc:0.55376\n",
      "[74]\tvalidation_0-auc:0.55459\n",
      "[75]\tvalidation_0-auc:0.56162\n",
      "[76]\tvalidation_0-auc:0.56431\n",
      "[77]\tvalidation_0-auc:0.55790\n",
      "[78]\tvalidation_0-auc:0.56162\n",
      "[79]\tvalidation_0-auc:0.56328\n",
      "Test Accuracy for Layer 3: 0.4857\n",
      "Test AUC for Layer 3: 0.3946\n",
      "\n",
      "Average Test Accuracy across all layers: 0.6121\n",
      "Average Test AUC across all layers: 0.5062\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Full text and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.50484\n",
      "[1]\tvalidation_0-auc:0.57886\n",
      "[2]\tvalidation_0-auc:0.57511\n",
      "[3]\tvalidation_0-auc:0.51693\n",
      "[4]\tvalidation_0-auc:0.54221\n",
      "[5]\tvalidation_0-auc:0.52286\n",
      "[6]\tvalidation_0-auc:0.54729\n",
      "[7]\tvalidation_0-auc:0.52903\n",
      "[8]\tvalidation_0-auc:0.55346\n",
      "[9]\tvalidation_0-auc:0.53580\n",
      "[10]\tvalidation_0-auc:0.56858\n",
      "[11]\tvalidation_0-auc:0.55140\n",
      "[12]\tvalidation_0-auc:0.55624\n",
      "[13]\tvalidation_0-auc:0.56507\n",
      "[14]\tvalidation_0-auc:0.53931\n",
      "[15]\tvalidation_0-auc:0.52419\n",
      "[16]\tvalidation_0-auc:0.52879\n",
      "[17]\tvalidation_0-auc:0.54499\n",
      "[18]\tvalidation_0-auc:0.55273\n",
      "[19]\tvalidation_0-auc:0.54741\n",
      "[20]\tvalidation_0-auc:0.55431\n",
      "[21]\tvalidation_0-auc:0.54463\n",
      "[22]\tvalidation_0-auc:0.54378\n",
      "[23]\tvalidation_0-auc:0.53399\n",
      "[24]\tvalidation_0-auc:0.53870\n",
      "[25]\tvalidation_0-auc:0.53628\n",
      "[26]\tvalidation_0-auc:0.52818\n",
      "[27]\tvalidation_0-auc:0.52758\n",
      "[28]\tvalidation_0-auc:0.53241\n",
      "[29]\tvalidation_0-auc:0.53580\n",
      "[30]\tvalidation_0-auc:0.53411\n",
      "[31]\tvalidation_0-auc:0.53967\n",
      "[32]\tvalidation_0-auc:0.54777\n",
      "[33]\tvalidation_0-auc:0.54644\n",
      "[34]\tvalidation_0-auc:0.55237\n",
      "[35]\tvalidation_0-auc:0.55080\n",
      "[36]\tvalidation_0-auc:0.53822\n",
      "[37]\tvalidation_0-auc:0.53604\n",
      "[38]\tvalidation_0-auc:0.55648\n",
      "[39]\tvalidation_0-auc:0.55044\n",
      "[40]\tvalidation_0-auc:0.53991\n",
      "[41]\tvalidation_0-auc:0.52975\n",
      "[42]\tvalidation_0-auc:0.53145\n",
      "[43]\tvalidation_0-auc:0.53834\n",
      "[44]\tvalidation_0-auc:0.54015\n",
      "[45]\tvalidation_0-auc:0.52274\n",
      "[46]\tvalidation_0-auc:0.53108\n",
      "[47]\tvalidation_0-auc:0.51524\n",
      "[48]\tvalidation_0-auc:0.52068\n",
      "[49]\tvalidation_0-auc:0.51754\n",
      "[50]\tvalidation_0-auc:0.51500\n",
      "[51]\tvalidation_0-auc:0.51064\n",
      "[52]\tvalidation_0-auc:0.51064\n",
      "[53]\tvalidation_0-auc:0.51064\n",
      "[54]\tvalidation_0-auc:0.50363\n",
      "[55]\tvalidation_0-auc:0.50387\n",
      "[56]\tvalidation_0-auc:0.51500\n",
      "[57]\tvalidation_0-auc:0.50701\n",
      "[58]\tvalidation_0-auc:0.50169\n",
      "[59]\tvalidation_0-auc:0.49879\n",
      "[60]\tvalidation_0-auc:0.49686\n",
      "[61]\tvalidation_0-auc:0.49976\n",
      "[62]\tvalidation_0-auc:0.49806\n",
      "[63]\tvalidation_0-auc:0.50000\n",
      "[64]\tvalidation_0-auc:0.49927\n",
      "[65]\tvalidation_0-auc:0.49855\n",
      "[66]\tvalidation_0-auc:0.50750\n",
      "[67]\tvalidation_0-auc:0.51403\n",
      "[68]\tvalidation_0-auc:0.51669\n",
      "[69]\tvalidation_0-auc:0.51016\n",
      "[70]\tvalidation_0-auc:0.51355\n",
      "[71]\tvalidation_0-auc:0.51343\n",
      "[72]\tvalidation_0-auc:0.50581\n",
      "[73]\tvalidation_0-auc:0.51161\n",
      "[74]\tvalidation_0-auc:0.51185\n",
      "[75]\tvalidation_0-auc:0.51669\n",
      "[76]\tvalidation_0-auc:0.51863\n",
      "[77]\tvalidation_0-auc:0.51113\n",
      "[78]\tvalidation_0-auc:0.51234\n",
      "[79]\tvalidation_0-auc:0.50774\n",
      "Test Accuracy for Layer 1: 0.5489\n",
      "Test AUC for Layer 1: 0.3775\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.41495\n",
      "[1]\tvalidation_0-auc:0.33767\n",
      "[2]\tvalidation_0-auc:0.37500\n",
      "[3]\tvalidation_0-auc:0.39201\n",
      "[4]\tvalidation_0-auc:0.37146\n",
      "[5]\tvalidation_0-auc:0.37694\n",
      "[6]\tvalidation_0-auc:0.34121\n",
      "[7]\tvalidation_0-auc:0.33676\n",
      "[8]\tvalidation_0-auc:0.33995\n",
      "[9]\tvalidation_0-auc:0.34201\n",
      "[10]\tvalidation_0-auc:0.32089\n",
      "[11]\tvalidation_0-auc:0.32717\n",
      "[12]\tvalidation_0-auc:0.32192\n",
      "[13]\tvalidation_0-auc:0.32854\n",
      "[14]\tvalidation_0-auc:0.34189\n",
      "[15]\tvalidation_0-auc:0.35491\n",
      "[16]\tvalidation_0-auc:0.34600\n",
      "[17]\tvalidation_0-auc:0.34749\n",
      "[18]\tvalidation_0-auc:0.35263\n",
      "[19]\tvalidation_0-auc:0.37260\n",
      "[20]\tvalidation_0-auc:0.37888\n",
      "[21]\tvalidation_0-auc:0.39007\n",
      "[22]\tvalidation_0-auc:0.37717\n",
      "[23]\tvalidation_0-auc:0.37158\n",
      "[24]\tvalidation_0-auc:0.37865\n",
      "[25]\tvalidation_0-auc:0.38231\n",
      "[26]\tvalidation_0-auc:0.38847\n",
      "[27]\tvalidation_0-auc:0.37968\n",
      "[28]\tvalidation_0-auc:0.38082\n",
      "[29]\tvalidation_0-auc:0.37808\n",
      "[30]\tvalidation_0-auc:0.37877\n",
      "[31]\tvalidation_0-auc:0.38836\n",
      "[32]\tvalidation_0-auc:0.39566\n",
      "[33]\tvalidation_0-auc:0.38584\n",
      "[34]\tvalidation_0-auc:0.39292\n",
      "[35]\tvalidation_0-auc:0.38813\n",
      "[36]\tvalidation_0-auc:0.39281\n",
      "[37]\tvalidation_0-auc:0.38562\n",
      "[38]\tvalidation_0-auc:0.39121\n",
      "[39]\tvalidation_0-auc:0.38493\n",
      "[40]\tvalidation_0-auc:0.38584\n",
      "[41]\tvalidation_0-auc:0.38927\n",
      "[42]\tvalidation_0-auc:0.38630\n",
      "[43]\tvalidation_0-auc:0.38756\n",
      "[44]\tvalidation_0-auc:0.38824\n",
      "[45]\tvalidation_0-auc:0.39772\n",
      "[46]\tvalidation_0-auc:0.39886\n",
      "[47]\tvalidation_0-auc:0.40091\n",
      "[48]\tvalidation_0-auc:0.40320\n",
      "[49]\tvalidation_0-auc:0.39817\n",
      "[50]\tvalidation_0-auc:0.39384\n",
      "[51]\tvalidation_0-auc:0.38881\n",
      "[52]\tvalidation_0-auc:0.39132\n",
      "[53]\tvalidation_0-auc:0.38950\n",
      "[54]\tvalidation_0-auc:0.38858\n",
      "[55]\tvalidation_0-auc:0.37626\n",
      "[56]\tvalidation_0-auc:0.37717\n",
      "[57]\tvalidation_0-auc:0.37192\n",
      "[58]\tvalidation_0-auc:0.36872\n",
      "[59]\tvalidation_0-auc:0.37146\n",
      "[60]\tvalidation_0-auc:0.37557\n",
      "[61]\tvalidation_0-auc:0.37317\n",
      "[62]\tvalidation_0-auc:0.37260\n",
      "[63]\tvalidation_0-auc:0.37329\n",
      "[64]\tvalidation_0-auc:0.37397\n",
      "[65]\tvalidation_0-auc:0.36941\n",
      "[66]\tvalidation_0-auc:0.36621\n",
      "[67]\tvalidation_0-auc:0.36461\n",
      "[68]\tvalidation_0-auc:0.35970\n",
      "[69]\tvalidation_0-auc:0.36119\n",
      "[70]\tvalidation_0-auc:0.35639\n",
      "[71]\tvalidation_0-auc:0.35890\n",
      "[72]\tvalidation_0-auc:0.36484\n",
      "[73]\tvalidation_0-auc:0.36381\n",
      "[74]\tvalidation_0-auc:0.36416\n",
      "[75]\tvalidation_0-auc:0.36233\n",
      "[76]\tvalidation_0-auc:0.36244\n",
      "[77]\tvalidation_0-auc:0.36849\n",
      "[78]\tvalidation_0-auc:0.36484\n",
      "[79]\tvalidation_0-auc:0.36758\n",
      "Test Accuracy for Layer 2: 0.5347\n",
      "Test AUC for Layer 2: 0.6677\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.50020\n",
      "[1]\tvalidation_0-auc:0.54058\n",
      "[2]\tvalidation_0-auc:0.55713\n",
      "[3]\tvalidation_0-auc:0.57742\n",
      "[4]\tvalidation_0-auc:0.58649\n",
      "[5]\tvalidation_0-auc:0.56738\n",
      "[6]\tvalidation_0-auc:0.53704\n",
      "[7]\tvalidation_0-auc:0.55162\n",
      "[8]\tvalidation_0-auc:0.59338\n",
      "[9]\tvalidation_0-auc:0.60126\n",
      "[10]\tvalidation_0-auc:0.58156\n",
      "[11]\tvalidation_0-auc:0.58806\n",
      "[12]\tvalidation_0-auc:0.59456\n",
      "[13]\tvalidation_0-auc:0.56974\n",
      "[14]\tvalidation_0-auc:0.54610\n",
      "[15]\tvalidation_0-auc:0.53999\n",
      "[16]\tvalidation_0-auc:0.51872\n",
      "[17]\tvalidation_0-auc:0.51458\n",
      "[18]\tvalidation_0-auc:0.50709\n",
      "[19]\tvalidation_0-auc:0.50552\n",
      "[20]\tvalidation_0-auc:0.50532\n",
      "[21]\tvalidation_0-auc:0.51024\n",
      "[22]\tvalidation_0-auc:0.51478\n",
      "[23]\tvalidation_0-auc:0.52246\n",
      "[24]\tvalidation_0-auc:0.52009\n",
      "[25]\tvalidation_0-auc:0.53073\n",
      "[26]\tvalidation_0-auc:0.52522\n",
      "[27]\tvalidation_0-auc:0.53546\n",
      "[28]\tvalidation_0-auc:0.54846\n",
      "[29]\tvalidation_0-auc:0.53920\n",
      "[30]\tvalidation_0-auc:0.53625\n",
      "[31]\tvalidation_0-auc:0.54925\n",
      "[32]\tvalidation_0-auc:0.54728\n",
      "[33]\tvalidation_0-auc:0.56304\n",
      "[34]\tvalidation_0-auc:0.55930\n",
      "[35]\tvalidation_0-auc:0.55792\n",
      "[36]\tvalidation_0-auc:0.54216\n",
      "[37]\tvalidation_0-auc:0.54610\n",
      "[38]\tvalidation_0-auc:0.53526\n",
      "[39]\tvalidation_0-auc:0.55280\n",
      "[40]\tvalidation_0-auc:0.55043\n",
      "[41]\tvalidation_0-auc:0.55910\n",
      "[42]\tvalidation_0-auc:0.57171\n",
      "[43]\tvalidation_0-auc:0.55556\n",
      "[44]\tvalidation_0-auc:0.55516\n",
      "[45]\tvalidation_0-auc:0.55280\n",
      "[46]\tvalidation_0-auc:0.55162\n",
      "[47]\tvalidation_0-auc:0.54945\n",
      "[48]\tvalidation_0-auc:0.54295\n",
      "[49]\tvalidation_0-auc:0.54708\n",
      "[50]\tvalidation_0-auc:0.53822\n",
      "[51]\tvalidation_0-auc:0.54374\n",
      "[52]\tvalidation_0-auc:0.54768\n",
      "[53]\tvalidation_0-auc:0.55516\n",
      "[54]\tvalidation_0-auc:0.55792\n",
      "[55]\tvalidation_0-auc:0.56166\n",
      "[56]\tvalidation_0-auc:0.56225\n",
      "[57]\tvalidation_0-auc:0.56068\n",
      "[58]\tvalidation_0-auc:0.55871\n",
      "[59]\tvalidation_0-auc:0.56816\n",
      "[60]\tvalidation_0-auc:0.56974\n",
      "[61]\tvalidation_0-auc:0.58195\n",
      "[62]\tvalidation_0-auc:0.57880\n",
      "[63]\tvalidation_0-auc:0.56895\n",
      "[64]\tvalidation_0-auc:0.57368\n",
      "[65]\tvalidation_0-auc:0.57959\n",
      "[66]\tvalidation_0-auc:0.58550\n",
      "[67]\tvalidation_0-auc:0.58668\n",
      "[68]\tvalidation_0-auc:0.59180\n",
      "[69]\tvalidation_0-auc:0.59259\n",
      "[70]\tvalidation_0-auc:0.58195\n",
      "[71]\tvalidation_0-auc:0.57959\n",
      "[72]\tvalidation_0-auc:0.57526\n",
      "[73]\tvalidation_0-auc:0.58589\n",
      "[74]\tvalidation_0-auc:0.57801\n",
      "[75]\tvalidation_0-auc:0.57723\n",
      "[76]\tvalidation_0-auc:0.57841\n",
      "[77]\tvalidation_0-auc:0.58235\n",
      "[78]\tvalidation_0-auc:0.57998\n",
      "[79]\tvalidation_0-auc:0.58511\n",
      "Test Accuracy for Layer 3: 0.5714\n",
      "Test AUC for Layer 3: 0.5617\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5517\n",
      "Average Test AUC across all layers: 0.5356\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Full text and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.43173\n",
      "[1]\tvalidation_0-auc:0.48750\n",
      "[2]\tvalidation_0-auc:0.55879\n",
      "[3]\tvalidation_0-auc:0.56168\n",
      "[4]\tvalidation_0-auc:0.56951\n",
      "[5]\tvalidation_0-auc:0.56951\n",
      "[6]\tvalidation_0-auc:0.57184\n",
      "[7]\tvalidation_0-auc:0.57555\n",
      "[8]\tvalidation_0-auc:0.56593\n",
      "[9]\tvalidation_0-auc:0.58571\n",
      "[10]\tvalidation_0-auc:0.56497\n",
      "[11]\tvalidation_0-auc:0.55962\n",
      "[12]\tvalidation_0-auc:0.55192\n",
      "[13]\tvalidation_0-auc:0.56126\n",
      "[14]\tvalidation_0-auc:0.56223\n",
      "[15]\tvalidation_0-auc:0.56264\n",
      "[16]\tvalidation_0-auc:0.54835\n",
      "[17]\tvalidation_0-auc:0.52967\n",
      "[18]\tvalidation_0-auc:0.52665\n",
      "[19]\tvalidation_0-auc:0.52720\n",
      "[20]\tvalidation_0-auc:0.54396\n",
      "[21]\tvalidation_0-auc:0.53764\n",
      "[22]\tvalidation_0-auc:0.54327\n",
      "[23]\tvalidation_0-auc:0.54615\n",
      "[24]\tvalidation_0-auc:0.55110\n",
      "[25]\tvalidation_0-auc:0.56016\n",
      "[26]\tvalidation_0-auc:0.54684\n",
      "[27]\tvalidation_0-auc:0.54066\n",
      "[28]\tvalidation_0-auc:0.52637\n",
      "[29]\tvalidation_0-auc:0.53462\n",
      "[30]\tvalidation_0-auc:0.54588\n",
      "[31]\tvalidation_0-auc:0.56676\n",
      "[32]\tvalidation_0-auc:0.57541\n",
      "[33]\tvalidation_0-auc:0.56593\n",
      "[34]\tvalidation_0-auc:0.57047\n",
      "[35]\tvalidation_0-auc:0.56099\n",
      "[36]\tvalidation_0-auc:0.57418\n",
      "[37]\tvalidation_0-auc:0.56484\n",
      "[38]\tvalidation_0-auc:0.56648\n",
      "[39]\tvalidation_0-auc:0.56731\n",
      "[40]\tvalidation_0-auc:0.55989\n",
      "[41]\tvalidation_0-auc:0.55549\n",
      "[42]\tvalidation_0-auc:0.55893\n",
      "[43]\tvalidation_0-auc:0.57376\n",
      "[44]\tvalidation_0-auc:0.56497\n",
      "[45]\tvalidation_0-auc:0.56717\n",
      "[46]\tvalidation_0-auc:0.56745\n",
      "[47]\tvalidation_0-auc:0.57280\n",
      "[48]\tvalidation_0-auc:0.57198\n",
      "[49]\tvalidation_0-auc:0.56703\n",
      "[50]\tvalidation_0-auc:0.57253\n",
      "[51]\tvalidation_0-auc:0.56951\n",
      "[52]\tvalidation_0-auc:0.55852\n",
      "[53]\tvalidation_0-auc:0.56044\n",
      "[54]\tvalidation_0-auc:0.56593\n",
      "[55]\tvalidation_0-auc:0.55907\n",
      "[56]\tvalidation_0-auc:0.55508\n",
      "[57]\tvalidation_0-auc:0.55275\n",
      "[58]\tvalidation_0-auc:0.54973\n",
      "[59]\tvalidation_0-auc:0.54753\n",
      "[60]\tvalidation_0-auc:0.55027\n",
      "[61]\tvalidation_0-auc:0.54753\n",
      "[62]\tvalidation_0-auc:0.55000\n",
      "[63]\tvalidation_0-auc:0.54918\n",
      "[64]\tvalidation_0-auc:0.55330\n",
      "[65]\tvalidation_0-auc:0.54794\n",
      "[66]\tvalidation_0-auc:0.55755\n",
      "[67]\tvalidation_0-auc:0.56346\n",
      "[68]\tvalidation_0-auc:0.56621\n",
      "[69]\tvalidation_0-auc:0.56676\n",
      "[70]\tvalidation_0-auc:0.56360\n",
      "[71]\tvalidation_0-auc:0.56154\n",
      "[72]\tvalidation_0-auc:0.56291\n",
      "[73]\tvalidation_0-auc:0.56016\n",
      "[74]\tvalidation_0-auc:0.55989\n",
      "[75]\tvalidation_0-auc:0.55769\n",
      "[76]\tvalidation_0-auc:0.55962\n",
      "[77]\tvalidation_0-auc:0.56484\n",
      "[78]\tvalidation_0-auc:0.56387\n",
      "[79]\tvalidation_0-auc:0.56154\n",
      "Test Accuracy for Layer 1: 0.7368\n",
      "Test AUC for Layer 1: 0.6519\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.56181\n",
      "[1]\tvalidation_0-auc:0.59431\n",
      "[2]\tvalidation_0-auc:0.60160\n",
      "[3]\tvalidation_0-auc:0.58499\n",
      "[4]\tvalidation_0-auc:0.59023\n",
      "[5]\tvalidation_0-auc:0.61020\n",
      "[6]\tvalidation_0-auc:0.60423\n",
      "[7]\tvalidation_0-auc:0.56181\n",
      "[8]\tvalidation_0-auc:0.58528\n",
      "[9]\tvalidation_0-auc:0.59446\n",
      "[10]\tvalidation_0-auc:0.62143\n",
      "[11]\tvalidation_0-auc:0.60117\n",
      "[12]\tvalidation_0-auc:0.60991\n",
      "[13]\tvalidation_0-auc:0.61501\n",
      "[14]\tvalidation_0-auc:0.61224\n",
      "[15]\tvalidation_0-auc:0.61079\n",
      "[16]\tvalidation_0-auc:0.62638\n",
      "[17]\tvalidation_0-auc:0.62770\n",
      "[18]\tvalidation_0-auc:0.61312\n",
      "[19]\tvalidation_0-auc:0.62915\n",
      "[20]\tvalidation_0-auc:0.63003\n",
      "[21]\tvalidation_0-auc:0.64169\n",
      "[22]\tvalidation_0-auc:0.62726\n",
      "[23]\tvalidation_0-auc:0.60466\n",
      "[24]\tvalidation_0-auc:0.60131\n",
      "[25]\tvalidation_0-auc:0.60233\n",
      "[26]\tvalidation_0-auc:0.59854\n",
      "[27]\tvalidation_0-auc:0.59956\n",
      "[28]\tvalidation_0-auc:0.59038\n",
      "[29]\tvalidation_0-auc:0.60379\n",
      "[30]\tvalidation_0-auc:0.60612\n",
      "[31]\tvalidation_0-auc:0.60671\n",
      "[32]\tvalidation_0-auc:0.60160\n",
      "[33]\tvalidation_0-auc:0.61035\n",
      "[34]\tvalidation_0-auc:0.61327\n",
      "[35]\tvalidation_0-auc:0.60335\n",
      "[36]\tvalidation_0-auc:0.60175\n",
      "[37]\tvalidation_0-auc:0.60292\n",
      "[38]\tvalidation_0-auc:0.60583\n",
      "[39]\tvalidation_0-auc:0.61429\n",
      "[40]\tvalidation_0-auc:0.61574\n",
      "[41]\tvalidation_0-auc:0.61239\n",
      "[42]\tvalidation_0-auc:0.62012\n",
      "[43]\tvalidation_0-auc:0.61837\n",
      "[44]\tvalidation_0-auc:0.62216\n",
      "[45]\tvalidation_0-auc:0.62391\n",
      "[46]\tvalidation_0-auc:0.62041\n",
      "[47]\tvalidation_0-auc:0.62624\n",
      "[48]\tvalidation_0-auc:0.63469\n",
      "[49]\tvalidation_0-auc:0.64344\n",
      "[50]\tvalidation_0-auc:0.63907\n",
      "[51]\tvalidation_0-auc:0.63761\n",
      "[52]\tvalidation_0-auc:0.62857\n",
      "[53]\tvalidation_0-auc:0.62391\n",
      "[54]\tvalidation_0-auc:0.62682\n",
      "[55]\tvalidation_0-auc:0.62624\n",
      "[56]\tvalidation_0-auc:0.62726\n",
      "[57]\tvalidation_0-auc:0.62609\n",
      "[58]\tvalidation_0-auc:0.62580\n",
      "[59]\tvalidation_0-auc:0.63149\n",
      "[60]\tvalidation_0-auc:0.63294\n",
      "[61]\tvalidation_0-auc:0.62624\n",
      "[62]\tvalidation_0-auc:0.62828\n",
      "[63]\tvalidation_0-auc:0.62741\n",
      "[64]\tvalidation_0-auc:0.63178\n",
      "[65]\tvalidation_0-auc:0.63265\n",
      "[66]\tvalidation_0-auc:0.63294\n",
      "[67]\tvalidation_0-auc:0.63703\n",
      "[68]\tvalidation_0-auc:0.63586\n",
      "[69]\tvalidation_0-auc:0.63761\n",
      "[70]\tvalidation_0-auc:0.63805\n",
      "[71]\tvalidation_0-auc:0.64490\n",
      "[72]\tvalidation_0-auc:0.64738\n",
      "[73]\tvalidation_0-auc:0.64169\n",
      "[74]\tvalidation_0-auc:0.64052\n",
      "[75]\tvalidation_0-auc:0.63848\n",
      "[76]\tvalidation_0-auc:0.64344\n",
      "[77]\tvalidation_0-auc:0.63965\n",
      "[78]\tvalidation_0-auc:0.63615\n",
      "[79]\tvalidation_0-auc:0.63863\n",
      "Test Accuracy for Layer 2: 0.6139\n",
      "Test AUC for Layer 2: 0.7039\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.58065\n",
      "[1]\tvalidation_0-auc:0.58085\n",
      "[2]\tvalidation_0-auc:0.62738\n",
      "[3]\tvalidation_0-auc:0.64578\n",
      "[4]\tvalidation_0-auc:0.65674\n",
      "[5]\tvalidation_0-auc:0.69086\n",
      "[6]\tvalidation_0-auc:0.73490\n",
      "[7]\tvalidation_0-auc:0.73346\n",
      "[8]\tvalidation_0-auc:0.73056\n",
      "[9]\tvalidation_0-auc:0.73883\n",
      "[10]\tvalidation_0-auc:0.73242\n",
      "[11]\tvalidation_0-auc:0.73222\n",
      "[12]\tvalidation_0-auc:0.71671\n",
      "[13]\tvalidation_0-auc:0.71092\n",
      "[14]\tvalidation_0-auc:0.71381\n",
      "[15]\tvalidation_0-auc:0.72891\n",
      "[16]\tvalidation_0-auc:0.72519\n",
      "[17]\tvalidation_0-auc:0.72581\n",
      "[18]\tvalidation_0-auc:0.72684\n",
      "[19]\tvalidation_0-auc:0.71836\n",
      "[20]\tvalidation_0-auc:0.73160\n",
      "[21]\tvalidation_0-auc:0.73863\n",
      "[22]\tvalidation_0-auc:0.73635\n",
      "[23]\tvalidation_0-auc:0.74256\n",
      "[24]\tvalidation_0-auc:0.72746\n",
      "[25]\tvalidation_0-auc:0.72870\n",
      "[26]\tvalidation_0-auc:0.73284\n",
      "[27]\tvalidation_0-auc:0.73739\n",
      "[28]\tvalidation_0-auc:0.73573\n",
      "[29]\tvalidation_0-auc:0.74380\n",
      "[30]\tvalidation_0-auc:0.74648\n",
      "[31]\tvalidation_0-auc:0.74524\n",
      "[32]\tvalidation_0-auc:0.74938\n",
      "[33]\tvalidation_0-auc:0.74648\n",
      "[34]\tvalidation_0-auc:0.74400\n",
      "[35]\tvalidation_0-auc:0.73656\n",
      "[36]\tvalidation_0-auc:0.73118\n",
      "[37]\tvalidation_0-auc:0.73325\n",
      "[38]\tvalidation_0-auc:0.74152\n",
      "[39]\tvalidation_0-auc:0.74276\n",
      "[40]\tvalidation_0-auc:0.73615\n",
      "[41]\tvalidation_0-auc:0.73284\n",
      "[42]\tvalidation_0-auc:0.72705\n",
      "[43]\tvalidation_0-auc:0.73656\n",
      "[44]\tvalidation_0-auc:0.72498\n",
      "[45]\tvalidation_0-auc:0.72250\n",
      "[46]\tvalidation_0-auc:0.70761\n",
      "[47]\tvalidation_0-auc:0.70678\n",
      "[48]\tvalidation_0-auc:0.70306\n",
      "[49]\tvalidation_0-auc:0.69768\n",
      "[50]\tvalidation_0-auc:0.70761\n",
      "[51]\tvalidation_0-auc:0.70740\n",
      "[52]\tvalidation_0-auc:0.70802\n",
      "[53]\tvalidation_0-auc:0.71050\n",
      "[54]\tvalidation_0-auc:0.71340\n",
      "[55]\tvalidation_0-auc:0.71712\n",
      "[56]\tvalidation_0-auc:0.70182\n",
      "[57]\tvalidation_0-auc:0.70347\n",
      "[58]\tvalidation_0-auc:0.71092\n",
      "[59]\tvalidation_0-auc:0.71423\n",
      "[60]\tvalidation_0-auc:0.71381\n",
      "[61]\tvalidation_0-auc:0.70968\n",
      "[62]\tvalidation_0-auc:0.71464\n",
      "[63]\tvalidation_0-auc:0.71836\n",
      "[64]\tvalidation_0-auc:0.71960\n",
      "[65]\tvalidation_0-auc:0.71774\n",
      "[66]\tvalidation_0-auc:0.71299\n",
      "[67]\tvalidation_0-auc:0.71009\n",
      "[68]\tvalidation_0-auc:0.70761\n",
      "[69]\tvalidation_0-auc:0.70885\n",
      "[70]\tvalidation_0-auc:0.70926\n",
      "[71]\tvalidation_0-auc:0.70430\n",
      "[72]\tvalidation_0-auc:0.70596\n",
      "[73]\tvalidation_0-auc:0.70306\n",
      "[74]\tvalidation_0-auc:0.70596\n",
      "[75]\tvalidation_0-auc:0.70761\n",
      "[76]\tvalidation_0-auc:0.71133\n",
      "[77]\tvalidation_0-auc:0.71547\n",
      "[78]\tvalidation_0-auc:0.71836\n",
      "[79]\tvalidation_0-auc:0.72084\n",
      "Test Accuracy for Layer 3: 0.4857\n",
      "Test AUC for Layer 3: 0.4007\n",
      "\n",
      "Average Test Accuracy across all layers: 0.6121\n",
      "Average Test AUC across all layers: 0.5855\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Title + S_label\n",
      "Average Accuracy: 0.5238\n",
      "Average AUC: 0.5019\n",
      "  Layer 1 - Accuracy: 0.4511, AUC: 0.5016\n",
      "  Layer 2 - Accuracy: 0.5347, AUC: 0.4350\n",
      "  Layer 3 - Accuracy: 0.5857, AUC: 0.5692\n",
      "\n",
      "Combination: MLP with Title + L_label\n",
      "Average Accuracy: 0.4542\n",
      "Average AUC: 0.4862\n",
      "  Layer 1 - Accuracy: 0.2632, AUC: 0.5032\n",
      "  Layer 2 - Accuracy: 0.6139, AUC: 0.5182\n",
      "  Layer 3 - Accuracy: 0.4857, AUC: 0.4371\n",
      "\n",
      "Combination: MLP with Full text + S_label\n",
      "Average Accuracy: 0.5318\n",
      "Average AUC: 0.4867\n",
      "  Layer 1 - Accuracy: 0.5489, AUC: 0.5619\n",
      "  Layer 2 - Accuracy: 0.4752, AUC: 0.4866\n",
      "  Layer 3 - Accuracy: 0.5714, AUC: 0.4117\n",
      "\n",
      "Combination: MLP with Full text + L_label\n",
      "Average Accuracy: 0.3831\n",
      "Average AUC: 0.5082\n",
      "  Layer 1 - Accuracy: 0.2632, AUC: 0.4834\n",
      "  Layer 2 - Accuracy: 0.3861, AUC: 0.5926\n",
      "  Layer 3 - Accuracy: 0.5000, AUC: 0.4485\n",
      "\n",
      "Combination: XGBoost with Title + S_label\n",
      "Average Accuracy: 0.5517\n",
      "Average AUC: 0.5277\n",
      "  Layer 1 - Accuracy: 0.5489, AUC: 0.5432\n",
      "  Layer 2 - Accuracy: 0.5347, AUC: 0.5508\n",
      "  Layer 3 - Accuracy: 0.5714, AUC: 0.4892\n",
      "\n",
      "Combination: XGBoost with Title + L_label\n",
      "Average Accuracy: 0.6121\n",
      "Average AUC: 0.5062\n",
      "  Layer 1 - Accuracy: 0.7368, AUC: 0.5685\n",
      "  Layer 2 - Accuracy: 0.6139, AUC: 0.5554\n",
      "  Layer 3 - Accuracy: 0.4857, AUC: 0.3946\n",
      "\n",
      "Combination: XGBoost with Full text + S_label\n",
      "Average Accuracy: 0.5517\n",
      "Average AUC: 0.5356\n",
      "  Layer 1 - Accuracy: 0.5489, AUC: 0.3775\n",
      "  Layer 2 - Accuracy: 0.5347, AUC: 0.6677\n",
      "  Layer 3 - Accuracy: 0.5714, AUC: 0.5617\n",
      "\n",
      "Combination: XGBoost with Full text + L_label\n",
      "Average Accuracy: 0.6121\n",
      "Average AUC: 0.5855\n",
      "  Layer 1 - Accuracy: 0.7368, AUC: 0.6519\n",
      "  Layer 2 - Accuracy: 0.6139, AUC: 0.7039\n",
      "  Layer 3 - Accuracy: 0.4857, AUC: 0.4007\n",
      "MLP summary comparison visualization saved as: OpenAI_MLP_XGB/visualizations_summary/MPC\\mlp_performance_comparison.png\n",
      "XGBoost summary comparison visualization saved as: OpenAI_MLP_XGB/visualizations_summary/MPC\\xgb_performance_comparison.png\n",
      "MLP layer performance visualization saved as: OpenAI_MLP_XGB/visualizations_summary/MPC\\mlp_layer_performance.png\n",
      "XGBoost layer performance visualization saved as: OpenAI_MLP_XGB/visualizations_summary/MPC\\xgboost_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'OpenAI_MLP_XGB' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "from xgboost import callback\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, model_type, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class ClimateNewsOpenAIPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'OpenAI_MLP_XGB/visualizations_mlp/MPC'\n",
    "        self.xgb_viz_dir = 'OpenAI_MLP_XGB/visualizations_xgb/MPC'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs(self.xgb_viz_dir, exist_ok=True)\n",
    "        os.makedirs('OpenAI_MLP_XGB/visualizations_summary/MPC', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert embeddings to numpy arrays\n",
    "        self.data['Title_embedding'] = self.data['Title_embedding_vector'].apply(parse_embedding)\n",
    "        self.data['Fulltext_embedding'] = self.data['Full_text_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_title_embedding = self.data['Title_embedding'].iloc[0]\n",
    "        sample_fulltext_embedding = self.data['Fulltext_embedding'].iloc[0]\n",
    "        \n",
    "        print(f\"Sample Title embedding shape: {sample_title_embedding.shape}\")\n",
    "        print(f\"Sample Fulltext embedding shape: {sample_fulltext_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2014-2020), validation (2020-2021), test (2021-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2020-10-31'),\n",
    "            'val_start': pd.Timestamp('2020-11-01'),\n",
    "            'val_end': pd.Timestamp('2021-10-31'),\n",
    "            'test_start': pd.Timestamp('2021-11-01'),\n",
    "            'test_end': pd.Timestamp('2022-10-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2014-2021), validation (2021-2022), test (2022-2023)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2021-10-31'),\n",
    "            'val_start': pd.Timestamp('2021-11-01'),\n",
    "            'val_end': pd.Timestamp('2022-10-31'),\n",
    "            'test_start': pd.Timestamp('2022-11-01'),\n",
    "            'test_end': pd.Timestamp('2023-10-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2014-2022), validation (2022-2023), test (2023-2024)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2022-10-31'),\n",
    "            'val_start': pd.Timestamp('2022-11-01'),\n",
    "            'val_end': pd.Timestamp('2023-10-31'),\n",
    "            'test_start': pd.Timestamp('2023-11-01'),\n",
    "            'test_end': pd.Timestamp('2024-11-01')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, text_col, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            text_col: The column containing the embeddings ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data[text_col].values)\n",
    "        X_val = np.stack(val_data[text_col].values)\n",
    "        X_test = np.stack(test_data[text_col].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def get_xgb_parameters(self):\n",
    "        \"\"\"\n",
    "        Get XGBoost parameters optimized for high-dimensional embeddings.\n",
    "        Uses a single parameter set for both Title and Full text embeddings since they have the same dimension (1536).\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of base parameters\n",
    "        \"\"\"\n",
    "        # Setup XGBoost base parameters optimized for high-dimensional embeddings\n",
    "        base_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'n_estimators': 80,\n",
    "            'max_depth': 3,\n",
    "            'learning_rate': 0.0001,\n",
    "            'subsample': 0.7,          # Row subsampling to prevent overfitting\n",
    "            'colsample_bytree': 0.5,   # Column subsampling to handle high dimensionality\n",
    "            'min_child_weight': 3,     # Prevents overfitting on high-dimensional embeddings\n",
    "            'reg_alpha': 1.0,          # L1 regularization\n",
    "            'reg_lambda': 2.0,         # L2 regularization\n",
    "            'random_state': 42,\n",
    "            'use_label_encoder': False # Avoid deprecation warning\n",
    "        }\n",
    "        \n",
    "        return base_params\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, model_type, text_col, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            model_type: Model type (MLP)\n",
    "            text_col: Text column used\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_{model_type.lower()}_{display_text.replace(' ', '_')}_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, text_col, label_col, model_type):\n",
    "        \"\"\"\n",
    "        Train and evaluate a model for a specific text column and label column.\n",
    "        \n",
    "        Args:\n",
    "            text_col: The embedding column to use ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "            model_type: The model type to use ('MLP' or 'XGBoost')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        combination_key = f\"{model_type}|{display_text}|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training {model_type} model for {display_text} and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        visualization_dir = self.mlp_viz_dir if model_type == 'MLP' else self.xgb_viz_dir\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, text_col, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            if model_type == 'MLP':\n",
    "                # MLP model training\n",
    "                model = self.create_mlp_model((1536,))\n",
    "                print(f\"Created MLP model for {display_text} ({label_col})\")\n",
    "                model.summary()\n",
    "                \n",
    "                # Setup callbacks\n",
    "                plot_callback = PlotLearningCallback(model_type, display_text, label_col, i+1, visualization_dir)\n",
    "                early_stopping = EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=15,\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                model_checkpoint = ModelCheckpoint(\n",
    "                    filepath=f\"{visualization_dir}/best_{model_type.lower()}_{display_text.lower().replace(' ', '_')}_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                    monitor='val_loss',\n",
    "                    save_weights_only=True,\n",
    "                    save_best_only=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Train model\n",
    "                print(f\"Training MLP model...\")\n",
    "                batch_size = 32  # Larger batch size for embeddings\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                y_pred_proba = model.predict(X_test)\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                # Create final learning curve visualization\n",
    "                self.plot_final_learning_curves(history, model_type, text_col, label_col, i+1, visualization_dir)\n",
    "                \n",
    "                # Store training history\n",
    "                training_history = history.history\n",
    "                \n",
    "            else:  # XGBoost - simplified approach to fix API issues\n",
    "                # Get XGBoost parameters\n",
    "                base_params = self.get_xgb_parameters()\n",
    "                \n",
    "                # Create and train XGBoost model\n",
    "                print(f\"Creating and training XGBoost model...\")\n",
    "                model = xgb.XGBClassifier(**base_params)\n",
    "                \n",
    "                # Only use validation set for evaluation (not training set)\n",
    "                eval_set = [(X_val, y_val)]\n",
    "                \n",
    "                # Train the model\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    verbose=True\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                # Create a simple summary for XGBoost (no detailed learning curves available)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.text(0.5, 0.5, f'XGBoost Model Trained Successfully\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\\nAUC: {roc_auc_score(y_test, y_pred_proba):.4f}',\n",
    "                         ha='center', va='center', size=14, fontweight='bold')\n",
    "                plt.title(f'XGBoost Results ({display_text}, {label_col}, Layer {i+1})')\n",
    "                plt.axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{visualization_dir}/xgb_{display_text.replace(' ', '_')}_{label_col}_layer_{i+1}.png\")\n",
    "                plt.close()\n",
    "                \n",
    "                # Use None for training history since detailed learning curves aren't available\n",
    "                training_history = None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'model_type': model_type,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': training_history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for all combinations of text inputs, label columns, and model types.\"\"\"\n",
    "        # Define all combinations\n",
    "        embedding_cols = ['Title_embedding', 'Fulltext_embedding']\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        model_types = ['MLP', 'XGBoost']\n",
    "        \n",
    "        # Run analysis for each combination\n",
    "        for model_type in model_types:\n",
    "            for embedding_col in embedding_cols:\n",
    "                for label_col in label_cols:\n",
    "                    self.train_and_evaluate_model(embedding_col, label_col, model_type)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Model': model_type,\n",
    "                'Text': text_col,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing all model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Split by model type\n",
    "        mlp_data = df[df['Model'] == 'MLP']\n",
    "        xgb_data = df[df['Model'] == 'XGBoost']\n",
    "        \n",
    "        # 1. Performance comparison for MLP\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot accuracy and AUC bars for MLP\n",
    "        x = np.arange(len(mlp_data))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, mlp_data['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, mlp_data['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('MLP Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with OpenAI Embeddings')\n",
    "        labels = [f\"{row['Text']} + {row['Label']}\" for _, row in mlp_data.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(mlp_data['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(mlp_data['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(mlp_data['Avg Accuracy'].max(), mlp_data['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/MPC', \"mlp_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Performance comparison for XGBoost\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot accuracy and AUC bars for XGBoost\n",
    "        x = np.arange(len(xgb_data))\n",
    "        \n",
    "        plt.bar(x - width/2, xgb_data['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, xgb_data['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('XGBoost Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of XGBoost Models with OpenAI Embeddings')\n",
    "        labels = [f\"{row['Text']} + {row['Label']}\" for _, row in xgb_data.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(xgb_data['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(xgb_data['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(xgb_data['Avg Accuracy'].max(), xgb_data['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/MPC', \"xgb_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"XGBoost summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP and XGBoost models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        mlp_layer_data = []\n",
    "        xgb_layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Model': model_type,\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                if model_type == 'MLP':\n",
    "                    mlp_layer_data.append(layer_info)\n",
    "                else:  # XGBoost\n",
    "                    xgb_layer_data.append(layer_info)\n",
    "        \n",
    "        # Create visualizations for each model type\n",
    "        self._create_model_layer_visualization(mlp_layer_data, 'MLP')\n",
    "        self._create_model_layer_visualization(xgb_layer_data, 'XGBoost')\n",
    "    \n",
    "    def _create_model_layer_visualization(self, layer_data, model_type):\n",
    "        \"\"\"Create layer-specific visualizations for a given model type.\"\"\"\n",
    "        if not layer_data:\n",
    "            print(f\"No layer data available for {model_type}\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} Accuracy by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} AUC by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/MPC', f\"{model_type.lower()}_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"{model_type} layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['OpenAI_MLP_XGB/visualizations_mlp/MPC', 'OpenAI_MLP_XGB/visualizations_xgb/MPC', \n",
    "                      'OpenAI_MLP_XGB/visualizations_summary/MPC']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/wall_street_news_semantics_SP500_database/wall_street_news_semantics_MPC_completed_openai.csv'\n",
    "    \n",
    "    # Initialize the predictor with OpenAI embeddings\n",
    "    predictor = ClimateNewsOpenAIPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'OpenAI_MLP_XGB' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing OpenAI embedding vectors from string format...\n",
      "Sample Title embedding shape: (1536,)\n",
      "Sample Fulltext embedding shape: (1536,)\n",
      "Loaded 838 climate change news articles spanning from 07/11/2014 to 24/09/2024\n",
      "Class distribution for short-term prediction: {0: 433, 1: 405}\n",
      "Class distribution for long-term prediction: {0: 437, 1: 401}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_157\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_604 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_447 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_314 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_605 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_448 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_315 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_606 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_449 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_607 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8894 - accuracy: 0.4972\n",
      "Epoch 1: val_loss improved from inf to 0.69155, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 31ms/step - loss: 0.9005 - accuracy: 0.4913 - val_loss: 0.6915 - val_accuracy: 0.5191\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8360 - accuracy: 0.5369\n",
      "Epoch 2: val_loss improved from 0.69155 to 0.69147, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.8408 - accuracy: 0.5337 - val_loss: 0.6915 - val_accuracy: 0.5191\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7333 - accuracy: 0.5739\n",
      "Epoch 3: val_loss did not improve from 0.69147\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7298 - accuracy: 0.5736 - val_loss: 0.6924 - val_accuracy: 0.5191\n",
      "Epoch 4/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.7676 - accuracy: 0.5813\n",
      "Epoch 4: val_loss did not improve from 0.69147\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.7645 - accuracy: 0.5810 - val_loss: 0.6938 - val_accuracy: 0.5191\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6809 - accuracy: 0.6506\n",
      "Epoch 5: val_loss did not improve from 0.69147\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6747 - accuracy: 0.6409 - val_loss: 0.6954 - val_accuracy: 0.5191\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6735 - accuracy: 0.6307\n",
      "Epoch 6: val_loss did not improve from 0.69147\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6728 - accuracy: 0.6334 - val_loss: 0.6971 - val_accuracy: 0.5191\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6196 - accuracy: 0.6733\n",
      "Epoch 7: val_loss did not improve from 0.69147\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6166 - accuracy: 0.6783 - val_loss: 0.6986 - val_accuracy: 0.5191\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5870 - accuracy: 0.6847\n",
      "Epoch 8: val_loss did not improve from 0.69147\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5840 - accuracy: 0.6883 - val_loss: 0.7003 - val_accuracy: 0.5191\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5646 - accuracy: 0.7159\n",
      "Epoch 9: val_loss did not improve from 0.69147\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5604 - accuracy: 0.7182 - val_loss: 0.7014 - val_accuracy: 0.5191\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5112 - accuracy: 0.7500\n",
      "Epoch 10: val_loss did not improve from 0.69147\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5161 - accuracy: 0.7556 - val_loss: 0.7024 - val_accuracy: 0.5191\n",
      "Epoch 11/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5038 - accuracy: 0.7812\n",
      "Epoch 11: val_loss did not improve from 0.69147\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5064 - accuracy: 0.7756 - val_loss: 0.7035 - val_accuracy: 0.5191\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4730 - accuracy: 0.7727\n",
      "Epoch 12: val_loss did not improve from 0.69147\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4669 - accuracy: 0.7781 - val_loss: 0.7046 - val_accuracy: 0.5191\n",
      "Epoch 13/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4435 - accuracy: 0.7891\n",
      "Epoch 13: val_loss did not improve from 0.69147\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4473 - accuracy: 0.7830 - val_loss: 0.7038 - val_accuracy: 0.5191\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4177 - accuracy: 0.8125\n",
      "Epoch 14: val_loss did not improve from 0.69147\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4136 - accuracy: 0.8204 - val_loss: 0.7024 - val_accuracy: 0.5191\n",
      "Epoch 15/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.3909 - accuracy: 0.8281\n",
      "Epoch 15: val_loss did not improve from 0.69147\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3976 - accuracy: 0.8304 - val_loss: 0.7027 - val_accuracy: 0.5191\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3604 - accuracy: 0.8466\n",
      "Epoch 16: val_loss did not improve from 0.69147\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3720 - accuracy: 0.8379 - val_loss: 0.7036 - val_accuracy: 0.5191\n",
      "Epoch 17/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.3669 - accuracy: 0.8500Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.69147\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3819 - accuracy: 0.8429 - val_loss: 0.7048 - val_accuracy: 0.5191\n",
      "Epoch 17: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.5113\n",
      "Test AUC for Layer 1: 0.4957\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_158\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_608 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_450 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_316 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_609 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_451 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_317 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_610 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_452 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_611 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.9421 - accuracy: 0.4833\n",
      "Epoch 1: val_loss improved from inf to 0.69289, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 26ms/step - loss: 0.9286 - accuracy: 0.4906 - val_loss: 0.6929 - val_accuracy: 0.5188\n",
      "Epoch 2/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.8563 - accuracy: 0.5223\n",
      "Epoch 2: val_loss improved from 0.69289 to 0.69237, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.8466 - accuracy: 0.5282 - val_loss: 0.6924 - val_accuracy: 0.5113\n",
      "Epoch 3/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.7958 - accuracy: 0.5583\n",
      "Epoch 3: val_loss improved from 0.69237 to 0.69205, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7940 - accuracy: 0.5639 - val_loss: 0.6920 - val_accuracy: 0.5113\n",
      "Epoch 4/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.7742 - accuracy: 0.5917\n",
      "Epoch 4: val_loss improved from 0.69205 to 0.69178, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.7619 - accuracy: 0.5959 - val_loss: 0.6918 - val_accuracy: 0.5338\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6990 - accuracy: 0.6074\n",
      "Epoch 5: val_loss improved from 0.69178 to 0.69166, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.6941 - accuracy: 0.6109 - val_loss: 0.6917 - val_accuracy: 0.5338\n",
      "Epoch 6/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.6340 - accuracy: 0.6562\n",
      "Epoch 6: val_loss improved from 0.69166 to 0.69140, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.6356 - accuracy: 0.6504 - val_loss: 0.6914 - val_accuracy: 0.5188\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6160 - accuracy: 0.6699\n",
      "Epoch 7: val_loss improved from 0.69140 to 0.69118, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6182 - accuracy: 0.6654 - val_loss: 0.6912 - val_accuracy: 0.5188\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6043 - accuracy: 0.6719\n",
      "Epoch 8: val_loss improved from 0.69118 to 0.69090, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6008 - accuracy: 0.6767 - val_loss: 0.6909 - val_accuracy: 0.5188\n",
      "Epoch 9/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.5803 - accuracy: 0.7031\n",
      "Epoch 9: val_loss improved from 0.69090 to 0.69085, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5716 - accuracy: 0.7049 - val_loss: 0.6909 - val_accuracy: 0.5188\n",
      "Epoch 10/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5417 - accuracy: 0.7396\n",
      "Epoch 10: val_loss improved from 0.69085 to 0.69033, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.5381 - accuracy: 0.7368 - val_loss: 0.6903 - val_accuracy: 0.5188\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4988 - accuracy: 0.7676\n",
      "Epoch 11: val_loss did not improve from 0.69033\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5061 - accuracy: 0.7650 - val_loss: 0.6907 - val_accuracy: 0.5188\n",
      "Epoch 12/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4730 - accuracy: 0.7604\n",
      "Epoch 12: val_loss did not improve from 0.69033\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4831 - accuracy: 0.7594 - val_loss: 0.6907 - val_accuracy: 0.5113\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4276 - accuracy: 0.7988\n",
      "Epoch 13: val_loss did not improve from 0.69033\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4374 - accuracy: 0.7895 - val_loss: 0.6910 - val_accuracy: 0.5113\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4457 - accuracy: 0.7930\n",
      "Epoch 14: val_loss did not improve from 0.69033\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4409 - accuracy: 0.7970 - val_loss: 0.6915 - val_accuracy: 0.5113\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4030 - accuracy: 0.8242\n",
      "Epoch 15: val_loss did not improve from 0.69033\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3973 - accuracy: 0.8289 - val_loss: 0.6926 - val_accuracy: 0.5038\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4036 - accuracy: 0.8242\n",
      "Epoch 16: val_loss did not improve from 0.69033\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4038 - accuracy: 0.8252 - val_loss: 0.6927 - val_accuracy: 0.4812\n",
      "Epoch 17/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3742 - accuracy: 0.8594\n",
      "Epoch 17: val_loss did not improve from 0.69033\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3719 - accuracy: 0.8609 - val_loss: 0.6936 - val_accuracy: 0.5038\n",
      "Epoch 18/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3802 - accuracy: 0.8301\n",
      "Epoch 18: val_loss did not improve from 0.69033\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3809 - accuracy: 0.8271 - val_loss: 0.6952 - val_accuracy: 0.4887\n",
      "Epoch 19/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.3783 - accuracy: 0.8354\n",
      "Epoch 19: val_loss did not improve from 0.69033\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3790 - accuracy: 0.8346 - val_loss: 0.6964 - val_accuracy: 0.5113\n",
      "Epoch 20/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3217 - accuracy: 0.8870\n",
      "Epoch 20: val_loss did not improve from 0.69033\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3194 - accuracy: 0.8872 - val_loss: 0.6979 - val_accuracy: 0.5038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3009 - accuracy: 0.8910\n",
      "Epoch 21: val_loss did not improve from 0.69033\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3009 - accuracy: 0.8910 - val_loss: 0.7009 - val_accuracy: 0.4962\n",
      "Epoch 22/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3270 - accuracy: 0.8750\n",
      "Epoch 22: val_loss did not improve from 0.69033\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3258 - accuracy: 0.8722 - val_loss: 0.7054 - val_accuracy: 0.4812\n",
      "Epoch 23/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2879 - accuracy: 0.8990\n",
      "Epoch 23: val_loss did not improve from 0.69033\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2821 - accuracy: 0.9041 - val_loss: 0.7086 - val_accuracy: 0.4962\n",
      "Epoch 24/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2963 - accuracy: 0.9062\n",
      "Epoch 24: val_loss did not improve from 0.69033\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2941 - accuracy: 0.9098 - val_loss: 0.7118 - val_accuracy: 0.4812\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2867 - accuracy: 0.8985Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.69033\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2867 - accuracy: 0.8985 - val_loss: 0.7172 - val_accuracy: 0.4662\n",
      "Epoch 25: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5347\n",
      "Test AUC for Layer 2: 0.3793\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_159\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_612 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_453 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_318 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_613 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_454 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_319 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_614 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_455 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_615 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9092 - accuracy: 0.4775\n",
      "Epoch 1: val_loss improved from inf to 0.69658, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 22ms/step - loss: 0.9092 - accuracy: 0.4775 - val_loss: 0.6966 - val_accuracy: 0.3663\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8301 - accuracy: 0.5541\n",
      "Epoch 2: val_loss did not improve from 0.69658\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.8301 - accuracy: 0.5541 - val_loss: 0.6970 - val_accuracy: 0.3564\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7951 - accuracy: 0.5616\n",
      "Epoch 3: val_loss did not improve from 0.69658\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7951 - accuracy: 0.5616 - val_loss: 0.6969 - val_accuracy: 0.3861\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6956 - accuracy: 0.6231\n",
      "Epoch 4: val_loss did not improve from 0.69658\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6956 - accuracy: 0.6231 - val_loss: 0.6972 - val_accuracy: 0.3663\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6848 - accuracy: 0.6291\n",
      "Epoch 5: val_loss improved from 0.69658 to 0.69643, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.6848 - accuracy: 0.6291 - val_loss: 0.6964 - val_accuracy: 0.4158\n",
      "Epoch 6/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6471 - accuracy: 0.6516\n",
      "Epoch 6: val_loss improved from 0.69643 to 0.69585, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 0.6483 - accuracy: 0.6456 - val_loss: 0.6958 - val_accuracy: 0.5050\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6361 - accuracy: 0.6592\n",
      "Epoch 7: val_loss improved from 0.69585 to 0.69573, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.6361 - accuracy: 0.6592 - val_loss: 0.6957 - val_accuracy: 0.5149\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5592 - accuracy: 0.7147\n",
      "Epoch 8: val_loss improved from 0.69573 to 0.69471, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5592 - accuracy: 0.7147 - val_loss: 0.6947 - val_accuracy: 0.5545\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5032 - accuracy: 0.7312\n",
      "Epoch 9: val_loss did not improve from 0.69471\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5032 - accuracy: 0.7312 - val_loss: 0.6961 - val_accuracy: 0.5347\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5030 - accuracy: 0.7508\n",
      "Epoch 10: val_loss did not improve from 0.69471\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5030 - accuracy: 0.7508 - val_loss: 0.6965 - val_accuracy: 0.5545\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4952 - accuracy: 0.7613\n",
      "Epoch 11: val_loss did not improve from 0.69471\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4952 - accuracy: 0.7613 - val_loss: 0.6976 - val_accuracy: 0.5446\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4602 - accuracy: 0.7838\n",
      "Epoch 12: val_loss did not improve from 0.69471\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4602 - accuracy: 0.7838 - val_loss: 0.6980 - val_accuracy: 0.5446\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4524 - accuracy: 0.7793\n",
      "Epoch 13: val_loss did not improve from 0.69471\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4524 - accuracy: 0.7793 - val_loss: 0.6986 - val_accuracy: 0.5347\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4234 - accuracy: 0.8063\n",
      "Epoch 14: val_loss did not improve from 0.69471\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4234 - accuracy: 0.8063 - val_loss: 0.7013 - val_accuracy: 0.5347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.4119 - accuracy: 0.7996\n",
      "Epoch 15: val_loss did not improve from 0.69471\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4159 - accuracy: 0.8003 - val_loss: 0.7057 - val_accuracy: 0.5149\n",
      "Epoch 16/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.3954 - accuracy: 0.8203\n",
      "Epoch 16: val_loss did not improve from 0.69471\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4165 - accuracy: 0.8048 - val_loss: 0.7064 - val_accuracy: 0.5446\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3841 - accuracy: 0.8318\n",
      "Epoch 17: val_loss did not improve from 0.69471\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3841 - accuracy: 0.8318 - val_loss: 0.7100 - val_accuracy: 0.5149\n",
      "Epoch 18/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3562 - accuracy: 0.8468\n",
      "Epoch 18: val_loss did not improve from 0.69471\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3562 - accuracy: 0.8468 - val_loss: 0.7152 - val_accuracy: 0.4950\n",
      "Epoch 19/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3664 - accuracy: 0.8559\n",
      "Epoch 19: val_loss did not improve from 0.69471\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3664 - accuracy: 0.8559 - val_loss: 0.7197 - val_accuracy: 0.5149\n",
      "Epoch 20/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3505 - accuracy: 0.8514\n",
      "Epoch 20: val_loss did not improve from 0.69471\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3505 - accuracy: 0.8514 - val_loss: 0.7266 - val_accuracy: 0.5248\n",
      "Epoch 21/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3171 - accuracy: 0.8859\n",
      "Epoch 21: val_loss did not improve from 0.69471\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3171 - accuracy: 0.8859 - val_loss: 0.7336 - val_accuracy: 0.5050\n",
      "Epoch 22/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3038 - accuracy: 0.8904\n",
      "Epoch 22: val_loss did not improve from 0.69471\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3038 - accuracy: 0.8904 - val_loss: 0.7431 - val_accuracy: 0.4950\n",
      "Epoch 23/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2736 - accuracy: 0.9174Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.69471\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2736 - accuracy: 0.9174 - val_loss: 0.7564 - val_accuracy: 0.4356\n",
      "Epoch 23: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4429\n",
      "Test AUC for Layer 3: 0.4036\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4963\n",
      "Average Test AUC across all layers: 0.4262\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_160\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_616 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_456 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_320 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_617 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_457 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_321 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_618 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_458 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_619 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.8640 - accuracy: 0.5000\n",
      "Epoch 1: val_loss improved from inf to 0.68538, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 30ms/step - loss: 0.8379 - accuracy: 0.5162 - val_loss: 0.6854 - val_accuracy: 0.6260\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8327 - accuracy: 0.5085\n",
      "Epoch 2: val_loss improved from 0.68538 to 0.67787, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.8240 - accuracy: 0.5062 - val_loss: 0.6779 - val_accuracy: 0.6260\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7061 - accuracy: 0.6051\n",
      "Epoch 3: val_loss improved from 0.67787 to 0.67223, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.7100 - accuracy: 0.5885 - val_loss: 0.6722 - val_accuracy: 0.6260\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7023 - accuracy: 0.6080\n",
      "Epoch 4: val_loss improved from 0.67223 to 0.66760, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.7039 - accuracy: 0.6010 - val_loss: 0.6676 - val_accuracy: 0.6260\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6055 - accuracy: 0.6847\n",
      "Epoch 5: val_loss improved from 0.66760 to 0.66393, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.6034 - accuracy: 0.6833 - val_loss: 0.6639 - val_accuracy: 0.6260\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6485 - accuracy: 0.6591\n",
      "Epoch 6: val_loss improved from 0.66393 to 0.66211, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.6340 - accuracy: 0.6683 - val_loss: 0.6621 - val_accuracy: 0.6260\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5511 - accuracy: 0.7528\n",
      "Epoch 7: val_loss improved from 0.66211 to 0.66148, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5522 - accuracy: 0.7481 - val_loss: 0.6615 - val_accuracy: 0.6260\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5331 - accuracy: 0.7216\n",
      "Epoch 8: val_loss did not improve from 0.66148\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5301 - accuracy: 0.7207 - val_loss: 0.6617 - val_accuracy: 0.6260\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5223 - accuracy: 0.7301\n",
      "Epoch 9: val_loss did not improve from 0.66148\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5376 - accuracy: 0.7132 - val_loss: 0.6629 - val_accuracy: 0.6260\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4622 - accuracy: 0.7898\n",
      "Epoch 10: val_loss did not improve from 0.66148\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4604 - accuracy: 0.7955 - val_loss: 0.6648 - val_accuracy: 0.6260\n",
      "Epoch 11/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4861 - accuracy: 0.7528\n",
      "Epoch 11: val_loss did not improve from 0.66148\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4758 - accuracy: 0.7706 - val_loss: 0.6674 - val_accuracy: 0.6260\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4087 - accuracy: 0.8324\n",
      "Epoch 12: val_loss did not improve from 0.66148\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4144 - accuracy: 0.8254 - val_loss: 0.6708 - val_accuracy: 0.6260\n",
      "Epoch 13/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4305 - accuracy: 0.7983\n",
      "Epoch 13: val_loss did not improve from 0.66148\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4304 - accuracy: 0.8005 - val_loss: 0.6739 - val_accuracy: 0.6260\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4318 - accuracy: 0.8210\n",
      "Epoch 14: val_loss did not improve from 0.66148\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4390 - accuracy: 0.8155 - val_loss: 0.6784 - val_accuracy: 0.6260\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3970 - accuracy: 0.8381\n",
      "Epoch 15: val_loss did not improve from 0.66148\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3957 - accuracy: 0.8454 - val_loss: 0.6819 - val_accuracy: 0.6260\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3603 - accuracy: 0.8608\n",
      "Epoch 16: val_loss did not improve from 0.66148\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3526 - accuracy: 0.8703 - val_loss: 0.6869 - val_accuracy: 0.6260\n",
      "Epoch 17/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3607 - accuracy: 0.8494\n",
      "Epoch 17: val_loss did not improve from 0.66148\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3544 - accuracy: 0.8529 - val_loss: 0.6893 - val_accuracy: 0.6260\n",
      "Epoch 18/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3361 - accuracy: 0.8665\n",
      "Epoch 18: val_loss did not improve from 0.66148\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3440 - accuracy: 0.8603 - val_loss: 0.6920 - val_accuracy: 0.6260\n",
      "Epoch 19/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3307 - accuracy: 0.8864\n",
      "Epoch 19: val_loss did not improve from 0.66148\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3340 - accuracy: 0.8778 - val_loss: 0.6945 - val_accuracy: 0.6260\n",
      "Epoch 20/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.2864 - accuracy: 0.9062\n",
      "Epoch 20: val_loss did not improve from 0.66148\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.2942 - accuracy: 0.8953 - val_loss: 0.6971 - val_accuracy: 0.6260\n",
      "Epoch 21/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3040 - accuracy: 0.9006\n",
      "Epoch 21: val_loss did not improve from 0.66148\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3011 - accuracy: 0.8978 - val_loss: 0.6996 - val_accuracy: 0.6260\n",
      "Epoch 22/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3060 - accuracy: 0.8835Restoring model weights from the end of the best epoch: 7.\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.66148\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3043 - accuracy: 0.8878 - val_loss: 0.7018 - val_accuracy: 0.6260\n",
      "Epoch 22: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.6316\n",
      "Test AUC for Layer 1: 0.5275\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_161\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_620 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_459 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_322 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_621 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_460 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_323 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_622 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_461 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_623 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.8461 - accuracy: 0.5063\n",
      "Epoch 1: val_loss improved from inf to 0.70230, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 26ms/step - loss: 0.8540 - accuracy: 0.4962 - val_loss: 0.7023 - val_accuracy: 0.3684\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7671 - accuracy: 0.5645\n",
      "Epoch 2: val_loss did not improve from 0.70230\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7643 - accuracy: 0.5658 - val_loss: 0.7065 - val_accuracy: 0.3684\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7150 - accuracy: 0.6015\n",
      "Epoch 3: val_loss did not improve from 0.70230\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7150 - accuracy: 0.6015 - val_loss: 0.7106 - val_accuracy: 0.3684\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7255 - accuracy: 0.6035\n",
      "Epoch 4: val_loss did not improve from 0.70230\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7248 - accuracy: 0.6053 - val_loss: 0.7148 - val_accuracy: 0.3684\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6345 - accuracy: 0.6729\n",
      "Epoch 5: val_loss did not improve from 0.70230\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6345 - accuracy: 0.6729 - val_loss: 0.7187 - val_accuracy: 0.3684\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5923 - accuracy: 0.7091\n",
      "Epoch 6: val_loss did not improve from 0.70230\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5824 - accuracy: 0.7124 - val_loss: 0.7209 - val_accuracy: 0.3684\n",
      "Epoch 7/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6175 - accuracy: 0.6901\n",
      "Epoch 7: val_loss did not improve from 0.70230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6321 - accuracy: 0.6692 - val_loss: 0.7211 - val_accuracy: 0.3684\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5481 - accuracy: 0.7305\n",
      "Epoch 8: val_loss did not improve from 0.70230\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5465 - accuracy: 0.7350 - val_loss: 0.7225 - val_accuracy: 0.3684\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5251 - accuracy: 0.7356\n",
      "Epoch 9: val_loss did not improve from 0.70230\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5164 - accuracy: 0.7387 - val_loss: 0.7227 - val_accuracy: 0.3684\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4771 - accuracy: 0.7637\n",
      "Epoch 10: val_loss did not improve from 0.70230\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4724 - accuracy: 0.7707 - val_loss: 0.7239 - val_accuracy: 0.3609\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4813 - accuracy: 0.7812\n",
      "Epoch 11: val_loss did not improve from 0.70230\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4845 - accuracy: 0.7782 - val_loss: 0.7224 - val_accuracy: 0.3759\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4596 - accuracy: 0.8125\n",
      "Epoch 12: val_loss did not improve from 0.70230\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4559 - accuracy: 0.8158 - val_loss: 0.7219 - val_accuracy: 0.3684\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4586 - accuracy: 0.8005\n",
      "Epoch 13: val_loss did not improve from 0.70230\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4570 - accuracy: 0.7914 - val_loss: 0.7240 - val_accuracy: 0.3684\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3940 - accuracy: 0.8421\n",
      "Epoch 14: val_loss did not improve from 0.70230\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3940 - accuracy: 0.8421 - val_loss: 0.7279 - val_accuracy: 0.3835\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3819 - accuracy: 0.8383\n",
      "Epoch 15: val_loss did not improve from 0.70230\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3819 - accuracy: 0.8383 - val_loss: 0.7296 - val_accuracy: 0.3985\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3855 - accuracy: 0.8317Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70230\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4023 - accuracy: 0.8158 - val_loss: 0.7336 - val_accuracy: 0.3910\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5149\n",
      "Test AUC for Layer 2: 0.5000\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_162\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_624 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_462 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_324 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_625 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_463 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_325 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_626 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_464 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_627 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.8340 - accuracy: 0.5132\n",
      "Epoch 1: val_loss improved from inf to 0.69252, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 23ms/step - loss: 0.8308 - accuracy: 0.5165 - val_loss: 0.6925 - val_accuracy: 0.5248\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7339 - accuracy: 0.5871\n",
      "Epoch 2: val_loss improved from 0.69252 to 0.69249, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7339 - accuracy: 0.5871 - val_loss: 0.6925 - val_accuracy: 0.5149\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6958 - accuracy: 0.6066\n",
      "Epoch 3: val_loss did not improve from 0.69249\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6958 - accuracy: 0.6066 - val_loss: 0.6925 - val_accuracy: 0.5149\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6766 - accuracy: 0.6216\n",
      "Epoch 4: val_loss improved from 0.69249 to 0.69226, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.6766 - accuracy: 0.6216 - val_loss: 0.6923 - val_accuracy: 0.5248\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6435 - accuracy: 0.6502\n",
      "Epoch 5: val_loss improved from 0.69226 to 0.69209, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.6435 - accuracy: 0.6502 - val_loss: 0.6921 - val_accuracy: 0.5347\n",
      "Epoch 6/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.6096 - accuracy: 0.6523\n",
      "Epoch 6: val_loss improved from 0.69209 to 0.69194, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6114 - accuracy: 0.6577 - val_loss: 0.6919 - val_accuracy: 0.5446\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5461 - accuracy: 0.7222\n",
      "Epoch 7: val_loss improved from 0.69194 to 0.69125, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5461 - accuracy: 0.7222 - val_loss: 0.6913 - val_accuracy: 0.5347\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5177 - accuracy: 0.7357\n",
      "Epoch 8: val_loss improved from 0.69125 to 0.69061, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5177 - accuracy: 0.7357 - val_loss: 0.6906 - val_accuracy: 0.5347\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4952 - accuracy: 0.7703\n",
      "Epoch 9: val_loss improved from 0.69061 to 0.68991, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4952 - accuracy: 0.7703 - val_loss: 0.6899 - val_accuracy: 0.5347\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5018 - accuracy: 0.7658\n",
      "Epoch 10: val_loss improved from 0.68991 to 0.68960, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5018 - accuracy: 0.7658 - val_loss: 0.6896 - val_accuracy: 0.5149\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4592 - accuracy: 0.7718\n",
      "Epoch 11: val_loss improved from 0.68960 to 0.68910, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4592 - accuracy: 0.7718 - val_loss: 0.6891 - val_accuracy: 0.5149\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4432 - accuracy: 0.7928\n",
      "Epoch 12: val_loss improved from 0.68910 to 0.68847, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4432 - accuracy: 0.7928 - val_loss: 0.6885 - val_accuracy: 0.4950\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4088 - accuracy: 0.8228\n",
      "Epoch 13: val_loss did not improve from 0.68847\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4088 - accuracy: 0.8228 - val_loss: 0.6890 - val_accuracy: 0.4950\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3959 - accuracy: 0.8348\n",
      "Epoch 14: val_loss improved from 0.68847 to 0.68808, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.3959 - accuracy: 0.8348 - val_loss: 0.6881 - val_accuracy: 0.5248\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3872 - accuracy: 0.8363\n",
      "Epoch 15: val_loss improved from 0.68808 to 0.68736, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.3872 - accuracy: 0.8363 - val_loss: 0.6874 - val_accuracy: 0.5446\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3432 - accuracy: 0.8679\n",
      "Epoch 16: val_loss improved from 0.68736 to 0.68556, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.3432 - accuracy: 0.8679 - val_loss: 0.6856 - val_accuracy: 0.5545\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3656 - accuracy: 0.8498\n",
      "Epoch 17: val_loss improved from 0.68556 to 0.68454, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.3656 - accuracy: 0.8498 - val_loss: 0.6845 - val_accuracy: 0.5347\n",
      "Epoch 18/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3168 - accuracy: 0.8814\n",
      "Epoch 18: val_loss improved from 0.68454 to 0.68382, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.3168 - accuracy: 0.8814 - val_loss: 0.6838 - val_accuracy: 0.5347\n",
      "Epoch 19/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3117 - accuracy: 0.8784\n",
      "Epoch 19: val_loss did not improve from 0.68382\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3117 - accuracy: 0.8784 - val_loss: 0.6855 - val_accuracy: 0.5248\n",
      "Epoch 20/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.8904\n",
      "Epoch 20: val_loss did not improve from 0.68382\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2925 - accuracy: 0.8904 - val_loss: 0.6880 - val_accuracy: 0.5149\n",
      "Epoch 21/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2918 - accuracy: 0.8949\n",
      "Epoch 21: val_loss did not improve from 0.68382\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2918 - accuracy: 0.8949 - val_loss: 0.6906 - val_accuracy: 0.5050\n",
      "Epoch 22/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2813 - accuracy: 0.8949\n",
      "Epoch 22: val_loss did not improve from 0.68382\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2813 - accuracy: 0.8949 - val_loss: 0.6951 - val_accuracy: 0.5050\n",
      "Epoch 23/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2721 - accuracy: 0.8984\n",
      "Epoch 23: val_loss did not improve from 0.68382\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2714 - accuracy: 0.9009 - val_loss: 0.6977 - val_accuracy: 0.5248\n",
      "Epoch 24/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2377 - accuracy: 0.9114\n",
      "Epoch 24: val_loss did not improve from 0.68382\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2377 - accuracy: 0.9114 - val_loss: 0.7011 - val_accuracy: 0.5248\n",
      "Epoch 25/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2345 - accuracy: 0.9264\n",
      "Epoch 25: val_loss did not improve from 0.68382\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2345 - accuracy: 0.9264 - val_loss: 0.7056 - val_accuracy: 0.5446\n",
      "Epoch 26/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2279 - accuracy: 0.9219\n",
      "Epoch 26: val_loss did not improve from 0.68382\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2278 - accuracy: 0.9279 - val_loss: 0.7087 - val_accuracy: 0.5347\n",
      "Epoch 27/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.2112 - accuracy: 0.9467\n",
      "Epoch 27: val_loss did not improve from 0.68382\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2148 - accuracy: 0.9459 - val_loss: 0.7170 - val_accuracy: 0.5644\n",
      "Epoch 28/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.9339\n",
      "Epoch 28: val_loss did not improve from 0.68382\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2048 - accuracy: 0.9339 - val_loss: 0.7269 - val_accuracy: 0.5545\n",
      "Epoch 29/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2005 - accuracy: 0.9444\n",
      "Epoch 29: val_loss did not improve from 0.68382\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.1992 - accuracy: 0.9429 - val_loss: 0.7368 - val_accuracy: 0.5743\n",
      "Epoch 30/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2003 - accuracy: 0.9414\n",
      "Epoch 30: val_loss did not improve from 0.68382\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2003 - accuracy: 0.9414 - val_loss: 0.7486 - val_accuracy: 0.5644\n",
      "Epoch 31/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1782 - accuracy: 0.9489\n",
      "Epoch 31: val_loss did not improve from 0.68382\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.1782 - accuracy: 0.9489 - val_loss: 0.7635 - val_accuracy: 0.5545\n",
      "Epoch 32/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1891 - accuracy: 0.9414\n",
      "Epoch 32: val_loss did not improve from 0.68382\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.1891 - accuracy: 0.9414 - val_loss: 0.7831 - val_accuracy: 0.5743\n",
      "Epoch 33/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.1767 - accuracy: 0.9504Restoring model weights from the end of the best epoch: 18.\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.68382\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.1774 - accuracy: 0.9505 - val_loss: 0.8046 - val_accuracy: 0.5446\n",
      "Epoch 33: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5143\n",
      "Test AUC for Layer 3: 0.4375\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5536\n",
      "Average Test AUC across all layers: 0.4883\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_163\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_628 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_465 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_326 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_629 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_466 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_327 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_630 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_467 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_631 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.9137 - accuracy: 0.4801\n",
      "Epoch 1: val_loss improved from inf to 0.69366, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 30ms/step - loss: 0.9142 - accuracy: 0.4813 - val_loss: 0.6937 - val_accuracy: 0.4504\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8041 - accuracy: 0.5568\n",
      "Epoch 2: val_loss did not improve from 0.69366\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.8069 - accuracy: 0.5387 - val_loss: 0.6942 - val_accuracy: 0.4885\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7217 - accuracy: 0.5966\n",
      "Epoch 3: val_loss did not improve from 0.69366\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7213 - accuracy: 0.5935 - val_loss: 0.6947 - val_accuracy: 0.4809\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6867 - accuracy: 0.6335\n",
      "Epoch 4: val_loss did not improve from 0.69366\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6879 - accuracy: 0.6309 - val_loss: 0.6953 - val_accuracy: 0.4809\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6867 - accuracy: 0.6562\n",
      "Epoch 5: val_loss did not improve from 0.69366\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6791 - accuracy: 0.6608 - val_loss: 0.6953 - val_accuracy: 0.4809\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6442 - accuracy: 0.6449\n",
      "Epoch 6: val_loss did not improve from 0.69366\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6444 - accuracy: 0.6459 - val_loss: 0.6953 - val_accuracy: 0.4656\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5986 - accuracy: 0.6562\n",
      "Epoch 7: val_loss did not improve from 0.69366\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6015 - accuracy: 0.6559 - val_loss: 0.6955 - val_accuracy: 0.4580\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6240 - accuracy: 0.6761\n",
      "Epoch 8: val_loss did not improve from 0.69366\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6156 - accuracy: 0.6733 - val_loss: 0.6958 - val_accuracy: 0.4809\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6014 - accuracy: 0.6875\n",
      "Epoch 9: val_loss did not improve from 0.69366\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5954 - accuracy: 0.6958 - val_loss: 0.6959 - val_accuracy: 0.4504\n",
      "Epoch 10/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.5215 - accuracy: 0.7375\n",
      "Epoch 10: val_loss did not improve from 0.69366\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5143 - accuracy: 0.7382 - val_loss: 0.6960 - val_accuracy: 0.4427\n",
      "Epoch 11/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5368 - accuracy: 0.7301\n",
      "Epoch 11: val_loss did not improve from 0.69366\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5336 - accuracy: 0.7307 - val_loss: 0.6960 - val_accuracy: 0.4733\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4977 - accuracy: 0.7699\n",
      "Epoch 12: val_loss did not improve from 0.69366\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4948 - accuracy: 0.7706 - val_loss: 0.6960 - val_accuracy: 0.4809\n",
      "Epoch 13/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4908 - accuracy: 0.7472\n",
      "Epoch 13: val_loss did not improve from 0.69366\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4831 - accuracy: 0.7531 - val_loss: 0.6961 - val_accuracy: 0.4962\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4554 - accuracy: 0.7869\n",
      "Epoch 14: val_loss did not improve from 0.69366\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4563 - accuracy: 0.7880 - val_loss: 0.6965 - val_accuracy: 0.4580\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4519 - accuracy: 0.7812\n",
      "Epoch 15: val_loss did not improve from 0.69366\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4553 - accuracy: 0.7830 - val_loss: 0.6968 - val_accuracy: 0.4580\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4160 - accuracy: 0.8295Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69366\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4252 - accuracy: 0.8229 - val_loss: 0.6974 - val_accuracy: 0.5038\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.4812\n",
      "Test AUC for Layer 1: 0.4627\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_164\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_632 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_468 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_328 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_633 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_469 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_329 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_634 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_470 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_635 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.9188 - accuracy: 0.4883\n",
      "Epoch 1: val_loss improved from inf to 0.69666, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.9220 - accuracy: 0.4887 - val_loss: 0.6967 - val_accuracy: 0.4887\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8227 - accuracy: 0.5352\n",
      "Epoch 2: val_loss did not improve from 0.69666\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.8217 - accuracy: 0.5376 - val_loss: 0.7018 - val_accuracy: 0.4887\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7752 - accuracy: 0.5527\n",
      "Epoch 3: val_loss did not improve from 0.69666\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7781 - accuracy: 0.5526 - val_loss: 0.7082 - val_accuracy: 0.4887\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7094 - accuracy: 0.5938\n",
      "Epoch 4: val_loss did not improve from 0.69666\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7086 - accuracy: 0.5940 - val_loss: 0.7150 - val_accuracy: 0.4887\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6794 - accuracy: 0.6543\n",
      "Epoch 5: val_loss did not improve from 0.69666\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6737 - accuracy: 0.6560 - val_loss: 0.7219 - val_accuracy: 0.4887\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6561 - accuracy: 0.6543\n",
      "Epoch 6: val_loss did not improve from 0.69666\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6556 - accuracy: 0.6560 - val_loss: 0.7278 - val_accuracy: 0.4887\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5850 - accuracy: 0.6953\n",
      "Epoch 7: val_loss did not improve from 0.69666\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5881 - accuracy: 0.6936 - val_loss: 0.7327 - val_accuracy: 0.4887\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5519 - accuracy: 0.7207\n",
      "Epoch 8: val_loss did not improve from 0.69666\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5558 - accuracy: 0.7143 - val_loss: 0.7385 - val_accuracy: 0.4887\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5779 - accuracy: 0.6914\n",
      "Epoch 9: val_loss did not improve from 0.69666\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5789 - accuracy: 0.6917 - val_loss: 0.7418 - val_accuracy: 0.4887\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5258 - accuracy: 0.7539\n",
      "Epoch 10: val_loss did not improve from 0.69666\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5234 - accuracy: 0.7575 - val_loss: 0.7437 - val_accuracy: 0.4887\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5172 - accuracy: 0.7383\n",
      "Epoch 11: val_loss did not improve from 0.69666\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5158 - accuracy: 0.7387 - val_loss: 0.7457 - val_accuracy: 0.4887\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5089 - accuracy: 0.7441\n",
      "Epoch 12: val_loss did not improve from 0.69666\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5057 - accuracy: 0.7481 - val_loss: 0.7467 - val_accuracy: 0.4887\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4611 - accuracy: 0.7726\n",
      "Epoch 13: val_loss did not improve from 0.69666\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4611 - accuracy: 0.7726 - val_loss: 0.7469 - val_accuracy: 0.4887\n",
      "Epoch 14/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4695 - accuracy: 0.7708\n",
      "Epoch 14: val_loss did not improve from 0.69666\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4618 - accuracy: 0.7782 - val_loss: 0.7491 - val_accuracy: 0.4887\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4397 - accuracy: 0.7932\n",
      "Epoch 15: val_loss did not improve from 0.69666\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4397 - accuracy: 0.7932 - val_loss: 0.7484 - val_accuracy: 0.4887\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4219 - accuracy: 0.8177Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69666\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4219 - accuracy: 0.8177 - val_loss: 0.7464 - val_accuracy: 0.4887\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4257\n",
      "Test AUC for Layer 2: 0.4659\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_165\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_636 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_471 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_330 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_637 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_472 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_331 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_638 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_473 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_639 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9224 - accuracy: 0.4850\n",
      "Epoch 1: val_loss improved from inf to 0.70086, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 22ms/step - loss: 0.9224 - accuracy: 0.4850 - val_loss: 0.7009 - val_accuracy: 0.4257\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8178 - accuracy: 0.5195\n",
      "Epoch 2: val_loss did not improve from 0.70086\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.8178 - accuracy: 0.5195 - val_loss: 0.7084 - val_accuracy: 0.4257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7502 - accuracy: 0.5736\n",
      "Epoch 3: val_loss did not improve from 0.70086\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7502 - accuracy: 0.5736 - val_loss: 0.7144 - val_accuracy: 0.4257\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7247 - accuracy: 0.5931\n",
      "Epoch 4: val_loss did not improve from 0.70086\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7247 - accuracy: 0.5931 - val_loss: 0.7174 - val_accuracy: 0.4257\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6882 - accuracy: 0.6096\n",
      "Epoch 5: val_loss did not improve from 0.70086\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6882 - accuracy: 0.6096 - val_loss: 0.7166 - val_accuracy: 0.4257\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6269 - accuracy: 0.6622\n",
      "Epoch 6: val_loss did not improve from 0.70086\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6269 - accuracy: 0.6622 - val_loss: 0.7166 - val_accuracy: 0.4257\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6035 - accuracy: 0.6727\n",
      "Epoch 7: val_loss did not improve from 0.70086\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6035 - accuracy: 0.6727 - val_loss: 0.7139 - val_accuracy: 0.4257\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5869 - accuracy: 0.7027\n",
      "Epoch 8: val_loss did not improve from 0.70086\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5869 - accuracy: 0.7027 - val_loss: 0.7099 - val_accuracy: 0.4257\n",
      "Epoch 9/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.5301 - accuracy: 0.7371\n",
      "Epoch 9: val_loss did not improve from 0.70086\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5560 - accuracy: 0.7177 - val_loss: 0.7073 - val_accuracy: 0.4356\n",
      "Epoch 10/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.5331 - accuracy: 0.7132\n",
      "Epoch 10: val_loss did not improve from 0.70086\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5301 - accuracy: 0.7267 - val_loss: 0.7045 - val_accuracy: 0.4455\n",
      "Epoch 11/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.5287 - accuracy: 0.7298\n",
      "Epoch 11: val_loss improved from 0.70086 to 0.70020, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5262 - accuracy: 0.7387 - val_loss: 0.7002 - val_accuracy: 0.4653\n",
      "Epoch 12/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.4981 - accuracy: 0.7461\n",
      "Epoch 12: val_loss improved from 0.70020 to 0.69869, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.4964 - accuracy: 0.7417 - val_loss: 0.6987 - val_accuracy: 0.5347\n",
      "Epoch 13/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.4921 - accuracy: 0.7695\n",
      "Epoch 13: val_loss improved from 0.69869 to 0.69868, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4837 - accuracy: 0.7778 - val_loss: 0.6987 - val_accuracy: 0.5050\n",
      "Epoch 14/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4857 - accuracy: 0.7859\n",
      "Epoch 14: val_loss improved from 0.69868 to 0.69705, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.4926 - accuracy: 0.7808 - val_loss: 0.6971 - val_accuracy: 0.5050\n",
      "Epoch 15/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4736 - accuracy: 0.7688\n",
      "Epoch 15: val_loss improved from 0.69705 to 0.69685, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4718 - accuracy: 0.7703 - val_loss: 0.6968 - val_accuracy: 0.5248\n",
      "Epoch 16/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4520 - accuracy: 0.7844\n",
      "Epoch 16: val_loss improved from 0.69685 to 0.69670, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4514 - accuracy: 0.7868 - val_loss: 0.6967 - val_accuracy: 0.5446\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4192 - accuracy: 0.8078\n",
      "Epoch 17: val_loss did not improve from 0.69670\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4192 - accuracy: 0.8078 - val_loss: 0.6984 - val_accuracy: 0.5050\n",
      "Epoch 18/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.3779 - accuracy: 0.8594\n",
      "Epoch 18: val_loss did not improve from 0.69670\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3754 - accuracy: 0.8619 - val_loss: 0.7006 - val_accuracy: 0.4950\n",
      "Epoch 19/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.3753 - accuracy: 0.8364\n",
      "Epoch 19: val_loss did not improve from 0.69670\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3819 - accuracy: 0.8288 - val_loss: 0.7048 - val_accuracy: 0.4851\n",
      "Epoch 20/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4013 - accuracy: 0.8303\n",
      "Epoch 20: val_loss did not improve from 0.69670\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4013 - accuracy: 0.8303 - val_loss: 0.7118 - val_accuracy: 0.4851\n",
      "Epoch 21/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3349 - accuracy: 0.8882\n",
      "Epoch 21: val_loss did not improve from 0.69670\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3412 - accuracy: 0.8844 - val_loss: 0.7180 - val_accuracy: 0.4752\n",
      "Epoch 22/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.3436 - accuracy: 0.8621\n",
      "Epoch 22: val_loss did not improve from 0.69670\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3539 - accuracy: 0.8498 - val_loss: 0.7239 - val_accuracy: 0.4653\n",
      "Epoch 23/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.3432 - accuracy: 0.8695\n",
      "Epoch 23: val_loss did not improve from 0.69670\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3405 - accuracy: 0.8679 - val_loss: 0.7338 - val_accuracy: 0.4554\n",
      "Epoch 24/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3259 - accuracy: 0.8769\n",
      "Epoch 24: val_loss did not improve from 0.69670\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3259 - accuracy: 0.8769 - val_loss: 0.7466 - val_accuracy: 0.4554\n",
      "Epoch 25/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3093 - accuracy: 0.8874\n",
      "Epoch 25: val_loss did not improve from 0.69670\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3093 - accuracy: 0.8874 - val_loss: 0.7621 - val_accuracy: 0.4554\n",
      "Epoch 26/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3135 - accuracy: 0.8829\n",
      "Epoch 26: val_loss did not improve from 0.69670\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3135 - accuracy: 0.8829 - val_loss: 0.7792 - val_accuracy: 0.4554\n",
      "Epoch 27/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2527 - accuracy: 0.9201\n",
      "Epoch 27: val_loss did not improve from 0.69670\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2607 - accuracy: 0.9204 - val_loss: 0.7926 - val_accuracy: 0.4554\n",
      "Epoch 28/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.2878 - accuracy: 0.8695\n",
      "Epoch 28: val_loss did not improve from 0.69670\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2802 - accuracy: 0.8784 - val_loss: 0.8015 - val_accuracy: 0.4455\n",
      "Epoch 29/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2691 - accuracy: 0.8976\n",
      "Epoch 29: val_loss did not improve from 0.69670\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2797 - accuracy: 0.8964 - val_loss: 0.8220 - val_accuracy: 0.4554\n",
      "Epoch 30/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2766 - accuracy: 0.8984\n",
      "Epoch 30: val_loss did not improve from 0.69670\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2772 - accuracy: 0.8964 - val_loss: 0.8413 - val_accuracy: 0.4455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.2359 - accuracy: 0.9141Restoring model weights from the end of the best epoch: 16.\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.69670\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2426 - accuracy: 0.9099 - val_loss: 0.8576 - val_accuracy: 0.4455\n",
      "Epoch 31: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5143\n",
      "Test AUC for Layer 3: 0.5037\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4737\n",
      "Average Test AUC across all layers: 0.4774\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_166\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_640 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_474 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_332 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_641 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_475 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_333 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_642 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_476 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_643 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.9518 - accuracy: 0.5000\n",
      "Epoch 1: val_loss improved from inf to 0.67662, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 30ms/step - loss: 0.9534 - accuracy: 0.5062 - val_loss: 0.6766 - val_accuracy: 0.6260\n",
      "Epoch 2/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.8581 - accuracy: 0.5219\n",
      "Epoch 2: val_loss improved from 0.67662 to 0.66610, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.8500 - accuracy: 0.5312 - val_loss: 0.6661 - val_accuracy: 0.6260\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7775 - accuracy: 0.5767\n",
      "Epoch 3: val_loss improved from 0.66610 to 0.66273, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.7753 - accuracy: 0.5810 - val_loss: 0.6627 - val_accuracy: 0.6260\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6935 - accuracy: 0.6307\n",
      "Epoch 4: val_loss did not improve from 0.66273\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6919 - accuracy: 0.6309 - val_loss: 0.6641 - val_accuracy: 0.6260\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6614 - accuracy: 0.6477\n",
      "Epoch 5: val_loss did not improve from 0.66273\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6575 - accuracy: 0.6509 - val_loss: 0.6694 - val_accuracy: 0.6260\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6440 - accuracy: 0.6420\n",
      "Epoch 6: val_loss did not improve from 0.66273\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6551 - accuracy: 0.6359 - val_loss: 0.6776 - val_accuracy: 0.6260\n",
      "Epoch 7/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.5391 - accuracy: 0.7312\n",
      "Epoch 7: val_loss did not improve from 0.66273\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5480 - accuracy: 0.7232 - val_loss: 0.6875 - val_accuracy: 0.6260\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5574 - accuracy: 0.7273\n",
      "Epoch 8: val_loss did not improve from 0.66273\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5526 - accuracy: 0.7307 - val_loss: 0.6971 - val_accuracy: 0.6260\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5610 - accuracy: 0.7045\n",
      "Epoch 9: val_loss did not improve from 0.66273\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5602 - accuracy: 0.7032 - val_loss: 0.7078 - val_accuracy: 0.6260\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5320 - accuracy: 0.7386\n",
      "Epoch 10: val_loss did not improve from 0.66273\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5225 - accuracy: 0.7456 - val_loss: 0.7191 - val_accuracy: 0.6260\n",
      "Epoch 11/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5132 - accuracy: 0.7301\n",
      "Epoch 11: val_loss did not improve from 0.66273\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4974 - accuracy: 0.7406 - val_loss: 0.7307 - val_accuracy: 0.6260\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4850 - accuracy: 0.7358\n",
      "Epoch 12: val_loss did not improve from 0.66273\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4843 - accuracy: 0.7481 - val_loss: 0.7407 - val_accuracy: 0.6260\n",
      "Epoch 13/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4724 - accuracy: 0.7642\n",
      "Epoch 13: val_loss did not improve from 0.66273\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4615 - accuracy: 0.7681 - val_loss: 0.7511 - val_accuracy: 0.6260\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4203 - accuracy: 0.8097\n",
      "Epoch 14: val_loss did not improve from 0.66273\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4105 - accuracy: 0.8180 - val_loss: 0.7602 - val_accuracy: 0.6260\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3830 - accuracy: 0.8352\n",
      "Epoch 15: val_loss did not improve from 0.66273\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3971 - accuracy: 0.8229 - val_loss: 0.7707 - val_accuracy: 0.6260\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3990 - accuracy: 0.8097\n",
      "Epoch 16: val_loss did not improve from 0.66273\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3939 - accuracy: 0.8130 - val_loss: 0.7799 - val_accuracy: 0.6260\n",
      "Epoch 17/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3835 - accuracy: 0.8153\n",
      "Epoch 17: val_loss did not improve from 0.66273\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3697 - accuracy: 0.8304 - val_loss: 0.7870 - val_accuracy: 0.6260\n",
      "Epoch 18/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3241 - accuracy: 0.8665Restoring model weights from the end of the best epoch: 3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: val_loss did not improve from 0.66273\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3416 - accuracy: 0.8554 - val_loss: 0.7945 - val_accuracy: 0.6260\n",
      "Epoch 18: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.6316\n",
      "Test AUC for Layer 1: 0.5211\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_167\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_644 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_477 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_334 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_645 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_478 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_335 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_646 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_479 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_647 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.8858 - accuracy: 0.5268\n",
      "Epoch 1: val_loss improved from inf to 0.69530, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 26ms/step - loss: 0.8840 - accuracy: 0.5169 - val_loss: 0.6953 - val_accuracy: 0.4286\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7995 - accuracy: 0.5508\n",
      "Epoch 2: val_loss improved from 0.69530 to 0.69462, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7972 - accuracy: 0.5508 - val_loss: 0.6946 - val_accuracy: 0.4436\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7564 - accuracy: 0.5977\n",
      "Epoch 3: val_loss improved from 0.69462 to 0.69369, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7604 - accuracy: 0.5921 - val_loss: 0.6937 - val_accuracy: 0.4737\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6923 - accuracy: 0.6133\n",
      "Epoch 4: val_loss improved from 0.69369 to 0.69257, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6963 - accuracy: 0.6109 - val_loss: 0.6926 - val_accuracy: 0.5038\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6880 - accuracy: 0.6113\n",
      "Epoch 5: val_loss improved from 0.69257 to 0.69187, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6914 - accuracy: 0.6090 - val_loss: 0.6919 - val_accuracy: 0.5639\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5683 - accuracy: 0.7070\n",
      "Epoch 6: val_loss improved from 0.69187 to 0.69157, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5657 - accuracy: 0.7086 - val_loss: 0.6916 - val_accuracy: 0.5489\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5693 - accuracy: 0.7070\n",
      "Epoch 7: val_loss improved from 0.69157 to 0.69002, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5732 - accuracy: 0.7030 - val_loss: 0.6900 - val_accuracy: 0.5714\n",
      "Epoch 8/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5641 - accuracy: 0.7063\n",
      "Epoch 8: val_loss did not improve from 0.69002\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5716 - accuracy: 0.7068 - val_loss: 0.6901 - val_accuracy: 0.5564\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5194 - accuracy: 0.7305\n",
      "Epoch 9: val_loss improved from 0.69002 to 0.68773, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5311 - accuracy: 0.7256 - val_loss: 0.6877 - val_accuracy: 0.5940\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5298 - accuracy: 0.7539\n",
      "Epoch 10: val_loss improved from 0.68773 to 0.68547, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5277 - accuracy: 0.7500 - val_loss: 0.6855 - val_accuracy: 0.5940\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4791 - accuracy: 0.7773\n",
      "Epoch 11: val_loss improved from 0.68547 to 0.68393, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4804 - accuracy: 0.7744 - val_loss: 0.6839 - val_accuracy: 0.5865\n",
      "Epoch 12/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4380 - accuracy: 0.8125\n",
      "Epoch 12: val_loss improved from 0.68393 to 0.68312, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.4505 - accuracy: 0.7970 - val_loss: 0.6831 - val_accuracy: 0.6015\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4555 - accuracy: 0.7773\n",
      "Epoch 13: val_loss improved from 0.68312 to 0.68220, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4581 - accuracy: 0.7763 - val_loss: 0.6822 - val_accuracy: 0.6015\n",
      "Epoch 14/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4048 - accuracy: 0.8229\n",
      "Epoch 14: val_loss did not improve from 0.68220\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4005 - accuracy: 0.8271 - val_loss: 0.6826 - val_accuracy: 0.5865\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4159 - accuracy: 0.8164\n",
      "Epoch 15: val_loss improved from 0.68220 to 0.68133, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.4116 - accuracy: 0.8177 - val_loss: 0.6813 - val_accuracy: 0.5865\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4233 - accuracy: 0.7891\n",
      "Epoch 16: val_loss did not improve from 0.68133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4263 - accuracy: 0.7914 - val_loss: 0.6828 - val_accuracy: 0.5940\n",
      "Epoch 17/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3795 - accuracy: 0.8340\n",
      "Epoch 17: val_loss did not improve from 0.68133\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3757 - accuracy: 0.8365 - val_loss: 0.6831 - val_accuracy: 0.5789\n",
      "Epoch 18/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3624 - accuracy: 0.8574\n",
      "Epoch 18: val_loss improved from 0.68133 to 0.67931, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.3573 - accuracy: 0.8628 - val_loss: 0.6793 - val_accuracy: 0.5714\n",
      "Epoch 19/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3380 - accuracy: 0.8672\n",
      "Epoch 19: val_loss improved from 0.67931 to 0.67744, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.3392 - accuracy: 0.8684 - val_loss: 0.6774 - val_accuracy: 0.5940\n",
      "Epoch 20/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3201 - accuracy: 0.8945\n",
      "Epoch 20: val_loss did not improve from 0.67744\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3278 - accuracy: 0.8891 - val_loss: 0.6785 - val_accuracy: 0.5639\n",
      "Epoch 21/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3276 - accuracy: 0.8574\n",
      "Epoch 21: val_loss improved from 0.67744 to 0.67645, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.3273 - accuracy: 0.8553 - val_loss: 0.6764 - val_accuracy: 0.5714\n",
      "Epoch 22/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3048 - accuracy: 0.8711\n",
      "Epoch 22: val_loss improved from 0.67645 to 0.67569, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.3013 - accuracy: 0.8741 - val_loss: 0.6757 - val_accuracy: 0.5940\n",
      "Epoch 23/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2870 - accuracy: 0.9160\n",
      "Epoch 23: val_loss improved from 0.67569 to 0.67476, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.2831 - accuracy: 0.9173 - val_loss: 0.6748 - val_accuracy: 0.5940\n",
      "Epoch 24/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2975 - accuracy: 0.8906\n",
      "Epoch 24: val_loss did not improve from 0.67476\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.2951 - accuracy: 0.8929 - val_loss: 0.6750 - val_accuracy: 0.6015\n",
      "Epoch 25/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2874 - accuracy: 0.8945\n",
      "Epoch 25: val_loss did not improve from 0.67476\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.2924 - accuracy: 0.8910 - val_loss: 0.6771 - val_accuracy: 0.5940\n",
      "Epoch 26/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2493 - accuracy: 0.9102\n",
      "Epoch 26: val_loss did not improve from 0.67476\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2514 - accuracy: 0.9079 - val_loss: 0.6763 - val_accuracy: 0.6015\n",
      "Epoch 27/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2469 - accuracy: 0.9258\n",
      "Epoch 27: val_loss did not improve from 0.67476\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.2476 - accuracy: 0.9248 - val_loss: 0.6770 - val_accuracy: 0.6015\n",
      "Epoch 28/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2345 - accuracy: 0.9297\n",
      "Epoch 28: val_loss did not improve from 0.67476\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.2380 - accuracy: 0.9267 - val_loss: 0.6785 - val_accuracy: 0.5940\n",
      "Epoch 29/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2501 - accuracy: 0.9121\n",
      "Epoch 29: val_loss did not improve from 0.67476\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2504 - accuracy: 0.9135 - val_loss: 0.6820 - val_accuracy: 0.5940\n",
      "Epoch 30/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2265 - accuracy: 0.9258\n",
      "Epoch 30: val_loss did not improve from 0.67476\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2258 - accuracy: 0.9267 - val_loss: 0.6899 - val_accuracy: 0.5940\n",
      "Epoch 31/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2263 - accuracy: 0.9336\n",
      "Epoch 31: val_loss did not improve from 0.67476\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.2271 - accuracy: 0.9286 - val_loss: 0.6943 - val_accuracy: 0.5865\n",
      "Epoch 32/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2264 - accuracy: 0.9258\n",
      "Epoch 32: val_loss did not improve from 0.67476\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2288 - accuracy: 0.9229 - val_loss: 0.6989 - val_accuracy: 0.5865\n",
      "Epoch 33/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2240 - accuracy: 0.9199\n",
      "Epoch 33: val_loss did not improve from 0.67476\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2306 - accuracy: 0.9173 - val_loss: 0.6972 - val_accuracy: 0.6316\n",
      "Epoch 34/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.1874 - accuracy: 0.9492\n",
      "Epoch 34: val_loss did not improve from 0.67476\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.1930 - accuracy: 0.9436 - val_loss: 0.6933 - val_accuracy: 0.6391\n",
      "Epoch 35/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2057 - accuracy: 0.9434\n",
      "Epoch 35: val_loss did not improve from 0.67476\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.2072 - accuracy: 0.9455 - val_loss: 0.6979 - val_accuracy: 0.6316\n",
      "Epoch 36/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.1653 - accuracy: 0.9609\n",
      "Epoch 36: val_loss did not improve from 0.67476\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.1661 - accuracy: 0.9605 - val_loss: 0.7056 - val_accuracy: 0.6316\n",
      "Epoch 37/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.1948 - accuracy: 0.9375\n",
      "Epoch 37: val_loss did not improve from 0.67476\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.1962 - accuracy: 0.9380 - val_loss: 0.7219 - val_accuracy: 0.6090\n",
      "Epoch 38/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2144 - accuracy: 0.9399Restoring model weights from the end of the best epoch: 23.\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.67476\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2030 - accuracy: 0.9436 - val_loss: 0.7345 - val_accuracy: 0.6165\n",
      "Epoch 38: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4950\n",
      "Test AUC for Layer 2: 0.5377\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_168\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_648 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_480 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_336 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_649 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_481 (B  (None, 256)               1024      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_337 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_650 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_482 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_651 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.9238 - accuracy: 0.4938\n",
      "Epoch 1: val_loss improved from inf to 0.69648, saving model to OpenAI_MLP_XGB/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 23ms/step - loss: 0.9316 - accuracy: 0.4940 - val_loss: 0.6965 - val_accuracy: 0.4851\n",
      "Epoch 2/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.8137 - accuracy: 0.5295\n",
      "Epoch 2: val_loss did not improve from 0.69648\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.8264 - accuracy: 0.5330 - val_loss: 0.7018 - val_accuracy: 0.4851\n",
      "Epoch 3/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.7511 - accuracy: 0.5839\n",
      "Epoch 3: val_loss did not improve from 0.69648\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7589 - accuracy: 0.5796 - val_loss: 0.7086 - val_accuracy: 0.4851\n",
      "Epoch 4/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.7065 - accuracy: 0.5984\n",
      "Epoch 4: val_loss did not improve from 0.69648\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7067 - accuracy: 0.5991 - val_loss: 0.7175 - val_accuracy: 0.4851\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6394 - accuracy: 0.6697\n",
      "Epoch 5: val_loss did not improve from 0.69648\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6394 - accuracy: 0.6697 - val_loss: 0.7266 - val_accuracy: 0.4851\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6371 - accuracy: 0.6682\n",
      "Epoch 6: val_loss did not improve from 0.69648\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6371 - accuracy: 0.6682 - val_loss: 0.7374 - val_accuracy: 0.4851\n",
      "Epoch 7/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5847 - accuracy: 0.6984\n",
      "Epoch 7: val_loss did not improve from 0.69648\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5824 - accuracy: 0.6997 - val_loss: 0.7475 - val_accuracy: 0.4851\n",
      "Epoch 8/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5796 - accuracy: 0.7047\n",
      "Epoch 8: val_loss did not improve from 0.69648\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5753 - accuracy: 0.7057 - val_loss: 0.7589 - val_accuracy: 0.4851\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5284 - accuracy: 0.7372\n",
      "Epoch 9: val_loss did not improve from 0.69648\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5284 - accuracy: 0.7372 - val_loss: 0.7667 - val_accuracy: 0.4851\n",
      "Epoch 10/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5051 - accuracy: 0.7500\n",
      "Epoch 10: val_loss did not improve from 0.69648\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5056 - accuracy: 0.7508 - val_loss: 0.7786 - val_accuracy: 0.4851\n",
      "Epoch 11/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4698 - accuracy: 0.7719\n",
      "Epoch 11: val_loss did not improve from 0.69648\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.4727 - accuracy: 0.7703 - val_loss: 0.7856 - val_accuracy: 0.4851\n",
      "Epoch 12/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4654 - accuracy: 0.7734\n",
      "Epoch 12: val_loss did not improve from 0.69648\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4721 - accuracy: 0.7673 - val_loss: 0.7893 - val_accuracy: 0.4851\n",
      "Epoch 13/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.4287 - accuracy: 0.7904\n",
      "Epoch 13: val_loss did not improve from 0.69648\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.4317 - accuracy: 0.7958 - val_loss: 0.7926 - val_accuracy: 0.4851\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4140 - accuracy: 0.8063\n",
      "Epoch 14: val_loss did not improve from 0.69648\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4140 - accuracy: 0.8063 - val_loss: 0.7972 - val_accuracy: 0.4851\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4203 - accuracy: 0.8033\n",
      "Epoch 15: val_loss did not improve from 0.69648\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4203 - accuracy: 0.8033 - val_loss: 0.8014 - val_accuracy: 0.4851\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3911 - accuracy: 0.8243Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69648\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3911 - accuracy: 0.8243 - val_loss: 0.8089 - val_accuracy: 0.4851\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.3429\n",
      "Test AUC for Layer 3: 0.5797\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4898\n",
      "Average Test AUC across all layers: 0.5462\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Title and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.46755\n",
      "[1]\tvalidation_0-auc:0.47724\n",
      "[2]\tvalidation_0-auc:0.46090\n",
      "[3]\tvalidation_0-auc:0.46965\n",
      "[4]\tvalidation_0-auc:0.45168\n",
      "[5]\tvalidation_0-auc:0.42355\n",
      "[6]\tvalidation_0-auc:0.44795\n",
      "[7]\tvalidation_0-auc:0.45331\n",
      "[8]\tvalidation_0-auc:0.47316\n",
      "[9]\tvalidation_0-auc:0.45028\n",
      "[10]\tvalidation_0-auc:0.46265\n",
      "[11]\tvalidation_0-auc:0.43651\n",
      "[12]\tvalidation_0-auc:0.41328\n",
      "[13]\tvalidation_0-auc:0.42519\n",
      "[14]\tvalidation_0-auc:0.41608\n",
      "[15]\tvalidation_0-auc:0.42810\n",
      "[16]\tvalidation_0-auc:0.42939\n",
      "[17]\tvalidation_0-auc:0.43102\n",
      "[18]\tvalidation_0-auc:0.42799\n",
      "[19]\tvalidation_0-auc:0.45950\n",
      "[20]\tvalidation_0-auc:0.46487\n",
      "[21]\tvalidation_0-auc:0.46312\n",
      "[22]\tvalidation_0-auc:0.46499\n",
      "[23]\tvalidation_0-auc:0.47502\n",
      "[24]\tvalidation_0-auc:0.46650\n",
      "[25]\tvalidation_0-auc:0.45425\n",
      "[26]\tvalidation_0-auc:0.45670\n",
      "[27]\tvalidation_0-auc:0.45682\n",
      "[28]\tvalidation_0-auc:0.45903\n",
      "[29]\tvalidation_0-auc:0.46452\n",
      "[30]\tvalidation_0-auc:0.46732\n",
      "[31]\tvalidation_0-auc:0.47257\n",
      "[32]\tvalidation_0-auc:0.47969\n",
      "[33]\tvalidation_0-auc:0.47386\n",
      "[34]\tvalidation_0-auc:0.45425\n",
      "[35]\tvalidation_0-auc:0.45378\n",
      "[36]\tvalidation_0-auc:0.45378\n",
      "[37]\tvalidation_0-auc:0.45110\n",
      "[38]\tvalidation_0-auc:0.44269\n",
      "[39]\tvalidation_0-auc:0.44479\n",
      "[40]\tvalidation_0-auc:0.44106\n",
      "[41]\tvalidation_0-auc:0.44678\n",
      "[42]\tvalidation_0-auc:0.44655\n",
      "[43]\tvalidation_0-auc:0.45425\n",
      "[44]\tvalidation_0-auc:0.45693\n",
      "[45]\tvalidation_0-auc:0.45366\n",
      "[46]\tvalidation_0-auc:0.45495\n",
      "[47]\tvalidation_0-auc:0.45623\n",
      "[48]\tvalidation_0-auc:0.45962\n",
      "[49]\tvalidation_0-auc:0.46218\n",
      "[50]\tvalidation_0-auc:0.45588\n",
      "[51]\tvalidation_0-auc:0.45740\n",
      "[52]\tvalidation_0-auc:0.46113\n",
      "[53]\tvalidation_0-auc:0.46265\n",
      "[54]\tvalidation_0-auc:0.45822\n",
      "[55]\tvalidation_0-auc:0.45892\n",
      "[56]\tvalidation_0-auc:0.45857\n",
      "[57]\tvalidation_0-auc:0.46289\n",
      "[58]\tvalidation_0-auc:0.46942\n",
      "[59]\tvalidation_0-auc:0.46837\n",
      "[60]\tvalidation_0-auc:0.47397\n",
      "[61]\tvalidation_0-auc:0.48179\n",
      "[62]\tvalidation_0-auc:0.48634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63]\tvalidation_0-auc:0.48121\n",
      "[64]\tvalidation_0-auc:0.48389\n",
      "[65]\tvalidation_0-auc:0.48121\n",
      "[66]\tvalidation_0-auc:0.47572\n",
      "[67]\tvalidation_0-auc:0.47409\n",
      "[68]\tvalidation_0-auc:0.47806\n",
      "[69]\tvalidation_0-auc:0.48063\n",
      "[70]\tvalidation_0-auc:0.48436\n",
      "[71]\tvalidation_0-auc:0.49020\n",
      "[72]\tvalidation_0-auc:0.49288\n",
      "[73]\tvalidation_0-auc:0.48296\n",
      "[74]\tvalidation_0-auc:0.48471\n",
      "[75]\tvalidation_0-auc:0.48413\n",
      "[76]\tvalidation_0-auc:0.48366\n",
      "[77]\tvalidation_0-auc:0.48786\n",
      "[78]\tvalidation_0-auc:0.48751\n",
      "[79]\tvalidation_0-auc:0.48564\n",
      "[80]\tvalidation_0-auc:0.49148\n",
      "[81]\tvalidation_0-auc:0.48739\n",
      "[82]\tvalidation_0-auc:0.48401\n",
      "[83]\tvalidation_0-auc:0.48926\n",
      "[84]\tvalidation_0-auc:0.48926\n",
      "[85]\tvalidation_0-auc:0.49218\n",
      "[86]\tvalidation_0-auc:0.48950\n",
      "[87]\tvalidation_0-auc:0.48378\n",
      "[88]\tvalidation_0-auc:0.48413\n",
      "[89]\tvalidation_0-auc:0.48646\n",
      "[90]\tvalidation_0-auc:0.48763\n",
      "[91]\tvalidation_0-auc:0.48599\n",
      "[92]\tvalidation_0-auc:0.48051\n",
      "[93]\tvalidation_0-auc:0.48191\n",
      "[94]\tvalidation_0-auc:0.48284\n",
      "[95]\tvalidation_0-auc:0.48284\n",
      "[96]\tvalidation_0-auc:0.48646\n",
      "[97]\tvalidation_0-auc:0.48319\n",
      "[98]\tvalidation_0-auc:0.48063\n",
      "[99]\tvalidation_0-auc:0.48343\n",
      "[100]\tvalidation_0-auc:0.48389\n",
      "[101]\tvalidation_0-auc:0.48389\n",
      "[102]\tvalidation_0-auc:0.48471\n",
      "[103]\tvalidation_0-auc:0.48775\n",
      "[104]\tvalidation_0-auc:0.48693\n",
      "[105]\tvalidation_0-auc:0.48518\n",
      "[106]\tvalidation_0-auc:0.49008\n",
      "[107]\tvalidation_0-auc:0.48833\n",
      "[108]\tvalidation_0-auc:0.48810\n",
      "[109]\tvalidation_0-auc:0.48599\n",
      "[110]\tvalidation_0-auc:0.48529\n",
      "[111]\tvalidation_0-auc:0.48109\n",
      "[112]\tvalidation_0-auc:0.47794\n",
      "[113]\tvalidation_0-auc:0.47631\n",
      "[114]\tvalidation_0-auc:0.47572\n",
      "[115]\tvalidation_0-auc:0.47502\n",
      "[116]\tvalidation_0-auc:0.47316\n",
      "[117]\tvalidation_0-auc:0.47456\n",
      "[118]\tvalidation_0-auc:0.47549\n",
      "[119]\tvalidation_0-auc:0.47642\n",
      "[120]\tvalidation_0-auc:0.47456\n",
      "[121]\tvalidation_0-auc:0.47432\n",
      "[122]\tvalidation_0-auc:0.47281\n",
      "[123]\tvalidation_0-auc:0.46779\n",
      "[124]\tvalidation_0-auc:0.46312\n",
      "[125]\tvalidation_0-auc:0.46265\n",
      "[126]\tvalidation_0-auc:0.45892\n",
      "[127]\tvalidation_0-auc:0.45798\n",
      "[128]\tvalidation_0-auc:0.45752\n",
      "[129]\tvalidation_0-auc:0.45775\n",
      "[130]\tvalidation_0-auc:0.45612\n",
      "[131]\tvalidation_0-auc:0.45728\n",
      "[132]\tvalidation_0-auc:0.45822\n",
      "[133]\tvalidation_0-auc:0.45507\n",
      "[134]\tvalidation_0-auc:0.45658\n",
      "[135]\tvalidation_0-auc:0.45635\n",
      "[136]\tvalidation_0-auc:0.45658\n",
      "[137]\tvalidation_0-auc:0.45355\n",
      "[138]\tvalidation_0-auc:0.45472\n",
      "[139]\tvalidation_0-auc:0.45518\n",
      "[140]\tvalidation_0-auc:0.45472\n",
      "[141]\tvalidation_0-auc:0.45472\n",
      "[142]\tvalidation_0-auc:0.45483\n",
      "[143]\tvalidation_0-auc:0.45472\n",
      "[144]\tvalidation_0-auc:0.44888\n",
      "[145]\tvalidation_0-auc:0.44549\n",
      "[146]\tvalidation_0-auc:0.44631\n",
      "[147]\tvalidation_0-auc:0.44234\n",
      "[148]\tvalidation_0-auc:0.43989\n",
      "[149]\tvalidation_0-auc:0.43651\n",
      "Test Accuracy for Layer 1: 0.4887\n",
      "Test AUC for Layer 1: 0.4292\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.40622\n",
      "[1]\tvalidation_0-auc:0.39932\n",
      "[2]\tvalidation_0-auc:0.37557\n",
      "[3]\tvalidation_0-auc:0.38394\n",
      "[4]\tvalidation_0-auc:0.38281\n",
      "[5]\tvalidation_0-auc:0.40962\n",
      "[6]\tvalidation_0-auc:0.39581\n",
      "[7]\tvalidation_0-auc:0.36222\n",
      "[8]\tvalidation_0-auc:0.34367\n",
      "[9]\tvalidation_0-auc:0.35667\n",
      "[10]\tvalidation_0-auc:0.36109\n",
      "[11]\tvalidation_0-auc:0.35328\n",
      "[12]\tvalidation_0-auc:0.36505\n",
      "[13]\tvalidation_0-auc:0.37002\n",
      "[14]\tvalidation_0-auc:0.34548\n",
      "[15]\tvalidation_0-auc:0.33450\n",
      "[16]\tvalidation_0-auc:0.36029\n",
      "[17]\tvalidation_0-auc:0.35260\n",
      "[18]\tvalidation_0-auc:0.33835\n",
      "[19]\tvalidation_0-auc:0.35079\n",
      "[20]\tvalidation_0-auc:0.35045\n",
      "[21]\tvalidation_0-auc:0.36663\n",
      "[22]\tvalidation_0-auc:0.37636\n",
      "[23]\tvalidation_0-auc:0.37726\n",
      "[24]\tvalidation_0-auc:0.38303\n",
      "[25]\tvalidation_0-auc:0.39593\n",
      "[26]\tvalidation_0-auc:0.39989\n",
      "[27]\tvalidation_0-auc:0.38382\n",
      "[28]\tvalidation_0-auc:0.38167\n",
      "[29]\tvalidation_0-auc:0.38439\n",
      "[30]\tvalidation_0-auc:0.38213\n",
      "[31]\tvalidation_0-auc:0.39276\n",
      "[32]\tvalidation_0-auc:0.39095\n",
      "[33]\tvalidation_0-auc:0.38778\n",
      "[34]\tvalidation_0-auc:0.38462\n",
      "[35]\tvalidation_0-auc:0.39219\n",
      "[36]\tvalidation_0-auc:0.37726\n",
      "[37]\tvalidation_0-auc:0.38405\n",
      "[38]\tvalidation_0-auc:0.37545\n",
      "[39]\tvalidation_0-auc:0.38676\n",
      "[40]\tvalidation_0-auc:0.38473\n",
      "[41]\tvalidation_0-auc:0.38394\n",
      "[42]\tvalidation_0-auc:0.37353\n",
      "[43]\tvalidation_0-auc:0.37975\n",
      "[44]\tvalidation_0-auc:0.37862\n",
      "[45]\tvalidation_0-auc:0.37443\n",
      "[46]\tvalidation_0-auc:0.38552\n",
      "[47]\tvalidation_0-auc:0.38620\n",
      "[48]\tvalidation_0-auc:0.38801\n",
      "[49]\tvalidation_0-auc:0.38541\n",
      "[50]\tvalidation_0-auc:0.38903\n",
      "[51]\tvalidation_0-auc:0.39672\n",
      "[52]\tvalidation_0-auc:0.39593\n",
      "[53]\tvalidation_0-auc:0.38778\n",
      "[54]\tvalidation_0-auc:0.38643\n",
      "[55]\tvalidation_0-auc:0.38122\n",
      "[56]\tvalidation_0-auc:0.37783\n",
      "[57]\tvalidation_0-auc:0.37828\n",
      "[58]\tvalidation_0-auc:0.37183\n",
      "[59]\tvalidation_0-auc:0.37149\n",
      "[60]\tvalidation_0-auc:0.37523\n",
      "[61]\tvalidation_0-auc:0.37364\n",
      "[62]\tvalidation_0-auc:0.37885\n",
      "[63]\tvalidation_0-auc:0.38167\n",
      "[64]\tvalidation_0-auc:0.37919\n",
      "[65]\tvalidation_0-auc:0.37805\n",
      "[66]\tvalidation_0-auc:0.37410\n",
      "[67]\tvalidation_0-auc:0.37794\n",
      "[68]\tvalidation_0-auc:0.37817\n",
      "[69]\tvalidation_0-auc:0.38133\n",
      "[70]\tvalidation_0-auc:0.37952\n",
      "[71]\tvalidation_0-auc:0.38473\n",
      "[72]\tvalidation_0-auc:0.39050\n",
      "[73]\tvalidation_0-auc:0.39095\n",
      "[74]\tvalidation_0-auc:0.38575\n",
      "[75]\tvalidation_0-auc:0.39050\n",
      "[76]\tvalidation_0-auc:0.39231\n",
      "[77]\tvalidation_0-auc:0.39412\n",
      "[78]\tvalidation_0-auc:0.39253\n",
      "[79]\tvalidation_0-auc:0.39457\n",
      "[80]\tvalidation_0-auc:0.39061\n",
      "[81]\tvalidation_0-auc:0.38903\n",
      "[82]\tvalidation_0-auc:0.38643\n",
      "[83]\tvalidation_0-auc:0.38778\n",
      "[84]\tvalidation_0-auc:0.38518\n",
      "[85]\tvalidation_0-auc:0.38529\n",
      "[86]\tvalidation_0-auc:0.38303\n",
      "[87]\tvalidation_0-auc:0.38247\n",
      "[88]\tvalidation_0-auc:0.37715\n",
      "[89]\tvalidation_0-auc:0.37681\n",
      "[90]\tvalidation_0-auc:0.37783\n",
      "[91]\tvalidation_0-auc:0.38032\n",
      "[92]\tvalidation_0-auc:0.37760\n",
      "[93]\tvalidation_0-auc:0.37670\n",
      "[94]\tvalidation_0-auc:0.37783\n",
      "[95]\tvalidation_0-auc:0.38247\n",
      "[96]\tvalidation_0-auc:0.38258\n",
      "[97]\tvalidation_0-auc:0.38145\n",
      "[98]\tvalidation_0-auc:0.38326\n",
      "[99]\tvalidation_0-auc:0.37477\n",
      "[100]\tvalidation_0-auc:0.37805\n",
      "[101]\tvalidation_0-auc:0.37817\n",
      "[102]\tvalidation_0-auc:0.37783\n",
      "[103]\tvalidation_0-auc:0.36900\n",
      "[104]\tvalidation_0-auc:0.37217\n",
      "[105]\tvalidation_0-auc:0.37217\n",
      "[106]\tvalidation_0-auc:0.36934\n",
      "[107]\tvalidation_0-auc:0.36357\n",
      "[108]\tvalidation_0-auc:0.36731\n",
      "[109]\tvalidation_0-auc:0.36652\n",
      "[110]\tvalidation_0-auc:0.36765\n",
      "[111]\tvalidation_0-auc:0.36991\n",
      "[112]\tvalidation_0-auc:0.36889\n",
      "[113]\tvalidation_0-auc:0.36900\n",
      "[114]\tvalidation_0-auc:0.36403\n",
      "[115]\tvalidation_0-auc:0.35984\n",
      "[116]\tvalidation_0-auc:0.36244\n",
      "[117]\tvalidation_0-auc:0.36471\n",
      "[118]\tvalidation_0-auc:0.36697\n",
      "[119]\tvalidation_0-auc:0.36357\n",
      "[120]\tvalidation_0-auc:0.36516\n",
      "[121]\tvalidation_0-auc:0.36776\n",
      "[122]\tvalidation_0-auc:0.36991\n",
      "[123]\tvalidation_0-auc:0.36889\n",
      "[124]\tvalidation_0-auc:0.37036\n",
      "[125]\tvalidation_0-auc:0.36833\n",
      "[126]\tvalidation_0-auc:0.36538\n",
      "[127]\tvalidation_0-auc:0.36799\n",
      "[128]\tvalidation_0-auc:0.36380\n",
      "[129]\tvalidation_0-auc:0.36493\n",
      "[130]\tvalidation_0-auc:0.36403\n",
      "[131]\tvalidation_0-auc:0.36606\n",
      "[132]\tvalidation_0-auc:0.36719\n",
      "[133]\tvalidation_0-auc:0.36391\n",
      "[134]\tvalidation_0-auc:0.36584\n",
      "[135]\tvalidation_0-auc:0.36527\n",
      "[136]\tvalidation_0-auc:0.36369\n",
      "[137]\tvalidation_0-auc:0.36063\n",
      "[138]\tvalidation_0-auc:0.36086\n",
      "[139]\tvalidation_0-auc:0.36199\n",
      "[140]\tvalidation_0-auc:0.35984\n",
      "[141]\tvalidation_0-auc:0.36075\n",
      "[142]\tvalidation_0-auc:0.36165\n",
      "[143]\tvalidation_0-auc:0.36018\n",
      "[144]\tvalidation_0-auc:0.36109\n",
      "[145]\tvalidation_0-auc:0.36199\n",
      "[146]\tvalidation_0-auc:0.36109\n",
      "[147]\tvalidation_0-auc:0.36199\n",
      "[148]\tvalidation_0-auc:0.36176\n",
      "[149]\tvalidation_0-auc:0.36278\n",
      "Test Accuracy for Layer 2: 0.4257\n",
      "Test AUC for Layer 2: 0.5140\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.53929\n",
      "[1]\tvalidation_0-auc:0.55032\n",
      "[2]\tvalidation_0-auc:0.48115\n",
      "[3]\tvalidation_0-auc:0.52947\n",
      "[4]\tvalidation_0-auc:0.51945\n",
      "[5]\tvalidation_0-auc:0.51824\n",
      "[6]\tvalidation_0-auc:0.51383\n",
      "[7]\tvalidation_0-auc:0.47654\n",
      "[8]\tvalidation_0-auc:0.48617\n",
      "[9]\tvalidation_0-auc:0.49900\n",
      "[10]\tvalidation_0-auc:0.48216\n",
      "[11]\tvalidation_0-auc:0.47113\n",
      "[12]\tvalidation_0-auc:0.45970\n",
      "[13]\tvalidation_0-auc:0.46091\n",
      "[14]\tvalidation_0-auc:0.45068\n",
      "[15]\tvalidation_0-auc:0.45369\n",
      "[16]\tvalidation_0-auc:0.45750\n",
      "[17]\tvalidation_0-auc:0.45209\n",
      "[18]\tvalidation_0-auc:0.45289\n",
      "[19]\tvalidation_0-auc:0.44387\n",
      "[20]\tvalidation_0-auc:0.44667\n",
      "[21]\tvalidation_0-auc:0.44507\n",
      "[22]\tvalidation_0-auc:0.45008\n",
      "[23]\tvalidation_0-auc:0.44166\n",
      "[24]\tvalidation_0-auc:0.42823\n",
      "[25]\tvalidation_0-auc:0.42622\n",
      "[26]\tvalidation_0-auc:0.41941\n",
      "[27]\tvalidation_0-auc:0.42983\n",
      "[28]\tvalidation_0-auc:0.42181\n",
      "[29]\tvalidation_0-auc:0.43324\n",
      "[30]\tvalidation_0-auc:0.42121\n",
      "[31]\tvalidation_0-auc:0.41560\n",
      "[32]\tvalidation_0-auc:0.41961\n",
      "[33]\tvalidation_0-auc:0.42642\n",
      "[34]\tvalidation_0-auc:0.42883\n",
      "[35]\tvalidation_0-auc:0.42843\n",
      "[36]\tvalidation_0-auc:0.43625\n",
      "[37]\tvalidation_0-auc:0.44306\n",
      "[38]\tvalidation_0-auc:0.43945\n",
      "[39]\tvalidation_0-auc:0.41921\n",
      "[40]\tvalidation_0-auc:0.42302\n",
      "[41]\tvalidation_0-auc:0.43204\n",
      "[42]\tvalidation_0-auc:0.47374\n",
      "[43]\tvalidation_0-auc:0.46391\n",
      "[44]\tvalidation_0-auc:0.45108\n",
      "[45]\tvalidation_0-auc:0.45569\n",
      "[46]\tvalidation_0-auc:0.44747\n",
      "[47]\tvalidation_0-auc:0.44186\n",
      "[48]\tvalidation_0-auc:0.43504\n",
      "[49]\tvalidation_0-auc:0.44286\n",
      "[50]\tvalidation_0-auc:0.44787\n",
      "[51]\tvalidation_0-auc:0.44587\n",
      "[52]\tvalidation_0-auc:0.44346\n",
      "[53]\tvalidation_0-auc:0.44186\n",
      "[54]\tvalidation_0-auc:0.44387\n",
      "[55]\tvalidation_0-auc:0.43825\n",
      "[56]\tvalidation_0-auc:0.43504\n",
      "[57]\tvalidation_0-auc:0.43063\n",
      "[58]\tvalidation_0-auc:0.43284\n",
      "[59]\tvalidation_0-auc:0.43865\n",
      "[60]\tvalidation_0-auc:0.43184\n",
      "[61]\tvalidation_0-auc:0.43043\n",
      "[62]\tvalidation_0-auc:0.42903\n",
      "[63]\tvalidation_0-auc:0.44226\n",
      "[64]\tvalidation_0-auc:0.44387\n",
      "[65]\tvalidation_0-auc:0.43224\n",
      "[66]\tvalidation_0-auc:0.43063\n",
      "[67]\tvalidation_0-auc:0.43264\n",
      "[68]\tvalidation_0-auc:0.43304\n",
      "[69]\tvalidation_0-auc:0.43825\n",
      "[70]\tvalidation_0-auc:0.43524\n",
      "[71]\tvalidation_0-auc:0.43284\n",
      "[72]\tvalidation_0-auc:0.43986\n",
      "[73]\tvalidation_0-auc:0.44186\n",
      "[74]\tvalidation_0-auc:0.43705\n",
      "[75]\tvalidation_0-auc:0.44146\n",
      "[76]\tvalidation_0-auc:0.43825\n",
      "[77]\tvalidation_0-auc:0.44026\n",
      "[78]\tvalidation_0-auc:0.44387\n",
      "[79]\tvalidation_0-auc:0.44346\n",
      "[80]\tvalidation_0-auc:0.43905\n",
      "[81]\tvalidation_0-auc:0.43504\n",
      "[82]\tvalidation_0-auc:0.43264\n",
      "[83]\tvalidation_0-auc:0.43364\n",
      "[84]\tvalidation_0-auc:0.43785\n",
      "[85]\tvalidation_0-auc:0.44066\n",
      "[86]\tvalidation_0-auc:0.44226\n",
      "[87]\tvalidation_0-auc:0.43765\n",
      "[88]\tvalidation_0-auc:0.44026\n",
      "[89]\tvalidation_0-auc:0.43204\n",
      "[90]\tvalidation_0-auc:0.43384\n",
      "[91]\tvalidation_0-auc:0.43384\n",
      "[92]\tvalidation_0-auc:0.42422\n",
      "[93]\tvalidation_0-auc:0.42302\n",
      "[94]\tvalidation_0-auc:0.42382\n",
      "[95]\tvalidation_0-auc:0.42642\n",
      "[96]\tvalidation_0-auc:0.42302\n",
      "[97]\tvalidation_0-auc:0.42221\n",
      "[98]\tvalidation_0-auc:0.43123\n",
      "[99]\tvalidation_0-auc:0.43424\n",
      "[100]\tvalidation_0-auc:0.42783\n",
      "[101]\tvalidation_0-auc:0.43023\n",
      "[102]\tvalidation_0-auc:0.43063\n",
      "[103]\tvalidation_0-auc:0.43264\n",
      "[104]\tvalidation_0-auc:0.42462\n",
      "[105]\tvalidation_0-auc:0.42662\n",
      "[106]\tvalidation_0-auc:0.42502\n",
      "[107]\tvalidation_0-auc:0.42061\n",
      "[108]\tvalidation_0-auc:0.41941\n",
      "[109]\tvalidation_0-auc:0.41379\n",
      "[110]\tvalidation_0-auc:0.41580\n",
      "[111]\tvalidation_0-auc:0.41860\n",
      "[112]\tvalidation_0-auc:0.42422\n",
      "[113]\tvalidation_0-auc:0.42021\n",
      "[114]\tvalidation_0-auc:0.41820\n",
      "[115]\tvalidation_0-auc:0.42081\n",
      "[116]\tvalidation_0-auc:0.42201\n",
      "[117]\tvalidation_0-auc:0.42021\n",
      "[118]\tvalidation_0-auc:0.42261\n",
      "[119]\tvalidation_0-auc:0.42021\n",
      "[120]\tvalidation_0-auc:0.42101\n",
      "[121]\tvalidation_0-auc:0.42201\n",
      "[122]\tvalidation_0-auc:0.42061\n",
      "[123]\tvalidation_0-auc:0.42261\n",
      "[124]\tvalidation_0-auc:0.42181\n",
      "[125]\tvalidation_0-auc:0.42101\n",
      "[126]\tvalidation_0-auc:0.41921\n",
      "[127]\tvalidation_0-auc:0.41640\n",
      "[128]\tvalidation_0-auc:0.41820\n",
      "[129]\tvalidation_0-auc:0.42181\n",
      "[130]\tvalidation_0-auc:0.42502\n",
      "[131]\tvalidation_0-auc:0.42502\n",
      "[132]\tvalidation_0-auc:0.42462\n",
      "[133]\tvalidation_0-auc:0.42582\n",
      "[134]\tvalidation_0-auc:0.42582\n",
      "[135]\tvalidation_0-auc:0.42582\n",
      "[136]\tvalidation_0-auc:0.42602\n",
      "[137]\tvalidation_0-auc:0.42783\n",
      "[138]\tvalidation_0-auc:0.42021\n",
      "[139]\tvalidation_0-auc:0.42302\n",
      "[140]\tvalidation_0-auc:0.42702\n",
      "[141]\tvalidation_0-auc:0.42422\n",
      "[142]\tvalidation_0-auc:0.42502\n",
      "[143]\tvalidation_0-auc:0.42702\n",
      "[144]\tvalidation_0-auc:0.43144\n",
      "[145]\tvalidation_0-auc:0.42783\n",
      "[146]\tvalidation_0-auc:0.42903\n",
      "[147]\tvalidation_0-auc:0.42863\n",
      "[148]\tvalidation_0-auc:0.42743\n",
      "[149]\tvalidation_0-auc:0.42342\n",
      "Test Accuracy for Layer 3: 0.5571\n",
      "Test AUC for Layer 3: 0.5658\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4905\n",
      "Average Test AUC across all layers: 0.5030\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Title and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.46205\n",
      "[1]\tvalidation_0-auc:0.47909\n",
      "[2]\tvalidation_0-auc:0.44350\n",
      "[3]\tvalidation_0-auc:0.43206\n",
      "[4]\tvalidation_0-auc:0.43405\n",
      "[5]\tvalidation_0-auc:0.44139\n",
      "[6]\tvalidation_0-auc:0.43529\n",
      "[7]\tvalidation_0-auc:0.44649\n",
      "[8]\tvalidation_0-auc:0.48743\n",
      "[9]\tvalidation_0-auc:0.51593\n",
      "[10]\tvalidation_0-auc:0.50249\n",
      "[11]\tvalidation_0-auc:0.49104\n",
      "[12]\tvalidation_0-auc:0.50299\n",
      "[13]\tvalidation_0-auc:0.50983\n",
      "[14]\tvalidation_0-auc:0.51220\n",
      "[15]\tvalidation_0-auc:0.51966\n",
      "[16]\tvalidation_0-auc:0.51891\n",
      "[17]\tvalidation_0-auc:0.52912\n",
      "[18]\tvalidation_0-auc:0.52850\n",
      "[19]\tvalidation_0-auc:0.50423\n",
      "[20]\tvalidation_0-auc:0.50398\n",
      "[21]\tvalidation_0-auc:0.50871\n",
      "[22]\tvalidation_0-auc:0.49602\n",
      "[23]\tvalidation_0-auc:0.49813\n",
      "[24]\tvalidation_0-auc:0.50075\n",
      "[25]\tvalidation_0-auc:0.49602\n",
      "[26]\tvalidation_0-auc:0.50510\n",
      "[27]\tvalidation_0-auc:0.50672\n",
      "[28]\tvalidation_0-auc:0.50784\n",
      "[29]\tvalidation_0-auc:0.51730\n",
      "[30]\tvalidation_0-auc:0.51979\n",
      "[31]\tvalidation_0-auc:0.52103\n",
      "[32]\tvalidation_0-auc:0.51419\n",
      "[33]\tvalidation_0-auc:0.52165\n",
      "[34]\tvalidation_0-auc:0.51817\n",
      "[35]\tvalidation_0-auc:0.51916\n",
      "[36]\tvalidation_0-auc:0.52364\n",
      "[37]\tvalidation_0-auc:0.52066\n",
      "[38]\tvalidation_0-auc:0.51406\n",
      "[39]\tvalidation_0-auc:0.50510\n",
      "[40]\tvalidation_0-auc:0.51319\n",
      "[41]\tvalidation_0-auc:0.50460\n",
      "[42]\tvalidation_0-auc:0.50734\n",
      "[43]\tvalidation_0-auc:0.51394\n",
      "[44]\tvalidation_0-auc:0.51730\n",
      "[45]\tvalidation_0-auc:0.51655\n",
      "[46]\tvalidation_0-auc:0.52190\n",
      "[47]\tvalidation_0-auc:0.53148\n",
      "[48]\tvalidation_0-auc:0.52999\n",
      "[49]\tvalidation_0-auc:0.52875\n",
      "[50]\tvalidation_0-auc:0.51842\n",
      "[51]\tvalidation_0-auc:0.51618\n",
      "[52]\tvalidation_0-auc:0.51680\n",
      "[53]\tvalidation_0-auc:0.51456\n",
      "[54]\tvalidation_0-auc:0.52028\n",
      "[55]\tvalidation_0-auc:0.51941\n",
      "[56]\tvalidation_0-auc:0.52688\n",
      "[57]\tvalidation_0-auc:0.52501\n",
      "[58]\tvalidation_0-auc:0.52115\n",
      "[59]\tvalidation_0-auc:0.52277\n",
      "[60]\tvalidation_0-auc:0.52912\n",
      "[61]\tvalidation_0-auc:0.53011\n",
      "[62]\tvalidation_0-auc:0.53385\n",
      "[63]\tvalidation_0-auc:0.53335\n",
      "[64]\tvalidation_0-auc:0.53186\n",
      "[65]\tvalidation_0-auc:0.53186\n",
      "[66]\tvalidation_0-auc:0.53385\n",
      "[67]\tvalidation_0-auc:0.52713\n",
      "[68]\tvalidation_0-auc:0.52240\n",
      "[69]\tvalidation_0-auc:0.52339\n",
      "[70]\tvalidation_0-auc:0.52265\n",
      "[71]\tvalidation_0-auc:0.52439\n",
      "[72]\tvalidation_0-auc:0.52489\n",
      "[73]\tvalidation_0-auc:0.52414\n",
      "[74]\tvalidation_0-auc:0.52464\n",
      "[75]\tvalidation_0-auc:0.52775\n",
      "[76]\tvalidation_0-auc:0.52875\n",
      "[77]\tvalidation_0-auc:0.52937\n",
      "[78]\tvalidation_0-auc:0.53211\n",
      "[79]\tvalidation_0-auc:0.53982\n",
      "[80]\tvalidation_0-auc:0.54206\n",
      "[81]\tvalidation_0-auc:0.54156\n",
      "[82]\tvalidation_0-auc:0.53982\n",
      "[83]\tvalidation_0-auc:0.53385\n",
      "[84]\tvalidation_0-auc:0.53074\n",
      "[85]\tvalidation_0-auc:0.52837\n",
      "[86]\tvalidation_0-auc:0.52651\n",
      "[87]\tvalidation_0-auc:0.52476\n",
      "[88]\tvalidation_0-auc:0.52899\n",
      "[89]\tvalidation_0-auc:0.52949\n",
      "[90]\tvalidation_0-auc:0.52750\n",
      "[91]\tvalidation_0-auc:0.53186\n",
      "[92]\tvalidation_0-auc:0.53584\n",
      "[93]\tvalidation_0-auc:0.53410\n",
      "[94]\tvalidation_0-auc:0.53683\n",
      "[95]\tvalidation_0-auc:0.53559\n",
      "[96]\tvalidation_0-auc:0.54044\n",
      "[97]\tvalidation_0-auc:0.54343\n",
      "[98]\tvalidation_0-auc:0.54405\n",
      "[99]\tvalidation_0-auc:0.54455\n",
      "[100]\tvalidation_0-auc:0.54331\n",
      "[101]\tvalidation_0-auc:0.54729\n",
      "[102]\tvalidation_0-auc:0.54480\n",
      "[103]\tvalidation_0-auc:0.54729\n",
      "[104]\tvalidation_0-auc:0.54853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105]\tvalidation_0-auc:0.55152\n",
      "[106]\tvalidation_0-auc:0.55587\n",
      "[107]\tvalidation_0-auc:0.55538\n",
      "[108]\tvalidation_0-auc:0.55525\n",
      "[109]\tvalidation_0-auc:0.55674\n",
      "[110]\tvalidation_0-auc:0.55177\n",
      "[111]\tvalidation_0-auc:0.55550\n",
      "[112]\tvalidation_0-auc:0.55338\n",
      "[113]\tvalidation_0-auc:0.55450\n",
      "[114]\tvalidation_0-auc:0.55376\n",
      "[115]\tvalidation_0-auc:0.55326\n",
      "[116]\tvalidation_0-auc:0.55824\n",
      "[117]\tvalidation_0-auc:0.56272\n",
      "[118]\tvalidation_0-auc:0.55774\n",
      "[119]\tvalidation_0-auc:0.55923\n",
      "[120]\tvalidation_0-auc:0.56010\n",
      "[121]\tvalidation_0-auc:0.55438\n",
      "[122]\tvalidation_0-auc:0.55127\n",
      "[123]\tvalidation_0-auc:0.54978\n",
      "[124]\tvalidation_0-auc:0.55202\n",
      "[125]\tvalidation_0-auc:0.55301\n",
      "[126]\tvalidation_0-auc:0.55027\n",
      "[127]\tvalidation_0-auc:0.55090\n",
      "[128]\tvalidation_0-auc:0.55276\n",
      "[129]\tvalidation_0-auc:0.55351\n",
      "[130]\tvalidation_0-auc:0.55376\n",
      "[131]\tvalidation_0-auc:0.55002\n",
      "[132]\tvalidation_0-auc:0.55301\n",
      "[133]\tvalidation_0-auc:0.55152\n",
      "[134]\tvalidation_0-auc:0.55002\n",
      "[135]\tvalidation_0-auc:0.55152\n",
      "[136]\tvalidation_0-auc:0.54953\n",
      "[137]\tvalidation_0-auc:0.55077\n",
      "[138]\tvalidation_0-auc:0.55052\n",
      "[139]\tvalidation_0-auc:0.55052\n",
      "[140]\tvalidation_0-auc:0.55139\n",
      "[141]\tvalidation_0-auc:0.54903\n",
      "[142]\tvalidation_0-auc:0.54953\n",
      "[143]\tvalidation_0-auc:0.54567\n",
      "[144]\tvalidation_0-auc:0.54766\n",
      "[145]\tvalidation_0-auc:0.54990\n",
      "[146]\tvalidation_0-auc:0.55226\n",
      "[147]\tvalidation_0-auc:0.55699\n",
      "[148]\tvalidation_0-auc:0.55724\n",
      "[149]\tvalidation_0-auc:0.55376\n",
      "Test Accuracy for Layer 1: 0.3684\n",
      "Test AUC for Layer 1: 0.4852\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.53620\n",
      "[1]\tvalidation_0-auc:0.46234\n",
      "[2]\tvalidation_0-auc:0.50777\n",
      "[3]\tvalidation_0-auc:0.54009\n",
      "[4]\tvalidation_0-auc:0.50729\n",
      "[5]\tvalidation_0-auc:0.50717\n",
      "[6]\tvalidation_0-auc:0.48190\n",
      "[7]\tvalidation_0-auc:0.49490\n",
      "[8]\tvalidation_0-auc:0.51470\n",
      "[9]\tvalidation_0-auc:0.52065\n",
      "[10]\tvalidation_0-auc:0.49089\n",
      "[11]\tvalidation_0-auc:0.49235\n",
      "[12]\tvalidation_0-auc:0.50607\n",
      "[13]\tvalidation_0-auc:0.50935\n",
      "[14]\tvalidation_0-auc:0.49040\n",
      "[15]\tvalidation_0-auc:0.50267\n",
      "[16]\tvalidation_0-auc:0.49891\n",
      "[17]\tvalidation_0-auc:0.49891\n",
      "[18]\tvalidation_0-auc:0.50401\n",
      "[19]\tvalidation_0-auc:0.51105\n",
      "[20]\tvalidation_0-auc:0.50996\n",
      "[21]\tvalidation_0-auc:0.51251\n",
      "[22]\tvalidation_0-auc:0.51931\n",
      "[23]\tvalidation_0-auc:0.52296\n",
      "[24]\tvalidation_0-auc:0.51506\n",
      "[25]\tvalidation_0-auc:0.51798\n",
      "[26]\tvalidation_0-auc:0.50923\n",
      "[27]\tvalidation_0-auc:0.49927\n",
      "[28]\tvalidation_0-auc:0.50389\n",
      "[29]\tvalidation_0-auc:0.50862\n",
      "[30]\tvalidation_0-auc:0.50559\n",
      "[31]\tvalidation_0-auc:0.51871\n",
      "[32]\tvalidation_0-auc:0.53426\n",
      "[33]\tvalidation_0-auc:0.52843\n",
      "[34]\tvalidation_0-auc:0.51628\n",
      "[35]\tvalidation_0-auc:0.52855\n",
      "[36]\tvalidation_0-auc:0.53353\n",
      "[37]\tvalidation_0-auc:0.52624\n",
      "[38]\tvalidation_0-auc:0.52187\n",
      "[39]\tvalidation_0-auc:0.53231\n",
      "[40]\tvalidation_0-auc:0.53158\n",
      "[41]\tvalidation_0-auc:0.52830\n",
      "[42]\tvalidation_0-auc:0.53146\n",
      "[43]\tvalidation_0-auc:0.52891\n",
      "[44]\tvalidation_0-auc:0.52794\n",
      "[45]\tvalidation_0-auc:0.52745\n",
      "[46]\tvalidation_0-auc:0.52964\n",
      "[47]\tvalidation_0-auc:0.53450\n",
      "[48]\tvalidation_0-auc:0.53778\n",
      "[49]\tvalidation_0-auc:0.53243\n",
      "[50]\tvalidation_0-auc:0.52575\n",
      "[51]\tvalidation_0-auc:0.51992\n",
      "[52]\tvalidation_0-auc:0.51761\n",
      "[53]\tvalidation_0-auc:0.52004\n",
      "[54]\tvalidation_0-auc:0.52259\n",
      "[55]\tvalidation_0-auc:0.51397\n",
      "[56]\tvalidation_0-auc:0.51020\n",
      "[57]\tvalidation_0-auc:0.51276\n",
      "[58]\tvalidation_0-auc:0.51567\n",
      "[59]\tvalidation_0-auc:0.51288\n",
      "[60]\tvalidation_0-auc:0.51689\n",
      "[61]\tvalidation_0-auc:0.52259\n",
      "[62]\tvalidation_0-auc:0.51676\n",
      "[63]\tvalidation_0-auc:0.52211\n",
      "[64]\tvalidation_0-auc:0.52089\n",
      "[65]\tvalidation_0-auc:0.52089\n",
      "[66]\tvalidation_0-auc:0.51676\n",
      "[67]\tvalidation_0-auc:0.51774\n",
      "[68]\tvalidation_0-auc:0.52041\n",
      "[69]\tvalidation_0-auc:0.51555\n",
      "[70]\tvalidation_0-auc:0.52017\n",
      "[71]\tvalidation_0-auc:0.51798\n",
      "[72]\tvalidation_0-auc:0.51980\n",
      "[73]\tvalidation_0-auc:0.51725\n",
      "[74]\tvalidation_0-auc:0.51968\n",
      "[75]\tvalidation_0-auc:0.51822\n",
      "[76]\tvalidation_0-auc:0.52915\n",
      "[77]\tvalidation_0-auc:0.52915\n",
      "[78]\tvalidation_0-auc:0.53365\n",
      "[79]\tvalidation_0-auc:0.53414\n",
      "[80]\tvalidation_0-auc:0.53353\n",
      "[81]\tvalidation_0-auc:0.53158\n",
      "[82]\tvalidation_0-auc:0.53304\n",
      "[83]\tvalidation_0-auc:0.53183\n",
      "[84]\tvalidation_0-auc:0.53256\n",
      "[85]\tvalidation_0-auc:0.53426\n",
      "[86]\tvalidation_0-auc:0.53499\n",
      "[87]\tvalidation_0-auc:0.52843\n",
      "[88]\tvalidation_0-auc:0.53280\n",
      "[89]\tvalidation_0-auc:0.52843\n",
      "[90]\tvalidation_0-auc:0.52843\n",
      "[91]\tvalidation_0-auc:0.52478\n",
      "[92]\tvalidation_0-auc:0.52259\n",
      "[93]\tvalidation_0-auc:0.52527\n",
      "[94]\tvalidation_0-auc:0.51895\n",
      "[95]\tvalidation_0-auc:0.52575\n",
      "[96]\tvalidation_0-auc:0.52454\n",
      "[97]\tvalidation_0-auc:0.51968\n",
      "[98]\tvalidation_0-auc:0.52259\n",
      "[99]\tvalidation_0-auc:0.52369\n",
      "[100]\tvalidation_0-auc:0.52490\n",
      "[101]\tvalidation_0-auc:0.52600\n",
      "[102]\tvalidation_0-auc:0.52745\n",
      "[103]\tvalidation_0-auc:0.52770\n",
      "[104]\tvalidation_0-auc:0.52405\n",
      "[105]\tvalidation_0-auc:0.52624\n",
      "[106]\tvalidation_0-auc:0.52466\n",
      "[107]\tvalidation_0-auc:0.52563\n",
      "[108]\tvalidation_0-auc:0.52430\n",
      "[109]\tvalidation_0-auc:0.52247\n",
      "[110]\tvalidation_0-auc:0.52466\n",
      "[111]\tvalidation_0-auc:0.52162\n",
      "[112]\tvalidation_0-auc:0.51871\n",
      "[113]\tvalidation_0-auc:0.51944\n",
      "[114]\tvalidation_0-auc:0.51628\n",
      "[115]\tvalidation_0-auc:0.51798\n",
      "[116]\tvalidation_0-auc:0.51725\n",
      "[117]\tvalidation_0-auc:0.51846\n",
      "[118]\tvalidation_0-auc:0.51980\n",
      "[119]\tvalidation_0-auc:0.52077\n",
      "[120]\tvalidation_0-auc:0.51895\n",
      "[121]\tvalidation_0-auc:0.51409\n",
      "[122]\tvalidation_0-auc:0.51676\n",
      "[123]\tvalidation_0-auc:0.51919\n",
      "[124]\tvalidation_0-auc:0.52150\n",
      "[125]\tvalidation_0-auc:0.52575\n",
      "[126]\tvalidation_0-auc:0.52478\n",
      "[127]\tvalidation_0-auc:0.52454\n",
      "[128]\tvalidation_0-auc:0.52624\n",
      "[129]\tvalidation_0-auc:0.52527\n",
      "[130]\tvalidation_0-auc:0.52672\n",
      "[131]\tvalidation_0-auc:0.52660\n",
      "[132]\tvalidation_0-auc:0.52417\n",
      "[133]\tvalidation_0-auc:0.52357\n",
      "[134]\tvalidation_0-auc:0.52259\n",
      "[135]\tvalidation_0-auc:0.52454\n",
      "[136]\tvalidation_0-auc:0.52065\n",
      "[137]\tvalidation_0-auc:0.52211\n",
      "[138]\tvalidation_0-auc:0.52223\n",
      "[139]\tvalidation_0-auc:0.51980\n",
      "[140]\tvalidation_0-auc:0.51458\n",
      "[141]\tvalidation_0-auc:0.51579\n",
      "[142]\tvalidation_0-auc:0.51385\n",
      "[143]\tvalidation_0-auc:0.50850\n",
      "[144]\tvalidation_0-auc:0.50620\n",
      "[145]\tvalidation_0-auc:0.50948\n",
      "[146]\tvalidation_0-auc:0.50948\n",
      "[147]\tvalidation_0-auc:0.50802\n",
      "[148]\tvalidation_0-auc:0.51118\n",
      "[149]\tvalidation_0-auc:0.50935\n",
      "Test Accuracy for Layer 2: 0.5149\n",
      "Test AUC for Layer 2: 0.4494\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.61401\n",
      "[1]\tvalidation_0-auc:0.59439\n",
      "[2]\tvalidation_0-auc:0.53336\n",
      "[3]\tvalidation_0-auc:0.55024\n",
      "[4]\tvalidation_0-auc:0.52296\n",
      "[5]\tvalidation_0-auc:0.48214\n",
      "[6]\tvalidation_0-auc:0.49745\n",
      "[7]\tvalidation_0-auc:0.49235\n",
      "[8]\tvalidation_0-auc:0.52374\n",
      "[9]\tvalidation_0-auc:0.51864\n",
      "[10]\tvalidation_0-auc:0.50589\n",
      "[11]\tvalidation_0-auc:0.52394\n",
      "[12]\tvalidation_0-auc:0.53257\n",
      "[13]\tvalidation_0-auc:0.52865\n",
      "[14]\tvalidation_0-auc:0.52649\n",
      "[15]\tvalidation_0-auc:0.53885\n",
      "[16]\tvalidation_0-auc:0.54474\n",
      "[17]\tvalidation_0-auc:0.54886\n",
      "[18]\tvalidation_0-auc:0.54435\n",
      "[19]\tvalidation_0-auc:0.53650\n",
      "[20]\tvalidation_0-auc:0.54160\n",
      "[21]\tvalidation_0-auc:0.56044\n",
      "[22]\tvalidation_0-auc:0.55063\n",
      "[23]\tvalidation_0-auc:0.54474\n",
      "[24]\tvalidation_0-auc:0.54180\n",
      "[25]\tvalidation_0-auc:0.54003\n",
      "[26]\tvalidation_0-auc:0.54651\n",
      "[27]\tvalidation_0-auc:0.53336\n",
      "[28]\tvalidation_0-auc:0.53257\n",
      "[29]\tvalidation_0-auc:0.53375\n",
      "[30]\tvalidation_0-auc:0.52786\n",
      "[31]\tvalidation_0-auc:0.54297\n",
      "[32]\tvalidation_0-auc:0.54121\n",
      "[33]\tvalidation_0-auc:0.53787\n",
      "[34]\tvalidation_0-auc:0.53100\n",
      "[35]\tvalidation_0-auc:0.52630\n",
      "[36]\tvalidation_0-auc:0.52786\n",
      "[37]\tvalidation_0-auc:0.51943\n",
      "[38]\tvalidation_0-auc:0.51864\n",
      "[39]\tvalidation_0-auc:0.53336\n",
      "[40]\tvalidation_0-auc:0.53434\n",
      "[41]\tvalidation_0-auc:0.53179\n",
      "[42]\tvalidation_0-auc:0.54278\n",
      "[43]\tvalidation_0-auc:0.54219\n",
      "[44]\tvalidation_0-auc:0.54670\n",
      "[45]\tvalidation_0-auc:0.54827\n",
      "[46]\tvalidation_0-auc:0.54768\n",
      "[47]\tvalidation_0-auc:0.54670\n",
      "[48]\tvalidation_0-auc:0.54631\n",
      "[49]\tvalidation_0-auc:0.53768\n",
      "[50]\tvalidation_0-auc:0.52924\n",
      "[51]\tvalidation_0-auc:0.52865\n",
      "[52]\tvalidation_0-auc:0.52943\n",
      "[53]\tvalidation_0-auc:0.52630\n",
      "[54]\tvalidation_0-auc:0.52355\n",
      "[55]\tvalidation_0-auc:0.52708\n",
      "[56]\tvalidation_0-auc:0.52590\n",
      "[57]\tvalidation_0-auc:0.52786\n",
      "[58]\tvalidation_0-auc:0.52060\n",
      "[59]\tvalidation_0-auc:0.52826\n",
      "[60]\tvalidation_0-auc:0.52826\n",
      "[61]\tvalidation_0-auc:0.53807\n",
      "[62]\tvalidation_0-auc:0.54003\n",
      "[63]\tvalidation_0-auc:0.53493\n",
      "[64]\tvalidation_0-auc:0.52551\n",
      "[65]\tvalidation_0-auc:0.52826\n",
      "[66]\tvalidation_0-auc:0.52826\n",
      "[67]\tvalidation_0-auc:0.52983\n",
      "[68]\tvalidation_0-auc:0.53375\n",
      "[69]\tvalidation_0-auc:0.53611\n",
      "[70]\tvalidation_0-auc:0.53670\n",
      "[71]\tvalidation_0-auc:0.53140\n",
      "[72]\tvalidation_0-auc:0.52708\n",
      "[73]\tvalidation_0-auc:0.51943\n",
      "[74]\tvalidation_0-auc:0.52237\n",
      "[75]\tvalidation_0-auc:0.52786\n",
      "[76]\tvalidation_0-auc:0.52649\n",
      "[77]\tvalidation_0-auc:0.52708\n",
      "[78]\tvalidation_0-auc:0.52316\n",
      "[79]\tvalidation_0-auc:0.52394\n",
      "[80]\tvalidation_0-auc:0.52002\n",
      "[81]\tvalidation_0-auc:0.52630\n",
      "[82]\tvalidation_0-auc:0.52688\n",
      "[83]\tvalidation_0-auc:0.52943\n",
      "[84]\tvalidation_0-auc:0.52512\n",
      "[85]\tvalidation_0-auc:0.52767\n",
      "[86]\tvalidation_0-auc:0.53061\n",
      "[87]\tvalidation_0-auc:0.53571\n",
      "[88]\tvalidation_0-auc:0.53571\n",
      "[89]\tvalidation_0-auc:0.53571\n",
      "[90]\tvalidation_0-auc:0.53395\n",
      "[91]\tvalidation_0-auc:0.52943\n",
      "[92]\tvalidation_0-auc:0.52943\n",
      "[93]\tvalidation_0-auc:0.52865\n",
      "[94]\tvalidation_0-auc:0.52610\n",
      "[95]\tvalidation_0-auc:0.52649\n",
      "[96]\tvalidation_0-auc:0.52747\n",
      "[97]\tvalidation_0-auc:0.52708\n",
      "[98]\tvalidation_0-auc:0.53061\n",
      "[99]\tvalidation_0-auc:0.52688\n",
      "[100]\tvalidation_0-auc:0.52119\n",
      "[101]\tvalidation_0-auc:0.52590\n",
      "[102]\tvalidation_0-auc:0.52492\n",
      "[103]\tvalidation_0-auc:0.52630\n",
      "[104]\tvalidation_0-auc:0.52728\n",
      "[105]\tvalidation_0-auc:0.52669\n",
      "[106]\tvalidation_0-auc:0.53218\n",
      "[107]\tvalidation_0-auc:0.53454\n",
      "[108]\tvalidation_0-auc:0.53120\n",
      "[109]\tvalidation_0-auc:0.53827\n",
      "[110]\tvalidation_0-auc:0.54141\n",
      "[111]\tvalidation_0-auc:0.54278\n",
      "[112]\tvalidation_0-auc:0.54867\n",
      "[113]\tvalidation_0-auc:0.54631\n",
      "[114]\tvalidation_0-auc:0.54749\n",
      "[115]\tvalidation_0-auc:0.54729\n",
      "[116]\tvalidation_0-auc:0.54592\n",
      "[117]\tvalidation_0-auc:0.54317\n",
      "[118]\tvalidation_0-auc:0.54749\n",
      "[119]\tvalidation_0-auc:0.54513\n",
      "[120]\tvalidation_0-auc:0.54121\n",
      "[121]\tvalidation_0-auc:0.53807\n",
      "[122]\tvalidation_0-auc:0.54435\n",
      "[123]\tvalidation_0-auc:0.54082\n",
      "[124]\tvalidation_0-auc:0.54003\n",
      "[125]\tvalidation_0-auc:0.54121\n",
      "[126]\tvalidation_0-auc:0.53846\n",
      "[127]\tvalidation_0-auc:0.54160\n",
      "[128]\tvalidation_0-auc:0.53611\n",
      "[129]\tvalidation_0-auc:0.53768\n",
      "[130]\tvalidation_0-auc:0.54003\n",
      "[131]\tvalidation_0-auc:0.53944\n",
      "[132]\tvalidation_0-auc:0.53375\n",
      "[133]\tvalidation_0-auc:0.53179\n",
      "[134]\tvalidation_0-auc:0.53140\n",
      "[135]\tvalidation_0-auc:0.53061\n",
      "[136]\tvalidation_0-auc:0.52826\n",
      "[137]\tvalidation_0-auc:0.52983\n",
      "[138]\tvalidation_0-auc:0.52943\n",
      "[139]\tvalidation_0-auc:0.53081\n",
      "[140]\tvalidation_0-auc:0.52767\n",
      "[141]\tvalidation_0-auc:0.52708\n",
      "[142]\tvalidation_0-auc:0.52473\n",
      "[143]\tvalidation_0-auc:0.52276\n",
      "[144]\tvalidation_0-auc:0.52237\n",
      "[145]\tvalidation_0-auc:0.52276\n",
      "[146]\tvalidation_0-auc:0.52531\n",
      "[147]\tvalidation_0-auc:0.52433\n",
      "[148]\tvalidation_0-auc:0.52237\n",
      "[149]\tvalidation_0-auc:0.52669\n",
      "Test Accuracy for Layer 3: 0.6571\n",
      "Test AUC for Layer 3: 0.5580\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5135\n",
      "Average Test AUC across all layers: 0.4975\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Full text and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.44246\n",
      "[1]\tvalidation_0-auc:0.38095\n",
      "[2]\tvalidation_0-auc:0.37488\n",
      "[3]\tvalidation_0-auc:0.41293\n",
      "[4]\tvalidation_0-auc:0.44830\n",
      "[5]\tvalidation_0-auc:0.44328\n",
      "[6]\tvalidation_0-auc:0.43522\n",
      "[7]\tvalidation_0-auc:0.44293\n",
      "[8]\tvalidation_0-auc:0.43032\n",
      "[9]\tvalidation_0-auc:0.43079\n",
      "[10]\tvalidation_0-auc:0.41573\n",
      "[11]\tvalidation_0-auc:0.40033\n",
      "[12]\tvalidation_0-auc:0.39776\n",
      "[13]\tvalidation_0-auc:0.38200\n",
      "[14]\tvalidation_0-auc:0.37710\n",
      "[15]\tvalidation_0-auc:0.38072\n",
      "[16]\tvalidation_0-auc:0.37010\n",
      "[17]\tvalidation_0-auc:0.37827\n",
      "[18]\tvalidation_0-auc:0.38189\n",
      "[19]\tvalidation_0-auc:0.37395\n",
      "[20]\tvalidation_0-auc:0.37407\n",
      "[21]\tvalidation_0-auc:0.37488\n",
      "[22]\tvalidation_0-auc:0.37535\n",
      "[23]\tvalidation_0-auc:0.37593\n",
      "[24]\tvalidation_0-auc:0.38329\n",
      "[25]\tvalidation_0-auc:0.38025\n",
      "[26]\tvalidation_0-auc:0.38410\n",
      "[27]\tvalidation_0-auc:0.38154\n",
      "[28]\tvalidation_0-auc:0.37722\n",
      "[29]\tvalidation_0-auc:0.38002\n",
      "[30]\tvalidation_0-auc:0.37862\n",
      "[31]\tvalidation_0-auc:0.37488\n",
      "[32]\tvalidation_0-auc:0.38352\n",
      "[33]\tvalidation_0-auc:0.38632\n",
      "[34]\tvalidation_0-auc:0.38702\n",
      "[35]\tvalidation_0-auc:0.39391\n",
      "[36]\tvalidation_0-auc:0.38235\n",
      "[37]\tvalidation_0-auc:0.37663\n",
      "[38]\tvalidation_0-auc:0.37243\n",
      "[39]\tvalidation_0-auc:0.37838\n",
      "[40]\tvalidation_0-auc:0.38399\n",
      "[41]\tvalidation_0-auc:0.38049\n",
      "[42]\tvalidation_0-auc:0.38235\n",
      "[43]\tvalidation_0-auc:0.38819\n",
      "[44]\tvalidation_0-auc:0.39099\n",
      "[45]\tvalidation_0-auc:0.39286\n",
      "[46]\tvalidation_0-auc:0.39449\n",
      "[47]\tvalidation_0-auc:0.39122\n",
      "[48]\tvalidation_0-auc:0.38644\n",
      "[49]\tvalidation_0-auc:0.38936\n",
      "[50]\tvalidation_0-auc:0.39122\n",
      "[51]\tvalidation_0-auc:0.39589\n",
      "[52]\tvalidation_0-auc:0.40453\n",
      "[53]\tvalidation_0-auc:0.40791\n",
      "[54]\tvalidation_0-auc:0.41118\n",
      "[55]\tvalidation_0-auc:0.41398\n",
      "[56]\tvalidation_0-auc:0.41527\n",
      "[57]\tvalidation_0-auc:0.41608\n",
      "[58]\tvalidation_0-auc:0.41246\n",
      "[59]\tvalidation_0-auc:0.41013\n",
      "[60]\tvalidation_0-auc:0.41480\n",
      "[61]\tvalidation_0-auc:0.41363\n",
      "[62]\tvalidation_0-auc:0.41970\n",
      "[63]\tvalidation_0-auc:0.41538\n",
      "[64]\tvalidation_0-auc:0.41853\n",
      "[65]\tvalidation_0-auc:0.41912\n",
      "[66]\tvalidation_0-auc:0.41468\n",
      "[67]\tvalidation_0-auc:0.41690\n",
      "[68]\tvalidation_0-auc:0.41106\n",
      "[69]\tvalidation_0-auc:0.41270\n",
      "[70]\tvalidation_0-auc:0.42274\n",
      "[71]\tvalidation_0-auc:0.41702\n",
      "[72]\tvalidation_0-auc:0.42262\n",
      "[73]\tvalidation_0-auc:0.42040\n",
      "[74]\tvalidation_0-auc:0.42227\n",
      "[75]\tvalidation_0-auc:0.42250\n",
      "[76]\tvalidation_0-auc:0.42227\n",
      "[77]\tvalidation_0-auc:0.42110\n",
      "[78]\tvalidation_0-auc:0.41457\n",
      "[79]\tvalidation_0-auc:0.41410\n",
      "[80]\tvalidation_0-auc:0.41503\n",
      "[81]\tvalidation_0-auc:0.41597\n",
      "[82]\tvalidation_0-auc:0.41842\n",
      "[83]\tvalidation_0-auc:0.42390\n",
      "[84]\tvalidation_0-auc:0.42647\n",
      "[85]\tvalidation_0-auc:0.42647\n",
      "[86]\tvalidation_0-auc:0.42180\n",
      "[87]\tvalidation_0-auc:0.42600\n",
      "[88]\tvalidation_0-auc:0.42157\n",
      "[89]\tvalidation_0-auc:0.42110\n",
      "[90]\tvalidation_0-auc:0.42297\n",
      "[91]\tvalidation_0-auc:0.42274\n",
      "[92]\tvalidation_0-auc:0.42414\n",
      "[93]\tvalidation_0-auc:0.42647\n",
      "[94]\tvalidation_0-auc:0.42740\n",
      "[95]\tvalidation_0-auc:0.42787\n",
      "[96]\tvalidation_0-auc:0.43254\n",
      "[97]\tvalidation_0-auc:0.42927\n",
      "[98]\tvalidation_0-auc:0.42927\n",
      "[99]\tvalidation_0-auc:0.42880\n",
      "[100]\tvalidation_0-auc:0.42834\n",
      "[101]\tvalidation_0-auc:0.42997\n",
      "[102]\tvalidation_0-auc:0.43534\n",
      "[103]\tvalidation_0-auc:0.43324\n",
      "[104]\tvalidation_0-auc:0.43569\n",
      "[105]\tvalidation_0-auc:0.43336\n",
      "[106]\tvalidation_0-auc:0.43231\n",
      "[107]\tvalidation_0-auc:0.43522\n",
      "[108]\tvalidation_0-auc:0.43371\n",
      "[109]\tvalidation_0-auc:0.43219\n",
      "[110]\tvalidation_0-auc:0.43627\n",
      "[111]\tvalidation_0-auc:0.43499\n",
      "[112]\tvalidation_0-auc:0.43534\n",
      "[113]\tvalidation_0-auc:0.43301\n",
      "[114]\tvalidation_0-auc:0.42997\n",
      "[115]\tvalidation_0-auc:0.43417\n",
      "[116]\tvalidation_0-auc:0.43627\n",
      "[117]\tvalidation_0-auc:0.43464\n",
      "[118]\tvalidation_0-auc:0.43721\n",
      "[119]\tvalidation_0-auc:0.43721\n",
      "[120]\tvalidation_0-auc:0.43978\n",
      "[121]\tvalidation_0-auc:0.44024\n",
      "[122]\tvalidation_0-auc:0.44001\n",
      "[123]\tvalidation_0-auc:0.43581\n",
      "[124]\tvalidation_0-auc:0.43487\n",
      "[125]\tvalidation_0-auc:0.43417\n",
      "[126]\tvalidation_0-auc:0.43627\n",
      "[127]\tvalidation_0-auc:0.43604\n",
      "[128]\tvalidation_0-auc:0.43908\n",
      "[129]\tvalidation_0-auc:0.43908\n",
      "[130]\tvalidation_0-auc:0.43931\n",
      "[131]\tvalidation_0-auc:0.43931\n",
      "[132]\tvalidation_0-auc:0.43674\n",
      "[133]\tvalidation_0-auc:0.43441\n",
      "[134]\tvalidation_0-auc:0.43814\n",
      "[135]\tvalidation_0-auc:0.43627\n",
      "[136]\tvalidation_0-auc:0.43674\n",
      "[137]\tvalidation_0-auc:0.43476\n",
      "[138]\tvalidation_0-auc:0.43919\n",
      "[139]\tvalidation_0-auc:0.43674\n",
      "[140]\tvalidation_0-auc:0.43884\n",
      "[141]\tvalidation_0-auc:0.43511\n",
      "[142]\tvalidation_0-auc:0.43534\n",
      "[143]\tvalidation_0-auc:0.43697\n",
      "[144]\tvalidation_0-auc:0.43429\n",
      "[145]\tvalidation_0-auc:0.43651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[146]\tvalidation_0-auc:0.43779\n",
      "[147]\tvalidation_0-auc:0.43487\n",
      "[148]\tvalidation_0-auc:0.43651\n",
      "[149]\tvalidation_0-auc:0.43441\n",
      "Test Accuracy for Layer 1: 0.4887\n",
      "Test AUC for Layer 1: 0.6656\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.48891\n",
      "[1]\tvalidation_0-auc:0.44106\n",
      "[2]\tvalidation_0-auc:0.46980\n",
      "[3]\tvalidation_0-auc:0.48371\n",
      "[4]\tvalidation_0-auc:0.46663\n",
      "[5]\tvalidation_0-auc:0.48224\n",
      "[6]\tvalidation_0-auc:0.48247\n",
      "[7]\tvalidation_0-auc:0.46437\n",
      "[8]\tvalidation_0-auc:0.45645\n",
      "[9]\tvalidation_0-auc:0.44581\n",
      "[10]\tvalidation_0-auc:0.41324\n",
      "[11]\tvalidation_0-auc:0.40328\n",
      "[12]\tvalidation_0-auc:0.38100\n",
      "[13]\tvalidation_0-auc:0.38914\n",
      "[14]\tvalidation_0-auc:0.38111\n",
      "[15]\tvalidation_0-auc:0.37579\n",
      "[16]\tvalidation_0-auc:0.35238\n",
      "[17]\tvalidation_0-auc:0.34231\n",
      "[18]\tvalidation_0-auc:0.35554\n",
      "[19]\tvalidation_0-auc:0.35701\n",
      "[20]\tvalidation_0-auc:0.37885\n",
      "[21]\tvalidation_0-auc:0.37036\n",
      "[22]\tvalidation_0-auc:0.38394\n",
      "[23]\tvalidation_0-auc:0.39491\n",
      "[24]\tvalidation_0-auc:0.39921\n",
      "[25]\tvalidation_0-auc:0.39321\n",
      "[26]\tvalidation_0-auc:0.39491\n",
      "[27]\tvalidation_0-auc:0.40577\n",
      "[28]\tvalidation_0-auc:0.40724\n",
      "[29]\tvalidation_0-auc:0.42455\n",
      "[30]\tvalidation_0-auc:0.43688\n",
      "[31]\tvalidation_0-auc:0.42952\n",
      "[32]\tvalidation_0-auc:0.42557\n",
      "[33]\tvalidation_0-auc:0.42229\n",
      "[34]\tvalidation_0-auc:0.42206\n",
      "[35]\tvalidation_0-auc:0.42511\n",
      "[36]\tvalidation_0-auc:0.42002\n",
      "[37]\tvalidation_0-auc:0.41855\n",
      "[38]\tvalidation_0-auc:0.42670\n",
      "[39]\tvalidation_0-auc:0.42500\n",
      "[40]\tvalidation_0-auc:0.42579\n",
      "[41]\tvalidation_0-auc:0.43473\n",
      "[42]\tvalidation_0-auc:0.43620\n",
      "[43]\tvalidation_0-auc:0.43937\n",
      "[44]\tvalidation_0-auc:0.44287\n",
      "[45]\tvalidation_0-auc:0.44095\n",
      "[46]\tvalidation_0-auc:0.43733\n",
      "[47]\tvalidation_0-auc:0.43778\n",
      "[48]\tvalidation_0-auc:0.43631\n",
      "[49]\tvalidation_0-auc:0.43462\n",
      "[50]\tvalidation_0-auc:0.42692\n",
      "[51]\tvalidation_0-auc:0.42229\n",
      "[52]\tvalidation_0-auc:0.42692\n",
      "[53]\tvalidation_0-auc:0.41731\n",
      "[54]\tvalidation_0-auc:0.41787\n",
      "[55]\tvalidation_0-auc:0.41697\n",
      "[56]\tvalidation_0-auc:0.41867\n",
      "[57]\tvalidation_0-auc:0.41957\n",
      "[58]\tvalidation_0-auc:0.42364\n",
      "[59]\tvalidation_0-auc:0.42342\n",
      "[60]\tvalidation_0-auc:0.42308\n",
      "[61]\tvalidation_0-auc:0.42715\n",
      "[62]\tvalidation_0-auc:0.42059\n",
      "[63]\tvalidation_0-auc:0.41267\n",
      "[64]\tvalidation_0-auc:0.40928\n",
      "[65]\tvalidation_0-auc:0.40441\n",
      "[66]\tvalidation_0-auc:0.40452\n",
      "[67]\tvalidation_0-auc:0.40339\n",
      "[68]\tvalidation_0-auc:0.40215\n",
      "[69]\tvalidation_0-auc:0.40317\n",
      "[70]\tvalidation_0-auc:0.40226\n",
      "[71]\tvalidation_0-auc:0.40000\n",
      "[72]\tvalidation_0-auc:0.40407\n",
      "[73]\tvalidation_0-auc:0.40814\n",
      "[74]\tvalidation_0-auc:0.40419\n",
      "[75]\tvalidation_0-auc:0.40385\n",
      "[76]\tvalidation_0-auc:0.40407\n",
      "[77]\tvalidation_0-auc:0.40136\n",
      "[78]\tvalidation_0-auc:0.40600\n",
      "[79]\tvalidation_0-auc:0.41109\n",
      "[80]\tvalidation_0-auc:0.41957\n",
      "[81]\tvalidation_0-auc:0.41833\n",
      "[82]\tvalidation_0-auc:0.41086\n",
      "[83]\tvalidation_0-auc:0.41199\n",
      "[84]\tvalidation_0-auc:0.41357\n",
      "[85]\tvalidation_0-auc:0.41222\n",
      "[86]\tvalidation_0-auc:0.41086\n",
      "[87]\tvalidation_0-auc:0.41290\n",
      "[88]\tvalidation_0-auc:0.40995\n",
      "[89]\tvalidation_0-auc:0.41538\n",
      "[90]\tvalidation_0-auc:0.41652\n",
      "[91]\tvalidation_0-auc:0.41878\n",
      "[92]\tvalidation_0-auc:0.41584\n",
      "[93]\tvalidation_0-auc:0.41946\n",
      "[94]\tvalidation_0-auc:0.42025\n",
      "[95]\tvalidation_0-auc:0.41991\n",
      "[96]\tvalidation_0-auc:0.42262\n",
      "[97]\tvalidation_0-auc:0.42296\n",
      "[98]\tvalidation_0-auc:0.42602\n",
      "[99]\tvalidation_0-auc:0.42127\n",
      "[100]\tvalidation_0-auc:0.42059\n",
      "[101]\tvalidation_0-auc:0.41900\n",
      "[102]\tvalidation_0-auc:0.41787\n",
      "[103]\tvalidation_0-auc:0.42455\n",
      "[104]\tvalidation_0-auc:0.42036\n",
      "[105]\tvalidation_0-auc:0.41810\n",
      "[106]\tvalidation_0-auc:0.41391\n",
      "[107]\tvalidation_0-auc:0.41652\n",
      "[108]\tvalidation_0-auc:0.41833\n",
      "[109]\tvalidation_0-auc:0.42511\n",
      "[110]\tvalidation_0-auc:0.42715\n",
      "[111]\tvalidation_0-auc:0.42670\n",
      "[112]\tvalidation_0-auc:0.42262\n",
      "[113]\tvalidation_0-auc:0.42149\n",
      "[114]\tvalidation_0-auc:0.41968\n",
      "[115]\tvalidation_0-auc:0.41719\n",
      "[116]\tvalidation_0-auc:0.41867\n",
      "[117]\tvalidation_0-auc:0.42014\n",
      "[118]\tvalidation_0-auc:0.42172\n",
      "[119]\tvalidation_0-auc:0.42376\n",
      "[120]\tvalidation_0-auc:0.42376\n",
      "[121]\tvalidation_0-auc:0.42715\n",
      "[122]\tvalidation_0-auc:0.42919\n",
      "[123]\tvalidation_0-auc:0.43235\n",
      "[124]\tvalidation_0-auc:0.43122\n",
      "[125]\tvalidation_0-auc:0.42828\n",
      "[126]\tvalidation_0-auc:0.42998\n",
      "[127]\tvalidation_0-auc:0.42862\n",
      "[128]\tvalidation_0-auc:0.42749\n",
      "[129]\tvalidation_0-auc:0.42986\n",
      "[130]\tvalidation_0-auc:0.42986\n",
      "[131]\tvalidation_0-auc:0.43009\n",
      "[132]\tvalidation_0-auc:0.43145\n",
      "[133]\tvalidation_0-auc:0.42907\n",
      "[134]\tvalidation_0-auc:0.43269\n",
      "[135]\tvalidation_0-auc:0.42975\n",
      "[136]\tvalidation_0-auc:0.42670\n",
      "[137]\tvalidation_0-auc:0.43281\n",
      "[138]\tvalidation_0-auc:0.43167\n",
      "[139]\tvalidation_0-auc:0.43462\n",
      "[140]\tvalidation_0-auc:0.43201\n",
      "[141]\tvalidation_0-auc:0.43100\n",
      "[142]\tvalidation_0-auc:0.42964\n",
      "[143]\tvalidation_0-auc:0.42783\n",
      "[144]\tvalidation_0-auc:0.42873\n",
      "[145]\tvalidation_0-auc:0.43348\n",
      "[146]\tvalidation_0-auc:0.43303\n",
      "[147]\tvalidation_0-auc:0.43405\n",
      "[148]\tvalidation_0-auc:0.43303\n",
      "[149]\tvalidation_0-auc:0.43654\n",
      "Test Accuracy for Layer 2: 0.4257\n",
      "Test AUC for Layer 2: 0.4663\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.41079\n",
      "[1]\tvalidation_0-auc:0.43123\n",
      "[2]\tvalidation_0-auc:0.40978\n",
      "[3]\tvalidation_0-auc:0.37670\n",
      "[4]\tvalidation_0-auc:0.37871\n",
      "[5]\tvalidation_0-auc:0.38573\n",
      "[6]\tvalidation_0-auc:0.38111\n",
      "[7]\tvalidation_0-auc:0.39374\n",
      "[8]\tvalidation_0-auc:0.39254\n",
      "[9]\tvalidation_0-auc:0.38232\n",
      "[10]\tvalidation_0-auc:0.38011\n",
      "[11]\tvalidation_0-auc:0.37029\n",
      "[12]\tvalidation_0-auc:0.36608\n",
      "[13]\tvalidation_0-auc:0.35385\n",
      "[14]\tvalidation_0-auc:0.36327\n",
      "[15]\tvalidation_0-auc:0.35405\n",
      "[16]\tvalidation_0-auc:0.36488\n",
      "[17]\tvalidation_0-auc:0.37029\n",
      "[18]\tvalidation_0-auc:0.36889\n",
      "[19]\tvalidation_0-auc:0.35966\n",
      "[20]\tvalidation_0-auc:0.36107\n",
      "[21]\tvalidation_0-auc:0.35325\n",
      "[22]\tvalidation_0-auc:0.33681\n",
      "[23]\tvalidation_0-auc:0.35305\n",
      "[24]\tvalidation_0-auc:0.34683\n",
      "[25]\tvalidation_0-auc:0.34683\n",
      "[26]\tvalidation_0-auc:0.35646\n",
      "[27]\tvalidation_0-auc:0.36427\n",
      "[28]\tvalidation_0-auc:0.36447\n",
      "[29]\tvalidation_0-auc:0.34864\n",
      "[30]\tvalidation_0-auc:0.34503\n",
      "[31]\tvalidation_0-auc:0.35345\n",
      "[32]\tvalidation_0-auc:0.34423\n",
      "[33]\tvalidation_0-auc:0.34443\n",
      "[34]\tvalidation_0-auc:0.34242\n",
      "[35]\tvalidation_0-auc:0.34423\n",
      "[36]\tvalidation_0-auc:0.34162\n",
      "[37]\tvalidation_0-auc:0.33861\n",
      "[38]\tvalidation_0-auc:0.34162\n",
      "[39]\tvalidation_0-auc:0.35044\n",
      "[40]\tvalidation_0-auc:0.34503\n",
      "[41]\tvalidation_0-auc:0.35024\n",
      "[42]\tvalidation_0-auc:0.33400\n",
      "[43]\tvalidation_0-auc:0.34282\n",
      "[44]\tvalidation_0-auc:0.32939\n",
      "[45]\tvalidation_0-auc:0.33561\n",
      "[46]\tvalidation_0-auc:0.33861\n",
      "[47]\tvalidation_0-auc:0.33320\n",
      "[48]\tvalidation_0-auc:0.33841\n",
      "[49]\tvalidation_0-auc:0.33641\n",
      "[50]\tvalidation_0-auc:0.34362\n",
      "[51]\tvalidation_0-auc:0.33841\n",
      "[52]\tvalidation_0-auc:0.33801\n",
      "[53]\tvalidation_0-auc:0.33801\n",
      "[54]\tvalidation_0-auc:0.34282\n",
      "[55]\tvalidation_0-auc:0.33480\n",
      "[56]\tvalidation_0-auc:0.33701\n",
      "[57]\tvalidation_0-auc:0.34102\n",
      "[58]\tvalidation_0-auc:0.34082\n",
      "[59]\tvalidation_0-auc:0.33721\n",
      "[60]\tvalidation_0-auc:0.34242\n",
      "[61]\tvalidation_0-auc:0.34483\n",
      "[62]\tvalidation_0-auc:0.34763\n",
      "[63]\tvalidation_0-auc:0.34443\n",
      "[64]\tvalidation_0-auc:0.35204\n",
      "[65]\tvalidation_0-auc:0.34643\n",
      "[66]\tvalidation_0-auc:0.34884\n",
      "[67]\tvalidation_0-auc:0.35445\n",
      "[68]\tvalidation_0-auc:0.35325\n",
      "[69]\tvalidation_0-auc:0.35325\n",
      "[70]\tvalidation_0-auc:0.35124\n",
      "[71]\tvalidation_0-auc:0.35806\n",
      "[72]\tvalidation_0-auc:0.35726\n",
      "[73]\tvalidation_0-auc:0.36127\n",
      "[74]\tvalidation_0-auc:0.35325\n",
      "[75]\tvalidation_0-auc:0.34844\n",
      "[76]\tvalidation_0-auc:0.35365\n",
      "[77]\tvalidation_0-auc:0.35124\n",
      "[78]\tvalidation_0-auc:0.35565\n",
      "[79]\tvalidation_0-auc:0.35726\n",
      "[80]\tvalidation_0-auc:0.35806\n",
      "[81]\tvalidation_0-auc:0.34884\n",
      "[82]\tvalidation_0-auc:0.34924\n",
      "[83]\tvalidation_0-auc:0.35164\n",
      "[84]\tvalidation_0-auc:0.35806\n",
      "[85]\tvalidation_0-auc:0.35605\n",
      "[86]\tvalidation_0-auc:0.35265\n",
      "[87]\tvalidation_0-auc:0.35365\n",
      "[88]\tvalidation_0-auc:0.34944\n",
      "[89]\tvalidation_0-auc:0.34964\n",
      "[90]\tvalidation_0-auc:0.35485\n",
      "[91]\tvalidation_0-auc:0.35806\n",
      "[92]\tvalidation_0-auc:0.36047\n",
      "[93]\tvalidation_0-auc:0.36367\n",
      "[94]\tvalidation_0-auc:0.36427\n",
      "[95]\tvalidation_0-auc:0.36407\n",
      "[96]\tvalidation_0-auc:0.35866\n",
      "[97]\tvalidation_0-auc:0.35365\n",
      "[98]\tvalidation_0-auc:0.35646\n",
      "[99]\tvalidation_0-auc:0.36247\n",
      "[100]\tvalidation_0-auc:0.36247\n",
      "[101]\tvalidation_0-auc:0.35946\n",
      "[102]\tvalidation_0-auc:0.35926\n",
      "[103]\tvalidation_0-auc:0.36287\n",
      "[104]\tvalidation_0-auc:0.36648\n",
      "[105]\tvalidation_0-auc:0.36929\n",
      "[106]\tvalidation_0-auc:0.36287\n",
      "[107]\tvalidation_0-auc:0.36488\n",
      "[108]\tvalidation_0-auc:0.36889\n",
      "[109]\tvalidation_0-auc:0.37049\n",
      "[110]\tvalidation_0-auc:0.36828\n",
      "[111]\tvalidation_0-auc:0.36127\n",
      "[112]\tvalidation_0-auc:0.36127\n",
      "[113]\tvalidation_0-auc:0.35766\n",
      "[114]\tvalidation_0-auc:0.36407\n",
      "[115]\tvalidation_0-auc:0.36548\n",
      "[116]\tvalidation_0-auc:0.36728\n",
      "[117]\tvalidation_0-auc:0.36608\n",
      "[118]\tvalidation_0-auc:0.36127\n",
      "[119]\tvalidation_0-auc:0.36407\n",
      "[120]\tvalidation_0-auc:0.36247\n",
      "[121]\tvalidation_0-auc:0.36207\n",
      "[122]\tvalidation_0-auc:0.36047\n",
      "[123]\tvalidation_0-auc:0.36768\n",
      "[124]\tvalidation_0-auc:0.36648\n",
      "[125]\tvalidation_0-auc:0.36889\n",
      "[126]\tvalidation_0-auc:0.37129\n",
      "[127]\tvalidation_0-auc:0.37731\n",
      "[128]\tvalidation_0-auc:0.37450\n",
      "[129]\tvalidation_0-auc:0.37731\n",
      "[130]\tvalidation_0-auc:0.37189\n",
      "[131]\tvalidation_0-auc:0.37410\n",
      "[132]\tvalidation_0-auc:0.37590\n",
      "[133]\tvalidation_0-auc:0.37189\n",
      "[134]\tvalidation_0-auc:0.37610\n",
      "[135]\tvalidation_0-auc:0.37490\n",
      "[136]\tvalidation_0-auc:0.37530\n",
      "[137]\tvalidation_0-auc:0.37771\n",
      "[138]\tvalidation_0-auc:0.37670\n",
      "[139]\tvalidation_0-auc:0.38212\n",
      "[140]\tvalidation_0-auc:0.38452\n",
      "[141]\tvalidation_0-auc:0.38292\n",
      "[142]\tvalidation_0-auc:0.38532\n",
      "[143]\tvalidation_0-auc:0.37951\n",
      "[144]\tvalidation_0-auc:0.37851\n",
      "[145]\tvalidation_0-auc:0.38252\n",
      "[146]\tvalidation_0-auc:0.38051\n",
      "[147]\tvalidation_0-auc:0.38372\n",
      "[148]\tvalidation_0-auc:0.38553\n",
      "[149]\tvalidation_0-auc:0.38733\n",
      "Test Accuracy for Layer 3: 0.5571\n",
      "Test AUC for Layer 3: 0.4545\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4905\n",
      "Average Test AUC across all layers: 0.5288\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Full text and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.51767\n",
      "[1]\tvalidation_0-auc:0.46416\n",
      "[2]\tvalidation_0-auc:0.48681\n",
      "[3]\tvalidation_0-auc:0.48731\n",
      "[4]\tvalidation_0-auc:0.52265\n",
      "[5]\tvalidation_0-auc:0.52489\n",
      "[6]\tvalidation_0-auc:0.53646\n",
      "[7]\tvalidation_0-auc:0.53410\n",
      "[8]\tvalidation_0-auc:0.51904\n",
      "[9]\tvalidation_0-auc:0.51655\n",
      "[10]\tvalidation_0-auc:0.51307\n",
      "[11]\tvalidation_0-auc:0.50212\n",
      "[12]\tvalidation_0-auc:0.52041\n",
      "[13]\tvalidation_0-auc:0.51568\n",
      "[14]\tvalidation_0-auc:0.52078\n",
      "[15]\tvalidation_0-auc:0.51991\n",
      "[16]\tvalidation_0-auc:0.49440\n",
      "[17]\tvalidation_0-auc:0.49328\n",
      "[18]\tvalidation_0-auc:0.50025\n",
      "[19]\tvalidation_0-auc:0.49652\n",
      "[20]\tvalidation_0-auc:0.48955\n",
      "[21]\tvalidation_0-auc:0.50000\n",
      "[22]\tvalidation_0-auc:0.49303\n",
      "[23]\tvalidation_0-auc:0.48731\n",
      "[24]\tvalidation_0-auc:0.49776\n",
      "[25]\tvalidation_0-auc:0.50560\n",
      "[26]\tvalidation_0-auc:0.51593\n",
      "[27]\tvalidation_0-auc:0.52003\n",
      "[28]\tvalidation_0-auc:0.52514\n",
      "[29]\tvalidation_0-auc:0.52563\n",
      "[30]\tvalidation_0-auc:0.52912\n",
      "[31]\tvalidation_0-auc:0.51842\n",
      "[32]\tvalidation_0-auc:0.52501\n",
      "[33]\tvalidation_0-auc:0.52389\n",
      "[34]\tvalidation_0-auc:0.52290\n",
      "[35]\tvalidation_0-auc:0.53011\n",
      "[36]\tvalidation_0-auc:0.53783\n",
      "[37]\tvalidation_0-auc:0.53758\n",
      "[38]\tvalidation_0-auc:0.52800\n",
      "[39]\tvalidation_0-auc:0.52912\n",
      "[40]\tvalidation_0-auc:0.52887\n",
      "[41]\tvalidation_0-auc:0.53086\n",
      "[42]\tvalidation_0-auc:0.53260\n",
      "[43]\tvalidation_0-auc:0.53783\n",
      "[44]\tvalidation_0-auc:0.54256\n",
      "[45]\tvalidation_0-auc:0.54530\n",
      "[46]\tvalidation_0-auc:0.54754\n",
      "[47]\tvalidation_0-auc:0.55562\n",
      "[48]\tvalidation_0-auc:0.55301\n",
      "[49]\tvalidation_0-auc:0.54878\n",
      "[50]\tvalidation_0-auc:0.54455\n",
      "[51]\tvalidation_0-auc:0.53683\n",
      "[52]\tvalidation_0-auc:0.53659\n",
      "[53]\tvalidation_0-auc:0.53609\n",
      "[54]\tvalidation_0-auc:0.54455\n",
      "[55]\tvalidation_0-auc:0.54629\n",
      "[56]\tvalidation_0-auc:0.54803\n",
      "[57]\tvalidation_0-auc:0.54368\n",
      "[58]\tvalidation_0-auc:0.53758\n",
      "[59]\tvalidation_0-auc:0.53186\n",
      "[60]\tvalidation_0-auc:0.53211\n",
      "[61]\tvalidation_0-auc:0.52937\n",
      "[62]\tvalidation_0-auc:0.53211\n",
      "[63]\tvalidation_0-auc:0.52713\n",
      "[64]\tvalidation_0-auc:0.53123\n",
      "[65]\tvalidation_0-auc:0.53385\n",
      "[66]\tvalidation_0-auc:0.53347\n",
      "[67]\tvalidation_0-auc:0.53310\n",
      "[68]\tvalidation_0-auc:0.53883\n",
      "[69]\tvalidation_0-auc:0.54044\n",
      "[70]\tvalidation_0-auc:0.53609\n",
      "[71]\tvalidation_0-auc:0.52663\n",
      "[72]\tvalidation_0-auc:0.53435\n",
      "[73]\tvalidation_0-auc:0.52787\n",
      "[74]\tvalidation_0-auc:0.52389\n",
      "[75]\tvalidation_0-auc:0.53571\n",
      "[76]\tvalidation_0-auc:0.52812\n",
      "[77]\tvalidation_0-auc:0.52763\n",
      "[78]\tvalidation_0-auc:0.53086\n",
      "[79]\tvalidation_0-auc:0.52763\n",
      "[80]\tvalidation_0-auc:0.52937\n",
      "[81]\tvalidation_0-auc:0.53360\n",
      "[82]\tvalidation_0-auc:0.53808\n",
      "[83]\tvalidation_0-auc:0.53808\n",
      "[84]\tvalidation_0-auc:0.54530\n",
      "[85]\tvalidation_0-auc:0.54256\n",
      "[86]\tvalidation_0-auc:0.54455\n",
      "[87]\tvalidation_0-auc:0.54903\n",
      "[88]\tvalidation_0-auc:0.54306\n",
      "[89]\tvalidation_0-auc:0.54654\n",
      "[90]\tvalidation_0-auc:0.54281\n",
      "[91]\tvalidation_0-auc:0.53609\n",
      "[92]\tvalidation_0-auc:0.53883\n",
      "[93]\tvalidation_0-auc:0.53659\n",
      "[94]\tvalidation_0-auc:0.53584\n",
      "[95]\tvalidation_0-auc:0.53497\n",
      "[96]\tvalidation_0-auc:0.53584\n",
      "[97]\tvalidation_0-auc:0.54181\n",
      "[98]\tvalidation_0-auc:0.54181\n",
      "[99]\tvalidation_0-auc:0.53820\n",
      "[100]\tvalidation_0-auc:0.53223\n",
      "[101]\tvalidation_0-auc:0.53148\n",
      "[102]\tvalidation_0-auc:0.53136\n",
      "[103]\tvalidation_0-auc:0.52875\n",
      "[104]\tvalidation_0-auc:0.53011\n",
      "[105]\tvalidation_0-auc:0.52563\n",
      "[106]\tvalidation_0-auc:0.52439\n",
      "[107]\tvalidation_0-auc:0.52302\n",
      "[108]\tvalidation_0-auc:0.52115\n",
      "[109]\tvalidation_0-auc:0.51966\n",
      "[110]\tvalidation_0-auc:0.51916\n",
      "[111]\tvalidation_0-auc:0.52016\n",
      "[112]\tvalidation_0-auc:0.51966\n",
      "[113]\tvalidation_0-auc:0.52003\n",
      "[114]\tvalidation_0-auc:0.52115\n",
      "[115]\tvalidation_0-auc:0.52638\n",
      "[116]\tvalidation_0-auc:0.52178\n",
      "[117]\tvalidation_0-auc:0.52091\n",
      "[118]\tvalidation_0-auc:0.52339\n",
      "[119]\tvalidation_0-auc:0.52103\n",
      "[120]\tvalidation_0-auc:0.52016\n",
      "[121]\tvalidation_0-auc:0.51543\n",
      "[122]\tvalidation_0-auc:0.51543\n",
      "[123]\tvalidation_0-auc:0.51493\n",
      "[124]\tvalidation_0-auc:0.51095\n",
      "[125]\tvalidation_0-auc:0.51518\n",
      "[126]\tvalidation_0-auc:0.51269\n",
      "[127]\tvalidation_0-auc:0.51058\n",
      "[128]\tvalidation_0-auc:0.51095\n",
      "[129]\tvalidation_0-auc:0.50796\n",
      "[130]\tvalidation_0-auc:0.50772\n",
      "[131]\tvalidation_0-auc:0.50473\n",
      "[132]\tvalidation_0-auc:0.50124\n",
      "[133]\tvalidation_0-auc:0.50299\n",
      "[134]\tvalidation_0-auc:0.49627\n",
      "[135]\tvalidation_0-auc:0.49676\n",
      "[136]\tvalidation_0-auc:0.49527\n",
      "[137]\tvalidation_0-auc:0.49851\n",
      "[138]\tvalidation_0-auc:0.49552\n",
      "[139]\tvalidation_0-auc:0.49801\n",
      "[140]\tvalidation_0-auc:0.50050\n",
      "[141]\tvalidation_0-auc:0.50398\n",
      "[142]\tvalidation_0-auc:0.50498\n",
      "[143]\tvalidation_0-auc:0.50100\n",
      "[144]\tvalidation_0-auc:0.49950\n",
      "[145]\tvalidation_0-auc:0.50423\n",
      "[146]\tvalidation_0-auc:0.50473\n",
      "[147]\tvalidation_0-auc:0.50697\n",
      "[148]\tvalidation_0-auc:0.50647\n",
      "[149]\tvalidation_0-auc:0.50572\n",
      "Test Accuracy for Layer 1: 0.3684\n",
      "Test AUC for Layer 1: 0.5824\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.46234\n",
      "[1]\tvalidation_0-auc:0.46914\n",
      "[2]\tvalidation_0-auc:0.48639\n",
      "[3]\tvalidation_0-auc:0.51458\n",
      "[4]\tvalidation_0-auc:0.59402\n",
      "[5]\tvalidation_0-auc:0.59342\n",
      "[6]\tvalidation_0-auc:0.60811\n",
      "[7]\tvalidation_0-auc:0.60313\n",
      "[8]\tvalidation_0-auc:0.58637\n",
      "[9]\tvalidation_0-auc:0.59026\n",
      "[10]\tvalidation_0-auc:0.59111\n",
      "[11]\tvalidation_0-auc:0.59487\n",
      "[12]\tvalidation_0-auc:0.60496\n",
      "[13]\tvalidation_0-auc:0.62694\n",
      "[14]\tvalidation_0-auc:0.63205\n",
      "[15]\tvalidation_0-auc:0.63666\n",
      "[16]\tvalidation_0-auc:0.63581\n",
      "[17]\tvalidation_0-auc:0.63703\n",
      "[18]\tvalidation_0-auc:0.64091\n",
      "[19]\tvalidation_0-auc:0.65039\n",
      "[20]\tvalidation_0-auc:0.64674\n",
      "[21]\tvalidation_0-auc:0.64942\n",
      "[22]\tvalidation_0-auc:0.65513\n",
      "[23]\tvalidation_0-auc:0.65452\n",
      "[24]\tvalidation_0-auc:0.64747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25]\tvalidation_0-auc:0.64723\n",
      "[26]\tvalidation_0-auc:0.65962\n",
      "[27]\tvalidation_0-auc:0.65428\n",
      "[28]\tvalidation_0-auc:0.65938\n",
      "[29]\tvalidation_0-auc:0.65986\n",
      "[30]\tvalidation_0-auc:0.65792\n",
      "[31]\tvalidation_0-auc:0.65136\n",
      "[32]\tvalidation_0-auc:0.65792\n",
      "[33]\tvalidation_0-auc:0.65075\n",
      "[34]\tvalidation_0-auc:0.64529\n",
      "[35]\tvalidation_0-auc:0.65403\n",
      "[36]\tvalidation_0-auc:0.65403\n",
      "[37]\tvalidation_0-auc:0.64832\n",
      "[38]\tvalidation_0-auc:0.64431\n",
      "[39]\tvalidation_0-auc:0.63338\n",
      "[40]\tvalidation_0-auc:0.64504\n",
      "[41]\tvalidation_0-auc:0.64480\n",
      "[42]\tvalidation_0-auc:0.64444\n",
      "[43]\tvalidation_0-auc:0.63958\n",
      "[44]\tvalidation_0-auc:0.63739\n",
      "[45]\tvalidation_0-auc:0.64128\n",
      "[46]\tvalidation_0-auc:0.63290\n",
      "[47]\tvalidation_0-auc:0.63022\n",
      "[48]\tvalidation_0-auc:0.62949\n",
      "[49]\tvalidation_0-auc:0.63630\n",
      "[50]\tvalidation_0-auc:0.64407\n",
      "[51]\tvalidation_0-auc:0.64456\n",
      "[52]\tvalidation_0-auc:0.64359\n",
      "[53]\tvalidation_0-auc:0.64662\n",
      "[54]\tvalidation_0-auc:0.63776\n",
      "[55]\tvalidation_0-auc:0.63776\n",
      "[56]\tvalidation_0-auc:0.63520\n",
      "[57]\tvalidation_0-auc:0.63435\n",
      "[58]\tvalidation_0-auc:0.63411\n",
      "[59]\tvalidation_0-auc:0.63970\n",
      "[60]\tvalidation_0-auc:0.63484\n",
      "[61]\tvalidation_0-auc:0.63921\n",
      "[62]\tvalidation_0-auc:0.64043\n",
      "[63]\tvalidation_0-auc:0.64091\n",
      "[64]\tvalidation_0-auc:0.64674\n",
      "[65]\tvalidation_0-auc:0.64164\n",
      "[66]\tvalidation_0-auc:0.64589\n",
      "[67]\tvalidation_0-auc:0.64990\n",
      "[68]\tvalidation_0-auc:0.65100\n",
      "[69]\tvalidation_0-auc:0.64772\n",
      "[70]\tvalidation_0-auc:0.64917\n",
      "[71]\tvalidation_0-auc:0.64966\n",
      "[72]\tvalidation_0-auc:0.64917\n",
      "[73]\tvalidation_0-auc:0.64942\n",
      "[74]\tvalidation_0-auc:0.65063\n",
      "[75]\tvalidation_0-auc:0.66266\n",
      "[76]\tvalidation_0-auc:0.66035\n",
      "[77]\tvalidation_0-auc:0.65525\n",
      "[78]\tvalidation_0-auc:0.65865\n",
      "[79]\tvalidation_0-auc:0.66181\n",
      "[80]\tvalidation_0-auc:0.66302\n",
      "[81]\tvalidation_0-auc:0.66557\n",
      "[82]\tvalidation_0-auc:0.66642\n",
      "[83]\tvalidation_0-auc:0.66764\n",
      "[84]\tvalidation_0-auc:0.66484\n",
      "[85]\tvalidation_0-auc:0.66594\n",
      "[86]\tvalidation_0-auc:0.66618\n",
      "[87]\tvalidation_0-auc:0.66448\n",
      "[88]\tvalidation_0-auc:0.66800\n",
      "[89]\tvalidation_0-auc:0.66630\n",
      "[90]\tvalidation_0-auc:0.66691\n",
      "[91]\tvalidation_0-auc:0.66727\n",
      "[92]\tvalidation_0-auc:0.66594\n",
      "[93]\tvalidation_0-auc:0.67080\n",
      "[94]\tvalidation_0-auc:0.67128\n",
      "[95]\tvalidation_0-auc:0.67177\n",
      "[96]\tvalidation_0-auc:0.67663\n",
      "[97]\tvalidation_0-auc:0.67979\n",
      "[98]\tvalidation_0-auc:0.67809\n",
      "[99]\tvalidation_0-auc:0.68149\n",
      "[100]\tvalidation_0-auc:0.67942\n",
      "[101]\tvalidation_0-auc:0.67638\n",
      "[102]\tvalidation_0-auc:0.67638\n",
      "[103]\tvalidation_0-auc:0.67881\n",
      "[104]\tvalidation_0-auc:0.67638\n",
      "[105]\tvalidation_0-auc:0.67481\n",
      "[106]\tvalidation_0-auc:0.67432\n",
      "[107]\tvalidation_0-auc:0.67347\n",
      "[108]\tvalidation_0-auc:0.67566\n",
      "[109]\tvalidation_0-auc:0.67274\n",
      "[110]\tvalidation_0-auc:0.67323\n",
      "[111]\tvalidation_0-auc:0.67201\n",
      "[112]\tvalidation_0-auc:0.67396\n",
      "[113]\tvalidation_0-auc:0.67614\n",
      "[114]\tvalidation_0-auc:0.67408\n",
      "[115]\tvalidation_0-auc:0.67456\n",
      "[116]\tvalidation_0-auc:0.67566\n",
      "[117]\tvalidation_0-auc:0.67468\n",
      "[118]\tvalidation_0-auc:0.67566\n",
      "[119]\tvalidation_0-auc:0.67396\n",
      "[120]\tvalidation_0-auc:0.67335\n",
      "[121]\tvalidation_0-auc:0.67493\n",
      "[122]\tvalidation_0-auc:0.67420\n",
      "[123]\tvalidation_0-auc:0.68088\n",
      "[124]\tvalidation_0-auc:0.67954\n",
      "[125]\tvalidation_0-auc:0.68319\n",
      "[126]\tvalidation_0-auc:0.68343\n",
      "[127]\tvalidation_0-auc:0.68440\n",
      "[128]\tvalidation_0-auc:0.68513\n",
      "[129]\tvalidation_0-auc:0.69023\n",
      "[130]\tvalidation_0-auc:0.69278\n",
      "[131]\tvalidation_0-auc:0.68890\n",
      "[132]\tvalidation_0-auc:0.69461\n",
      "[133]\tvalidation_0-auc:0.69388\n",
      "[134]\tvalidation_0-auc:0.69121\n",
      "[135]\tvalidation_0-auc:0.69145\n",
      "[136]\tvalidation_0-auc:0.69023\n",
      "[137]\tvalidation_0-auc:0.68975\n",
      "[138]\tvalidation_0-auc:0.68853\n",
      "[139]\tvalidation_0-auc:0.68805\n",
      "[140]\tvalidation_0-auc:0.68732\n",
      "[141]\tvalidation_0-auc:0.68829\n",
      "[142]\tvalidation_0-auc:0.68562\n",
      "[143]\tvalidation_0-auc:0.68404\n",
      "[144]\tvalidation_0-auc:0.68513\n",
      "[145]\tvalidation_0-auc:0.68780\n",
      "[146]\tvalidation_0-auc:0.68902\n",
      "[147]\tvalidation_0-auc:0.69121\n",
      "[148]\tvalidation_0-auc:0.69218\n",
      "[149]\tvalidation_0-auc:0.69169\n",
      "Test Accuracy for Layer 2: 0.5149\n",
      "Test AUC for Layer 2: 0.4368\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.48744\n",
      "[1]\tvalidation_0-auc:0.46978\n",
      "[2]\tvalidation_0-auc:0.45604\n",
      "[3]\tvalidation_0-auc:0.47606\n",
      "[4]\tvalidation_0-auc:0.45722\n",
      "[5]\tvalidation_0-auc:0.47469\n",
      "[6]\tvalidation_0-auc:0.48960\n",
      "[7]\tvalidation_0-auc:0.45055\n",
      "[8]\tvalidation_0-auc:0.47253\n",
      "[9]\tvalidation_0-auc:0.45604\n",
      "[10]\tvalidation_0-auc:0.44525\n",
      "[11]\tvalidation_0-auc:0.45016\n",
      "[12]\tvalidation_0-auc:0.47096\n",
      "[13]\tvalidation_0-auc:0.44878\n",
      "[14]\tvalidation_0-auc:0.47429\n",
      "[15]\tvalidation_0-auc:0.47979\n",
      "[16]\tvalidation_0-auc:0.48862\n",
      "[17]\tvalidation_0-auc:0.48744\n",
      "[18]\tvalidation_0-auc:0.49608\n",
      "[19]\tvalidation_0-auc:0.50059\n",
      "[20]\tvalidation_0-auc:0.46860\n",
      "[21]\tvalidation_0-auc:0.46075\n",
      "[22]\tvalidation_0-auc:0.47253\n",
      "[23]\tvalidation_0-auc:0.46958\n",
      "[24]\tvalidation_0-auc:0.46900\n",
      "[25]\tvalidation_0-auc:0.47115\n",
      "[26]\tvalidation_0-auc:0.47135\n",
      "[27]\tvalidation_0-auc:0.46880\n",
      "[28]\tvalidation_0-auc:0.45663\n",
      "[29]\tvalidation_0-auc:0.46330\n",
      "[30]\tvalidation_0-auc:0.46291\n",
      "[31]\tvalidation_0-auc:0.46546\n",
      "[32]\tvalidation_0-auc:0.47488\n",
      "[33]\tvalidation_0-auc:0.48842\n",
      "[34]\tvalidation_0-auc:0.48881\n",
      "[35]\tvalidation_0-auc:0.48450\n",
      "[36]\tvalidation_0-auc:0.49686\n",
      "[37]\tvalidation_0-auc:0.48901\n",
      "[38]\tvalidation_0-auc:0.48999\n",
      "[39]\tvalidation_0-auc:0.49686\n",
      "[40]\tvalidation_0-auc:0.48744\n",
      "[41]\tvalidation_0-auc:0.49215\n",
      "[42]\tvalidation_0-auc:0.49706\n",
      "[43]\tvalidation_0-auc:0.50235\n",
      "[44]\tvalidation_0-auc:0.50432\n",
      "[45]\tvalidation_0-auc:0.49882\n",
      "[46]\tvalidation_0-auc:0.50235\n",
      "[47]\tvalidation_0-auc:0.50392\n",
      "[48]\tvalidation_0-auc:0.51648\n",
      "[49]\tvalidation_0-auc:0.52041\n",
      "[50]\tvalidation_0-auc:0.52198\n",
      "[51]\tvalidation_0-auc:0.52433\n",
      "[52]\tvalidation_0-auc:0.52649\n",
      "[53]\tvalidation_0-auc:0.53179\n",
      "[54]\tvalidation_0-auc:0.52924\n",
      "[55]\tvalidation_0-auc:0.52786\n",
      "[56]\tvalidation_0-auc:0.52394\n",
      "[57]\tvalidation_0-auc:0.52885\n",
      "[58]\tvalidation_0-auc:0.52649\n",
      "[59]\tvalidation_0-auc:0.52276\n",
      "[60]\tvalidation_0-auc:0.52590\n",
      "[61]\tvalidation_0-auc:0.53061\n",
      "[62]\tvalidation_0-auc:0.52904\n",
      "[63]\tvalidation_0-auc:0.52669\n",
      "[64]\tvalidation_0-auc:0.52747\n",
      "[65]\tvalidation_0-auc:0.52826\n",
      "[66]\tvalidation_0-auc:0.52728\n",
      "[67]\tvalidation_0-auc:0.52983\n",
      "[68]\tvalidation_0-auc:0.53199\n",
      "[69]\tvalidation_0-auc:0.53100\n",
      "[70]\tvalidation_0-auc:0.52865\n",
      "[71]\tvalidation_0-auc:0.53100\n",
      "[72]\tvalidation_0-auc:0.53356\n",
      "[73]\tvalidation_0-auc:0.53434\n",
      "[74]\tvalidation_0-auc:0.53473\n",
      "[75]\tvalidation_0-auc:0.53709\n",
      "[76]\tvalidation_0-auc:0.54121\n",
      "[77]\tvalidation_0-auc:0.54278\n",
      "[78]\tvalidation_0-auc:0.54592\n",
      "[79]\tvalidation_0-auc:0.54513\n",
      "[80]\tvalidation_0-auc:0.54042\n",
      "[81]\tvalidation_0-auc:0.53807\n",
      "[82]\tvalidation_0-auc:0.53846\n",
      "[83]\tvalidation_0-auc:0.53571\n",
      "[84]\tvalidation_0-auc:0.53885\n",
      "[85]\tvalidation_0-auc:0.53846\n",
      "[86]\tvalidation_0-auc:0.54042\n",
      "[87]\tvalidation_0-auc:0.54239\n",
      "[88]\tvalidation_0-auc:0.54042\n",
      "[89]\tvalidation_0-auc:0.54042\n",
      "[90]\tvalidation_0-auc:0.53728\n",
      "[91]\tvalidation_0-auc:0.53807\n",
      "[92]\tvalidation_0-auc:0.54003\n",
      "[93]\tvalidation_0-auc:0.54121\n",
      "[94]\tvalidation_0-auc:0.54082\n",
      "[95]\tvalidation_0-auc:0.54199\n",
      "[96]\tvalidation_0-auc:0.54199\n",
      "[97]\tvalidation_0-auc:0.53728\n",
      "[98]\tvalidation_0-auc:0.53689\n",
      "[99]\tvalidation_0-auc:0.53925\n",
      "[100]\tvalidation_0-auc:0.53925\n",
      "[101]\tvalidation_0-auc:0.54356\n",
      "[102]\tvalidation_0-auc:0.54082\n",
      "[103]\tvalidation_0-auc:0.54239\n",
      "[104]\tvalidation_0-auc:0.54199\n",
      "[105]\tvalidation_0-auc:0.54003\n",
      "[106]\tvalidation_0-auc:0.54199\n",
      "[107]\tvalidation_0-auc:0.54121\n",
      "[108]\tvalidation_0-auc:0.54023\n",
      "[109]\tvalidation_0-auc:0.54513\n",
      "[110]\tvalidation_0-auc:0.54376\n",
      "[111]\tvalidation_0-auc:0.54396\n",
      "[112]\tvalidation_0-auc:0.54396\n",
      "[113]\tvalidation_0-auc:0.54376\n",
      "[114]\tvalidation_0-auc:0.54278\n",
      "[115]\tvalidation_0-auc:0.54474\n",
      "[116]\tvalidation_0-auc:0.54297\n",
      "[117]\tvalidation_0-auc:0.54062\n",
      "[118]\tvalidation_0-auc:0.54042\n",
      "[119]\tvalidation_0-auc:0.54003\n",
      "[120]\tvalidation_0-auc:0.54003\n",
      "[121]\tvalidation_0-auc:0.54101\n",
      "[122]\tvalidation_0-auc:0.54199\n",
      "[123]\tvalidation_0-auc:0.54396\n",
      "[124]\tvalidation_0-auc:0.54474\n",
      "[125]\tvalidation_0-auc:0.54160\n",
      "[126]\tvalidation_0-auc:0.53944\n",
      "[127]\tvalidation_0-auc:0.53630\n",
      "[128]\tvalidation_0-auc:0.53807\n",
      "[129]\tvalidation_0-auc:0.54160\n",
      "[130]\tvalidation_0-auc:0.53807\n",
      "[131]\tvalidation_0-auc:0.53571\n",
      "[132]\tvalidation_0-auc:0.54199\n",
      "[133]\tvalidation_0-auc:0.54042\n",
      "[134]\tvalidation_0-auc:0.54435\n",
      "[135]\tvalidation_0-auc:0.54278\n",
      "[136]\tvalidation_0-auc:0.54199\n",
      "[137]\tvalidation_0-auc:0.54160\n",
      "[138]\tvalidation_0-auc:0.54023\n",
      "[139]\tvalidation_0-auc:0.54160\n",
      "[140]\tvalidation_0-auc:0.53925\n",
      "[141]\tvalidation_0-auc:0.53925\n",
      "[142]\tvalidation_0-auc:0.53728\n",
      "[143]\tvalidation_0-auc:0.53964\n",
      "[144]\tvalidation_0-auc:0.54239\n",
      "[145]\tvalidation_0-auc:0.54160\n",
      "[146]\tvalidation_0-auc:0.53984\n",
      "[147]\tvalidation_0-auc:0.53885\n",
      "[148]\tvalidation_0-auc:0.53611\n",
      "[149]\tvalidation_0-auc:0.53513\n",
      "Test Accuracy for Layer 3: 0.6571\n",
      "Test AUC for Layer 3: 0.5643\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5135\n",
      "Average Test AUC across all layers: 0.5278\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Title + S_label\n",
      "Average Accuracy: 0.4963\n",
      "Average AUC: 0.4262\n",
      "  Layer 1 - Accuracy: 0.5113, AUC: 0.4957\n",
      "  Layer 2 - Accuracy: 0.5347, AUC: 0.3793\n",
      "  Layer 3 - Accuracy: 0.4429, AUC: 0.4036\n",
      "\n",
      "Combination: MLP with Title + L_label\n",
      "Average Accuracy: 0.5536\n",
      "Average AUC: 0.4883\n",
      "  Layer 1 - Accuracy: 0.6316, AUC: 0.5275\n",
      "  Layer 2 - Accuracy: 0.5149, AUC: 0.5000\n",
      "  Layer 3 - Accuracy: 0.5143, AUC: 0.4375\n",
      "\n",
      "Combination: MLP with Full text + S_label\n",
      "Average Accuracy: 0.4737\n",
      "Average AUC: 0.4774\n",
      "  Layer 1 - Accuracy: 0.4812, AUC: 0.4627\n",
      "  Layer 2 - Accuracy: 0.4257, AUC: 0.4659\n",
      "  Layer 3 - Accuracy: 0.5143, AUC: 0.5037\n",
      "\n",
      "Combination: MLP with Full text + L_label\n",
      "Average Accuracy: 0.4898\n",
      "Average AUC: 0.5462\n",
      "  Layer 1 - Accuracy: 0.6316, AUC: 0.5211\n",
      "  Layer 2 - Accuracy: 0.4950, AUC: 0.5377\n",
      "  Layer 3 - Accuracy: 0.3429, AUC: 0.5797\n",
      "\n",
      "Combination: XGBoost with Title + S_label\n",
      "Average Accuracy: 0.4905\n",
      "Average AUC: 0.5030\n",
      "  Layer 1 - Accuracy: 0.4887, AUC: 0.4292\n",
      "  Layer 2 - Accuracy: 0.4257, AUC: 0.5140\n",
      "  Layer 3 - Accuracy: 0.5571, AUC: 0.5658\n",
      "\n",
      "Combination: XGBoost with Title + L_label\n",
      "Average Accuracy: 0.5135\n",
      "Average AUC: 0.4975\n",
      "  Layer 1 - Accuracy: 0.3684, AUC: 0.4852\n",
      "  Layer 2 - Accuracy: 0.5149, AUC: 0.4494\n",
      "  Layer 3 - Accuracy: 0.6571, AUC: 0.5580\n",
      "\n",
      "Combination: XGBoost with Full text + S_label\n",
      "Average Accuracy: 0.4905\n",
      "Average AUC: 0.5288\n",
      "  Layer 1 - Accuracy: 0.4887, AUC: 0.6656\n",
      "  Layer 2 - Accuracy: 0.4257, AUC: 0.4663\n",
      "  Layer 3 - Accuracy: 0.5571, AUC: 0.4545\n",
      "\n",
      "Combination: XGBoost with Full text + L_label\n",
      "Average Accuracy: 0.5135\n",
      "Average AUC: 0.5278\n",
      "  Layer 1 - Accuracy: 0.3684, AUC: 0.5824\n",
      "  Layer 2 - Accuracy: 0.5149, AUC: 0.4368\n",
      "  Layer 3 - Accuracy: 0.6571, AUC: 0.5643\n",
      "MLP summary comparison visualization saved as: OpenAI_MLP_XGB/visualizations_summary/SLB\\mlp_performance_comparison.png\n",
      "XGBoost summary comparison visualization saved as: OpenAI_MLP_XGB/visualizations_summary/SLB\\xgb_performance_comparison.png\n",
      "MLP layer performance visualization saved as: OpenAI_MLP_XGB/visualizations_summary/SLB\\mlp_layer_performance.png\n",
      "XGBoost layer performance visualization saved as: OpenAI_MLP_XGB/visualizations_summary/SLB\\xgboost_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'OpenAI_MLP_XGB' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "from xgboost import callback\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, model_type, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class ClimateNewsOpenAIPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'OpenAI_MLP_XGB/visualizations_mlp/SLB'\n",
    "        self.xgb_viz_dir = 'OpenAI_MLP_XGB/visualizations_xgb/SLB'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs(self.xgb_viz_dir, exist_ok=True)\n",
    "        os.makedirs('OpenAI_MLP_XGB/visualizations_summary/SLB', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert embeddings to numpy arrays\n",
    "        self.data['Title_embedding'] = self.data['Title_embedding_vector'].apply(parse_embedding)\n",
    "        self.data['Fulltext_embedding'] = self.data['Full_text_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_title_embedding = self.data['Title_embedding'].iloc[0]\n",
    "        sample_fulltext_embedding = self.data['Fulltext_embedding'].iloc[0]\n",
    "        \n",
    "        print(f\"Sample Title embedding shape: {sample_title_embedding.shape}\")\n",
    "        print(f\"Sample Fulltext embedding shape: {sample_fulltext_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2014-2020), validation (2020-2021), test (2021-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2020-10-31'),\n",
    "            'val_start': pd.Timestamp('2020-11-01'),\n",
    "            'val_end': pd.Timestamp('2021-10-31'),\n",
    "            'test_start': pd.Timestamp('2021-11-01'),\n",
    "            'test_end': pd.Timestamp('2022-10-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2014-2021), validation (2021-2022), test (2022-2023)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2021-10-31'),\n",
    "            'val_start': pd.Timestamp('2021-11-01'),\n",
    "            'val_end': pd.Timestamp('2022-10-31'),\n",
    "            'test_start': pd.Timestamp('2022-11-01'),\n",
    "            'test_end': pd.Timestamp('2023-10-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2014-2022), validation (2022-2023), test (2023-2024)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2022-10-31'),\n",
    "            'val_start': pd.Timestamp('2022-11-01'),\n",
    "            'val_end': pd.Timestamp('2023-10-31'),\n",
    "            'test_start': pd.Timestamp('2023-11-01'),\n",
    "            'test_end': pd.Timestamp('2024-11-01')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, text_col, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            text_col: The column containing the embeddings ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data[text_col].values)\n",
    "        X_val = np.stack(val_data[text_col].values)\n",
    "        X_test = np.stack(test_data[text_col].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def get_xgb_parameters(self):\n",
    "        \"\"\"\n",
    "        Get XGBoost parameters optimized for high-dimensional embeddings.\n",
    "        Uses a single parameter set for both Title and Full text embeddings since they have the same dimension (1536).\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of base parameters\n",
    "        \"\"\"\n",
    "        # Setup XGBoost base parameters optimized for high-dimensional embeddings\n",
    "        base_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'n_estimators': 150,\n",
    "            'max_depth': 3,\n",
    "            'learning_rate': 0.00005,\n",
    "            'subsample': 0.7,          # Row subsampling to prevent overfitting\n",
    "            'colsample_bytree': 0.5,   # Column subsampling to handle high dimensionality\n",
    "            'min_child_weight': 3,     # Prevents overfitting on high-dimensional embeddings\n",
    "            'reg_alpha': 1.0,          # L1 regularization\n",
    "            'reg_lambda': 2.0,         # L2 regularization\n",
    "            'random_state': 42,\n",
    "            'use_label_encoder': False # Avoid deprecation warning\n",
    "        }\n",
    "        \n",
    "        return base_params\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, model_type, text_col, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            model_type: Model type (MLP)\n",
    "            text_col: Text column used\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_{model_type.lower()}_{display_text.replace(' ', '_')}_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, text_col, label_col, model_type):\n",
    "        \"\"\"\n",
    "        Train and evaluate a model for a specific text column and label column.\n",
    "        \n",
    "        Args:\n",
    "            text_col: The embedding column to use ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "            model_type: The model type to use ('MLP' or 'XGBoost')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        combination_key = f\"{model_type}|{display_text}|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training {model_type} model for {display_text} and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        visualization_dir = self.mlp_viz_dir if model_type == 'MLP' else self.xgb_viz_dir\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, text_col, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            if model_type == 'MLP':\n",
    "                # MLP model training\n",
    "                model = self.create_mlp_model((1536,))\n",
    "                print(f\"Created MLP model for {display_text} ({label_col})\")\n",
    "                model.summary()\n",
    "                \n",
    "                # Setup callbacks\n",
    "                plot_callback = PlotLearningCallback(model_type, display_text, label_col, i+1, visualization_dir)\n",
    "                early_stopping = EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=15,\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                model_checkpoint = ModelCheckpoint(\n",
    "                    filepath=f\"{visualization_dir}/best_{model_type.lower()}_{display_text.lower().replace(' ', '_')}_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                    monitor='val_loss',\n",
    "                    save_weights_only=True,\n",
    "                    save_best_only=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Train model\n",
    "                print(f\"Training MLP model...\")\n",
    "                batch_size = 32  # Larger batch size for embeddings\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                y_pred_proba = model.predict(X_test)\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                # Create final learning curve visualization\n",
    "                self.plot_final_learning_curves(history, model_type, text_col, label_col, i+1, visualization_dir)\n",
    "                \n",
    "                # Store training history\n",
    "                training_history = history.history\n",
    "                \n",
    "            else:  # XGBoost - simplified approach to fix API issues\n",
    "                # Get XGBoost parameters\n",
    "                base_params = self.get_xgb_parameters()\n",
    "                \n",
    "                # Create and train XGBoost model\n",
    "                print(f\"Creating and training XGBoost model...\")\n",
    "                model = xgb.XGBClassifier(**base_params)\n",
    "                \n",
    "                # Only use validation set for evaluation (not training set)\n",
    "                eval_set = [(X_val, y_val)]\n",
    "                \n",
    "                # Train the model\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    verbose=True\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                # Create a simple summary for XGBoost (no detailed learning curves available)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.text(0.5, 0.5, f'XGBoost Model Trained Successfully\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\\nAUC: {roc_auc_score(y_test, y_pred_proba):.4f}',\n",
    "                         ha='center', va='center', size=14, fontweight='bold')\n",
    "                plt.title(f'XGBoost Results ({display_text}, {label_col}, Layer {i+1})')\n",
    "                plt.axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{visualization_dir}/xgb_{display_text.replace(' ', '_')}_{label_col}_layer_{i+1}.png\")\n",
    "                plt.close()\n",
    "                \n",
    "                # Use None for training history since detailed learning curves aren't available\n",
    "                training_history = None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'model_type': model_type,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': training_history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for all combinations of text inputs, label columns, and model types.\"\"\"\n",
    "        # Define all combinations\n",
    "        embedding_cols = ['Title_embedding', 'Fulltext_embedding']\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        model_types = ['MLP', 'XGBoost']\n",
    "        \n",
    "        # Run analysis for each combination\n",
    "        for model_type in model_types:\n",
    "            for embedding_col in embedding_cols:\n",
    "                for label_col in label_cols:\n",
    "                    self.train_and_evaluate_model(embedding_col, label_col, model_type)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Model': model_type,\n",
    "                'Text': text_col,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing all model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Split by model type\n",
    "        mlp_data = df[df['Model'] == 'MLP']\n",
    "        xgb_data = df[df['Model'] == 'XGBoost']\n",
    "        \n",
    "        # 1. Performance comparison for MLP\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot accuracy and AUC bars for MLP\n",
    "        x = np.arange(len(mlp_data))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, mlp_data['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, mlp_data['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('MLP Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with OpenAI Embeddings')\n",
    "        labels = [f\"{row['Text']} + {row['Label']}\" for _, row in mlp_data.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(mlp_data['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(mlp_data['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(mlp_data['Avg Accuracy'].max(), mlp_data['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/SLB', \"mlp_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Performance comparison for XGBoost\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot accuracy and AUC bars for XGBoost\n",
    "        x = np.arange(len(xgb_data))\n",
    "        \n",
    "        plt.bar(x - width/2, xgb_data['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, xgb_data['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('XGBoost Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of XGBoost Models with OpenAI Embeddings')\n",
    "        labels = [f\"{row['Text']} + {row['Label']}\" for _, row in xgb_data.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(xgb_data['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(xgb_data['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(xgb_data['Avg Accuracy'].max(), xgb_data['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/SLB', \"xgb_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"XGBoost summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP and XGBoost models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        mlp_layer_data = []\n",
    "        xgb_layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Model': model_type,\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                if model_type == 'MLP':\n",
    "                    mlp_layer_data.append(layer_info)\n",
    "                else:  # XGBoost\n",
    "                    xgb_layer_data.append(layer_info)\n",
    "        \n",
    "        # Create visualizations for each model type\n",
    "        self._create_model_layer_visualization(mlp_layer_data, 'MLP')\n",
    "        self._create_model_layer_visualization(xgb_layer_data, 'XGBoost')\n",
    "    \n",
    "    def _create_model_layer_visualization(self, layer_data, model_type):\n",
    "        \"\"\"Create layer-specific visualizations for a given model type.\"\"\"\n",
    "        if not layer_data:\n",
    "            print(f\"No layer data available for {model_type}\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} Accuracy by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} AUC by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/SLB', f\"{model_type.lower()}_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"{model_type} layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['OpenAI_MLP_XGB/visualizations_mlp/SLB', 'OpenAI_MLP_XGB/visualizations_xgb/SLB', \n",
    "                      'OpenAI_MLP_XGB/visualizations_summary/SLB']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/wall_street_news_semantics_SP500_database/wall_street_news_semantics_SLB_completed_openai.csv'\n",
    "    \n",
    "    # Initialize the predictor with OpenAI embeddings\n",
    "    predictor = ClimateNewsOpenAIPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'OpenAI_MLP_XGB' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing OpenAI embedding vectors from string format...\n",
      "Sample Title embedding shape: (1536,)\n",
      "Sample Fulltext embedding shape: (1536,)\n",
      "Loaded 838 climate change news articles spanning from 07/11/2014 to 24/09/2024\n",
      "Class distribution for short-term prediction: {1: 427, 0: 411}\n",
      "Class distribution for long-term prediction: {1: 428, 0: 410}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_169\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_652 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_483 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_338 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_653 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_484 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_339 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_654 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_485 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_655 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.9228 - accuracy: 0.4875\n",
      "Epoch 1: val_loss improved from inf to 0.69500, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 30ms/step - loss: 0.9453 - accuracy: 0.4838 - val_loss: 0.6950 - val_accuracy: 0.4504\n",
      "Epoch 2/100\n",
      " 9/13 [===================>..........] - ETA: 0s - loss: 0.8540 - accuracy: 0.5243\n",
      "Epoch 2: val_loss did not improve from 0.69500\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.8229 - accuracy: 0.5436 - val_loss: 0.6965 - val_accuracy: 0.4351\n",
      "Epoch 3/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.7240 - accuracy: 0.5938\n",
      "Epoch 3: val_loss did not improve from 0.69500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.7375 - accuracy: 0.5786 - val_loss: 0.6971 - val_accuracy: 0.4427\n",
      "Epoch 4/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.6536 - accuracy: 0.6375\n",
      "Epoch 4: val_loss did not improve from 0.69500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6771 - accuracy: 0.6259 - val_loss: 0.6982 - val_accuracy: 0.4427\n",
      "Epoch 5/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.6365 - accuracy: 0.6432\n",
      "Epoch 5: val_loss did not improve from 0.69500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6393 - accuracy: 0.6359 - val_loss: 0.6989 - val_accuracy: 0.4427\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6804 - accuracy: 0.6307\n",
      "Epoch 6: val_loss did not improve from 0.69500\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6571 - accuracy: 0.6484 - val_loss: 0.6998 - val_accuracy: 0.4427\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6386 - accuracy: 0.6449\n",
      "Epoch 7: val_loss did not improve from 0.69500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6127 - accuracy: 0.6633 - val_loss: 0.7007 - val_accuracy: 0.4427\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5602 - accuracy: 0.7188\n",
      "Epoch 8: val_loss did not improve from 0.69500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5536 - accuracy: 0.7157 - val_loss: 0.7017 - val_accuracy: 0.4427\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5054 - accuracy: 0.7557\n",
      "Epoch 9: val_loss did not improve from 0.69500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5121 - accuracy: 0.7481 - val_loss: 0.7019 - val_accuracy: 0.4427\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4919 - accuracy: 0.7642\n",
      "Epoch 10: val_loss did not improve from 0.69500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4983 - accuracy: 0.7606 - val_loss: 0.7017 - val_accuracy: 0.4427\n",
      "Epoch 11/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.4788 - accuracy: 0.7625\n",
      "Epoch 11: val_loss did not improve from 0.69500\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4690 - accuracy: 0.7781 - val_loss: 0.7019 - val_accuracy: 0.4504\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4534 - accuracy: 0.7869\n",
      "Epoch 12: val_loss did not improve from 0.69500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4550 - accuracy: 0.7880 - val_loss: 0.7011 - val_accuracy: 0.4733\n",
      "Epoch 13/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4157 - accuracy: 0.8333\n",
      "Epoch 13: val_loss did not improve from 0.69500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4167 - accuracy: 0.8304 - val_loss: 0.7024 - val_accuracy: 0.4809\n",
      "Epoch 14/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.4183 - accuracy: 0.8344\n",
      "Epoch 14: val_loss did not improve from 0.69500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4253 - accuracy: 0.8254 - val_loss: 0.7029 - val_accuracy: 0.4885\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4010 - accuracy: 0.8381\n",
      "Epoch 15: val_loss did not improve from 0.69500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4019 - accuracy: 0.8379 - val_loss: 0.7031 - val_accuracy: 0.4962\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3534 - accuracy: 0.8523Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3563 - accuracy: 0.8504 - val_loss: 0.7043 - val_accuracy: 0.4809\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.4060\n",
      "Test AUC for Layer 1: 0.4180\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_170\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_656 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_486 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_340 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_657 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_487 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_341 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_658 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_488 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_659 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.8923 - accuracy: 0.5000\n",
      "Epoch 1: val_loss improved from inf to 0.68922, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 26ms/step - loss: 0.8848 - accuracy: 0.5056 - val_loss: 0.6892 - val_accuracy: 0.5564\n",
      "Epoch 2/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.8159 - accuracy: 0.5479\n",
      "Epoch 2: val_loss improved from 0.68922 to 0.68680, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.8273 - accuracy: 0.5432 - val_loss: 0.6868 - val_accuracy: 0.5564\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7550 - accuracy: 0.5781\n",
      "Epoch 3: val_loss improved from 0.68680 to 0.68582, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.7447 - accuracy: 0.5827 - val_loss: 0.6858 - val_accuracy: 0.5564\n",
      "Epoch 4/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6934 - accuracy: 0.6187\n",
      "Epoch 4: val_loss did not improve from 0.68582\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7002 - accuracy: 0.6109 - val_loss: 0.6863 - val_accuracy: 0.5564\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6811 - accuracy: 0.6191\n",
      "Epoch 5: val_loss did not improve from 0.68582\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6862 - accuracy: 0.6165 - val_loss: 0.6874 - val_accuracy: 0.5564\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6584 - accuracy: 0.6230\n",
      "Epoch 6: val_loss did not improve from 0.68582\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6596 - accuracy: 0.6259 - val_loss: 0.6895 - val_accuracy: 0.5564\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6121 - accuracy: 0.6777\n",
      "Epoch 7: val_loss did not improve from 0.68582\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6225 - accuracy: 0.6673 - val_loss: 0.6924 - val_accuracy: 0.5564\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5821 - accuracy: 0.6875\n",
      "Epoch 8: val_loss did not improve from 0.68582\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5784 - accuracy: 0.6880 - val_loss: 0.6945 - val_accuracy: 0.5564\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5402 - accuracy: 0.7324\n",
      "Epoch 9: val_loss did not improve from 0.68582\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5417 - accuracy: 0.7293 - val_loss: 0.6977 - val_accuracy: 0.5564\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5543 - accuracy: 0.6992\n",
      "Epoch 10: val_loss did not improve from 0.68582\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5508 - accuracy: 0.7030 - val_loss: 0.7014 - val_accuracy: 0.5564\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4824 - accuracy: 0.7598\n",
      "Epoch 11: val_loss did not improve from 0.68582\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4975 - accuracy: 0.7481 - val_loss: 0.7066 - val_accuracy: 0.5564\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4361 - accuracy: 0.7914\n",
      "Epoch 12: val_loss did not improve from 0.68582\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4361 - accuracy: 0.7914 - val_loss: 0.7092 - val_accuracy: 0.5564\n",
      "Epoch 13/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3930 - accuracy: 0.8359\n",
      "Epoch 13: val_loss did not improve from 0.68582\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4094 - accuracy: 0.8158 - val_loss: 0.7134 - val_accuracy: 0.5564\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4051 - accuracy: 0.8421\n",
      "Epoch 14: val_loss did not improve from 0.68582\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4051 - accuracy: 0.8421 - val_loss: 0.7189 - val_accuracy: 0.5564\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3755 - accuracy: 0.8281\n",
      "Epoch 15: val_loss did not improve from 0.68582\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3749 - accuracy: 0.8289 - val_loss: 0.7220 - val_accuracy: 0.5564\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4033 - accuracy: 0.8340\n",
      "Epoch 16: val_loss did not improve from 0.68582\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3989 - accuracy: 0.8383 - val_loss: 0.7242 - val_accuracy: 0.5564\n",
      "Epoch 17/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3734 - accuracy: 0.8418\n",
      "Epoch 17: val_loss did not improve from 0.68582\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3724 - accuracy: 0.8421 - val_loss: 0.7283 - val_accuracy: 0.5564\n",
      "Epoch 18/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3593 - accuracy: 0.8652Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.68582\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3651 - accuracy: 0.8609 - val_loss: 0.7293 - val_accuracy: 0.5564\n",
      "Epoch 18: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5446\n",
      "Test AUC for Layer 2: 0.4850\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_171\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_660 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_489 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_342 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_661 (Dense)           (None, 256)               131328    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " batch_normalization_490 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_343 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_662 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_491 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_663 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9058 - accuracy: 0.5060\n",
      "Epoch 1: val_loss improved from inf to 0.70372, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 23ms/step - loss: 0.9058 - accuracy: 0.5060 - val_loss: 0.7037 - val_accuracy: 0.4554\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8070 - accuracy: 0.5330\n",
      "Epoch 2: val_loss did not improve from 0.70372\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.8070 - accuracy: 0.5330 - val_loss: 0.7225 - val_accuracy: 0.4554\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7213 - accuracy: 0.6006\n",
      "Epoch 3: val_loss did not improve from 0.70372\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7213 - accuracy: 0.6006 - val_loss: 0.7428 - val_accuracy: 0.4554\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.5826\n",
      "Epoch 4: val_loss did not improve from 0.70372\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6939 - accuracy: 0.5826 - val_loss: 0.7668 - val_accuracy: 0.4554\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6987 - accuracy: 0.6066\n",
      "Epoch 5: val_loss did not improve from 0.70372\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6987 - accuracy: 0.6066 - val_loss: 0.7939 - val_accuracy: 0.4554\n",
      "Epoch 6/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.6260 - accuracy: 0.6654\n",
      "Epoch 6: val_loss did not improve from 0.70372\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6167 - accuracy: 0.6682 - val_loss: 0.8263 - val_accuracy: 0.4554\n",
      "Epoch 7/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.6058 - accuracy: 0.6746\n",
      "Epoch 7: val_loss did not improve from 0.70372\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5937 - accuracy: 0.6922 - val_loss: 0.8515 - val_accuracy: 0.4554\n",
      "Epoch 8/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.5852 - accuracy: 0.6738\n",
      "Epoch 8: val_loss did not improve from 0.70372\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5763 - accuracy: 0.6832 - val_loss: 0.8760 - val_accuracy: 0.4554\n",
      "Epoch 9/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.5343 - accuracy: 0.7363\n",
      "Epoch 9: val_loss did not improve from 0.70372\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5325 - accuracy: 0.7312 - val_loss: 0.9011 - val_accuracy: 0.4554\n",
      "Epoch 10/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.5370 - accuracy: 0.7151\n",
      "Epoch 10: val_loss did not improve from 0.70372\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5261 - accuracy: 0.7222 - val_loss: 0.9229 - val_accuracy: 0.4554\n",
      "Epoch 11/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.4894 - accuracy: 0.7574\n",
      "Epoch 11: val_loss did not improve from 0.70372\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4893 - accuracy: 0.7523 - val_loss: 0.9416 - val_accuracy: 0.4554\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4451 - accuracy: 0.7988\n",
      "Epoch 12: val_loss did not improve from 0.70372\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4451 - accuracy: 0.7988 - val_loss: 0.9592 - val_accuracy: 0.4554\n",
      "Epoch 13/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.4430 - accuracy: 0.7923\n",
      "Epoch 13: val_loss did not improve from 0.70372\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4439 - accuracy: 0.7943 - val_loss: 0.9749 - val_accuracy: 0.4554\n",
      "Epoch 14/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4318 - accuracy: 0.8174\n",
      "Epoch 14: val_loss did not improve from 0.70372\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4202 - accuracy: 0.8243 - val_loss: 0.9808 - val_accuracy: 0.4554\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3785 - accuracy: 0.8498\n",
      "Epoch 15: val_loss did not improve from 0.70372\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3785 - accuracy: 0.8498 - val_loss: 0.9815 - val_accuracy: 0.4554\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3808 - accuracy: 0.8318Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70372\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3808 - accuracy: 0.8318 - val_loss: 0.9772 - val_accuracy: 0.4554\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5571\n",
      "Test AUC for Layer 3: 0.5227\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5026\n",
      "Average Test AUC across all layers: 0.4752\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_172\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_664 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_492 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_344 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_665 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_493 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_345 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_666 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_494 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_667 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8772 - accuracy: 0.5114\n",
      "Epoch 1: val_loss improved from inf to 0.70047, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 30ms/step - loss: 0.9124 - accuracy: 0.5062 - val_loss: 0.7005 - val_accuracy: 0.3130\n",
      "Epoch 2/100\n",
      " 9/13 [===================>..........] - ETA: 0s - loss: 0.8189 - accuracy: 0.5347\n",
      "Epoch 2: val_loss did not improve from 0.70047\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.8047 - accuracy: 0.5411 - val_loss: 0.7128 - val_accuracy: 0.3130\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7467 - accuracy: 0.5824\n",
      "Epoch 3: val_loss did not improve from 0.70047\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7410 - accuracy: 0.5860 - val_loss: 0.7230 - val_accuracy: 0.3130\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7320 - accuracy: 0.5455\n",
      "Epoch 4: val_loss did not improve from 0.70047\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.7159 - accuracy: 0.5586 - val_loss: 0.7340 - val_accuracy: 0.3130\n",
      "Epoch 5/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.6490 - accuracy: 0.6667\n",
      "Epoch 5: val_loss did not improve from 0.70047\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6464 - accuracy: 0.6658 - val_loss: 0.7449 - val_accuracy: 0.3130\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6282 - accuracy: 0.6506\n",
      "Epoch 6: val_loss did not improve from 0.70047\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6281 - accuracy: 0.6559 - val_loss: 0.7568 - val_accuracy: 0.3130\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5952 - accuracy: 0.6875\n",
      "Epoch 7: val_loss did not improve from 0.70047\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5913 - accuracy: 0.6858 - val_loss: 0.7657 - val_accuracy: 0.3130\n",
      "Epoch 8/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5429 - accuracy: 0.7500\n",
      "Epoch 8: val_loss did not improve from 0.70047\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5360 - accuracy: 0.7556 - val_loss: 0.7782 - val_accuracy: 0.3130\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4636 - accuracy: 0.7926\n",
      "Epoch 9: val_loss did not improve from 0.70047\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4945 - accuracy: 0.7781 - val_loss: 0.7870 - val_accuracy: 0.3130\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5102 - accuracy: 0.7301\n",
      "Epoch 10: val_loss did not improve from 0.70047\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5233 - accuracy: 0.7157 - val_loss: 0.7973 - val_accuracy: 0.3130\n",
      "Epoch 11/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4926 - accuracy: 0.7699\n",
      "Epoch 11: val_loss did not improve from 0.70047\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4844 - accuracy: 0.7706 - val_loss: 0.8078 - val_accuracy: 0.3130\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4378 - accuracy: 0.8125\n",
      "Epoch 12: val_loss did not improve from 0.70047\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4386 - accuracy: 0.8055 - val_loss: 0.8186 - val_accuracy: 0.3130\n",
      "Epoch 13/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4311 - accuracy: 0.7869\n",
      "Epoch 13: val_loss did not improve from 0.70047\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4344 - accuracy: 0.7880 - val_loss: 0.8298 - val_accuracy: 0.3130\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4404 - accuracy: 0.7898\n",
      "Epoch 14: val_loss did not improve from 0.70047\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4404 - accuracy: 0.7930 - val_loss: 0.8401 - val_accuracy: 0.3130\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3643 - accuracy: 0.8324\n",
      "Epoch 15: val_loss did not improve from 0.70047\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3687 - accuracy: 0.8304 - val_loss: 0.8534 - val_accuracy: 0.3130\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3792 - accuracy: 0.8438Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70047\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3656 - accuracy: 0.8529 - val_loss: 0.8682 - val_accuracy: 0.3130\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.2707\n",
      "Test AUC for Layer 1: 0.5845\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_173\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_668 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_495 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_346 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_669 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_496 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_347 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_670 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_497 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_671 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8763 - accuracy: 0.5254\n",
      "Epoch 1: val_loss improved from inf to 0.71194, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8725 - accuracy: 0.5282 - val_loss: 0.7119 - val_accuracy: 0.2707\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8367 - accuracy: 0.5449\n",
      "Epoch 2: val_loss did not improve from 0.71194\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.8405 - accuracy: 0.5414 - val_loss: 0.7232 - val_accuracy: 0.2707\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7210 - accuracy: 0.6133\n",
      "Epoch 3: val_loss did not improve from 0.71194\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7198 - accuracy: 0.6109 - val_loss: 0.7295 - val_accuracy: 0.2707\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7032 - accuracy: 0.6211\n",
      "Epoch 4: val_loss did not improve from 0.71194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7080 - accuracy: 0.6147 - val_loss: 0.7349 - val_accuracy: 0.2707\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6580 - accuracy: 0.6621\n",
      "Epoch 5: val_loss did not improve from 0.71194\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6582 - accuracy: 0.6579 - val_loss: 0.7327 - val_accuracy: 0.2707\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6191 - accuracy: 0.6514\n",
      "Epoch 6: val_loss did not improve from 0.71194\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6327 - accuracy: 0.6504 - val_loss: 0.7306 - val_accuracy: 0.2707\n",
      "Epoch 7/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5388 - accuracy: 0.7292\n",
      "Epoch 7: val_loss did not improve from 0.71194\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5646 - accuracy: 0.7180 - val_loss: 0.7220 - val_accuracy: 0.2707\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5611 - accuracy: 0.7051\n",
      "Epoch 8: val_loss did not improve from 0.71194\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5591 - accuracy: 0.7068 - val_loss: 0.7157 - val_accuracy: 0.3008\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5286 - accuracy: 0.7308\n",
      "Epoch 9: val_loss improved from 0.71194 to 0.70502, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5252 - accuracy: 0.7293 - val_loss: 0.7050 - val_accuracy: 0.2932\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4914 - accuracy: 0.7734\n",
      "Epoch 10: val_loss improved from 0.70502 to 0.69758, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4931 - accuracy: 0.7688 - val_loss: 0.6976 - val_accuracy: 0.3985\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4684 - accuracy: 0.7910\n",
      "Epoch 11: val_loss improved from 0.69758 to 0.68913, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4763 - accuracy: 0.7838 - val_loss: 0.6891 - val_accuracy: 0.5865\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4656 - accuracy: 0.7754\n",
      "Epoch 12: val_loss improved from 0.68913 to 0.68111, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4717 - accuracy: 0.7707 - val_loss: 0.6811 - val_accuracy: 0.6617\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4421 - accuracy: 0.7949\n",
      "Epoch 13: val_loss improved from 0.68111 to 0.67405, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4458 - accuracy: 0.7895 - val_loss: 0.6740 - val_accuracy: 0.6992\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4440 - accuracy: 0.7969\n",
      "Epoch 14: val_loss improved from 0.67405 to 0.66974, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4352 - accuracy: 0.8026 - val_loss: 0.6697 - val_accuracy: 0.6992\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4166 - accuracy: 0.8418\n",
      "Epoch 15: val_loss improved from 0.66974 to 0.66425, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4201 - accuracy: 0.8365 - val_loss: 0.6643 - val_accuracy: 0.6992\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3734 - accuracy: 0.8652\n",
      "Epoch 16: val_loss improved from 0.66425 to 0.65600, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.3734 - accuracy: 0.8647 - val_loss: 0.6560 - val_accuracy: 0.7143\n",
      "Epoch 17/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4111 - accuracy: 0.8066\n",
      "Epoch 17: val_loss improved from 0.65600 to 0.65304, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4081 - accuracy: 0.8083 - val_loss: 0.6530 - val_accuracy: 0.7068\n",
      "Epoch 18/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3640 - accuracy: 0.8594\n",
      "Epoch 18: val_loss improved from 0.65304 to 0.65098, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.3620 - accuracy: 0.8590 - val_loss: 0.6510 - val_accuracy: 0.7068\n",
      "Epoch 19/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3081 - accuracy: 0.8906\n",
      "Epoch 19: val_loss improved from 0.65098 to 0.64917, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.3083 - accuracy: 0.8929 - val_loss: 0.6492 - val_accuracy: 0.7068\n",
      "Epoch 20/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2938 - accuracy: 0.9023\n",
      "Epoch 20: val_loss improved from 0.64917 to 0.64814, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.2918 - accuracy: 0.9060 - val_loss: 0.6481 - val_accuracy: 0.7068\n",
      "Epoch 21/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2939 - accuracy: 0.8965\n",
      "Epoch 21: val_loss did not improve from 0.64814\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.2949 - accuracy: 0.8947 - val_loss: 0.6493 - val_accuracy: 0.7143\n",
      "Epoch 22/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2917 - accuracy: 0.8984\n",
      "Epoch 22: val_loss did not improve from 0.64814\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.2982 - accuracy: 0.8910 - val_loss: 0.6519 - val_accuracy: 0.7068\n",
      "Epoch 23/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2923 - accuracy: 0.8887\n",
      "Epoch 23: val_loss did not improve from 0.64814\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2914 - accuracy: 0.8891 - val_loss: 0.6546 - val_accuracy: 0.6917\n",
      "Epoch 24/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2916 - accuracy: 0.9004\n",
      "Epoch 24: val_loss did not improve from 0.64814\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2894 - accuracy: 0.9023 - val_loss: 0.6621 - val_accuracy: 0.6391\n",
      "Epoch 25/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2340 - accuracy: 0.9355\n",
      "Epoch 25: val_loss did not improve from 0.64814\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2342 - accuracy: 0.9361 - val_loss: 0.6661 - val_accuracy: 0.6241\n",
      "Epoch 26/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2436 - accuracy: 0.9160\n",
      "Epoch 26: val_loss did not improve from 0.64814\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2492 - accuracy: 0.9135 - val_loss: 0.6735 - val_accuracy: 0.5865\n",
      "Epoch 27/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2197 - accuracy: 0.9336\n",
      "Epoch 27: val_loss did not improve from 0.64814\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2213 - accuracy: 0.9323 - val_loss: 0.6811 - val_accuracy: 0.5865\n",
      "Epoch 28/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2329 - accuracy: 0.9349\n",
      "Epoch 28: val_loss did not improve from 0.64814\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2233 - accuracy: 0.9361 - val_loss: 0.6882 - val_accuracy: 0.5414\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2154 - accuracy: 0.9267\n",
      "Epoch 29: val_loss did not improve from 0.64814\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2154 - accuracy: 0.9267 - val_loss: 0.6999 - val_accuracy: 0.5338\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 0.2005 - accuracy: 0.9455\n",
      "Epoch 30: val_loss did not improve from 0.64814\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2005 - accuracy: 0.9455 - val_loss: 0.7107 - val_accuracy: 0.5113\n",
      "Epoch 31/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2043 - accuracy: 0.9399\n",
      "Epoch 31: val_loss did not improve from 0.64814\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2012 - accuracy: 0.9380 - val_loss: 0.7204 - val_accuracy: 0.4887\n",
      "Epoch 32/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2048 - accuracy: 0.9453\n",
      "Epoch 32: val_loss did not improve from 0.64814\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2101 - accuracy: 0.9474 - val_loss: 0.7301 - val_accuracy: 0.4962\n",
      "Epoch 33/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.1750 - accuracy: 0.9505\n",
      "Epoch 33: val_loss did not improve from 0.64814\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.1767 - accuracy: 0.9492 - val_loss: 0.7408 - val_accuracy: 0.4812\n",
      "Epoch 34/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2120 - accuracy: 0.9355\n",
      "Epoch 34: val_loss did not improve from 0.64814\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2132 - accuracy: 0.9323 - val_loss: 0.7558 - val_accuracy: 0.4887\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1768 - accuracy: 0.9511Restoring model weights from the end of the best epoch: 20.\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.64814\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.1768 - accuracy: 0.9511 - val_loss: 0.7738 - val_accuracy: 0.4586\n",
      "Epoch 35: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4653\n",
      "Test AUC for Layer 2: 0.5807\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_174\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_672 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_498 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_348 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_673 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_499 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_349 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_674 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_500 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_675 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.8950 - accuracy: 0.5297\n",
      "Epoch 1: val_loss improved from inf to 0.68660, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 23ms/step - loss: 0.8966 - accuracy: 0.5285 - val_loss: 0.6866 - val_accuracy: 0.5842\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8128 - accuracy: 0.5330\n",
      "Epoch 2: val_loss improved from 0.68660 to 0.68119, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.8128 - accuracy: 0.5330 - val_loss: 0.6812 - val_accuracy: 0.5842\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7299 - accuracy: 0.6126\n",
      "Epoch 3: val_loss improved from 0.68119 to 0.67895, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7299 - accuracy: 0.6126 - val_loss: 0.6789 - val_accuracy: 0.5842\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6458 - accuracy: 0.6456\n",
      "Epoch 4: val_loss improved from 0.67895 to 0.67801, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 0.6458 - accuracy: 0.6456 - val_loss: 0.6780 - val_accuracy: 0.5842\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6710 - accuracy: 0.6306\n",
      "Epoch 5: val_loss did not improve from 0.67801\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6710 - accuracy: 0.6306 - val_loss: 0.6780 - val_accuracy: 0.5842\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6347 - accuracy: 0.6321\n",
      "Epoch 6: val_loss improved from 0.67801 to 0.67795, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 0.6347 - accuracy: 0.6321 - val_loss: 0.6780 - val_accuracy: 0.5842\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5873 - accuracy: 0.6997\n",
      "Epoch 7: val_loss improved from 0.67795 to 0.67785, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5873 - accuracy: 0.6997 - val_loss: 0.6778 - val_accuracy: 0.5842\n",
      "Epoch 8/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5797 - accuracy: 0.6859\n",
      "Epoch 8: val_loss did not improve from 0.67785\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5836 - accuracy: 0.6832 - val_loss: 0.6780 - val_accuracy: 0.5842\n",
      "Epoch 9/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.5360 - accuracy: 0.7309\n",
      "Epoch 9: val_loss improved from 0.67785 to 0.67695, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5305 - accuracy: 0.7312 - val_loss: 0.6770 - val_accuracy: 0.5842\n",
      "Epoch 10/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5075 - accuracy: 0.7344\n",
      "Epoch 10: val_loss improved from 0.67695 to 0.67611, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5084 - accuracy: 0.7327 - val_loss: 0.6761 - val_accuracy: 0.5842\n",
      "Epoch 11/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4872 - accuracy: 0.7703\n",
      "Epoch 11: val_loss did not improve from 0.67611\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4889 - accuracy: 0.7703 - val_loss: 0.6763 - val_accuracy: 0.5842\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4577 - accuracy: 0.7733\n",
      "Epoch 12: val_loss improved from 0.67611 to 0.67543, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4577 - accuracy: 0.7733 - val_loss: 0.6754 - val_accuracy: 0.5842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4549 - accuracy: 0.8016\n",
      "Epoch 13: val_loss improved from 0.67543 to 0.67453, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 0.4513 - accuracy: 0.8048 - val_loss: 0.6745 - val_accuracy: 0.5842\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4110 - accuracy: 0.8168\n",
      "Epoch 14: val_loss improved from 0.67453 to 0.67425, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4110 - accuracy: 0.8168 - val_loss: 0.6742 - val_accuracy: 0.5842\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4117 - accuracy: 0.8138\n",
      "Epoch 15: val_loss improved from 0.67425 to 0.67377, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4117 - accuracy: 0.8138 - val_loss: 0.6738 - val_accuracy: 0.5842\n",
      "Epoch 16/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3786 - accuracy: 0.8359\n",
      "Epoch 16: val_loss improved from 0.67377 to 0.67196, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 0.3785 - accuracy: 0.8363 - val_loss: 0.6720 - val_accuracy: 0.5842\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3556 - accuracy: 0.8619\n",
      "Epoch 17: val_loss did not improve from 0.67196\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3556 - accuracy: 0.8619 - val_loss: 0.6731 - val_accuracy: 0.5644\n",
      "Epoch 18/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.3758 - accuracy: 0.8419\n",
      "Epoch 18: val_loss did not improve from 0.67196\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3723 - accuracy: 0.8438 - val_loss: 0.6734 - val_accuracy: 0.5446\n",
      "Epoch 19/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3497 - accuracy: 0.8664\n",
      "Epoch 19: val_loss did not improve from 0.67196\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3497 - accuracy: 0.8664 - val_loss: 0.6748 - val_accuracy: 0.5347\n",
      "Epoch 20/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.3147 - accuracy: 0.8924\n",
      "Epoch 20: val_loss did not improve from 0.67196\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3159 - accuracy: 0.8889 - val_loss: 0.6782 - val_accuracy: 0.5347\n",
      "Epoch 21/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3101 - accuracy: 0.9114\n",
      "Epoch 21: val_loss did not improve from 0.67196\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3101 - accuracy: 0.9114 - val_loss: 0.6783 - val_accuracy: 0.5545\n",
      "Epoch 22/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.2846 - accuracy: 0.9118\n",
      "Epoch 22: val_loss did not improve from 0.67196\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2878 - accuracy: 0.9069 - val_loss: 0.6805 - val_accuracy: 0.5743\n",
      "Epoch 23/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2933 - accuracy: 0.8859\n",
      "Epoch 23: val_loss did not improve from 0.67196\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2933 - accuracy: 0.8859 - val_loss: 0.6864 - val_accuracy: 0.5644\n",
      "Epoch 24/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.9024\n",
      "Epoch 24: val_loss did not improve from 0.67196\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2857 - accuracy: 0.9024 - val_loss: 0.6896 - val_accuracy: 0.5743\n",
      "Epoch 25/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.9189\n",
      "Epoch 25: val_loss did not improve from 0.67196\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2712 - accuracy: 0.9189 - val_loss: 0.6922 - val_accuracy: 0.5842\n",
      "Epoch 26/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2585 - accuracy: 0.9129\n",
      "Epoch 26: val_loss did not improve from 0.67196\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2585 - accuracy: 0.9129 - val_loss: 0.6991 - val_accuracy: 0.5842\n",
      "Epoch 27/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.9264\n",
      "Epoch 27: val_loss did not improve from 0.67196\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2411 - accuracy: 0.9264 - val_loss: 0.7109 - val_accuracy: 0.5941\n",
      "Epoch 28/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2316 - accuracy: 0.9234\n",
      "Epoch 28: val_loss did not improve from 0.67196\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2316 - accuracy: 0.9234 - val_loss: 0.7193 - val_accuracy: 0.5842\n",
      "Epoch 29/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.9234\n",
      "Epoch 29: val_loss did not improve from 0.67196\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2321 - accuracy: 0.9234 - val_loss: 0.7373 - val_accuracy: 0.6040\n",
      "Epoch 30/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2134 - accuracy: 0.9384\n",
      "Epoch 30: val_loss did not improve from 0.67196\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.2134 - accuracy: 0.9384 - val_loss: 0.7504 - val_accuracy: 0.6040\n",
      "Epoch 31/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1899 - accuracy: 0.9459Restoring model weights from the end of the best epoch: 16.\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.67196\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.1899 - accuracy: 0.9459 - val_loss: 0.7702 - val_accuracy: 0.5842\n",
      "Epoch 31: early stopping\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4857\n",
      "Test AUC for Layer 3: 0.3698\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4072\n",
      "Average Test AUC across all layers: 0.5117\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_175\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_676 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_501 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_350 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_677 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_502 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_351 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_678 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_503 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_679 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.9097 - accuracy: 0.4830\n",
      "Epoch 1: val_loss improved from inf to 0.70095, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 30ms/step - loss: 0.9055 - accuracy: 0.4838 - val_loss: 0.7010 - val_accuracy: 0.4427\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8380 - accuracy: 0.5227\n",
      "Epoch 2: val_loss did not improve from 0.70095\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.8202 - accuracy: 0.5312 - val_loss: 0.7076 - val_accuracy: 0.4427\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7667 - accuracy: 0.5568\n",
      "Epoch 3: val_loss did not improve from 0.70095\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.7627 - accuracy: 0.5686 - val_loss: 0.7146 - val_accuracy: 0.4427\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7480 - accuracy: 0.5739\n",
      "Epoch 4: val_loss did not improve from 0.70095\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.7358 - accuracy: 0.5761 - val_loss: 0.7218 - val_accuracy: 0.4427\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7007 - accuracy: 0.6307\n",
      "Epoch 5: val_loss did not improve from 0.70095\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6984 - accuracy: 0.6259 - val_loss: 0.7289 - val_accuracy: 0.4427\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5589 - accuracy: 0.7244\n",
      "Epoch 6: val_loss did not improve from 0.70095\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5567 - accuracy: 0.7207 - val_loss: 0.7360 - val_accuracy: 0.4427\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5848 - accuracy: 0.7045\n",
      "Epoch 7: val_loss did not improve from 0.70095\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5833 - accuracy: 0.6983 - val_loss: 0.7433 - val_accuracy: 0.4427\n",
      "Epoch 8/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5187 - accuracy: 0.7396\n",
      "Epoch 8: val_loss did not improve from 0.70095\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5206 - accuracy: 0.7382 - val_loss: 0.7502 - val_accuracy: 0.4427\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5283 - accuracy: 0.7358\n",
      "Epoch 9: val_loss did not improve from 0.70095\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5241 - accuracy: 0.7431 - val_loss: 0.7585 - val_accuracy: 0.4427\n",
      "Epoch 10/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5317 - accuracy: 0.7330\n",
      "Epoch 10: val_loss did not improve from 0.70095\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5493 - accuracy: 0.7257 - val_loss: 0.7641 - val_accuracy: 0.4427\n",
      "Epoch 11/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4570 - accuracy: 0.7699\n",
      "Epoch 11: val_loss did not improve from 0.70095\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4499 - accuracy: 0.7805 - val_loss: 0.7707 - val_accuracy: 0.4427\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4827 - accuracy: 0.7642\n",
      "Epoch 12: val_loss did not improve from 0.70095\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4953 - accuracy: 0.7556 - val_loss: 0.7739 - val_accuracy: 0.4427\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4330 - accuracy: 0.8005\n",
      "Epoch 13: val_loss did not improve from 0.70095\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4330 - accuracy: 0.8005 - val_loss: 0.7805 - val_accuracy: 0.4427\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4409 - accuracy: 0.7812\n",
      "Epoch 14: val_loss did not improve from 0.70095\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4447 - accuracy: 0.7805 - val_loss: 0.7832 - val_accuracy: 0.4427\n",
      "Epoch 15/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3767 - accuracy: 0.8580\n",
      "Epoch 15: val_loss did not improve from 0.70095\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3726 - accuracy: 0.8554 - val_loss: 0.7903 - val_accuracy: 0.4427\n",
      "Epoch 16/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4561 - accuracy: 0.7841Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70095\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4618 - accuracy: 0.7805 - val_loss: 0.7933 - val_accuracy: 0.4427\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.4436\n",
      "Test AUC for Layer 1: 0.4647\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_176\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_680 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_504 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_352 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_681 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_505 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_353 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_682 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_506 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_683 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.9018 - accuracy: 0.4844\n",
      "Epoch 1: val_loss improved from inf to 0.69254, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 27ms/step - loss: 0.8780 - accuracy: 0.5000 - val_loss: 0.6925 - val_accuracy: 0.5564\n",
      "Epoch 2/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.8108 - accuracy: 0.5379\n",
      "Epoch 2: val_loss improved from 0.69254 to 0.69206, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.8152 - accuracy: 0.5338 - val_loss: 0.6921 - val_accuracy: 0.5414\n",
      "Epoch 3/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.7643 - accuracy: 0.5769\n",
      "Epoch 3: val_loss improved from 0.69206 to 0.69149, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.7571 - accuracy: 0.5883 - val_loss: 0.6915 - val_accuracy: 0.5414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6885 - accuracy: 0.6090\n",
      "Epoch 4: val_loss did not improve from 0.69149\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.6885 - accuracy: 0.6090 - val_loss: 0.6917 - val_accuracy: 0.5564\n",
      "Epoch 5/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6408 - accuracy: 0.6667\n",
      "Epoch 5: val_loss did not improve from 0.69149\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6459 - accuracy: 0.6560 - val_loss: 0.6923 - val_accuracy: 0.5263\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6291 - accuracy: 0.6758\n",
      "Epoch 6: val_loss did not improve from 0.69149\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6257 - accuracy: 0.6767 - val_loss: 0.6930 - val_accuracy: 0.5188\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5817 - accuracy: 0.6914\n",
      "Epoch 7: val_loss did not improve from 0.69149\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5793 - accuracy: 0.6936 - val_loss: 0.6940 - val_accuracy: 0.4737\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6005 - accuracy: 0.6914\n",
      "Epoch 8: val_loss did not improve from 0.69149\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5985 - accuracy: 0.6898 - val_loss: 0.6949 - val_accuracy: 0.4662\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5613 - accuracy: 0.7188\n",
      "Epoch 9: val_loss did not improve from 0.69149\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5581 - accuracy: 0.7218 - val_loss: 0.6960 - val_accuracy: 0.4361\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5347 - accuracy: 0.7148\n",
      "Epoch 10: val_loss did not improve from 0.69149\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5384 - accuracy: 0.7068 - val_loss: 0.6974 - val_accuracy: 0.4361\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4893 - accuracy: 0.7578\n",
      "Epoch 11: val_loss did not improve from 0.69149\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4857 - accuracy: 0.7632 - val_loss: 0.6979 - val_accuracy: 0.4511\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5026 - accuracy: 0.7598\n",
      "Epoch 12: val_loss did not improve from 0.69149\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5087 - accuracy: 0.7575 - val_loss: 0.6994 - val_accuracy: 0.4511\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4449 - accuracy: 0.8105\n",
      "Epoch 13: val_loss did not improve from 0.69149\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4481 - accuracy: 0.8102 - val_loss: 0.6992 - val_accuracy: 0.4586\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4723 - accuracy: 0.7734\n",
      "Epoch 14: val_loss did not improve from 0.69149\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4687 - accuracy: 0.7763 - val_loss: 0.7022 - val_accuracy: 0.4662\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4235 - accuracy: 0.7988\n",
      "Epoch 15: val_loss did not improve from 0.69149\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4220 - accuracy: 0.8026 - val_loss: 0.7040 - val_accuracy: 0.4511\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4470 - accuracy: 0.7832\n",
      "Epoch 16: val_loss did not improve from 0.69149\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4453 - accuracy: 0.7838 - val_loss: 0.7057 - val_accuracy: 0.4586\n",
      "Epoch 17/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4194 - accuracy: 0.8164\n",
      "Epoch 17: val_loss did not improve from 0.69149\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4204 - accuracy: 0.8158 - val_loss: 0.7077 - val_accuracy: 0.4586\n",
      "Epoch 18/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.3856 - accuracy: 0.8354Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.69149\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3740 - accuracy: 0.8440 - val_loss: 0.7088 - val_accuracy: 0.4662\n",
      "Epoch 18: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5050\n",
      "Test AUC for Layer 2: 0.3822\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_177\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_684 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_507 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_354 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_685 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_508 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_355 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_686 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_509 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_687 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.8688 - accuracy: 0.5016\n",
      "Epoch 1: val_loss improved from inf to 0.69602, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 25ms/step - loss: 0.8639 - accuracy: 0.5045 - val_loss: 0.6960 - val_accuracy: 0.4554\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7502 - accuracy: 0.5586\n",
      "Epoch 2: val_loss did not improve from 0.69602\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7502 - accuracy: 0.5586 - val_loss: 0.7004 - val_accuracy: 0.4554\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7127 - accuracy: 0.5856\n",
      "Epoch 3: val_loss did not improve from 0.69602\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7127 - accuracy: 0.5856 - val_loss: 0.7053 - val_accuracy: 0.4554\n",
      "Epoch 4/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6590 - accuracy: 0.6422\n",
      "Epoch 4: val_loss did not improve from 0.69602\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6543 - accuracy: 0.6441 - val_loss: 0.7096 - val_accuracy: 0.4554\n",
      "Epoch 5/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.6674 - accuracy: 0.6181\n",
      "Epoch 5: val_loss did not improve from 0.69602\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6639 - accuracy: 0.6231 - val_loss: 0.7159 - val_accuracy: 0.4554\n",
      "Epoch 6/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.6219 - accuracy: 0.6777\n",
      "Epoch 6: val_loss did not improve from 0.69602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6265 - accuracy: 0.6712 - val_loss: 0.7226 - val_accuracy: 0.4554\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5982 - accuracy: 0.6772\n",
      "Epoch 7: val_loss did not improve from 0.69602\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5982 - accuracy: 0.6772 - val_loss: 0.7260 - val_accuracy: 0.4554\n",
      "Epoch 8/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.5559 - accuracy: 0.7261\n",
      "Epoch 8: val_loss did not improve from 0.69602\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5365 - accuracy: 0.7372 - val_loss: 0.7310 - val_accuracy: 0.4554\n",
      "Epoch 9/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5447 - accuracy: 0.7250\n",
      "Epoch 9: val_loss did not improve from 0.69602\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5473 - accuracy: 0.7207 - val_loss: 0.7347 - val_accuracy: 0.4554\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5166 - accuracy: 0.7372\n",
      "Epoch 10: val_loss did not improve from 0.69602\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5166 - accuracy: 0.7372 - val_loss: 0.7387 - val_accuracy: 0.4554\n",
      "Epoch 11/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.4755 - accuracy: 0.7702\n",
      "Epoch 11: val_loss did not improve from 0.69602\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4826 - accuracy: 0.7613 - val_loss: 0.7422 - val_accuracy: 0.4554\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4996 - accuracy: 0.7568\n",
      "Epoch 12: val_loss did not improve from 0.69602\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4996 - accuracy: 0.7568 - val_loss: 0.7468 - val_accuracy: 0.4554\n",
      "Epoch 13/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4506 - accuracy: 0.7830\n",
      "Epoch 13: val_loss did not improve from 0.69602\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4627 - accuracy: 0.7748 - val_loss: 0.7486 - val_accuracy: 0.4554\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4408 - accuracy: 0.7943\n",
      "Epoch 14: val_loss did not improve from 0.69602\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4408 - accuracy: 0.7943 - val_loss: 0.7478 - val_accuracy: 0.4554\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4431 - accuracy: 0.7958\n",
      "Epoch 15: val_loss did not improve from 0.69602\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4431 - accuracy: 0.7958 - val_loss: 0.7500 - val_accuracy: 0.4554\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4193 - accuracy: 0.8078Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69602\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4193 - accuracy: 0.8078 - val_loss: 0.7485 - val_accuracy: 0.4554\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5571\n",
      "Test AUC for Layer 3: 0.5798\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5019\n",
      "Average Test AUC across all layers: 0.4756\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_178\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_688 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_510 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_356 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_689 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_511 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_357 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_690 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_512 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_691 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      " 9/13 [===================>..........] - ETA: 0s - loss: 0.8361 - accuracy: 0.5104\n",
      "Epoch 1: val_loss improved from inf to 0.69433, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 31ms/step - loss: 0.8496 - accuracy: 0.5062 - val_loss: 0.6943 - val_accuracy: 0.4275\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7834 - accuracy: 0.5653\n",
      "Epoch 2: val_loss improved from 0.69433 to 0.69416, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.7927 - accuracy: 0.5611 - val_loss: 0.6942 - val_accuracy: 0.4580\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7850 - accuracy: 0.5483\n",
      "Epoch 3: val_loss improved from 0.69416 to 0.69403, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.7856 - accuracy: 0.5362 - val_loss: 0.6940 - val_accuracy: 0.4580\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6815 - accuracy: 0.6278\n",
      "Epoch 4: val_loss did not improve from 0.69403\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6722 - accuracy: 0.6334 - val_loss: 0.6947 - val_accuracy: 0.4656\n",
      "Epoch 5/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.6337 - accuracy: 0.6969\n",
      "Epoch 5: val_loss did not improve from 0.69403\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6297 - accuracy: 0.6933 - val_loss: 0.6941 - val_accuracy: 0.4580\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6222 - accuracy: 0.6506\n",
      "Epoch 6: val_loss did not improve from 0.69403\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6315 - accuracy: 0.6459 - val_loss: 0.6943 - val_accuracy: 0.4656\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5988 - accuracy: 0.6983\n",
      "Epoch 7: val_loss improved from 0.69403 to 0.69365, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.5988 - accuracy: 0.6983 - val_loss: 0.6936 - val_accuracy: 0.4809\n",
      "Epoch 8/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.5674 - accuracy: 0.7125\n",
      "Epoch 8: val_loss improved from 0.69365 to 0.69080, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_1.weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5622 - accuracy: 0.7132 - val_loss: 0.6908 - val_accuracy: 0.5344\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5320 - accuracy: 0.7330\n",
      "Epoch 9: val_loss improved from 0.69080 to 0.69053, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.5309 - accuracy: 0.7307 - val_loss: 0.6905 - val_accuracy: 0.5496\n",
      "Epoch 10/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.5110 - accuracy: 0.7469\n",
      "Epoch 10: val_loss improved from 0.69053 to 0.68969, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.5087 - accuracy: 0.7456 - val_loss: 0.6897 - val_accuracy: 0.5420\n",
      "Epoch 11/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4509 - accuracy: 0.7926\n",
      "Epoch 11: val_loss did not improve from 0.68969\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4669 - accuracy: 0.7805 - val_loss: 0.6902 - val_accuracy: 0.5267\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4576 - accuracy: 0.7699\n",
      "Epoch 12: val_loss did not improve from 0.68969\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4593 - accuracy: 0.7731 - val_loss: 0.6905 - val_accuracy: 0.5191\n",
      "Epoch 13/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4731 - accuracy: 0.7756\n",
      "Epoch 13: val_loss did not improve from 0.68969\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4739 - accuracy: 0.7781 - val_loss: 0.6917 - val_accuracy: 0.5267\n",
      "Epoch 14/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.4299 - accuracy: 0.7937\n",
      "Epoch 14: val_loss did not improve from 0.68969\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4295 - accuracy: 0.7980 - val_loss: 0.6930 - val_accuracy: 0.5420\n",
      "Epoch 15/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4790 - accuracy: 0.7682\n",
      "Epoch 15: val_loss did not improve from 0.68969\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4754 - accuracy: 0.7706 - val_loss: 0.6927 - val_accuracy: 0.5267\n",
      "Epoch 16/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3998 - accuracy: 0.8229\n",
      "Epoch 16: val_loss did not improve from 0.68969\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4003 - accuracy: 0.8254 - val_loss: 0.6956 - val_accuracy: 0.5038\n",
      "Epoch 17/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3953 - accuracy: 0.8097\n",
      "Epoch 17: val_loss did not improve from 0.68969\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4086 - accuracy: 0.8030 - val_loss: 0.6973 - val_accuracy: 0.4885\n",
      "Epoch 18/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3744 - accuracy: 0.8551\n",
      "Epoch 18: val_loss did not improve from 0.68969\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3692 - accuracy: 0.8603 - val_loss: 0.6989 - val_accuracy: 0.4809\n",
      "Epoch 19/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3590 - accuracy: 0.8523\n",
      "Epoch 19: val_loss did not improve from 0.68969\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3550 - accuracy: 0.8579 - val_loss: 0.7001 - val_accuracy: 0.4885\n",
      "Epoch 20/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3893 - accuracy: 0.8153\n",
      "Epoch 20: val_loss did not improve from 0.68969\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3922 - accuracy: 0.8180 - val_loss: 0.7017 - val_accuracy: 0.4809\n",
      "Epoch 21/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3475 - accuracy: 0.8580\n",
      "Epoch 21: val_loss did not improve from 0.68969\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3508 - accuracy: 0.8529 - val_loss: 0.7053 - val_accuracy: 0.4809\n",
      "Epoch 22/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3330 - accuracy: 0.8665\n",
      "Epoch 22: val_loss did not improve from 0.68969\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3213 - accuracy: 0.8778 - val_loss: 0.7121 - val_accuracy: 0.4504\n",
      "Epoch 23/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3352 - accuracy: 0.8693\n",
      "Epoch 23: val_loss did not improve from 0.68969\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3414 - accuracy: 0.8628 - val_loss: 0.7159 - val_accuracy: 0.4351\n",
      "Epoch 24/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3328 - accuracy: 0.8722\n",
      "Epoch 24: val_loss did not improve from 0.68969\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3234 - accuracy: 0.8778 - val_loss: 0.7190 - val_accuracy: 0.4351\n",
      "Epoch 25/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3178 - accuracy: 0.8665Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.68969\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.3202 - accuracy: 0.8603 - val_loss: 0.7213 - val_accuracy: 0.4351\n",
      "Epoch 25: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.6391\n",
      "Test AUC for Layer 1: 0.3774\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_179\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_692 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_513 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_358 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_693 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_514 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_359 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_694 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_515 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_695 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.8942 - accuracy: 0.4750\n",
      "Epoch 1: val_loss improved from inf to 0.68378, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 26ms/step - loss: 0.8951 - accuracy: 0.4737 - val_loss: 0.6838 - val_accuracy: 0.7293\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7939 - accuracy: 0.5215\n",
      "Epoch 2: val_loss improved from 0.68378 to 0.67380, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.7889 - accuracy: 0.5301 - val_loss: 0.6738 - val_accuracy: 0.7293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7531 - accuracy: 0.5684\n",
      "Epoch 3: val_loss improved from 0.67380 to 0.66758, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7513 - accuracy: 0.5695 - val_loss: 0.6676 - val_accuracy: 0.7293\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6928 - accuracy: 0.6172\n",
      "Epoch 4: val_loss improved from 0.66758 to 0.66167, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.6818 - accuracy: 0.6278 - val_loss: 0.6617 - val_accuracy: 0.7293\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6759 - accuracy: 0.6074\n",
      "Epoch 5: val_loss improved from 0.66167 to 0.65636, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6730 - accuracy: 0.6090 - val_loss: 0.6564 - val_accuracy: 0.7293\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6569 - accuracy: 0.6309\n",
      "Epoch 6: val_loss improved from 0.65636 to 0.65533, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6633 - accuracy: 0.6241 - val_loss: 0.6553 - val_accuracy: 0.7293\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6039 - accuracy: 0.6836\n",
      "Epoch 7: val_loss improved from 0.65533 to 0.65523, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6157 - accuracy: 0.6729 - val_loss: 0.6552 - val_accuracy: 0.7293\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5736 - accuracy: 0.6914\n",
      "Epoch 8: val_loss did not improve from 0.65523\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5797 - accuracy: 0.6842 - val_loss: 0.6555 - val_accuracy: 0.7293\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5234 - accuracy: 0.7500\n",
      "Epoch 9: val_loss did not improve from 0.65523\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5199 - accuracy: 0.7500 - val_loss: 0.6561 - val_accuracy: 0.7293\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4963 - accuracy: 0.7695\n",
      "Epoch 10: val_loss did not improve from 0.65523\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4935 - accuracy: 0.7744 - val_loss: 0.6576 - val_accuracy: 0.7293\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4940 - accuracy: 0.7578\n",
      "Epoch 11: val_loss did not improve from 0.65523\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4894 - accuracy: 0.7632 - val_loss: 0.6589 - val_accuracy: 0.7293\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4690 - accuracy: 0.7910\n",
      "Epoch 12: val_loss did not improve from 0.65523\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4768 - accuracy: 0.7801 - val_loss: 0.6620 - val_accuracy: 0.7293\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4786 - accuracy: 0.7617\n",
      "Epoch 13: val_loss did not improve from 0.65523\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4769 - accuracy: 0.7613 - val_loss: 0.6679 - val_accuracy: 0.6692\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4390 - accuracy: 0.7988\n",
      "Epoch 14: val_loss did not improve from 0.65523\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4412 - accuracy: 0.7989 - val_loss: 0.6701 - val_accuracy: 0.6541\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4244 - accuracy: 0.8027\n",
      "Epoch 15: val_loss did not improve from 0.65523\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4254 - accuracy: 0.8045 - val_loss: 0.6715 - val_accuracy: 0.6466\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4141 - accuracy: 0.8120\n",
      "Epoch 16: val_loss did not improve from 0.65523\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4141 - accuracy: 0.8120 - val_loss: 0.6731 - val_accuracy: 0.6165\n",
      "Epoch 17/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4105 - accuracy: 0.8164\n",
      "Epoch 17: val_loss did not improve from 0.65523\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4120 - accuracy: 0.8139 - val_loss: 0.6766 - val_accuracy: 0.6015\n",
      "Epoch 18/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3660 - accuracy: 0.8535\n",
      "Epoch 18: val_loss did not improve from 0.65523\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3699 - accuracy: 0.8515 - val_loss: 0.6797 - val_accuracy: 0.6090\n",
      "Epoch 19/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3647 - accuracy: 0.8496\n",
      "Epoch 19: val_loss did not improve from 0.65523\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3685 - accuracy: 0.8421 - val_loss: 0.6857 - val_accuracy: 0.5940\n",
      "Epoch 20/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3416 - accuracy: 0.8652\n",
      "Epoch 20: val_loss did not improve from 0.65523\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3430 - accuracy: 0.8665 - val_loss: 0.6869 - val_accuracy: 0.6015\n",
      "Epoch 21/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3155 - accuracy: 0.8965\n",
      "Epoch 21: val_loss did not improve from 0.65523\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3181 - accuracy: 0.8966 - val_loss: 0.6917 - val_accuracy: 0.6015\n",
      "Epoch 22/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3171 - accuracy: 0.8730Restoring model weights from the end of the best epoch: 7.\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.65523\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3198 - accuracy: 0.8703 - val_loss: 0.6948 - val_accuracy: 0.6165\n",
      "Epoch 22: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4158\n",
      "Test AUC for Layer 2: 0.4552\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_180\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_696 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_516 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_360 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_697 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_517 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_361 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_698 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_518 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_699 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.8718 - accuracy: 0.5000\n",
      "Epoch 1: val_loss improved from inf to 0.69177, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 23ms/step - loss: 0.8831 - accuracy: 0.4925 - val_loss: 0.6918 - val_accuracy: 0.5842\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7585 - accuracy: 0.5721\n",
      "Epoch 2: val_loss improved from 0.69177 to 0.69112, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7585 - accuracy: 0.5721 - val_loss: 0.6911 - val_accuracy: 0.5842\n",
      "Epoch 3/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6871 - accuracy: 0.6328\n",
      "Epoch 3: val_loss improved from 0.69112 to 0.69090, saving model to OpenAI_MLP_XGB/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 0.6876 - accuracy: 0.6351 - val_loss: 0.6909 - val_accuracy: 0.5743\n",
      "Epoch 4/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6968 - accuracy: 0.6000\n",
      "Epoch 4: val_loss did not improve from 0.69090\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6970 - accuracy: 0.5961 - val_loss: 0.6915 - val_accuracy: 0.5545\n",
      "Epoch 5/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.6491 - accuracy: 0.6426\n",
      "Epoch 5: val_loss did not improve from 0.69090\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6275 - accuracy: 0.6517 - val_loss: 0.6920 - val_accuracy: 0.5149\n",
      "Epoch 6/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.5802 - accuracy: 0.7049\n",
      "Epoch 6: val_loss did not improve from 0.69090\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5934 - accuracy: 0.6937 - val_loss: 0.6930 - val_accuracy: 0.4752\n",
      "Epoch 7/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5741 - accuracy: 0.6812\n",
      "Epoch 7: val_loss did not improve from 0.69090\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5690 - accuracy: 0.6877 - val_loss: 0.6940 - val_accuracy: 0.4752\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5334 - accuracy: 0.7252\n",
      "Epoch 8: val_loss did not improve from 0.69090\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5334 - accuracy: 0.7252 - val_loss: 0.6954 - val_accuracy: 0.4653\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5269 - accuracy: 0.7372\n",
      "Epoch 9: val_loss did not improve from 0.69090\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5269 - accuracy: 0.7372 - val_loss: 0.6951 - val_accuracy: 0.4851\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4917 - accuracy: 0.7658\n",
      "Epoch 10: val_loss did not improve from 0.69090\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4917 - accuracy: 0.7658 - val_loss: 0.6966 - val_accuracy: 0.5248\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.7613\n",
      "Epoch 11: val_loss did not improve from 0.69090\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4749 - accuracy: 0.7613 - val_loss: 0.6988 - val_accuracy: 0.4851\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4391 - accuracy: 0.7988\n",
      "Epoch 12: val_loss did not improve from 0.69090\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4391 - accuracy: 0.7988 - val_loss: 0.6998 - val_accuracy: 0.5050\n",
      "Epoch 13/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4433 - accuracy: 0.7953\n",
      "Epoch 13: val_loss did not improve from 0.69090\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4421 - accuracy: 0.7943 - val_loss: 0.7004 - val_accuracy: 0.5149\n",
      "Epoch 14/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.4081 - accuracy: 0.8125\n",
      "Epoch 14: val_loss did not improve from 0.69090\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4053 - accuracy: 0.8228 - val_loss: 0.7046 - val_accuracy: 0.5050\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3939 - accuracy: 0.8393\n",
      "Epoch 15: val_loss did not improve from 0.69090\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3939 - accuracy: 0.8393 - val_loss: 0.7065 - val_accuracy: 0.4851\n",
      "Epoch 16/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3962 - accuracy: 0.8470\n",
      "Epoch 16: val_loss did not improve from 0.69090\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3939 - accuracy: 0.8498 - val_loss: 0.7076 - val_accuracy: 0.5050\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3619 - accuracy: 0.8604\n",
      "Epoch 17: val_loss did not improve from 0.69090\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3619 - accuracy: 0.8604 - val_loss: 0.7065 - val_accuracy: 0.4851\n",
      "Epoch 18/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3605 - accuracy: 0.8388Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.69090\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3543 - accuracy: 0.8423 - val_loss: 0.7084 - val_accuracy: 0.4950\n",
      "Epoch 18: early stopping\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5000\n",
      "Test AUC for Layer 3: 0.4816\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5183\n",
      "Average Test AUC across all layers: 0.4381\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Title and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.49114\n",
      "[1]\tvalidation_0-auc:0.43529\n",
      "[2]\tvalidation_0-auc:0.44391\n",
      "[3]\tvalidation_0-auc:0.42241\n",
      "[4]\tvalidation_0-auc:0.38274\n",
      "[5]\tvalidation_0-auc:0.37695\n",
      "[6]\tvalidation_0-auc:0.39537\n",
      "[7]\tvalidation_0-auc:0.41781\n",
      "[8]\tvalidation_0-auc:0.43765\n",
      "[9]\tvalidation_0-auc:0.42300\n",
      "[10]\tvalidation_0-auc:0.41497\n",
      "[11]\tvalidation_0-auc:0.41828\n",
      "[12]\tvalidation_0-auc:0.41568\n",
      "[13]\tvalidation_0-auc:0.42088\n",
      "[14]\tvalidation_0-auc:0.40801\n",
      "[15]\tvalidation_0-auc:0.40269\n",
      "[16]\tvalidation_0-auc:0.41025\n",
      "[17]\tvalidation_0-auc:0.42596\n",
      "[18]\tvalidation_0-auc:0.43245\n",
      "[19]\tvalidation_0-auc:0.42926\n",
      "[20]\tvalidation_0-auc:0.42985\n",
      "[21]\tvalidation_0-auc:0.42454\n",
      "[22]\tvalidation_0-auc:0.42584\n",
      "[23]\tvalidation_0-auc:0.42300\n",
      "[24]\tvalidation_0-auc:0.42915\n",
      "[25]\tvalidation_0-auc:0.42135\n",
      "[26]\tvalidation_0-auc:0.41722\n",
      "[27]\tvalidation_0-auc:0.40694\n",
      "[28]\tvalidation_0-auc:0.39927\n",
      "[29]\tvalidation_0-auc:0.38250\n",
      "[30]\tvalidation_0-auc:0.38545\n",
      "[31]\tvalidation_0-auc:0.38876\n",
      "[32]\tvalidation_0-auc:0.39100\n",
      "[33]\tvalidation_0-auc:0.39962\n",
      "[34]\tvalidation_0-auc:0.40529\n",
      "[35]\tvalidation_0-auc:0.40600\n",
      "[36]\tvalidation_0-auc:0.39372\n",
      "[37]\tvalidation_0-auc:0.39974\n",
      "[38]\tvalidation_0-auc:0.39608\n",
      "[39]\tvalidation_0-auc:0.39950\n",
      "[40]\tvalidation_0-auc:0.39832\n",
      "[41]\tvalidation_0-auc:0.40718\n",
      "[42]\tvalidation_0-auc:0.40187\n",
      "[43]\tvalidation_0-auc:0.40246\n",
      "[44]\tvalidation_0-auc:0.40458\n",
      "[45]\tvalidation_0-auc:0.40694\n",
      "[46]\tvalidation_0-auc:0.40706\n",
      "[47]\tvalidation_0-auc:0.41084\n",
      "[48]\tvalidation_0-auc:0.41993\n",
      "[49]\tvalidation_0-auc:0.42088\n",
      "[50]\tvalidation_0-auc:0.41993\n",
      "[51]\tvalidation_0-auc:0.42064\n",
      "[52]\tvalidation_0-auc:0.41828\n",
      "[53]\tvalidation_0-auc:0.40990\n",
      "[54]\tvalidation_0-auc:0.40789\n",
      "[55]\tvalidation_0-auc:0.40848\n",
      "[56]\tvalidation_0-auc:0.41013\n",
      "[57]\tvalidation_0-auc:0.41556\n",
      "[58]\tvalidation_0-auc:0.40777\n",
      "[59]\tvalidation_0-auc:0.40848\n",
      "[60]\tvalidation_0-auc:0.41001\n",
      "[61]\tvalidation_0-auc:0.40718\n",
      "[62]\tvalidation_0-auc:0.40600\n",
      "[63]\tvalidation_0-auc:0.40435\n",
      "[64]\tvalidation_0-auc:0.40045\n",
      "[65]\tvalidation_0-auc:0.40187\n",
      "[66]\tvalidation_0-auc:0.40198\n",
      "[67]\tvalidation_0-auc:0.40824\n",
      "[68]\tvalidation_0-auc:0.40883\n",
      "[69]\tvalidation_0-auc:0.41001\n",
      "[70]\tvalidation_0-auc:0.40836\n",
      "[71]\tvalidation_0-auc:0.41120\n",
      "[72]\tvalidation_0-auc:0.40564\n",
      "[73]\tvalidation_0-auc:0.40399\n",
      "[74]\tvalidation_0-auc:0.40505\n",
      "[75]\tvalidation_0-auc:0.40624\n",
      "[76]\tvalidation_0-auc:0.40730\n",
      "[77]\tvalidation_0-auc:0.40753\n",
      "[78]\tvalidation_0-auc:0.40895\n",
      "[79]\tvalidation_0-auc:0.41060\n",
      "[80]\tvalidation_0-auc:0.41686\n",
      "[81]\tvalidation_0-auc:0.41804\n",
      "[82]\tvalidation_0-auc:0.41627\n",
      "[83]\tvalidation_0-auc:0.42064\n",
      "[84]\tvalidation_0-auc:0.42100\n",
      "[85]\tvalidation_0-auc:0.42111\n",
      "[86]\tvalidation_0-auc:0.42560\n",
      "[87]\tvalidation_0-auc:0.42478\n",
      "[88]\tvalidation_0-auc:0.42513\n",
      "[89]\tvalidation_0-auc:0.42466\n",
      "[90]\tvalidation_0-auc:0.42808\n",
      "[91]\tvalidation_0-auc:0.42785\n",
      "[92]\tvalidation_0-auc:0.43375\n",
      "[93]\tvalidation_0-auc:0.43540\n",
      "[94]\tvalidation_0-auc:0.43977\n",
      "[95]\tvalidation_0-auc:0.43576\n",
      "[96]\tvalidation_0-auc:0.43836\n",
      "[97]\tvalidation_0-auc:0.43576\n",
      "[98]\tvalidation_0-auc:0.44025\n",
      "[99]\tvalidation_0-auc:0.44379\n",
      "Test Accuracy for Layer 1: 0.4436\n",
      "Test AUC for Layer 1: 0.4718\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.53688\n",
      "[1]\tvalidation_0-auc:0.52359\n",
      "[2]\tvalidation_0-auc:0.50939\n",
      "[3]\tvalidation_0-auc:0.49508\n",
      "[4]\tvalidation_0-auc:0.50893\n",
      "[5]\tvalidation_0-auc:0.46038\n",
      "[6]\tvalidation_0-auc:0.46805\n",
      "[7]\tvalidation_0-auc:0.44480\n",
      "[8]\tvalidation_0-auc:0.41606\n",
      "[9]\tvalidation_0-auc:0.41434\n",
      "[10]\tvalidation_0-auc:0.41686\n",
      "[11]\tvalidation_0-auc:0.41915\n",
      "[12]\tvalidation_0-auc:0.38250\n",
      "[13]\tvalidation_0-auc:0.39361\n",
      "[14]\tvalidation_0-auc:0.39487\n",
      "[15]\tvalidation_0-auc:0.39487\n",
      "[16]\tvalidation_0-auc:0.38937\n",
      "[17]\tvalidation_0-auc:0.38548\n",
      "[18]\tvalidation_0-auc:0.37689\n",
      "[19]\tvalidation_0-auc:0.37712\n",
      "[20]\tvalidation_0-auc:0.36842\n",
      "[21]\tvalidation_0-auc:0.37494\n",
      "[22]\tvalidation_0-auc:0.39430\n",
      "[23]\tvalidation_0-auc:0.40060\n",
      "[24]\tvalidation_0-auc:0.38926\n",
      "[25]\tvalidation_0-auc:0.38296\n",
      "[26]\tvalidation_0-auc:0.38204\n",
      "[27]\tvalidation_0-auc:0.38777\n",
      "[28]\tvalidation_0-auc:0.39556\n",
      "[29]\tvalidation_0-auc:0.38960\n",
      "[30]\tvalidation_0-auc:0.38307\n",
      "[31]\tvalidation_0-auc:0.37700\n",
      "[32]\tvalidation_0-auc:0.39120\n",
      "[33]\tvalidation_0-auc:0.39727\n",
      "[34]\tvalidation_0-auc:0.40060\n",
      "[35]\tvalidation_0-auc:0.40792\n",
      "[36]\tvalidation_0-auc:0.41022\n",
      "[37]\tvalidation_0-auc:0.41136\n",
      "[38]\tvalidation_0-auc:0.40300\n",
      "[39]\tvalidation_0-auc:0.39395\n",
      "[40]\tvalidation_0-auc:0.38937\n",
      "[41]\tvalidation_0-auc:0.39624\n",
      "[42]\tvalidation_0-auc:0.39991\n",
      "[43]\tvalidation_0-auc:0.39556\n",
      "[44]\tvalidation_0-auc:0.39189\n",
      "[45]\tvalidation_0-auc:0.38708\n",
      "[46]\tvalidation_0-auc:0.37952\n",
      "[47]\tvalidation_0-auc:0.38124\n",
      "[48]\tvalidation_0-auc:0.38170\n",
      "[49]\tvalidation_0-auc:0.37952\n",
      "[50]\tvalidation_0-auc:0.39006\n",
      "[51]\tvalidation_0-auc:0.38181\n",
      "[52]\tvalidation_0-auc:0.39017\n",
      "[53]\tvalidation_0-auc:0.38273\n",
      "[54]\tvalidation_0-auc:0.38869\n",
      "[55]\tvalidation_0-auc:0.38995\n",
      "[56]\tvalidation_0-auc:0.38319\n",
      "[57]\tvalidation_0-auc:0.37861\n",
      "[58]\tvalidation_0-auc:0.37746\n",
      "[59]\tvalidation_0-auc:0.37632\n",
      "[60]\tvalidation_0-auc:0.37380\n",
      "[61]\tvalidation_0-auc:0.38158\n",
      "[62]\tvalidation_0-auc:0.38639\n",
      "[63]\tvalidation_0-auc:0.38639\n",
      "[64]\tvalidation_0-auc:0.38502\n",
      "[65]\tvalidation_0-auc:0.39040\n",
      "[66]\tvalidation_0-auc:0.39017\n",
      "[67]\tvalidation_0-auc:0.38777\n",
      "[68]\tvalidation_0-auc:0.38983\n",
      "[69]\tvalidation_0-auc:0.39579\n",
      "[70]\tvalidation_0-auc:0.39510\n",
      "[71]\tvalidation_0-auc:0.38754\n",
      "[72]\tvalidation_0-auc:0.38193\n",
      "[73]\tvalidation_0-auc:0.38388\n",
      "[74]\tvalidation_0-auc:0.37941\n",
      "[75]\tvalidation_0-auc:0.37746\n",
      "[76]\tvalidation_0-auc:0.37884\n",
      "[77]\tvalidation_0-auc:0.37311\n",
      "[78]\tvalidation_0-auc:0.37552\n",
      "[79]\tvalidation_0-auc:0.37998\n",
      "[80]\tvalidation_0-auc:0.38319\n",
      "[81]\tvalidation_0-auc:0.38708\n",
      "[82]\tvalidation_0-auc:0.38777\n",
      "[83]\tvalidation_0-auc:0.38605\n",
      "[84]\tvalidation_0-auc:0.38342\n",
      "[85]\tvalidation_0-auc:0.38697\n",
      "[86]\tvalidation_0-auc:0.38960\n",
      "[87]\tvalidation_0-auc:0.38605\n",
      "[88]\tvalidation_0-auc:0.38639\n",
      "[89]\tvalidation_0-auc:0.38617\n",
      "[90]\tvalidation_0-auc:0.38685\n",
      "[91]\tvalidation_0-auc:0.38754\n",
      "[92]\tvalidation_0-auc:0.38479\n",
      "[93]\tvalidation_0-auc:0.38651\n",
      "[94]\tvalidation_0-auc:0.38055\n",
      "[95]\tvalidation_0-auc:0.39040\n",
      "[96]\tvalidation_0-auc:0.39372\n",
      "[97]\tvalidation_0-auc:0.39040\n",
      "[98]\tvalidation_0-auc:0.39132\n",
      "[99]\tvalidation_0-auc:0.39407\n",
      "Test Accuracy for Layer 2: 0.5446\n",
      "Test AUC for Layer 2: 0.5613\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.37806\n",
      "[1]\tvalidation_0-auc:0.44545\n",
      "[2]\tvalidation_0-auc:0.46937\n",
      "[3]\tvalidation_0-auc:0.46640\n",
      "[4]\tvalidation_0-auc:0.49111\n",
      "[5]\tvalidation_0-auc:0.44032\n",
      "[6]\tvalidation_0-auc:0.45632\n",
      "[7]\tvalidation_0-auc:0.45534\n",
      "[8]\tvalidation_0-auc:0.48458\n",
      "[9]\tvalidation_0-auc:0.50079\n",
      "[10]\tvalidation_0-auc:0.50474\n",
      "[11]\tvalidation_0-auc:0.51818\n",
      "[12]\tvalidation_0-auc:0.48755\n",
      "[13]\tvalidation_0-auc:0.51126\n",
      "[14]\tvalidation_0-auc:0.53577\n",
      "[15]\tvalidation_0-auc:0.51482\n",
      "[16]\tvalidation_0-auc:0.54071\n",
      "[17]\tvalidation_0-auc:0.56126\n",
      "[18]\tvalidation_0-auc:0.54427\n",
      "[19]\tvalidation_0-auc:0.55711\n",
      "[20]\tvalidation_0-auc:0.56561\n",
      "[21]\tvalidation_0-auc:0.55711\n",
      "[22]\tvalidation_0-auc:0.55494\n",
      "[23]\tvalidation_0-auc:0.55296\n",
      "[24]\tvalidation_0-auc:0.56700\n",
      "[25]\tvalidation_0-auc:0.60435\n",
      "[26]\tvalidation_0-auc:0.60909\n",
      "[27]\tvalidation_0-auc:0.60810\n",
      "[28]\tvalidation_0-auc:0.58340\n",
      "[29]\tvalidation_0-auc:0.57945\n",
      "[30]\tvalidation_0-auc:0.56502\n",
      "[31]\tvalidation_0-auc:0.55553\n",
      "[32]\tvalidation_0-auc:0.55277\n",
      "[33]\tvalidation_0-auc:0.55217\n",
      "[34]\tvalidation_0-auc:0.55850\n",
      "[35]\tvalidation_0-auc:0.56008\n",
      "[36]\tvalidation_0-auc:0.56700\n",
      "[37]\tvalidation_0-auc:0.56996\n",
      "[38]\tvalidation_0-auc:0.57213\n",
      "[39]\tvalidation_0-auc:0.56838\n",
      "[40]\tvalidation_0-auc:0.55652\n",
      "[41]\tvalidation_0-auc:0.54980\n",
      "[42]\tvalidation_0-auc:0.55573\n",
      "[43]\tvalidation_0-auc:0.56126\n",
      "[44]\tvalidation_0-auc:0.55613\n",
      "[45]\tvalidation_0-auc:0.54941\n",
      "[46]\tvalidation_0-auc:0.56047\n",
      "[47]\tvalidation_0-auc:0.56877\n",
      "[48]\tvalidation_0-auc:0.57431\n",
      "[49]\tvalidation_0-auc:0.57431\n",
      "[50]\tvalidation_0-auc:0.57075\n",
      "[51]\tvalidation_0-auc:0.56522\n",
      "[52]\tvalidation_0-auc:0.57194\n",
      "[53]\tvalidation_0-auc:0.57708\n",
      "[54]\tvalidation_0-auc:0.57549\n",
      "[55]\tvalidation_0-auc:0.57589\n",
      "[56]\tvalidation_0-auc:0.57451\n",
      "[57]\tvalidation_0-auc:0.57431\n",
      "[58]\tvalidation_0-auc:0.56818\n",
      "[59]\tvalidation_0-auc:0.57174\n",
      "[60]\tvalidation_0-auc:0.58004\n",
      "[61]\tvalidation_0-auc:0.59585\n",
      "[62]\tvalidation_0-auc:0.58617\n",
      "[63]\tvalidation_0-auc:0.57747\n",
      "[64]\tvalidation_0-auc:0.56917\n",
      "[65]\tvalidation_0-auc:0.55731\n",
      "[66]\tvalidation_0-auc:0.56917\n",
      "[67]\tvalidation_0-auc:0.56047\n",
      "[68]\tvalidation_0-auc:0.55455\n",
      "[69]\tvalidation_0-auc:0.55889\n",
      "[70]\tvalidation_0-auc:0.56146\n",
      "[71]\tvalidation_0-auc:0.55889\n",
      "[72]\tvalidation_0-auc:0.54111\n",
      "[73]\tvalidation_0-auc:0.53992\n",
      "[74]\tvalidation_0-auc:0.53854\n",
      "[75]\tvalidation_0-auc:0.54387\n",
      "[76]\tvalidation_0-auc:0.54822\n",
      "[77]\tvalidation_0-auc:0.53696\n",
      "[78]\tvalidation_0-auc:0.53439\n",
      "[79]\tvalidation_0-auc:0.53162\n",
      "[80]\tvalidation_0-auc:0.53320\n",
      "[81]\tvalidation_0-auc:0.54664\n",
      "[82]\tvalidation_0-auc:0.55059\n",
      "[83]\tvalidation_0-auc:0.55099\n",
      "[84]\tvalidation_0-auc:0.55257\n",
      "[85]\tvalidation_0-auc:0.54585\n",
      "[86]\tvalidation_0-auc:0.54506\n",
      "[87]\tvalidation_0-auc:0.54822\n",
      "[88]\tvalidation_0-auc:0.54407\n",
      "[89]\tvalidation_0-auc:0.54249\n",
      "[90]\tvalidation_0-auc:0.54111\n",
      "[91]\tvalidation_0-auc:0.54783\n",
      "[92]\tvalidation_0-auc:0.55020\n",
      "[93]\tvalidation_0-auc:0.54743\n",
      "[94]\tvalidation_0-auc:0.55336\n",
      "[95]\tvalidation_0-auc:0.56206\n",
      "[96]\tvalidation_0-auc:0.56917\n",
      "[97]\tvalidation_0-auc:0.56621\n",
      "[98]\tvalidation_0-auc:0.55909\n",
      "[99]\tvalidation_0-auc:0.55810\n",
      "Test Accuracy for Layer 3: 0.4429\n",
      "Test AUC for Layer 3: 0.5136\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4770\n",
      "Average Test AUC across all layers: 0.5156\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Title and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.51856\n",
      "[1]\tvalidation_0-auc:0.53455\n",
      "[2]\tvalidation_0-auc:0.50474\n",
      "[3]\tvalidation_0-auc:0.48130\n",
      "[4]\tvalidation_0-auc:0.46179\n",
      "[5]\tvalidation_0-auc:0.45908\n",
      "[6]\tvalidation_0-auc:0.50528\n",
      "[7]\tvalidation_0-auc:0.50298\n",
      "[8]\tvalidation_0-auc:0.50190\n",
      "[9]\tvalidation_0-auc:0.49864\n",
      "[10]\tvalidation_0-auc:0.49485\n",
      "[11]\tvalidation_0-auc:0.49228\n",
      "[12]\tvalidation_0-auc:0.51165\n",
      "[13]\tvalidation_0-auc:0.50081\n",
      "[14]\tvalidation_0-auc:0.49675\n",
      "[15]\tvalidation_0-auc:0.49146\n",
      "[16]\tvalidation_0-auc:0.50759\n",
      "[17]\tvalidation_0-auc:0.52629\n",
      "[18]\tvalidation_0-auc:0.51165\n",
      "[19]\tvalidation_0-auc:0.51396\n",
      "[20]\tvalidation_0-auc:0.50556\n",
      "[21]\tvalidation_0-auc:0.49702\n",
      "[22]\tvalidation_0-auc:0.50190\n",
      "[23]\tvalidation_0-auc:0.49444\n",
      "[24]\tvalidation_0-auc:0.49539\n",
      "[25]\tvalidation_0-auc:0.48970\n",
      "[26]\tvalidation_0-auc:0.48753\n",
      "[27]\tvalidation_0-auc:0.48604\n",
      "[28]\tvalidation_0-auc:0.47602\n",
      "[29]\tvalidation_0-auc:0.47669\n",
      "[30]\tvalidation_0-auc:0.47886\n",
      "[31]\tvalidation_0-auc:0.48076\n",
      "[32]\tvalidation_0-auc:0.48821\n",
      "[33]\tvalidation_0-auc:0.47724\n",
      "[34]\tvalidation_0-auc:0.47832\n",
      "[35]\tvalidation_0-auc:0.48455\n",
      "[36]\tvalidation_0-auc:0.47263\n",
      "[37]\tvalidation_0-auc:0.47100\n",
      "[38]\tvalidation_0-auc:0.47507\n",
      "[39]\tvalidation_0-auc:0.48049\n",
      "[40]\tvalidation_0-auc:0.48320\n",
      "[41]\tvalidation_0-auc:0.49350\n",
      "[42]\tvalidation_0-auc:0.48780\n",
      "[43]\tvalidation_0-auc:0.47995\n",
      "[44]\tvalidation_0-auc:0.47344\n",
      "[45]\tvalidation_0-auc:0.46504\n",
      "[46]\tvalidation_0-auc:0.47561\n",
      "[47]\tvalidation_0-auc:0.47575\n",
      "[48]\tvalidation_0-auc:0.47127\n",
      "[49]\tvalidation_0-auc:0.46667\n",
      "[50]\tvalidation_0-auc:0.46382\n",
      "[51]\tvalidation_0-auc:0.45488\n",
      "[52]\tvalidation_0-auc:0.46287\n",
      "[53]\tvalidation_0-auc:0.46748\n",
      "[54]\tvalidation_0-auc:0.46992\n",
      "[55]\tvalidation_0-auc:0.47507\n",
      "[56]\tvalidation_0-auc:0.47669\n",
      "[57]\tvalidation_0-auc:0.48130\n",
      "[58]\tvalidation_0-auc:0.48062\n",
      "[59]\tvalidation_0-auc:0.48076\n",
      "[60]\tvalidation_0-auc:0.47209\n",
      "[61]\tvalidation_0-auc:0.46856\n",
      "[62]\tvalidation_0-auc:0.47141\n",
      "[63]\tvalidation_0-auc:0.47696\n",
      "[64]\tvalidation_0-auc:0.47737\n",
      "[65]\tvalidation_0-auc:0.47588\n",
      "[66]\tvalidation_0-auc:0.47453\n",
      "[67]\tvalidation_0-auc:0.47263\n",
      "[68]\tvalidation_0-auc:0.47534\n",
      "[69]\tvalidation_0-auc:0.47073\n",
      "[70]\tvalidation_0-auc:0.47263\n",
      "[71]\tvalidation_0-auc:0.47480\n",
      "[72]\tvalidation_0-auc:0.47954\n",
      "[73]\tvalidation_0-auc:0.47290\n",
      "[74]\tvalidation_0-auc:0.46734\n",
      "[75]\tvalidation_0-auc:0.46870\n",
      "[76]\tvalidation_0-auc:0.46992\n",
      "[77]\tvalidation_0-auc:0.46802\n",
      "[78]\tvalidation_0-auc:0.46992\n",
      "[79]\tvalidation_0-auc:0.46883\n",
      "[80]\tvalidation_0-auc:0.46992\n",
      "[81]\tvalidation_0-auc:0.47412\n",
      "[82]\tvalidation_0-auc:0.47818\n",
      "[83]\tvalidation_0-auc:0.48130\n",
      "[84]\tvalidation_0-auc:0.47873\n",
      "[85]\tvalidation_0-auc:0.47805\n",
      "[86]\tvalidation_0-auc:0.47615\n",
      "[87]\tvalidation_0-auc:0.47358\n",
      "[88]\tvalidation_0-auc:0.47412\n",
      "[89]\tvalidation_0-auc:0.47561\n",
      "[90]\tvalidation_0-auc:0.47710\n",
      "[91]\tvalidation_0-auc:0.48076\n",
      "[92]\tvalidation_0-auc:0.48293\n",
      "[93]\tvalidation_0-auc:0.48726\n",
      "[94]\tvalidation_0-auc:0.48591\n",
      "[95]\tvalidation_0-auc:0.48835\n",
      "[96]\tvalidation_0-auc:0.48130\n",
      "[97]\tvalidation_0-auc:0.48564\n",
      "[98]\tvalidation_0-auc:0.48537\n",
      "[99]\tvalidation_0-auc:0.48862\n",
      "Test Accuracy for Layer 1: 0.2707\n",
      "Test AUC for Layer 1: 0.5263\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.51360\n",
      "[1]\tvalidation_0-auc:0.53923\n",
      "[2]\tvalidation_0-auc:0.51618\n",
      "[3]\tvalidation_0-auc:0.54997\n",
      "[4]\tvalidation_0-auc:0.53637\n",
      "[5]\tvalidation_0-auc:0.54940\n",
      "[6]\tvalidation_0-auc:0.55212\n",
      "[7]\tvalidation_0-auc:0.53580\n",
      "[8]\tvalidation_0-auc:0.51360\n",
      "[9]\tvalidation_0-auc:0.53522\n",
      "[10]\tvalidation_0-auc:0.55427\n",
      "[11]\tvalidation_0-auc:0.54868\n",
      "[12]\tvalidation_0-auc:0.54124\n",
      "[13]\tvalidation_0-auc:0.54081\n",
      "[14]\tvalidation_0-auc:0.54782\n",
      "[15]\tvalidation_0-auc:0.55298\n",
      "[16]\tvalidation_0-auc:0.54467\n",
      "[17]\tvalidation_0-auc:0.52620\n",
      "[18]\tvalidation_0-auc:0.52620\n",
      "[19]\tvalidation_0-auc:0.52005\n",
      "[20]\tvalidation_0-auc:0.53021\n",
      "[21]\tvalidation_0-auc:0.52191\n",
      "[22]\tvalidation_0-auc:0.52420\n",
      "[23]\tvalidation_0-auc:0.52348\n",
      "[24]\tvalidation_0-auc:0.52363\n",
      "[25]\tvalidation_0-auc:0.51919\n",
      "[26]\tvalidation_0-auc:0.52692\n",
      "[27]\tvalidation_0-auc:0.52090\n",
      "[28]\tvalidation_0-auc:0.52763\n",
      "[29]\tvalidation_0-auc:0.52477\n",
      "[30]\tvalidation_0-auc:0.51375\n",
      "[31]\tvalidation_0-auc:0.51260\n",
      "[32]\tvalidation_0-auc:0.51661\n",
      "[33]\tvalidation_0-auc:0.51060\n",
      "[34]\tvalidation_0-auc:0.51675\n",
      "[35]\tvalidation_0-auc:0.52606\n",
      "[36]\tvalidation_0-auc:0.51804\n",
      "[37]\tvalidation_0-auc:0.52320\n",
      "[38]\tvalidation_0-auc:0.52005\n",
      "[39]\tvalidation_0-auc:0.52148\n",
      "[40]\tvalidation_0-auc:0.51589\n",
      "[41]\tvalidation_0-auc:0.52105\n",
      "[42]\tvalidation_0-auc:0.51790\n",
      "[43]\tvalidation_0-auc:0.51775\n",
      "[44]\tvalidation_0-auc:0.51575\n",
      "[45]\tvalidation_0-auc:0.51360\n",
      "[46]\tvalidation_0-auc:0.49971\n",
      "[47]\tvalidation_0-auc:0.50845\n",
      "[48]\tvalidation_0-auc:0.50687\n",
      "[49]\tvalidation_0-auc:0.50272\n",
      "[50]\tvalidation_0-auc:0.50315\n",
      "[51]\tvalidation_0-auc:0.49971\n",
      "[52]\tvalidation_0-auc:0.49341\n",
      "[53]\tvalidation_0-auc:0.50143\n",
      "[54]\tvalidation_0-auc:0.51074\n",
      "[55]\tvalidation_0-auc:0.51103\n",
      "[56]\tvalidation_0-auc:0.51188\n",
      "[57]\tvalidation_0-auc:0.50315\n",
      "[58]\tvalidation_0-auc:0.50515\n",
      "[59]\tvalidation_0-auc:0.50258\n",
      "[60]\tvalidation_0-auc:0.50830\n",
      "[61]\tvalidation_0-auc:0.50172\n",
      "[62]\tvalidation_0-auc:0.51031\n",
      "[63]\tvalidation_0-auc:0.51246\n",
      "[64]\tvalidation_0-auc:0.50802\n",
      "[65]\tvalidation_0-auc:0.50630\n",
      "[66]\tvalidation_0-auc:0.50830\n",
      "[67]\tvalidation_0-auc:0.51403\n",
      "[68]\tvalidation_0-auc:0.50830\n",
      "[69]\tvalidation_0-auc:0.50644\n",
      "[70]\tvalidation_0-auc:0.50401\n",
      "[71]\tvalidation_0-auc:0.49785\n",
      "[72]\tvalidation_0-auc:0.49943\n",
      "[73]\tvalidation_0-auc:0.50000\n",
      "[74]\tvalidation_0-auc:0.50029\n",
      "[75]\tvalidation_0-auc:0.50401\n",
      "[76]\tvalidation_0-auc:0.51031\n",
      "[77]\tvalidation_0-auc:0.50859\n",
      "[78]\tvalidation_0-auc:0.50888\n",
      "[79]\tvalidation_0-auc:0.51031\n",
      "[80]\tvalidation_0-auc:0.51747\n",
      "[81]\tvalidation_0-auc:0.52205\n",
      "[82]\tvalidation_0-auc:0.52377\n",
      "[83]\tvalidation_0-auc:0.52262\n",
      "[84]\tvalidation_0-auc:0.52348\n",
      "[85]\tvalidation_0-auc:0.52463\n",
      "[86]\tvalidation_0-auc:0.52262\n",
      "[87]\tvalidation_0-auc:0.52062\n",
      "[88]\tvalidation_0-auc:0.52434\n",
      "[89]\tvalidation_0-auc:0.51890\n",
      "[90]\tvalidation_0-auc:0.51947\n",
      "[91]\tvalidation_0-auc:0.51575\n",
      "[92]\tvalidation_0-auc:0.51747\n",
      "[93]\tvalidation_0-auc:0.52062\n",
      "[94]\tvalidation_0-auc:0.51947\n",
      "[95]\tvalidation_0-auc:0.51790\n",
      "[96]\tvalidation_0-auc:0.51990\n",
      "[97]\tvalidation_0-auc:0.51976\n",
      "[98]\tvalidation_0-auc:0.52262\n",
      "[99]\tvalidation_0-auc:0.52262\n",
      "Test Accuracy for Layer 2: 0.5842\n",
      "Test AUC for Layer 2: 0.3705\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.48991\n",
      "[1]\tvalidation_0-auc:0.55569\n",
      "[2]\tvalidation_0-auc:0.56094\n",
      "[3]\tvalidation_0-auc:0.54520\n",
      "[4]\tvalidation_0-auc:0.55145\n",
      "[5]\tvalidation_0-auc:0.55952\n",
      "[6]\tvalidation_0-auc:0.54903\n",
      "[7]\tvalidation_0-auc:0.52341\n",
      "[8]\tvalidation_0-auc:0.52865\n",
      "[9]\tvalidation_0-auc:0.52361\n",
      "[10]\tvalidation_0-auc:0.50767\n",
      "[11]\tvalidation_0-auc:0.46953\n",
      "[12]\tvalidation_0-auc:0.45299\n",
      "[13]\tvalidation_0-auc:0.47337\n",
      "[14]\tvalidation_0-auc:0.45722\n",
      "[15]\tvalidation_0-auc:0.46529\n",
      "[16]\tvalidation_0-auc:0.47175\n",
      "[17]\tvalidation_0-auc:0.49153\n",
      "[18]\tvalidation_0-auc:0.48386\n",
      "[19]\tvalidation_0-auc:0.49758\n",
      "[20]\tvalidation_0-auc:0.48870\n",
      "[21]\tvalidation_0-auc:0.48749\n",
      "[22]\tvalidation_0-auc:0.48608\n",
      "[23]\tvalidation_0-auc:0.48931\n",
      "[24]\tvalidation_0-auc:0.49213\n",
      "[25]\tvalidation_0-auc:0.47559\n",
      "[26]\tvalidation_0-auc:0.47599\n",
      "[27]\tvalidation_0-auc:0.48507\n",
      "[28]\tvalidation_0-auc:0.48830\n",
      "[29]\tvalidation_0-auc:0.48709\n",
      "[30]\tvalidation_0-auc:0.48467\n",
      "[31]\tvalidation_0-auc:0.49031\n",
      "[32]\tvalidation_0-auc:0.48265\n",
      "[33]\tvalidation_0-auc:0.47780\n",
      "[34]\tvalidation_0-auc:0.47296\n",
      "[35]\tvalidation_0-auc:0.46408\n",
      "[36]\tvalidation_0-auc:0.45480\n",
      "[37]\tvalidation_0-auc:0.44734\n",
      "[38]\tvalidation_0-auc:0.44552\n",
      "[39]\tvalidation_0-auc:0.43947\n",
      "[40]\tvalidation_0-auc:0.44068\n",
      "[41]\tvalidation_0-auc:0.44088\n",
      "[42]\tvalidation_0-auc:0.42938\n",
      "[43]\tvalidation_0-auc:0.42676\n",
      "[44]\tvalidation_0-auc:0.42252\n",
      "[45]\tvalidation_0-auc:0.41626\n",
      "[46]\tvalidation_0-auc:0.41687\n",
      "[47]\tvalidation_0-auc:0.41606\n",
      "[48]\tvalidation_0-auc:0.41162\n",
      "[49]\tvalidation_0-auc:0.40839\n",
      "[50]\tvalidation_0-auc:0.40597\n",
      "[51]\tvalidation_0-auc:0.40517\n",
      "[52]\tvalidation_0-auc:0.40597\n",
      "[53]\tvalidation_0-auc:0.40577\n",
      "[54]\tvalidation_0-auc:0.40880\n",
      "[55]\tvalidation_0-auc:0.40779\n",
      "[56]\tvalidation_0-auc:0.40335\n",
      "[57]\tvalidation_0-auc:0.39891\n",
      "[58]\tvalidation_0-auc:0.40355\n",
      "[59]\tvalidation_0-auc:0.40315\n",
      "[60]\tvalidation_0-auc:0.40174\n",
      "[61]\tvalidation_0-auc:0.39669\n",
      "[62]\tvalidation_0-auc:0.40174\n",
      "[63]\tvalidation_0-auc:0.39185\n",
      "[64]\tvalidation_0-auc:0.38660\n",
      "[65]\tvalidation_0-auc:0.38983\n",
      "[66]\tvalidation_0-auc:0.39306\n",
      "[67]\tvalidation_0-auc:0.39629\n",
      "[68]\tvalidation_0-auc:0.39225\n",
      "[69]\tvalidation_0-auc:0.39023\n",
      "[70]\tvalidation_0-auc:0.39508\n",
      "[71]\tvalidation_0-auc:0.39790\n",
      "[72]\tvalidation_0-auc:0.39588\n",
      "[73]\tvalidation_0-auc:0.39144\n",
      "[74]\tvalidation_0-auc:0.39770\n",
      "[75]\tvalidation_0-auc:0.40194\n",
      "[76]\tvalidation_0-auc:0.39709\n",
      "[77]\tvalidation_0-auc:0.39023\n",
      "[78]\tvalidation_0-auc:0.39326\n",
      "[79]\tvalidation_0-auc:0.39306\n",
      "[80]\tvalidation_0-auc:0.39225\n",
      "[81]\tvalidation_0-auc:0.38983\n",
      "[82]\tvalidation_0-auc:0.39023\n",
      "[83]\tvalidation_0-auc:0.38983\n",
      "[84]\tvalidation_0-auc:0.39689\n",
      "[85]\tvalidation_0-auc:0.39266\n",
      "[86]\tvalidation_0-auc:0.39205\n",
      "[87]\tvalidation_0-auc:0.39487\n",
      "[88]\tvalidation_0-auc:0.39346\n",
      "[89]\tvalidation_0-auc:0.39669\n",
      "[90]\tvalidation_0-auc:0.39972\n",
      "[91]\tvalidation_0-auc:0.39629\n",
      "[92]\tvalidation_0-auc:0.39871\n",
      "[93]\tvalidation_0-auc:0.39508\n",
      "[94]\tvalidation_0-auc:0.39588\n",
      "[95]\tvalidation_0-auc:0.39346\n",
      "[96]\tvalidation_0-auc:0.39588\n",
      "[97]\tvalidation_0-auc:0.39831\n",
      "[98]\tvalidation_0-auc:0.40153\n",
      "[99]\tvalidation_0-auc:0.40153\n",
      "Test Accuracy for Layer 3: 0.5000\n",
      "Test AUC for Layer 3: 0.4714\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4516\n",
      "Average Test AUC across all layers: 0.4561\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Full text and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.51795\n",
      "[1]\tvalidation_0-auc:0.45949\n",
      "[2]\tvalidation_0-auc:0.44698\n",
      "[3]\tvalidation_0-auc:0.46079\n",
      "[4]\tvalidation_0-auc:0.46894\n",
      "[5]\tvalidation_0-auc:0.47308\n",
      "[6]\tvalidation_0-auc:0.46646\n",
      "[7]\tvalidation_0-auc:0.45749\n",
      "[8]\tvalidation_0-auc:0.47296\n",
      "[9]\tvalidation_0-auc:0.45843\n",
      "[10]\tvalidation_0-auc:0.45194\n",
      "[11]\tvalidation_0-auc:0.45394\n",
      "[12]\tvalidation_0-auc:0.46245\n",
      "[13]\tvalidation_0-auc:0.47803\n",
      "[14]\tvalidation_0-auc:0.48819\n",
      "[15]\tvalidation_0-auc:0.49386\n",
      "[16]\tvalidation_0-auc:0.49622\n",
      "[17]\tvalidation_0-auc:0.49823\n",
      "[18]\tvalidation_0-auc:0.49894\n",
      "[19]\tvalidation_0-auc:0.49764\n",
      "[20]\tvalidation_0-auc:0.47662\n",
      "[21]\tvalidation_0-auc:0.48347\n",
      "[22]\tvalidation_0-auc:0.47992\n",
      "[23]\tvalidation_0-auc:0.47130\n",
      "[24]\tvalidation_0-auc:0.45583\n",
      "[25]\tvalidation_0-auc:0.47815\n",
      "[26]\tvalidation_0-auc:0.46693\n",
      "[27]\tvalidation_0-auc:0.47650\n",
      "[28]\tvalidation_0-auc:0.48111\n",
      "[29]\tvalidation_0-auc:0.47095\n",
      "[30]\tvalidation_0-auc:0.46587\n",
      "[31]\tvalidation_0-auc:0.46800\n",
      "[32]\tvalidation_0-auc:0.47000\n",
      "[33]\tvalidation_0-auc:0.46682\n",
      "[34]\tvalidation_0-auc:0.47095\n",
      "[35]\tvalidation_0-auc:0.47225\n",
      "[36]\tvalidation_0-auc:0.47685\n",
      "[37]\tvalidation_0-auc:0.48111\n",
      "[38]\tvalidation_0-auc:0.47803\n",
      "[39]\tvalidation_0-auc:0.48996\n",
      "[40]\tvalidation_0-auc:0.49303\n",
      "[41]\tvalidation_0-auc:0.49563\n",
      "[42]\tvalidation_0-auc:0.48772\n",
      "[43]\tvalidation_0-auc:0.48736\n",
      "[44]\tvalidation_0-auc:0.49032\n",
      "[45]\tvalidation_0-auc:0.49433\n",
      "[46]\tvalidation_0-auc:0.49504\n",
      "[47]\tvalidation_0-auc:0.49823\n",
      "[48]\tvalidation_0-auc:0.49575\n",
      "[49]\tvalidation_0-auc:0.49953\n",
      "[50]\tvalidation_0-auc:0.49362\n",
      "[51]\tvalidation_0-auc:0.50189\n",
      "[52]\tvalidation_0-auc:0.50720\n",
      "[53]\tvalidation_0-auc:0.51500\n",
      "[54]\tvalidation_0-auc:0.51346\n",
      "[55]\tvalidation_0-auc:0.50437\n",
      "[56]\tvalidation_0-auc:0.50413\n",
      "[57]\tvalidation_0-auc:0.50449\n",
      "[58]\tvalidation_0-auc:0.50354\n",
      "[59]\tvalidation_0-auc:0.50260\n",
      "[60]\tvalidation_0-auc:0.50106\n",
      "[61]\tvalidation_0-auc:0.50425\n",
      "[62]\tvalidation_0-auc:0.50224\n",
      "[63]\tvalidation_0-auc:0.50472\n",
      "[64]\tvalidation_0-auc:0.50579\n",
      "[65]\tvalidation_0-auc:0.51181\n",
      "[66]\tvalidation_0-auc:0.51252\n",
      "[67]\tvalidation_0-auc:0.50779\n",
      "[68]\tvalidation_0-auc:0.50283\n",
      "[69]\tvalidation_0-auc:0.50272\n",
      "[70]\tvalidation_0-auc:0.50142\n",
      "[71]\tvalidation_0-auc:0.50307\n",
      "[72]\tvalidation_0-auc:0.50071\n",
      "[73]\tvalidation_0-auc:0.50555\n",
      "[74]\tvalidation_0-auc:0.50520\n",
      "[75]\tvalidation_0-auc:0.50827\n",
      "[76]\tvalidation_0-auc:0.50543\n",
      "[77]\tvalidation_0-auc:0.50968\n",
      "[78]\tvalidation_0-auc:0.51228\n",
      "[79]\tvalidation_0-auc:0.51464\n",
      "[80]\tvalidation_0-auc:0.51453\n",
      "[81]\tvalidation_0-auc:0.51417\n",
      "[82]\tvalidation_0-auc:0.51488\n",
      "[83]\tvalidation_0-auc:0.51417\n",
      "[84]\tvalidation_0-auc:0.51346\n",
      "[85]\tvalidation_0-auc:0.50874\n",
      "[86]\tvalidation_0-auc:0.51134\n",
      "[87]\tvalidation_0-auc:0.51157\n",
      "[88]\tvalidation_0-auc:0.51453\n",
      "[89]\tvalidation_0-auc:0.51441\n",
      "[90]\tvalidation_0-auc:0.51346\n",
      "[91]\tvalidation_0-auc:0.51086\n",
      "[92]\tvalidation_0-auc:0.50886\n",
      "[93]\tvalidation_0-auc:0.50874\n",
      "[94]\tvalidation_0-auc:0.50590\n",
      "[95]\tvalidation_0-auc:0.50815\n",
      "[96]\tvalidation_0-auc:0.50756\n",
      "[97]\tvalidation_0-auc:0.50614\n",
      "[98]\tvalidation_0-auc:0.50638\n",
      "[99]\tvalidation_0-auc:0.51016\n",
      "Test Accuracy for Layer 1: 0.4436\n",
      "Test AUC for Layer 1: 0.4828\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.52130\n",
      "[1]\tvalidation_0-auc:0.50183\n",
      "[2]\tvalidation_0-auc:0.52199\n",
      "[3]\tvalidation_0-auc:0.52142\n",
      "[4]\tvalidation_0-auc:0.50252\n",
      "[5]\tvalidation_0-auc:0.50069\n",
      "[6]\tvalidation_0-auc:0.47813\n",
      "[7]\tvalidation_0-auc:0.47927\n",
      "[8]\tvalidation_0-auc:0.46736\n",
      "[9]\tvalidation_0-auc:0.46370\n",
      "[10]\tvalidation_0-auc:0.46919\n",
      "[11]\tvalidation_0-auc:0.45980\n",
      "[12]\tvalidation_0-auc:0.45957\n",
      "[13]\tvalidation_0-auc:0.46232\n",
      "[14]\tvalidation_0-auc:0.45442\n",
      "[15]\tvalidation_0-auc:0.45511\n",
      "[16]\tvalidation_0-auc:0.46404\n",
      "[17]\tvalidation_0-auc:0.45774\n",
      "[18]\tvalidation_0-auc:0.45660\n",
      "[19]\tvalidation_0-auc:0.45843\n",
      "[20]\tvalidation_0-auc:0.45740\n",
      "[21]\tvalidation_0-auc:0.47274\n",
      "[22]\tvalidation_0-auc:0.47481\n",
      "[23]\tvalidation_0-auc:0.48236\n",
      "[24]\tvalidation_0-auc:0.47343\n",
      "[25]\tvalidation_0-auc:0.46782\n",
      "[26]\tvalidation_0-auc:0.46748\n",
      "[27]\tvalidation_0-auc:0.46633\n",
      "[28]\tvalidation_0-auc:0.47057\n",
      "[29]\tvalidation_0-auc:0.47526\n",
      "[30]\tvalidation_0-auc:0.47229\n",
      "[31]\tvalidation_0-auc:0.47000\n",
      "[32]\tvalidation_0-auc:0.47034\n",
      "[33]\tvalidation_0-auc:0.46519\n",
      "[34]\tvalidation_0-auc:0.46312\n",
      "[35]\tvalidation_0-auc:0.46312\n",
      "[36]\tvalidation_0-auc:0.45671\n",
      "[37]\tvalidation_0-auc:0.45694\n",
      "[38]\tvalidation_0-auc:0.45625\n",
      "[39]\tvalidation_0-auc:0.45889\n",
      "[40]\tvalidation_0-auc:0.46244\n",
      "[41]\tvalidation_0-auc:0.46450\n",
      "[42]\tvalidation_0-auc:0.46633\n",
      "[43]\tvalidation_0-auc:0.46221\n",
      "[44]\tvalidation_0-auc:0.45900\n",
      "[45]\tvalidation_0-auc:0.45557\n",
      "[46]\tvalidation_0-auc:0.46152\n",
      "[47]\tvalidation_0-auc:0.46507\n",
      "[48]\tvalidation_0-auc:0.46530\n",
      "[49]\tvalidation_0-auc:0.46358\n",
      "[50]\tvalidation_0-auc:0.46519\n",
      "[51]\tvalidation_0-auc:0.46564\n",
      "[52]\tvalidation_0-auc:0.46198\n",
      "[53]\tvalidation_0-auc:0.46015\n",
      "[54]\tvalidation_0-auc:0.45671\n",
      "[55]\tvalidation_0-auc:0.45625\n",
      "[56]\tvalidation_0-auc:0.45854\n",
      "[57]\tvalidation_0-auc:0.45877\n",
      "[58]\tvalidation_0-auc:0.45648\n",
      "[59]\tvalidation_0-auc:0.45660\n",
      "[60]\tvalidation_0-auc:0.46152\n",
      "[61]\tvalidation_0-auc:0.46060\n",
      "[62]\tvalidation_0-auc:0.45900\n",
      "[63]\tvalidation_0-auc:0.45671\n",
      "[64]\tvalidation_0-auc:0.45637\n",
      "[65]\tvalidation_0-auc:0.45282\n",
      "[66]\tvalidation_0-auc:0.45350\n",
      "[67]\tvalidation_0-auc:0.45053\n",
      "[68]\tvalidation_0-auc:0.44755\n",
      "[69]\tvalidation_0-auc:0.44961\n",
      "[70]\tvalidation_0-auc:0.45259\n",
      "[71]\tvalidation_0-auc:0.45511\n",
      "[72]\tvalidation_0-auc:0.45282\n",
      "[73]\tvalidation_0-auc:0.44961\n",
      "[74]\tvalidation_0-auc:0.44927\n",
      "[75]\tvalidation_0-auc:0.45202\n",
      "[76]\tvalidation_0-auc:0.45625\n",
      "[77]\tvalidation_0-auc:0.46083\n",
      "[78]\tvalidation_0-auc:0.45980\n",
      "[79]\tvalidation_0-auc:0.45969\n",
      "[80]\tvalidation_0-auc:0.46072\n",
      "[81]\tvalidation_0-auc:0.46049\n",
      "[82]\tvalidation_0-auc:0.46083\n",
      "[83]\tvalidation_0-auc:0.46404\n",
      "[84]\tvalidation_0-auc:0.46381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85]\tvalidation_0-auc:0.46530\n",
      "[86]\tvalidation_0-auc:0.46748\n",
      "[87]\tvalidation_0-auc:0.46656\n",
      "[88]\tvalidation_0-auc:0.46748\n",
      "[89]\tvalidation_0-auc:0.46473\n",
      "[90]\tvalidation_0-auc:0.46679\n",
      "[91]\tvalidation_0-auc:0.46713\n",
      "[92]\tvalidation_0-auc:0.46759\n",
      "[93]\tvalidation_0-auc:0.46862\n",
      "[94]\tvalidation_0-auc:0.46839\n",
      "[95]\tvalidation_0-auc:0.46942\n",
      "[96]\tvalidation_0-auc:0.47000\n",
      "[97]\tvalidation_0-auc:0.46862\n",
      "[98]\tvalidation_0-auc:0.46427\n",
      "[99]\tvalidation_0-auc:0.46496\n",
      "Test Accuracy for Layer 2: 0.5446\n",
      "Test AUC for Layer 2: 0.3759\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.51858\n",
      "[1]\tvalidation_0-auc:0.45652\n",
      "[2]\tvalidation_0-auc:0.49348\n",
      "[3]\tvalidation_0-auc:0.49190\n",
      "[4]\tvalidation_0-auc:0.48360\n",
      "[5]\tvalidation_0-auc:0.46265\n",
      "[6]\tvalidation_0-auc:0.44862\n",
      "[7]\tvalidation_0-auc:0.42115\n",
      "[8]\tvalidation_0-auc:0.43439\n",
      "[9]\tvalidation_0-auc:0.41838\n",
      "[10]\tvalidation_0-auc:0.39466\n",
      "[11]\tvalidation_0-auc:0.39387\n",
      "[12]\tvalidation_0-auc:0.38834\n",
      "[13]\tvalidation_0-auc:0.40474\n",
      "[14]\tvalidation_0-auc:0.42134\n",
      "[15]\tvalidation_0-auc:0.41067\n",
      "[16]\tvalidation_0-auc:0.42134\n",
      "[17]\tvalidation_0-auc:0.42984\n",
      "[18]\tvalidation_0-auc:0.42964\n",
      "[19]\tvalidation_0-auc:0.43202\n",
      "[20]\tvalidation_0-auc:0.43656\n",
      "[21]\tvalidation_0-auc:0.43439\n",
      "[22]\tvalidation_0-auc:0.42312\n",
      "[23]\tvalidation_0-auc:0.42964\n",
      "[24]\tvalidation_0-auc:0.43794\n",
      "[25]\tvalidation_0-auc:0.43854\n",
      "[26]\tvalidation_0-auc:0.44545\n",
      "[27]\tvalidation_0-auc:0.44901\n",
      "[28]\tvalidation_0-auc:0.44664\n",
      "[29]\tvalidation_0-auc:0.44644\n",
      "[30]\tvalidation_0-auc:0.43261\n",
      "[31]\tvalidation_0-auc:0.43617\n",
      "[32]\tvalidation_0-auc:0.42885\n",
      "[33]\tvalidation_0-auc:0.41700\n",
      "[34]\tvalidation_0-auc:0.41344\n",
      "[35]\tvalidation_0-auc:0.41858\n",
      "[36]\tvalidation_0-auc:0.40435\n",
      "[37]\tvalidation_0-auc:0.41067\n",
      "[38]\tvalidation_0-auc:0.41087\n",
      "[39]\tvalidation_0-auc:0.40988\n",
      "[40]\tvalidation_0-auc:0.40889\n",
      "[41]\tvalidation_0-auc:0.40632\n",
      "[42]\tvalidation_0-auc:0.41265\n",
      "[43]\tvalidation_0-auc:0.42372\n",
      "[44]\tvalidation_0-auc:0.42134\n",
      "[45]\tvalidation_0-auc:0.42688\n",
      "[46]\tvalidation_0-auc:0.42964\n",
      "[47]\tvalidation_0-auc:0.42451\n",
      "[48]\tvalidation_0-auc:0.42490\n",
      "[49]\tvalidation_0-auc:0.41344\n",
      "[50]\tvalidation_0-auc:0.41739\n",
      "[51]\tvalidation_0-auc:0.41976\n",
      "[52]\tvalidation_0-auc:0.42055\n",
      "[53]\tvalidation_0-auc:0.42708\n",
      "[54]\tvalidation_0-auc:0.42648\n",
      "[55]\tvalidation_0-auc:0.43597\n",
      "[56]\tvalidation_0-auc:0.43557\n",
      "[57]\tvalidation_0-auc:0.42826\n",
      "[58]\tvalidation_0-auc:0.42609\n",
      "[59]\tvalidation_0-auc:0.42431\n",
      "[60]\tvalidation_0-auc:0.43123\n",
      "[61]\tvalidation_0-auc:0.42885\n",
      "[62]\tvalidation_0-auc:0.42292\n",
      "[63]\tvalidation_0-auc:0.41759\n",
      "[64]\tvalidation_0-auc:0.41265\n",
      "[65]\tvalidation_0-auc:0.41621\n",
      "[66]\tvalidation_0-auc:0.41087\n",
      "[67]\tvalidation_0-auc:0.41462\n",
      "[68]\tvalidation_0-auc:0.41383\n",
      "[69]\tvalidation_0-auc:0.41146\n",
      "[70]\tvalidation_0-auc:0.40909\n",
      "[71]\tvalidation_0-auc:0.41146\n",
      "[72]\tvalidation_0-auc:0.40988\n",
      "[73]\tvalidation_0-auc:0.41186\n",
      "[74]\tvalidation_0-auc:0.40988\n",
      "[75]\tvalidation_0-auc:0.40632\n",
      "[76]\tvalidation_0-auc:0.40395\n",
      "[77]\tvalidation_0-auc:0.40514\n",
      "[78]\tvalidation_0-auc:0.40316\n",
      "[79]\tvalidation_0-auc:0.39802\n",
      "[80]\tvalidation_0-auc:0.39644\n",
      "[81]\tvalidation_0-auc:0.39289\n",
      "[82]\tvalidation_0-auc:0.39605\n",
      "[83]\tvalidation_0-auc:0.39585\n",
      "[84]\tvalidation_0-auc:0.39723\n",
      "[85]\tvalidation_0-auc:0.39368\n",
      "[86]\tvalidation_0-auc:0.39565\n",
      "[87]\tvalidation_0-auc:0.39407\n",
      "[88]\tvalidation_0-auc:0.39526\n",
      "[89]\tvalidation_0-auc:0.39130\n",
      "[90]\tvalidation_0-auc:0.39407\n",
      "[91]\tvalidation_0-auc:0.39605\n",
      "[92]\tvalidation_0-auc:0.38854\n",
      "[93]\tvalidation_0-auc:0.39723\n",
      "[94]\tvalidation_0-auc:0.40040\n",
      "[95]\tvalidation_0-auc:0.39881\n",
      "[96]\tvalidation_0-auc:0.39565\n",
      "[97]\tvalidation_0-auc:0.39506\n",
      "[98]\tvalidation_0-auc:0.39289\n",
      "[99]\tvalidation_0-auc:0.39684\n",
      "Test Accuracy for Layer 3: 0.4429\n",
      "Test AUC for Layer 3: 0.5806\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4770\n",
      "Average Test AUC across all layers: 0.4798\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost model for Full text and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.47561\n",
      "[1]\tvalidation_0-auc:0.49715\n",
      "[2]\tvalidation_0-auc:0.50840\n",
      "[3]\tvalidation_0-auc:0.49756\n",
      "[4]\tvalidation_0-auc:0.48631\n",
      "[5]\tvalidation_0-auc:0.51369\n",
      "[6]\tvalidation_0-auc:0.49566\n",
      "[7]\tvalidation_0-auc:0.46721\n",
      "[8]\tvalidation_0-auc:0.46640\n",
      "[9]\tvalidation_0-auc:0.47385\n",
      "[10]\tvalidation_0-auc:0.46287\n",
      "[11]\tvalidation_0-auc:0.47276\n",
      "[12]\tvalidation_0-auc:0.49133\n",
      "[13]\tvalidation_0-auc:0.50976\n",
      "[14]\tvalidation_0-auc:0.51775\n",
      "[15]\tvalidation_0-auc:0.51152\n",
      "[16]\tvalidation_0-auc:0.51314\n",
      "[17]\tvalidation_0-auc:0.51640\n",
      "[18]\tvalidation_0-auc:0.52398\n",
      "[19]\tvalidation_0-auc:0.51789\n",
      "[20]\tvalidation_0-auc:0.51233\n",
      "[21]\tvalidation_0-auc:0.51450\n",
      "[22]\tvalidation_0-auc:0.50176\n",
      "[23]\tvalidation_0-auc:0.50650\n",
      "[24]\tvalidation_0-auc:0.50678\n",
      "[25]\tvalidation_0-auc:0.50339\n",
      "[26]\tvalidation_0-auc:0.49661\n",
      "[27]\tvalidation_0-auc:0.49336\n",
      "[28]\tvalidation_0-auc:0.50190\n",
      "[29]\tvalidation_0-auc:0.50203\n",
      "[30]\tvalidation_0-auc:0.49024\n",
      "[31]\tvalidation_0-auc:0.49214\n",
      "[32]\tvalidation_0-auc:0.49851\n",
      "[33]\tvalidation_0-auc:0.50678\n",
      "[34]\tvalidation_0-auc:0.51328\n",
      "[35]\tvalidation_0-auc:0.51260\n",
      "[36]\tvalidation_0-auc:0.50257\n",
      "[37]\tvalidation_0-auc:0.50908\n",
      "[38]\tvalidation_0-auc:0.50976\n",
      "[39]\tvalidation_0-auc:0.50650\n",
      "[40]\tvalidation_0-auc:0.51098\n",
      "[41]\tvalidation_0-auc:0.51734\n",
      "[42]\tvalidation_0-auc:0.51856\n",
      "[43]\tvalidation_0-auc:0.52439\n",
      "[44]\tvalidation_0-auc:0.52005\n",
      "[45]\tvalidation_0-auc:0.51762\n",
      "[46]\tvalidation_0-auc:0.51775\n",
      "[47]\tvalidation_0-auc:0.52466\n",
      "[48]\tvalidation_0-auc:0.52304\n",
      "[49]\tvalidation_0-auc:0.52764\n",
      "[50]\tvalidation_0-auc:0.52168\n",
      "[51]\tvalidation_0-auc:0.51138\n",
      "[52]\tvalidation_0-auc:0.51328\n",
      "[53]\tvalidation_0-auc:0.51545\n",
      "[54]\tvalidation_0-auc:0.50691\n",
      "[55]\tvalidation_0-auc:0.50136\n",
      "[56]\tvalidation_0-auc:0.49932\n",
      "[57]\tvalidation_0-auc:0.49282\n",
      "[58]\tvalidation_0-auc:0.49919\n",
      "[59]\tvalidation_0-auc:0.50325\n",
      "[60]\tvalidation_0-auc:0.50501\n",
      "[61]\tvalidation_0-auc:0.50867\n",
      "[62]\tvalidation_0-auc:0.51382\n",
      "[63]\tvalidation_0-auc:0.51694\n",
      "[64]\tvalidation_0-auc:0.51314\n",
      "[65]\tvalidation_0-auc:0.51098\n",
      "[66]\tvalidation_0-auc:0.51870\n",
      "[67]\tvalidation_0-auc:0.51762\n",
      "[68]\tvalidation_0-auc:0.52222\n",
      "[69]\tvalidation_0-auc:0.52087\n",
      "[70]\tvalidation_0-auc:0.52114\n",
      "[71]\tvalidation_0-auc:0.51734\n",
      "[72]\tvalidation_0-auc:0.51707\n",
      "[73]\tvalidation_0-auc:0.52209\n",
      "[74]\tvalidation_0-auc:0.52805\n",
      "[75]\tvalidation_0-auc:0.52778\n",
      "[76]\tvalidation_0-auc:0.52331\n",
      "[77]\tvalidation_0-auc:0.52981\n",
      "[78]\tvalidation_0-auc:0.52832\n",
      "[79]\tvalidation_0-auc:0.52724\n",
      "[80]\tvalidation_0-auc:0.52683\n",
      "[81]\tvalidation_0-auc:0.52927\n",
      "[82]\tvalidation_0-auc:0.52764\n",
      "[83]\tvalidation_0-auc:0.51775\n",
      "[84]\tvalidation_0-auc:0.51762\n",
      "[85]\tvalidation_0-auc:0.52005\n",
      "[86]\tvalidation_0-auc:0.52304\n",
      "[87]\tvalidation_0-auc:0.52073\n",
      "[88]\tvalidation_0-auc:0.52263\n",
      "[89]\tvalidation_0-auc:0.52954\n",
      "[90]\tvalidation_0-auc:0.52818\n",
      "[91]\tvalidation_0-auc:0.52656\n",
      "[92]\tvalidation_0-auc:0.52954\n",
      "[93]\tvalidation_0-auc:0.52954\n",
      "[94]\tvalidation_0-auc:0.53062\n",
      "[95]\tvalidation_0-auc:0.52791\n",
      "[96]\tvalidation_0-auc:0.53225\n",
      "[97]\tvalidation_0-auc:0.52900\n",
      "[98]\tvalidation_0-auc:0.52818\n",
      "[99]\tvalidation_0-auc:0.52547\n",
      "Test Accuracy for Layer 1: 0.2707\n",
      "Test AUC for Layer 1: 0.4966\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.51188\n",
      "[1]\tvalidation_0-auc:0.55126\n",
      "[2]\tvalidation_0-auc:0.56701\n",
      "[3]\tvalidation_0-auc:0.50673\n",
      "[4]\tvalidation_0-auc:0.51990\n",
      "[5]\tvalidation_0-auc:0.54167\n",
      "[6]\tvalidation_0-auc:0.53379\n",
      "[7]\tvalidation_0-auc:0.52821\n",
      "[8]\tvalidation_0-auc:0.51332\n",
      "[9]\tvalidation_0-auc:0.51661\n",
      "[10]\tvalidation_0-auc:0.53064\n",
      "[11]\tvalidation_0-auc:0.54840\n",
      "[12]\tvalidation_0-auc:0.55527\n",
      "[13]\tvalidation_0-auc:0.56085\n",
      "[14]\tvalidation_0-auc:0.54424\n",
      "[15]\tvalidation_0-auc:0.54210\n",
      "[16]\tvalidation_0-auc:0.52749\n",
      "[17]\tvalidation_0-auc:0.53265\n",
      "[18]\tvalidation_0-auc:0.53522\n",
      "[19]\tvalidation_0-auc:0.54296\n",
      "[20]\tvalidation_0-auc:0.54052\n",
      "[21]\tvalidation_0-auc:0.54754\n",
      "[22]\tvalidation_0-auc:0.54596\n",
      "[23]\tvalidation_0-auc:0.56300\n",
      "[24]\tvalidation_0-auc:0.55212\n",
      "[25]\tvalidation_0-auc:0.55269\n",
      "[26]\tvalidation_0-auc:0.54525\n",
      "[27]\tvalidation_0-auc:0.54281\n",
      "[28]\tvalidation_0-auc:0.54611\n",
      "[29]\tvalidation_0-auc:0.54811\n",
      "[30]\tvalidation_0-auc:0.55513\n",
      "[31]\tvalidation_0-auc:0.54754\n",
      "[32]\tvalidation_0-auc:0.55670\n",
      "[33]\tvalidation_0-auc:0.55212\n",
      "[34]\tvalidation_0-auc:0.54038\n",
      "[35]\tvalidation_0-auc:0.53322\n",
      "[36]\tvalidation_0-auc:0.54167\n",
      "[37]\tvalidation_0-auc:0.53909\n",
      "[38]\tvalidation_0-auc:0.54081\n",
      "[39]\tvalidation_0-auc:0.52721\n",
      "[40]\tvalidation_0-auc:0.52090\n",
      "[41]\tvalidation_0-auc:0.53465\n",
      "[42]\tvalidation_0-auc:0.53537\n",
      "[43]\tvalidation_0-auc:0.53522\n",
      "[44]\tvalidation_0-auc:0.52305\n",
      "[45]\tvalidation_0-auc:0.52405\n",
      "[46]\tvalidation_0-auc:0.52119\n",
      "[47]\tvalidation_0-auc:0.52549\n",
      "[48]\tvalidation_0-auc:0.51231\n",
      "[49]\tvalidation_0-auc:0.51260\n",
      "[50]\tvalidation_0-auc:0.51604\n",
      "[51]\tvalidation_0-auc:0.51375\n",
      "[52]\tvalidation_0-auc:0.50601\n",
      "[53]\tvalidation_0-auc:0.51060\n",
      "[54]\tvalidation_0-auc:0.51074\n",
      "[55]\tvalidation_0-auc:0.51317\n",
      "[56]\tvalidation_0-auc:0.50544\n",
      "[57]\tvalidation_0-auc:0.51260\n",
      "[58]\tvalidation_0-auc:0.50730\n",
      "[59]\tvalidation_0-auc:0.50659\n",
      "[60]\tvalidation_0-auc:0.50458\n",
      "[61]\tvalidation_0-auc:0.50873\n",
      "[62]\tvalidation_0-auc:0.51475\n",
      "[63]\tvalidation_0-auc:0.51718\n",
      "[64]\tvalidation_0-auc:0.51604\n",
      "[65]\tvalidation_0-auc:0.51446\n",
      "[66]\tvalidation_0-auc:0.51346\n",
      "[67]\tvalidation_0-auc:0.51647\n",
      "[68]\tvalidation_0-auc:0.51145\n",
      "[69]\tvalidation_0-auc:0.51303\n",
      "[70]\tvalidation_0-auc:0.51647\n",
      "[71]\tvalidation_0-auc:0.51690\n",
      "[72]\tvalidation_0-auc:0.51546\n",
      "[73]\tvalidation_0-auc:0.51518\n",
      "[74]\tvalidation_0-auc:0.51260\n",
      "[75]\tvalidation_0-auc:0.51661\n",
      "[76]\tvalidation_0-auc:0.51260\n",
      "[77]\tvalidation_0-auc:0.51518\n",
      "[78]\tvalidation_0-auc:0.51375\n",
      "[79]\tvalidation_0-auc:0.50473\n",
      "[80]\tvalidation_0-auc:0.50616\n",
      "[81]\tvalidation_0-auc:0.50745\n",
      "[82]\tvalidation_0-auc:0.51403\n",
      "[83]\tvalidation_0-auc:0.51088\n",
      "[84]\tvalidation_0-auc:0.51317\n",
      "[85]\tvalidation_0-auc:0.51375\n",
      "[86]\tvalidation_0-auc:0.51418\n",
      "[87]\tvalidation_0-auc:0.51002\n",
      "[88]\tvalidation_0-auc:0.50745\n",
      "[89]\tvalidation_0-auc:0.50659\n",
      "[90]\tvalidation_0-auc:0.51103\n",
      "[91]\tvalidation_0-auc:0.51460\n",
      "[92]\tvalidation_0-auc:0.51088\n",
      "[93]\tvalidation_0-auc:0.50974\n",
      "[94]\tvalidation_0-auc:0.50916\n",
      "[95]\tvalidation_0-auc:0.51002\n",
      "[96]\tvalidation_0-auc:0.51217\n",
      "[97]\tvalidation_0-auc:0.51317\n",
      "[98]\tvalidation_0-auc:0.51775\n",
      "[99]\tvalidation_0-auc:0.52062\n",
      "Test Accuracy for Layer 2: 0.5842\n",
      "Test AUC for Layer 2: 0.4790\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating and training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.53652\n",
      "[1]\tvalidation_0-auc:0.51755\n",
      "[2]\tvalidation_0-auc:0.54237\n",
      "[3]\tvalidation_0-auc:0.54318\n",
      "[4]\tvalidation_0-auc:0.48467\n",
      "[5]\tvalidation_0-auc:0.50081\n",
      "[6]\tvalidation_0-auc:0.49112\n",
      "[7]\tvalidation_0-auc:0.48144\n",
      "[8]\tvalidation_0-auc:0.46933\n",
      "[9]\tvalidation_0-auc:0.47619\n",
      "[10]\tvalidation_0-auc:0.48123\n",
      "[11]\tvalidation_0-auc:0.48446\n",
      "[12]\tvalidation_0-auc:0.48426\n",
      "[13]\tvalidation_0-auc:0.48467\n",
      "[14]\tvalidation_0-auc:0.47619\n",
      "[15]\tvalidation_0-auc:0.46933\n",
      "[16]\tvalidation_0-auc:0.44855\n",
      "[17]\tvalidation_0-auc:0.45964\n",
      "[18]\tvalidation_0-auc:0.45621\n",
      "[19]\tvalidation_0-auc:0.47498\n",
      "[20]\tvalidation_0-auc:0.47881\n",
      "[21]\tvalidation_0-auc:0.47538\n",
      "[22]\tvalidation_0-auc:0.48507\n",
      "[23]\tvalidation_0-auc:0.49718\n",
      "[24]\tvalidation_0-auc:0.50444\n",
      "[25]\tvalidation_0-auc:0.50484\n",
      "[26]\tvalidation_0-auc:0.49818\n",
      "[27]\tvalidation_0-auc:0.49193\n",
      "[28]\tvalidation_0-auc:0.48830\n",
      "[29]\tvalidation_0-auc:0.48628\n",
      "[30]\tvalidation_0-auc:0.49839\n",
      "[31]\tvalidation_0-auc:0.48931\n",
      "[32]\tvalidation_0-auc:0.48648\n",
      "[33]\tvalidation_0-auc:0.48083\n",
      "[34]\tvalidation_0-auc:0.48285\n",
      "[35]\tvalidation_0-auc:0.48245\n",
      "[36]\tvalidation_0-auc:0.49536\n",
      "[37]\tvalidation_0-auc:0.48789\n",
      "[38]\tvalidation_0-auc:0.49052\n",
      "[39]\tvalidation_0-auc:0.47821\n",
      "[40]\tvalidation_0-auc:0.47942\n",
      "[41]\tvalidation_0-auc:0.48305\n",
      "[42]\tvalidation_0-auc:0.48567\n",
      "[43]\tvalidation_0-auc:0.47801\n",
      "[44]\tvalidation_0-auc:0.48325\n",
      "[45]\tvalidation_0-auc:0.48749\n",
      "[46]\tvalidation_0-auc:0.49374\n",
      "[47]\tvalidation_0-auc:0.48386\n",
      "[48]\tvalidation_0-auc:0.48971\n",
      "[49]\tvalidation_0-auc:0.49496\n",
      "[50]\tvalidation_0-auc:0.50363\n",
      "[51]\tvalidation_0-auc:0.50040\n",
      "[52]\tvalidation_0-auc:0.49011\n",
      "[53]\tvalidation_0-auc:0.48769\n",
      "[54]\tvalidation_0-auc:0.48467\n",
      "[55]\tvalidation_0-auc:0.48749\n",
      "[56]\tvalidation_0-auc:0.48507\n",
      "[57]\tvalidation_0-auc:0.48345\n",
      "[58]\tvalidation_0-auc:0.48103\n",
      "[59]\tvalidation_0-auc:0.47881\n",
      "[60]\tvalidation_0-auc:0.48386\n",
      "[61]\tvalidation_0-auc:0.48325\n",
      "[62]\tvalidation_0-auc:0.48688\n",
      "[63]\tvalidation_0-auc:0.48789\n",
      "[64]\tvalidation_0-auc:0.49233\n",
      "[65]\tvalidation_0-auc:0.49314\n",
      "[66]\tvalidation_0-auc:0.48870\n",
      "[67]\tvalidation_0-auc:0.48224\n",
      "[68]\tvalidation_0-auc:0.48709\n",
      "[69]\tvalidation_0-auc:0.48467\n",
      "[70]\tvalidation_0-auc:0.48305\n",
      "[71]\tvalidation_0-auc:0.48204\n",
      "[72]\tvalidation_0-auc:0.47962\n",
      "[73]\tvalidation_0-auc:0.47377\n",
      "[74]\tvalidation_0-auc:0.47175\n",
      "[75]\tvalidation_0-auc:0.47135\n",
      "[76]\tvalidation_0-auc:0.46287\n",
      "[77]\tvalidation_0-auc:0.46247\n",
      "[78]\tvalidation_0-auc:0.46045\n",
      "[79]\tvalidation_0-auc:0.45803\n",
      "[80]\tvalidation_0-auc:0.46328\n",
      "[81]\tvalidation_0-auc:0.45964\n",
      "[82]\tvalidation_0-auc:0.45924\n",
      "[83]\tvalidation_0-auc:0.45884\n",
      "[84]\tvalidation_0-auc:0.45884\n",
      "[85]\tvalidation_0-auc:0.45985\n",
      "[86]\tvalidation_0-auc:0.46287\n",
      "[87]\tvalidation_0-auc:0.46045\n",
      "[88]\tvalidation_0-auc:0.46328\n",
      "[89]\tvalidation_0-auc:0.46207\n",
      "[90]\tvalidation_0-auc:0.46328\n",
      "[91]\tvalidation_0-auc:0.46207\n",
      "[92]\tvalidation_0-auc:0.46126\n",
      "[93]\tvalidation_0-auc:0.46045\n",
      "[94]\tvalidation_0-auc:0.46106\n",
      "[95]\tvalidation_0-auc:0.46005\n",
      "[96]\tvalidation_0-auc:0.46651\n",
      "[97]\tvalidation_0-auc:0.46812\n",
      "[98]\tvalidation_0-auc:0.46570\n",
      "[99]\tvalidation_0-auc:0.46651\n",
      "Test Accuracy for Layer 3: 0.5000\n",
      "Test AUC for Layer 3: 0.5641\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4516\n",
      "Average Test AUC across all layers: 0.5132\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Title + S_label\n",
      "Average Accuracy: 0.5026\n",
      "Average AUC: 0.4752\n",
      "  Layer 1 - Accuracy: 0.4060, AUC: 0.4180\n",
      "  Layer 2 - Accuracy: 0.5446, AUC: 0.4850\n",
      "  Layer 3 - Accuracy: 0.5571, AUC: 0.5227\n",
      "\n",
      "Combination: MLP with Title + L_label\n",
      "Average Accuracy: 0.4072\n",
      "Average AUC: 0.5117\n",
      "  Layer 1 - Accuracy: 0.2707, AUC: 0.5845\n",
      "  Layer 2 - Accuracy: 0.4653, AUC: 0.5807\n",
      "  Layer 3 - Accuracy: 0.4857, AUC: 0.3698\n",
      "\n",
      "Combination: MLP with Full text + S_label\n",
      "Average Accuracy: 0.5019\n",
      "Average AUC: 0.4756\n",
      "  Layer 1 - Accuracy: 0.4436, AUC: 0.4647\n",
      "  Layer 2 - Accuracy: 0.5050, AUC: 0.3822\n",
      "  Layer 3 - Accuracy: 0.5571, AUC: 0.5798\n",
      "\n",
      "Combination: MLP with Full text + L_label\n",
      "Average Accuracy: 0.5183\n",
      "Average AUC: 0.4381\n",
      "  Layer 1 - Accuracy: 0.6391, AUC: 0.3774\n",
      "  Layer 2 - Accuracy: 0.4158, AUC: 0.4552\n",
      "  Layer 3 - Accuracy: 0.5000, AUC: 0.4816\n",
      "\n",
      "Combination: XGBoost with Title + S_label\n",
      "Average Accuracy: 0.4770\n",
      "Average AUC: 0.5156\n",
      "  Layer 1 - Accuracy: 0.4436, AUC: 0.4718\n",
      "  Layer 2 - Accuracy: 0.5446, AUC: 0.5613\n",
      "  Layer 3 - Accuracy: 0.4429, AUC: 0.5136\n",
      "\n",
      "Combination: XGBoost with Title + L_label\n",
      "Average Accuracy: 0.4516\n",
      "Average AUC: 0.4561\n",
      "  Layer 1 - Accuracy: 0.2707, AUC: 0.5263\n",
      "  Layer 2 - Accuracy: 0.5842, AUC: 0.3705\n",
      "  Layer 3 - Accuracy: 0.5000, AUC: 0.4714\n",
      "\n",
      "Combination: XGBoost with Full text + S_label\n",
      "Average Accuracy: 0.4770\n",
      "Average AUC: 0.4798\n",
      "  Layer 1 - Accuracy: 0.4436, AUC: 0.4828\n",
      "  Layer 2 - Accuracy: 0.5446, AUC: 0.3759\n",
      "  Layer 3 - Accuracy: 0.4429, AUC: 0.5806\n",
      "\n",
      "Combination: XGBoost with Full text + L_label\n",
      "Average Accuracy: 0.4516\n",
      "Average AUC: 0.5132\n",
      "  Layer 1 - Accuracy: 0.2707, AUC: 0.4966\n",
      "  Layer 2 - Accuracy: 0.5842, AUC: 0.4790\n",
      "  Layer 3 - Accuracy: 0.5000, AUC: 0.5641\n",
      "MLP summary comparison visualization saved as: OpenAI_MLP_XGB/visualizations_summary/XOM\\mlp_performance_comparison.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost summary comparison visualization saved as: OpenAI_MLP_XGB/visualizations_summary/XOM\\xgb_performance_comparison.png\n",
      "MLP layer performance visualization saved as: OpenAI_MLP_XGB/visualizations_summary/XOM\\mlp_layer_performance.png\n",
      "XGBoost layer performance visualization saved as: OpenAI_MLP_XGB/visualizations_summary/XOM\\xgboost_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'OpenAI_MLP_XGB' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "from xgboost import callback\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, model_type, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class ClimateNewsOpenAIPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'OpenAI_MLP_XGB/visualizations_mlp/XOM'\n",
    "        self.xgb_viz_dir = 'OpenAI_MLP_XGB/visualizations_xgb/XOM'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs(self.xgb_viz_dir, exist_ok=True)\n",
    "        os.makedirs('OpenAI_MLP_XGB/visualizations_summary/XOM', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert embeddings to numpy arrays\n",
    "        self.data['Title_embedding'] = self.data['Title_embedding_vector'].apply(parse_embedding)\n",
    "        self.data['Fulltext_embedding'] = self.data['Full_text_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_title_embedding = self.data['Title_embedding'].iloc[0]\n",
    "        sample_fulltext_embedding = self.data['Fulltext_embedding'].iloc[0]\n",
    "        \n",
    "        print(f\"Sample Title embedding shape: {sample_title_embedding.shape}\")\n",
    "        print(f\"Sample Fulltext embedding shape: {sample_fulltext_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2014-2020), validation (2020-2021), test (2021-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2020-10-31'),\n",
    "            'val_start': pd.Timestamp('2020-11-01'),\n",
    "            'val_end': pd.Timestamp('2021-10-31'),\n",
    "            'test_start': pd.Timestamp('2021-11-01'),\n",
    "            'test_end': pd.Timestamp('2022-10-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2014-2021), validation (2021-2022), test (2022-2023)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2021-10-31'),\n",
    "            'val_start': pd.Timestamp('2021-11-01'),\n",
    "            'val_end': pd.Timestamp('2022-10-31'),\n",
    "            'test_start': pd.Timestamp('2022-11-01'),\n",
    "            'test_end': pd.Timestamp('2023-10-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2014-2022), validation (2022-2023), test (2023-2024)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2022-10-31'),\n",
    "            'val_start': pd.Timestamp('2022-11-01'),\n",
    "            'val_end': pd.Timestamp('2023-10-31'),\n",
    "            'test_start': pd.Timestamp('2023-11-01'),\n",
    "            'test_end': pd.Timestamp('2024-11-01')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, text_col, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            text_col: The column containing the embeddings ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data[text_col].values)\n",
    "        X_val = np.stack(val_data[text_col].values)\n",
    "        X_test = np.stack(test_data[text_col].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def get_xgb_parameters(self):\n",
    "        \"\"\"\n",
    "        Get XGBoost parameters optimized for high-dimensional embeddings.\n",
    "        Uses a single parameter set for both Title and Full text embeddings since they have the same dimension (1536).\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of base parameters\n",
    "        \"\"\"\n",
    "        # Setup XGBoost base parameters optimized for high-dimensional embeddings\n",
    "        base_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 3,\n",
    "            'learning_rate': 0.00005,\n",
    "            'subsample': 0.7,          # Row subsampling to prevent overfitting\n",
    "            'colsample_bytree': 0.5,   # Column subsampling to handle high dimensionality\n",
    "            'min_child_weight': 3,     # Prevents overfitting on high-dimensional embeddings\n",
    "            'reg_alpha': 1.0,          # L1 regularization\n",
    "            'reg_lambda': 2.0,         # L2 regularization\n",
    "            'random_state': 42,\n",
    "            'use_label_encoder': False # Avoid deprecation warning\n",
    "        }\n",
    "        \n",
    "        return base_params\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, model_type, text_col, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            model_type: Model type (MLP)\n",
    "            text_col: Text column used\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_{model_type.lower()}_{display_text.replace(' ', '_')}_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, text_col, label_col, model_type):\n",
    "        \"\"\"\n",
    "        Train and evaluate a model for a specific text column and label column.\n",
    "        \n",
    "        Args:\n",
    "            text_col: The embedding column to use ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "            model_type: The model type to use ('MLP' or 'XGBoost')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        combination_key = f\"{model_type}|{display_text}|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training {model_type} model for {display_text} and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        visualization_dir = self.mlp_viz_dir if model_type == 'MLP' else self.xgb_viz_dir\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, text_col, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            if model_type == 'MLP':\n",
    "                # MLP model training\n",
    "                model = self.create_mlp_model((1536,))\n",
    "                print(f\"Created MLP model for {display_text} ({label_col})\")\n",
    "                model.summary()\n",
    "                \n",
    "                # Setup callbacks\n",
    "                plot_callback = PlotLearningCallback(model_type, display_text, label_col, i+1, visualization_dir)\n",
    "                early_stopping = EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=15,\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                model_checkpoint = ModelCheckpoint(\n",
    "                    filepath=f\"{visualization_dir}/best_{model_type.lower()}_{display_text.lower().replace(' ', '_')}_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                    monitor='val_loss',\n",
    "                    save_weights_only=True,\n",
    "                    save_best_only=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Train model\n",
    "                print(f\"Training MLP model...\")\n",
    "                batch_size = 32  # Larger batch size for embeddings\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                y_pred_proba = model.predict(X_test)\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                # Create final learning curve visualization\n",
    "                self.plot_final_learning_curves(history, model_type, text_col, label_col, i+1, visualization_dir)\n",
    "                \n",
    "                # Store training history\n",
    "                training_history = history.history\n",
    "                \n",
    "            else:  # XGBoost - simplified approach to fix API issues\n",
    "                # Get XGBoost parameters\n",
    "                base_params = self.get_xgb_parameters()\n",
    "                \n",
    "                # Create and train XGBoost model\n",
    "                print(f\"Creating and training XGBoost model...\")\n",
    "                model = xgb.XGBClassifier(**base_params)\n",
    "                \n",
    "                # Only use validation set for evaluation (not training set)\n",
    "                eval_set = [(X_val, y_val)]\n",
    "                \n",
    "                # Train the model\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    verbose=True\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                # Create a simple summary for XGBoost (no detailed learning curves available)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.text(0.5, 0.5, f'XGBoost Model Trained Successfully\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\\nAUC: {roc_auc_score(y_test, y_pred_proba):.4f}',\n",
    "                         ha='center', va='center', size=14, fontweight='bold')\n",
    "                plt.title(f'XGBoost Results ({display_text}, {label_col}, Layer {i+1})')\n",
    "                plt.axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{visualization_dir}/xgb_{display_text.replace(' ', '_')}_{label_col}_layer_{i+1}.png\")\n",
    "                plt.close()\n",
    "                \n",
    "                # Use None for training history since detailed learning curves aren't available\n",
    "                training_history = None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'model_type': model_type,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': training_history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for all combinations of text inputs, label columns, and model types.\"\"\"\n",
    "        # Define all combinations\n",
    "        embedding_cols = ['Title_embedding', 'Fulltext_embedding']\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        model_types = ['MLP', 'XGBoost']\n",
    "        \n",
    "        # Run analysis for each combination\n",
    "        for model_type in model_types:\n",
    "            for embedding_col in embedding_cols:\n",
    "                for label_col in label_cols:\n",
    "                    self.train_and_evaluate_model(embedding_col, label_col, model_type)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Model': model_type,\n",
    "                'Text': text_col,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing all model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Split by model type\n",
    "        mlp_data = df[df['Model'] == 'MLP']\n",
    "        xgb_data = df[df['Model'] == 'XGBoost']\n",
    "        \n",
    "        # 1. Performance comparison for MLP\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot accuracy and AUC bars for MLP\n",
    "        x = np.arange(len(mlp_data))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, mlp_data['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, mlp_data['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('MLP Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with OpenAI Embeddings')\n",
    "        labels = [f\"{row['Text']} + {row['Label']}\" for _, row in mlp_data.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(mlp_data['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(mlp_data['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(mlp_data['Avg Accuracy'].max(), mlp_data['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/XOM', \"mlp_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Performance comparison for XGBoost\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot accuracy and AUC bars for XGBoost\n",
    "        x = np.arange(len(xgb_data))\n",
    "        \n",
    "        plt.bar(x - width/2, xgb_data['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, xgb_data['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('XGBoost Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of XGBoost Models with OpenAI Embeddings')\n",
    "        labels = [f\"{row['Text']} + {row['Label']}\" for _, row in xgb_data.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(xgb_data['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(xgb_data['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(xgb_data['Avg Accuracy'].max(), xgb_data['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/XOM', \"xgb_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"XGBoost summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP and XGBoost models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        mlp_layer_data = []\n",
    "        xgb_layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Model': model_type,\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                if model_type == 'MLP':\n",
    "                    mlp_layer_data.append(layer_info)\n",
    "                else:  # XGBoost\n",
    "                    xgb_layer_data.append(layer_info)\n",
    "        \n",
    "        # Create visualizations for each model type\n",
    "        self._create_model_layer_visualization(mlp_layer_data, 'MLP')\n",
    "        self._create_model_layer_visualization(xgb_layer_data, 'XGBoost')\n",
    "    \n",
    "    def _create_model_layer_visualization(self, layer_data, model_type):\n",
    "        \"\"\"Create layer-specific visualizations for a given model type.\"\"\"\n",
    "        if not layer_data:\n",
    "            print(f\"No layer data available for {model_type}\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} Accuracy by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} AUC by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP_XGB/visualizations_summary/XOM', f\"{model_type.lower()}_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"{model_type} layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['OpenAI_MLP_XGB/visualizations_mlp/XOM', 'OpenAI_MLP_XGB/visualizations_xgb/XOM', \n",
    "                      'OpenAI_MLP_XGB/visualizations_summary/XOM']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/wall_street_news_semantics_SP500_database/wall_street_news_semantics_XOM_completed_openai.csv'\n",
    "    \n",
    "    # Initialize the predictor with OpenAI embeddings\n",
    "    predictor = ClimateNewsOpenAIPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'OpenAI_MLP_XGB' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing merged OpenAI embedding vectors from string format...\n",
      "Sample Merged embedding shape: (1536,)\n",
      "Loaded 838 climate change news articles spanning from 07/11/2014 to 24/09/2024\n",
      "Class distribution for short-term prediction: {0: 431, 1: 407}\n",
      "Class distribution for long-term prediction: {1: 440, 0: 398}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 512)               2048      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8153 - accuracy: 0.5411\n",
      "Epoch 1: val_loss improved from inf to 0.69248, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 34ms/step - loss: 0.8153 - accuracy: 0.5411 - val_loss: 0.6925 - val_accuracy: 0.5191\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7515 - accuracy: 0.5795\n",
      "Epoch 2: val_loss improved from 0.69248 to 0.69139, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.7486 - accuracy: 0.5835 - val_loss: 0.6914 - val_accuracy: 0.5496\n",
      "Epoch 3/100\n",
      " 7/13 [===============>..............] - ETA: 0s - loss: 0.6643 - accuracy: 0.6384\n",
      "Epoch 3: val_loss improved from 0.69139 to 0.69033, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6788 - accuracy: 0.6284 - val_loss: 0.6903 - val_accuracy: 0.5420\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6841 - accuracy: 0.6160\n",
      "Epoch 4: val_loss improved from 0.69033 to 0.68975, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6841 - accuracy: 0.6160 - val_loss: 0.6898 - val_accuracy: 0.5420\n",
      "Epoch 5/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.6728 - accuracy: 0.6198\n",
      "Epoch 5: val_loss improved from 0.68975 to 0.68911, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.6751 - accuracy: 0.6209 - val_loss: 0.6891 - val_accuracy: 0.5420\n",
      "Epoch 6/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.6061 - accuracy: 0.6979\n",
      "Epoch 6: val_loss improved from 0.68911 to 0.68884, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6065 - accuracy: 0.6983 - val_loss: 0.6888 - val_accuracy: 0.5420\n",
      "Epoch 7/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5639 - accuracy: 0.7292\n",
      "Epoch 7: val_loss improved from 0.68884 to 0.68880, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.5590 - accuracy: 0.7332 - val_loss: 0.6888 - val_accuracy: 0.5420\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5097 - accuracy: 0.7273\n",
      "Epoch 8: val_loss did not improve from 0.68880\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5164 - accuracy: 0.7257 - val_loss: 0.6889 - val_accuracy: 0.5420\n",
      "Epoch 9/100\n",
      " 7/13 [===============>..............] - ETA: 0s - loss: 0.5106 - accuracy: 0.7366\n",
      "Epoch 9: val_loss did not improve from 0.68880\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5186 - accuracy: 0.7307 - val_loss: 0.6891 - val_accuracy: 0.5420\n",
      "Epoch 10/100\n",
      " 7/13 [===============>..............] - ETA: 0s - loss: 0.4695 - accuracy: 0.8036\n",
      "Epoch 10: val_loss did not improve from 0.68880\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5033 - accuracy: 0.7581 - val_loss: 0.6895 - val_accuracy: 0.5420\n",
      "Epoch 11/100\n",
      " 7/13 [===============>..............] - ETA: 0s - loss: 0.5039 - accuracy: 0.7366\n",
      "Epoch 11: val_loss did not improve from 0.68880\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.4890 - accuracy: 0.7431 - val_loss: 0.6902 - val_accuracy: 0.5420\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4613 - accuracy: 0.7805\n",
      "Epoch 12: val_loss did not improve from 0.68880\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4613 - accuracy: 0.7805 - val_loss: 0.6913 - val_accuracy: 0.5420\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4504 - accuracy: 0.8030\n",
      "Epoch 13: val_loss did not improve from 0.68880\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4504 - accuracy: 0.8030 - val_loss: 0.6924 - val_accuracy: 0.5420\n",
      "Epoch 14/100\n",
      " 7/13 [===============>..............] - ETA: 0s - loss: 0.3840 - accuracy: 0.8080\n",
      "Epoch 14: val_loss did not improve from 0.68880\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.3791 - accuracy: 0.8279 - val_loss: 0.6939 - val_accuracy: 0.5420\n",
      "Epoch 15/100\n",
      " 7/13 [===============>..............] - ETA: 0s - loss: 0.4442 - accuracy: 0.7723\n",
      "Epoch 15: val_loss did not improve from 0.68880\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.4091 - accuracy: 0.8204 - val_loss: 0.6948 - val_accuracy: 0.5420\n",
      "Epoch 16/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4023 - accuracy: 0.8177\n",
      "Epoch 16: val_loss did not improve from 0.68880\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4041 - accuracy: 0.8155 - val_loss: 0.6962 - val_accuracy: 0.5420\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3655 - accuracy: 0.8653\n",
      "Epoch 17: val_loss did not improve from 0.68880\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3655 - accuracy: 0.8653 - val_loss: 0.6982 - val_accuracy: 0.5420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      " 7/13 [===============>..............] - ETA: 0s - loss: 0.3637 - accuracy: 0.8259\n",
      "Epoch 18: val_loss did not improve from 0.68880\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.3947 - accuracy: 0.8080 - val_loss: 0.7010 - val_accuracy: 0.5420\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3872 - accuracy: 0.8204\n",
      "Epoch 19: val_loss did not improve from 0.68880\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3872 - accuracy: 0.8204 - val_loss: 0.7027 - val_accuracy: 0.5420\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3320 - accuracy: 0.8703\n",
      "Epoch 20: val_loss did not improve from 0.68880\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3320 - accuracy: 0.8703 - val_loss: 0.7045 - val_accuracy: 0.5420\n",
      "Epoch 21/100\n",
      " 7/13 [===============>..............] - ETA: 0s - loss: 0.3610 - accuracy: 0.8527\n",
      "Epoch 21: val_loss did not improve from 0.68880\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.3257 - accuracy: 0.8678 - val_loss: 0.7057 - val_accuracy: 0.5420\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3130 - accuracy: 0.8753Restoring model weights from the end of the best epoch: 7.\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.68880\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.3130 - accuracy: 0.8753 - val_loss: 0.7077 - val_accuracy: 0.5420\n",
      "Epoch 22: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.5338\n",
      "Test AUC for Layer 1: 0.5395\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.9050 - accuracy: 0.4766\n",
      "Epoch 1: val_loss improved from inf to 0.69768, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8950 - accuracy: 0.5019 - val_loss: 0.6977 - val_accuracy: 0.4662\n",
      "Epoch 2/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.7755 - accuracy: 0.5481\n",
      "Epoch 2: val_loss did not improve from 0.69768\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7655 - accuracy: 0.5658 - val_loss: 0.7062 - val_accuracy: 0.4662\n",
      "Epoch 3/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6936 - accuracy: 0.6154\n",
      "Epoch 3: val_loss did not improve from 0.69768\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6795 - accuracy: 0.6222 - val_loss: 0.7183 - val_accuracy: 0.4662\n",
      "Epoch 4/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6741 - accuracy: 0.6322\n",
      "Epoch 4: val_loss did not improve from 0.69768\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6717 - accuracy: 0.6278 - val_loss: 0.7322 - val_accuracy: 0.4662\n",
      "Epoch 5/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6252 - accuracy: 0.6635\n",
      "Epoch 5: val_loss did not improve from 0.69768\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6582 - accuracy: 0.6485 - val_loss: 0.7486 - val_accuracy: 0.4662\n",
      "Epoch 6/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.6420 - accuracy: 0.6562\n",
      "Epoch 6: val_loss did not improve from 0.69768\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6470 - accuracy: 0.6504 - val_loss: 0.7615 - val_accuracy: 0.4662\n",
      "Epoch 7/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6250 - accuracy: 0.6587\n",
      "Epoch 7: val_loss did not improve from 0.69768\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6152 - accuracy: 0.6729 - val_loss: 0.7765 - val_accuracy: 0.4662\n",
      "Epoch 8/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5729 - accuracy: 0.7115\n",
      "Epoch 8: val_loss did not improve from 0.69768\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5703 - accuracy: 0.7162 - val_loss: 0.7915 - val_accuracy: 0.4662\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5382 - accuracy: 0.7332\n",
      "Epoch 9: val_loss did not improve from 0.69768\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5419 - accuracy: 0.7312 - val_loss: 0.8090 - val_accuracy: 0.4662\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5132 - accuracy: 0.7260\n",
      "Epoch 10: val_loss did not improve from 0.69768\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5102 - accuracy: 0.7368 - val_loss: 0.8292 - val_accuracy: 0.4662\n",
      "Epoch 11/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5028 - accuracy: 0.7428\n",
      "Epoch 11: val_loss did not improve from 0.69768\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4976 - accuracy: 0.7481 - val_loss: 0.8478 - val_accuracy: 0.4662\n",
      "Epoch 12/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4515 - accuracy: 0.7957\n",
      "Epoch 12: val_loss did not improve from 0.69768\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4544 - accuracy: 0.7895 - val_loss: 0.8650 - val_accuracy: 0.4662\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4499 - accuracy: 0.7837\n",
      "Epoch 13: val_loss did not improve from 0.69768\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4373 - accuracy: 0.8008 - val_loss: 0.8797 - val_accuracy: 0.4662\n",
      "Epoch 14/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4463 - accuracy: 0.8005\n",
      "Epoch 14: val_loss did not improve from 0.69768\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4327 - accuracy: 0.8045 - val_loss: 0.8960 - val_accuracy: 0.4662\n",
      "Epoch 15/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4310 - accuracy: 0.8029\n",
      "Epoch 15: val_loss did not improve from 0.69768\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.4307 - accuracy: 0.8026 - val_loss: 0.9108 - val_accuracy: 0.4662\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4152 - accuracy: 0.8221Restoring model weights from the end of the best epoch: 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: val_loss did not improve from 0.69768\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4239 - accuracy: 0.8214 - val_loss: 0.9272 - val_accuracy: 0.4662\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4455\n",
      "Test AUC for Layer 2: 0.5040\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.8809 - accuracy: 0.5104\n",
      "Epoch 1: val_loss improved from inf to 0.68834, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 22ms/step - loss: 0.8726 - accuracy: 0.5120 - val_loss: 0.6883 - val_accuracy: 0.5545\n",
      "Epoch 2/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.7782 - accuracy: 0.5724\n",
      "Epoch 2: val_loss improved from 0.68834 to 0.68787, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.7730 - accuracy: 0.5661 - val_loss: 0.6879 - val_accuracy: 0.5545\n",
      "Epoch 3/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.7179 - accuracy: 0.6053\n",
      "Epoch 3: val_loss did not improve from 0.68787\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.7326 - accuracy: 0.5961 - val_loss: 0.6901 - val_accuracy: 0.5545\n",
      "Epoch 4/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6987 - accuracy: 0.6109\n",
      "Epoch 4: val_loss did not improve from 0.68787\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.6971 - accuracy: 0.6111 - val_loss: 0.6941 - val_accuracy: 0.5545\n",
      "Epoch 5/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6723 - accuracy: 0.6328\n",
      "Epoch 5: val_loss did not improve from 0.68787\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.6820 - accuracy: 0.6261 - val_loss: 0.6990 - val_accuracy: 0.5545\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6384 - accuracy: 0.6622\n",
      "Epoch 6: val_loss did not improve from 0.68787\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.6384 - accuracy: 0.6622 - val_loss: 0.7059 - val_accuracy: 0.5545\n",
      "Epoch 7/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5940 - accuracy: 0.7016\n",
      "Epoch 7: val_loss did not improve from 0.68787\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5934 - accuracy: 0.7027 - val_loss: 0.7125 - val_accuracy: 0.5545\n",
      "Epoch 8/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5478 - accuracy: 0.7155\n",
      "Epoch 8: val_loss did not improve from 0.68787\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5547 - accuracy: 0.7117 - val_loss: 0.7176 - val_accuracy: 0.5545\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5256 - accuracy: 0.7462\n",
      "Epoch 9: val_loss did not improve from 0.68787\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5256 - accuracy: 0.7462 - val_loss: 0.7232 - val_accuracy: 0.5545\n",
      "Epoch 10/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5091 - accuracy: 0.7352\n",
      "Epoch 10: val_loss did not improve from 0.68787\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5091 - accuracy: 0.7342 - val_loss: 0.7277 - val_accuracy: 0.5545\n",
      "Epoch 11/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5102 - accuracy: 0.7566\n",
      "Epoch 11: val_loss did not improve from 0.68787\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5088 - accuracy: 0.7583 - val_loss: 0.7321 - val_accuracy: 0.5545\n",
      "Epoch 12/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4963 - accuracy: 0.7552\n",
      "Epoch 12: val_loss did not improve from 0.68787\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4962 - accuracy: 0.7553 - val_loss: 0.7370 - val_accuracy: 0.5545\n",
      "Epoch 13/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4522 - accuracy: 0.8010\n",
      "Epoch 13: val_loss did not improve from 0.68787\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4595 - accuracy: 0.7913 - val_loss: 0.7409 - val_accuracy: 0.5545\n",
      "Epoch 14/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4339 - accuracy: 0.7928\n",
      "Epoch 14: val_loss did not improve from 0.68787\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4346 - accuracy: 0.7943 - val_loss: 0.7449 - val_accuracy: 0.5545\n",
      "Epoch 15/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4043 - accuracy: 0.8174\n",
      "Epoch 15: val_loss did not improve from 0.68787\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3986 - accuracy: 0.8243 - val_loss: 0.7481 - val_accuracy: 0.5545\n",
      "Epoch 16/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4352 - accuracy: 0.8092\n",
      "Epoch 16: val_loss did not improve from 0.68787\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4274 - accuracy: 0.8123 - val_loss: 0.7510 - val_accuracy: 0.5545\n",
      "Epoch 17/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3918 - accuracy: 0.8240Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.68787\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3949 - accuracy: 0.8243 - val_loss: 0.7553 - val_accuracy: 0.5545\n",
      "Epoch 17: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4000\n",
      "Test AUC for Layer 3: 0.6395\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4598\n",
      "Average Test AUC across all layers: 0.5610\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_3\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.9066 - accuracy: 0.4773\n",
      "Epoch 1: val_loss improved from inf to 0.68062, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 29ms/step - loss: 0.9103 - accuracy: 0.4738 - val_loss: 0.6806 - val_accuracy: 0.6718\n",
      "Epoch 2/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.8055 - accuracy: 0.5156\n",
      "Epoch 2: val_loss improved from 0.68062 to 0.66537, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.8039 - accuracy: 0.5187 - val_loss: 0.6654 - val_accuracy: 0.6718\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7513 - accuracy: 0.5860\n",
      "Epoch 3: val_loss improved from 0.66537 to 0.65474, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7513 - accuracy: 0.5860 - val_loss: 0.6547 - val_accuracy: 0.6718\n",
      "Epoch 4/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.7091 - accuracy: 0.5651\n",
      "Epoch 4: val_loss improved from 0.65474 to 0.64743, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.7044 - accuracy: 0.5711 - val_loss: 0.6474 - val_accuracy: 0.6718\n",
      "Epoch 5/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.7011 - accuracy: 0.6328\n",
      "Epoch 5: val_loss improved from 0.64743 to 0.64132, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.7011 - accuracy: 0.6284 - val_loss: 0.6413 - val_accuracy: 0.6718\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6534 - accuracy: 0.6477\n",
      "Epoch 6: val_loss improved from 0.64132 to 0.63779, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6503 - accuracy: 0.6484 - val_loss: 0.6378 - val_accuracy: 0.6718\n",
      "Epoch 7/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.6004 - accuracy: 0.6901\n",
      "Epoch 7: val_loss improved from 0.63779 to 0.63526, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6099 - accuracy: 0.6833 - val_loss: 0.6353 - val_accuracy: 0.6718\n",
      "Epoch 8/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5466 - accuracy: 0.7214\n",
      "Epoch 8: val_loss improved from 0.63526 to 0.63349, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.5473 - accuracy: 0.7157 - val_loss: 0.6335 - val_accuracy: 0.6718\n",
      "Epoch 9/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5140 - accuracy: 0.7422\n",
      "Epoch 9: val_loss improved from 0.63349 to 0.63256, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5219 - accuracy: 0.7406 - val_loss: 0.6326 - val_accuracy: 0.6718\n",
      "Epoch 10/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4966 - accuracy: 0.7604\n",
      "Epoch 10: val_loss improved from 0.63256 to 0.63216, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.5024 - accuracy: 0.7556 - val_loss: 0.6322 - val_accuracy: 0.6718\n",
      "Epoch 11/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4684 - accuracy: 0.7682\n",
      "Epoch 11: val_loss improved from 0.63216 to 0.63195, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.4657 - accuracy: 0.7731 - val_loss: 0.6319 - val_accuracy: 0.6718\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4387 - accuracy: 0.8125\n",
      "Epoch 12: val_loss improved from 0.63195 to 0.63176, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.4459 - accuracy: 0.8030 - val_loss: 0.6318 - val_accuracy: 0.6718\n",
      "Epoch 13/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4523 - accuracy: 0.8177\n",
      "Epoch 13: val_loss did not improve from 0.63176\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4506 - accuracy: 0.8180 - val_loss: 0.6319 - val_accuracy: 0.6718\n",
      "Epoch 14/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4342 - accuracy: 0.8047\n",
      "Epoch 14: val_loss did not improve from 0.63176\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4304 - accuracy: 0.8080 - val_loss: 0.6320 - val_accuracy: 0.6718\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4297 - accuracy: 0.8155\n",
      "Epoch 15: val_loss did not improve from 0.63176\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4297 - accuracy: 0.8155 - val_loss: 0.6328 - val_accuracy: 0.6718\n",
      "Epoch 16/100\n",
      " 7/13 [===============>..............] - ETA: 0s - loss: 0.3919 - accuracy: 0.8259\n",
      "Epoch 16: val_loss did not improve from 0.63176\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.4171 - accuracy: 0.8105 - val_loss: 0.6339 - val_accuracy: 0.6718\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3614 - accuracy: 0.8579\n",
      "Epoch 17: val_loss did not improve from 0.63176\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3614 - accuracy: 0.8579 - val_loss: 0.6344 - val_accuracy: 0.6718\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3568 - accuracy: 0.8628\n",
      "Epoch 18: val_loss did not improve from 0.63176\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3568 - accuracy: 0.8628 - val_loss: 0.6342 - val_accuracy: 0.6718\n",
      "Epoch 19/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3746 - accuracy: 0.8125\n",
      "Epoch 19: val_loss did not improve from 0.63176\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3733 - accuracy: 0.8130 - val_loss: 0.6352 - val_accuracy: 0.6718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3191 - accuracy: 0.8878\n",
      "Epoch 20: val_loss did not improve from 0.63176\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3191 - accuracy: 0.8878 - val_loss: 0.6368 - val_accuracy: 0.6718\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3304 - accuracy: 0.8653\n",
      "Epoch 21: val_loss did not improve from 0.63176\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3304 - accuracy: 0.8653 - val_loss: 0.6374 - val_accuracy: 0.6718\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3175 - accuracy: 0.8928\n",
      "Epoch 22: val_loss did not improve from 0.63176\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3175 - accuracy: 0.8928 - val_loss: 0.6386 - val_accuracy: 0.6718\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3332 - accuracy: 0.8628\n",
      "Epoch 23: val_loss did not improve from 0.63176\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3332 - accuracy: 0.8628 - val_loss: 0.6404 - val_accuracy: 0.6718\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2957 - accuracy: 0.9077\n",
      "Epoch 24: val_loss did not improve from 0.63176\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.2957 - accuracy: 0.9077 - val_loss: 0.6422 - val_accuracy: 0.6718\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2996 - accuracy: 0.8828\n",
      "Epoch 25: val_loss did not improve from 0.63176\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.2996 - accuracy: 0.8828 - val_loss: 0.6421 - val_accuracy: 0.6718\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2982 - accuracy: 0.8903\n",
      "Epoch 26: val_loss did not improve from 0.63176\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.2982 - accuracy: 0.8903 - val_loss: 0.6426 - val_accuracy: 0.6718\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.9377Restoring model weights from the end of the best epoch: 12.\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.63176\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.2497 - accuracy: 0.9377 - val_loss: 0.6435 - val_accuracy: 0.6718\n",
      "Epoch 27: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.7368\n",
      "Test AUC for Layer 1: 0.6222\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8794 - accuracy: 0.5019\n",
      "Epoch 1: val_loss improved from inf to 0.66479, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 24ms/step - loss: 0.8794 - accuracy: 0.5019 - val_loss: 0.6648 - val_accuracy: 0.7368\n",
      "Epoch 2/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.7886 - accuracy: 0.5697\n",
      "Epoch 2: val_loss improved from 0.66479 to 0.64019, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7782 - accuracy: 0.5752 - val_loss: 0.6402 - val_accuracy: 0.7368\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7454 - accuracy: 0.5564\n",
      "Epoch 3: val_loss improved from 0.64019 to 0.62185, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.7454 - accuracy: 0.5564 - val_loss: 0.6219 - val_accuracy: 0.7368\n",
      "Epoch 4/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6511 - accuracy: 0.6394\n",
      "Epoch 4: val_loss improved from 0.62185 to 0.61003, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6461 - accuracy: 0.6391 - val_loss: 0.6100 - val_accuracy: 0.7368\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6270 - accuracy: 0.6621\n",
      "Epoch 5: val_loss improved from 0.61003 to 0.60195, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.6326 - accuracy: 0.6598 - val_loss: 0.6019 - val_accuracy: 0.7368\n",
      "Epoch 6/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6618 - accuracy: 0.6458\n",
      "Epoch 6: val_loss improved from 0.60195 to 0.59704, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6522 - accuracy: 0.6579 - val_loss: 0.5970 - val_accuracy: 0.7368\n",
      "Epoch 7/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5715 - accuracy: 0.6771\n",
      "Epoch 7: val_loss improved from 0.59704 to 0.59330, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.5700 - accuracy: 0.6861 - val_loss: 0.5933 - val_accuracy: 0.7368\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5542 - accuracy: 0.7109\n",
      "Epoch 8: val_loss improved from 0.59330 to 0.58986, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5516 - accuracy: 0.7105 - val_loss: 0.5899 - val_accuracy: 0.7368\n",
      "Epoch 9/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5478 - accuracy: 0.7448\n",
      "Epoch 9: val_loss improved from 0.58986 to 0.58864, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5417 - accuracy: 0.7500 - val_loss: 0.5886 - val_accuracy: 0.7368\n",
      "Epoch 10/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5034 - accuracy: 0.7604\n",
      "Epoch 10: val_loss improved from 0.58864 to 0.58774, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5142 - accuracy: 0.7462 - val_loss: 0.5877 - val_accuracy: 0.7368\n",
      "Epoch 11/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4786 - accuracy: 0.7760\n",
      "Epoch 11: val_loss improved from 0.58774 to 0.58707, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4870 - accuracy: 0.7726 - val_loss: 0.5871 - val_accuracy: 0.7368\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4843 - accuracy: 0.7688\n",
      "Epoch 12: val_loss improved from 0.58707 to 0.58595, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.4843 - accuracy: 0.7688 - val_loss: 0.5859 - val_accuracy: 0.7368\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4517 - accuracy: 0.7788\n",
      "Epoch 13: val_loss improved from 0.58595 to 0.58586, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4485 - accuracy: 0.7820 - val_loss: 0.5859 - val_accuracy: 0.7368\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4477 - accuracy: 0.8120\n",
      "Epoch 14: val_loss improved from 0.58586 to 0.58537, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4477 - accuracy: 0.8120 - val_loss: 0.5854 - val_accuracy: 0.7368\n",
      "Epoch 15/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3845 - accuracy: 0.8594\n",
      "Epoch 15: val_loss improved from 0.58537 to 0.58425, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3897 - accuracy: 0.8440 - val_loss: 0.5843 - val_accuracy: 0.7368\n",
      "Epoch 16/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3976 - accuracy: 0.8151\n",
      "Epoch 16: val_loss did not improve from 0.58425\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3859 - accuracy: 0.8346 - val_loss: 0.5844 - val_accuracy: 0.7368\n",
      "Epoch 17/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3603 - accuracy: 0.8630\n",
      "Epoch 17: val_loss did not improve from 0.58425\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3649 - accuracy: 0.8534 - val_loss: 0.5851 - val_accuracy: 0.7368\n",
      "Epoch 18/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3361 - accuracy: 0.8678\n",
      "Epoch 18: val_loss improved from 0.58425 to 0.58402, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3548 - accuracy: 0.8609 - val_loss: 0.5840 - val_accuracy: 0.7368\n",
      "Epoch 19/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3551 - accuracy: 0.8702\n",
      "Epoch 19: val_loss improved from 0.58402 to 0.58349, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3561 - accuracy: 0.8665 - val_loss: 0.5835 - val_accuracy: 0.7368\n",
      "Epoch 20/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3212 - accuracy: 0.8724\n",
      "Epoch 20: val_loss did not improve from 0.58349\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3259 - accuracy: 0.8665 - val_loss: 0.5853 - val_accuracy: 0.7368\n",
      "Epoch 21/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3150 - accuracy: 0.8918\n",
      "Epoch 21: val_loss did not improve from 0.58349\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3185 - accuracy: 0.8853 - val_loss: 0.5875 - val_accuracy: 0.7368\n",
      "Epoch 22/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3081 - accuracy: 0.8750\n",
      "Epoch 22: val_loss did not improve from 0.58349\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3113 - accuracy: 0.8778 - val_loss: 0.5881 - val_accuracy: 0.7368\n",
      "Epoch 23/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2631 - accuracy: 0.9193\n",
      "Epoch 23: val_loss did not improve from 0.58349\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2627 - accuracy: 0.9192 - val_loss: 0.5897 - val_accuracy: 0.7368\n",
      "Epoch 24/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2891 - accuracy: 0.9014\n",
      "Epoch 24: val_loss did not improve from 0.58349\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2843 - accuracy: 0.8966 - val_loss: 0.5902 - val_accuracy: 0.7368\n",
      "Epoch 25/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2789 - accuracy: 0.8942\n",
      "Epoch 25: val_loss did not improve from 0.58349\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2797 - accuracy: 0.8966 - val_loss: 0.5931 - val_accuracy: 0.7368\n",
      "Epoch 26/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2668 - accuracy: 0.9014\n",
      "Epoch 26: val_loss did not improve from 0.58349\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2746 - accuracy: 0.8966 - val_loss: 0.5966 - val_accuracy: 0.7519\n",
      "Epoch 27/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2450 - accuracy: 0.9279\n",
      "Epoch 27: val_loss did not improve from 0.58349\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2457 - accuracy: 0.9248 - val_loss: 0.6005 - val_accuracy: 0.7444\n",
      "Epoch 28/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2505 - accuracy: 0.9231\n",
      "Epoch 28: val_loss did not improve from 0.58349\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2445 - accuracy: 0.9267 - val_loss: 0.6063 - val_accuracy: 0.7368\n",
      "Epoch 29/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2494 - accuracy: 0.9279\n",
      "Epoch 29: val_loss did not improve from 0.58349\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2635 - accuracy: 0.9173 - val_loss: 0.6119 - val_accuracy: 0.7293\n",
      "Epoch 30/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2085 - accuracy: 0.9453\n",
      "Epoch 30: val_loss did not improve from 0.58349\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2313 - accuracy: 0.9323 - val_loss: 0.6189 - val_accuracy: 0.7293\n",
      "Epoch 31/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2384 - accuracy: 0.9297\n",
      "Epoch 31: val_loss did not improve from 0.58349\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2384 - accuracy: 0.9286 - val_loss: 0.6298 - val_accuracy: 0.7143\n",
      "Epoch 32/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.2208 - accuracy: 0.9375\n",
      "Epoch 32: val_loss did not improve from 0.58349\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.2206 - accuracy: 0.9361 - val_loss: 0.6390 - val_accuracy: 0.7068\n",
      "Epoch 33/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.1868 - accuracy: 0.9531\n",
      "Epoch 33: val_loss did not improve from 0.58349\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1817 - accuracy: 0.9568 - val_loss: 0.6473 - val_accuracy: 0.6992\n",
      "Epoch 34/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2154 - accuracy: 0.9279Restoring model weights from the end of the best epoch: 19.\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.58349\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2067 - accuracy: 0.9380 - val_loss: 0.6567 - val_accuracy: 0.7068\n",
      "Epoch 34: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5050\n",
      "Test AUC for Layer 2: 0.4165\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_5\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.8787 - accuracy: 0.4983\n",
      "Epoch 1: val_loss improved from inf to 0.69375, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 20ms/step - loss: 0.8795 - accuracy: 0.4985 - val_loss: 0.6937 - val_accuracy: 0.5050\n",
      "Epoch 2/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.8084 - accuracy: 0.5451\n",
      "Epoch 2: val_loss did not improve from 0.69375\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.7954 - accuracy: 0.5465 - val_loss: 0.6958 - val_accuracy: 0.5050\n",
      "Epoch 3/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.7331 - accuracy: 0.5729\n",
      "Epoch 3: val_loss did not improve from 0.69375\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.7260 - accuracy: 0.5796 - val_loss: 0.6994 - val_accuracy: 0.5050\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6911 - accuracy: 0.5961\n",
      "Epoch 4: val_loss did not improve from 0.69375\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.6911 - accuracy: 0.5961 - val_loss: 0.7039 - val_accuracy: 0.5050\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6104 - accuracy: 0.6622\n",
      "Epoch 5: val_loss did not improve from 0.69375\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.6104 - accuracy: 0.6622 - val_loss: 0.7065 - val_accuracy: 0.5050\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6280 - accuracy: 0.6547\n",
      "Epoch 6: val_loss did not improve from 0.69375\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.6280 - accuracy: 0.6547 - val_loss: 0.7104 - val_accuracy: 0.5050\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5519 - accuracy: 0.7357\n",
      "Epoch 7: val_loss did not improve from 0.69375\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5519 - accuracy: 0.7357 - val_loss: 0.7141 - val_accuracy: 0.5050\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5436 - accuracy: 0.7417\n",
      "Epoch 8: val_loss did not improve from 0.69375\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5436 - accuracy: 0.7417 - val_loss: 0.7183 - val_accuracy: 0.5050\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5194 - accuracy: 0.7462\n",
      "Epoch 9: val_loss did not improve from 0.69375\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5194 - accuracy: 0.7462 - val_loss: 0.7219 - val_accuracy: 0.5050\n",
      "Epoch 10/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5092 - accuracy: 0.7714\n",
      "Epoch 10: val_loss did not improve from 0.69375\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5119 - accuracy: 0.7688 - val_loss: 0.7256 - val_accuracy: 0.5050\n",
      "Epoch 11/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4880 - accuracy: 0.7664\n",
      "Epoch 11: val_loss did not improve from 0.69375\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4803 - accuracy: 0.7718 - val_loss: 0.7274 - val_accuracy: 0.5050\n",
      "Epoch 12/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4615 - accuracy: 0.7961\n",
      "Epoch 12: val_loss did not improve from 0.69375\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4671 - accuracy: 0.7913 - val_loss: 0.7299 - val_accuracy: 0.5050\n",
      "Epoch 13/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4497 - accuracy: 0.8003\n",
      "Epoch 13: val_loss did not improve from 0.69375\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4489 - accuracy: 0.8033 - val_loss: 0.7341 - val_accuracy: 0.5050\n",
      "Epoch 14/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4359 - accuracy: 0.8062\n",
      "Epoch 14: val_loss did not improve from 0.69375\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4362 - accuracy: 0.8048 - val_loss: 0.7375 - val_accuracy: 0.5050\n",
      "Epoch 15/100\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 0.4057 - accuracy: 0.8188\n",
      "Epoch 15: val_loss did not improve from 0.69375\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4091 - accuracy: 0.8093 - val_loss: 0.7404 - val_accuracy: 0.5050\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4176 - accuracy: 0.8273Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69375\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4176 - accuracy: 0.8273 - val_loss: 0.7443 - val_accuracy: 0.5050\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4429\n",
      "Test AUC for Layer 3: 0.5261\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5615\n",
      "Average Test AUC across all layers: 0.5216\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS (COP)\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Merged + S_label\n",
      "Average Accuracy: 0.4598\n",
      "Average AUC: 0.5610\n",
      "  Layer 1 - Accuracy: 0.5338, AUC: 0.5395\n",
      "  Layer 2 - Accuracy: 0.4455, AUC: 0.5040\n",
      "  Layer 3 - Accuracy: 0.4000, AUC: 0.6395\n",
      "\n",
      "Combination: MLP with Merged + L_label\n",
      "Average Accuracy: 0.5615\n",
      "Average AUC: 0.5216\n",
      "  Layer 1 - Accuracy: 0.7368, AUC: 0.6222\n",
      "  Layer 2 - Accuracy: 0.5050, AUC: 0.4165\n",
      "  Layer 3 - Accuracy: 0.4429, AUC: 0.5261\n",
      "MLP summary comparison visualization saved as: Merged_OpenAI_MLP/visualizations_summary/COP\\mlp_merged_performance_comparison.png\n",
      "MLP layer performance visualization saved as: Merged_OpenAI_MLP/visualizations_summary/COP\\mlp_merged_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class MergedEmbeddingMLPPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using merged OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with merged OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'Merged_OpenAI_MLP/visualizations_mlp/COP'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs('Merged_OpenAI_MLP/visualizations_summary/COP', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with merged OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing merged OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert merged embeddings to numpy arrays\n",
    "        self.data['Merged_embedding'] = self.data['Merged_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_merged_embedding = self.data['Merged_embedding'].iloc[0]\n",
    "        print(f\"Sample Merged embedding shape: {sample_merged_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2014-2020), validation (2020-2021), test (2021-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2020-10-31'),\n",
    "            'val_start': pd.Timestamp('2020-11-01'),\n",
    "            'val_end': pd.Timestamp('2021-10-31'),\n",
    "            'test_start': pd.Timestamp('2021-11-01'),\n",
    "            'test_end': pd.Timestamp('2022-10-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2014-2021), validation (2021-2022), test (2022-2023)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2021-10-31'),\n",
    "            'val_start': pd.Timestamp('2021-11-01'),\n",
    "            'val_end': pd.Timestamp('2022-10-31'),\n",
    "            'test_start': pd.Timestamp('2022-11-01'),\n",
    "            'test_end': pd.Timestamp('2023-10-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2014-2022), validation (2022-2023), test (2023-2024)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2022-10-31'),\n",
    "            'val_start': pd.Timestamp('2022-11-01'),\n",
    "            'val_end': pd.Timestamp('2023-10-31'),\n",
    "            'test_start': pd.Timestamp('2023-11-01'),\n",
    "            'test_end': pd.Timestamp('2024-11-01')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data['Merged_embedding'].values)\n",
    "        X_val = np.stack(val_data['Merged_embedding'].values)\n",
    "        X_test = np.stack(test_data['Merged_embedding'].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_mlp_merged_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, label_col):\n",
    "        \"\"\"\n",
    "        Train and evaluate an MLP model for merged embeddings and a specific label column.\n",
    "        \n",
    "        Args:\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        combination_key = f\"MLP|Merged|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training MLP model for Merged Embeddings and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            model = self.create_mlp_model((1536,))\n",
    "            print(f\"Created MLP model for Merged Embeddings ({label_col})\")\n",
    "            model.summary()\n",
    "            \n",
    "            # Setup callbacks\n",
    "            plot_callback = PlotLearningCallback('Merged', label_col, i+1, self.mlp_viz_dir)\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=15,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            model_checkpoint = ModelCheckpoint(\n",
    "                filepath=f\"{self.mlp_viz_dir}/best_mlp_merged_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                monitor='val_loss',\n",
    "                save_weights_only=True,\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            print(f\"Training MLP model...\")\n",
    "            batch_size = 32  # Larger batch size for embeddings\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred_proba = model.predict(X_test)\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "            \n",
    "            # Create final learning curve visualization\n",
    "            self.plot_final_learning_curves(history, label_col, i+1, self.mlp_viz_dir)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': history.history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for merged embeddings with both short-term and long-term labels.\"\"\"\n",
    "        # Define label columns\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        \n",
    "        # Run analysis for each label column\n",
    "        for label_col in label_cols:\n",
    "            self.train_and_evaluate_model(label_col)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS (COP)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Performance comparison for MLP with merged embeddings\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Plot accuracy and AUC bars\n",
    "        x = np.arange(len(df))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, df['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, df['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with Merged OpenAI Embeddings')\n",
    "        labels = [f\"Merged + {row['Label']}\" for _, row in df.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(df['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(df['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(df['Avg Accuracy'].max(), df['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/COP', \"mlp_merged_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                layer_data.append(layer_info)\n",
    "        \n",
    "        if not layer_data:\n",
    "            print(\"No layer data available\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title('MLP Accuracy by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title('MLP AUC by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/COP', \"mlp_merged_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['Merged_OpenAI_MLP/visualizations_mlp/COP', 'Merged_OpenAI_MLP/visualizations_summary/COP']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/wall_street_news_title_full_text_SP500_database/semantic/wall_street_news_semantics_COP_completed_openai_Merged.csv'\n",
    "    \n",
    "    # Initialize the predictor with merged OpenAI embeddings\n",
    "    predictor = MergedEmbeddingMLPPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing merged OpenAI embedding vectors from string format...\n",
      "Sample Merged embedding shape: (1536,)\n",
      "Loaded 838 climate change news articles spanning from 07/11/2014 to 24/09/2024\n",
      "Class distribution for short-term prediction: {1: 452, 0: 386}\n",
      "Class distribution for long-term prediction: {1: 464, 0: 374}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_18 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_19 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_20 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.9074 - accuracy: 0.5199\n",
      "Epoch 1: val_loss improved from inf to 0.70245, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 29ms/step - loss: 0.9075 - accuracy: 0.5062 - val_loss: 0.7024 - val_accuracy: 0.4351\n",
      "Epoch 2/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.7367 - accuracy: 0.5755\n",
      "Epoch 2: val_loss did not improve from 0.70245\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.7367 - accuracy: 0.5736 - val_loss: 0.7163 - val_accuracy: 0.4351\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7682 - accuracy: 0.5786\n",
      "Epoch 3: val_loss did not improve from 0.70245\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.7682 - accuracy: 0.5786 - val_loss: 0.7334 - val_accuracy: 0.4351\n",
      "Epoch 4/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.6908 - accuracy: 0.6250\n",
      "Epoch 4: val_loss did not improve from 0.70245\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6821 - accuracy: 0.6309 - val_loss: 0.7522 - val_accuracy: 0.4351\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6580 - accuracy: 0.6658\n",
      "Epoch 5: val_loss did not improve from 0.70245\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6580 - accuracy: 0.6658 - val_loss: 0.7704 - val_accuracy: 0.4351\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5895 - accuracy: 0.6833\n",
      "Epoch 6: val_loss did not improve from 0.70245\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5895 - accuracy: 0.6833 - val_loss: 0.7907 - val_accuracy: 0.4351\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5589 - accuracy: 0.7232\n",
      "Epoch 7: val_loss did not improve from 0.70245\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5589 - accuracy: 0.7232 - val_loss: 0.8094 - val_accuracy: 0.4351\n",
      "Epoch 8/100\n",
      " 7/13 [===============>..............] - ETA: 0s - loss: 0.5672 - accuracy: 0.7277\n",
      "Epoch 8: val_loss did not improve from 0.70245\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5565 - accuracy: 0.7282 - val_loss: 0.8275 - val_accuracy: 0.4351\n",
      "Epoch 9/100\n",
      " 7/13 [===============>..............] - ETA: 0s - loss: 0.5190 - accuracy: 0.7455\n",
      "Epoch 9: val_loss did not improve from 0.70245\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5683 - accuracy: 0.7082 - val_loss: 0.8448 - val_accuracy: 0.4351\n",
      "Epoch 10/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4746 - accuracy: 0.7865\n",
      "Epoch 10: val_loss did not improve from 0.70245\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4801 - accuracy: 0.7880 - val_loss: 0.8629 - val_accuracy: 0.4351\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4827 - accuracy: 0.7756\n",
      "Epoch 11: val_loss did not improve from 0.70245\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.4827 - accuracy: 0.7756 - val_loss: 0.8820 - val_accuracy: 0.4351\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4906 - accuracy: 0.7531\n",
      "Epoch 12: val_loss did not improve from 0.70245\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4906 - accuracy: 0.7531 - val_loss: 0.8999 - val_accuracy: 0.4351\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4515 - accuracy: 0.7905\n",
      "Epoch 13: val_loss did not improve from 0.70245\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4515 - accuracy: 0.7905 - val_loss: 0.9162 - val_accuracy: 0.4351\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4455 - accuracy: 0.8040\n",
      "Epoch 14: val_loss did not improve from 0.70245\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4413 - accuracy: 0.8030 - val_loss: 0.9315 - val_accuracy: 0.4351\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4406 - accuracy: 0.8030\n",
      "Epoch 15: val_loss did not improve from 0.70245\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4406 - accuracy: 0.8030 - val_loss: 0.9487 - val_accuracy: 0.4351\n",
      "Epoch 16/100\n",
      " 7/13 [===============>..............] - ETA: 0s - loss: 0.4204 - accuracy: 0.8036Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70245\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.4056 - accuracy: 0.8204 - val_loss: 0.9646 - val_accuracy: 0.4351\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.3910\n",
      "Test AUC for Layer 1: 0.4463\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_28 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_21 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_22 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_23 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8745 - accuracy: 0.5357\n",
      "Epoch 1: val_loss improved from inf to 0.70093, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 24ms/step - loss: 0.8745 - accuracy: 0.5357 - val_loss: 0.7009 - val_accuracy: 0.3910\n",
      "Epoch 2/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.8330 - accuracy: 0.5312\n",
      "Epoch 2: val_loss did not improve from 0.70093\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.8124 - accuracy: 0.5526 - val_loss: 0.7081 - val_accuracy: 0.3910\n",
      "Epoch 3/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.7836 - accuracy: 0.5649\n",
      "Epoch 3: val_loss did not improve from 0.70093\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.8045 - accuracy: 0.5489 - val_loss: 0.7137 - val_accuracy: 0.3910\n",
      "Epoch 4/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.7086 - accuracy: 0.5938\n",
      "Epoch 4: val_loss did not improve from 0.70093\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.7006 - accuracy: 0.6015 - val_loss: 0.7161 - val_accuracy: 0.3910\n",
      "Epoch 5/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5992 - accuracy: 0.6755\n",
      "Epoch 5: val_loss did not improve from 0.70093\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6132 - accuracy: 0.6711 - val_loss: 0.7171 - val_accuracy: 0.3910\n",
      "Epoch 6/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6174 - accuracy: 0.6693\n",
      "Epoch 6: val_loss did not improve from 0.70093\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6062 - accuracy: 0.6917 - val_loss: 0.7180 - val_accuracy: 0.3910\n",
      "Epoch 7/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5844 - accuracy: 0.6899\n",
      "Epoch 7: val_loss did not improve from 0.70093\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6068 - accuracy: 0.6842 - val_loss: 0.7155 - val_accuracy: 0.3910\n",
      "Epoch 8/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5551 - accuracy: 0.6947\n",
      "Epoch 8: val_loss did not improve from 0.70093\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5541 - accuracy: 0.6917 - val_loss: 0.7134 - val_accuracy: 0.3910\n",
      "Epoch 9/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.5375 - accuracy: 0.7478\n",
      "Epoch 9: val_loss did not improve from 0.70093\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.5460 - accuracy: 0.7406 - val_loss: 0.7092 - val_accuracy: 0.3910\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5533 - accuracy: 0.7139\n",
      "Epoch 10: val_loss did not improve from 0.70093\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5384 - accuracy: 0.7274 - val_loss: 0.7065 - val_accuracy: 0.4060\n",
      "Epoch 11/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.5151 - accuracy: 0.7656\n",
      "Epoch 11: val_loss did not improve from 0.70093\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.5147 - accuracy: 0.7613 - val_loss: 0.7032 - val_accuracy: 0.4511\n",
      "Epoch 12/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4797 - accuracy: 0.7861\n",
      "Epoch 12: val_loss improved from 0.70093 to 0.69965, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4694 - accuracy: 0.7989 - val_loss: 0.6997 - val_accuracy: 0.4887\n",
      "Epoch 13/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4666 - accuracy: 0.7734\n",
      "Epoch 13: val_loss improved from 0.69965 to 0.69613, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4789 - accuracy: 0.7763 - val_loss: 0.6961 - val_accuracy: 0.4662\n",
      "Epoch 14/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4460 - accuracy: 0.7933\n",
      "Epoch 14: val_loss improved from 0.69613 to 0.69281, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4384 - accuracy: 0.7951 - val_loss: 0.6928 - val_accuracy: 0.4511\n",
      "Epoch 15/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4530 - accuracy: 0.7656\n",
      "Epoch 15: val_loss improved from 0.69281 to 0.68805, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4562 - accuracy: 0.7632 - val_loss: 0.6880 - val_accuracy: 0.5038\n",
      "Epoch 16/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4235 - accuracy: 0.8047\n",
      "Epoch 16: val_loss improved from 0.68805 to 0.68462, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4123 - accuracy: 0.8139 - val_loss: 0.6846 - val_accuracy: 0.5639\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4102 - accuracy: 0.8252\n",
      "Epoch 17: val_loss improved from 0.68462 to 0.68013, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4102 - accuracy: 0.8252 - val_loss: 0.6801 - val_accuracy: 0.6090\n",
      "Epoch 18/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3818 - accuracy: 0.8281\n",
      "Epoch 18: val_loss improved from 0.68013 to 0.67953, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3861 - accuracy: 0.8177 - val_loss: 0.6795 - val_accuracy: 0.6316\n",
      "Epoch 19/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3788 - accuracy: 0.8516\n",
      "Epoch 19: val_loss improved from 0.67953 to 0.67859, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3674 - accuracy: 0.8590 - val_loss: 0.6786 - val_accuracy: 0.6090\n",
      "Epoch 20/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3511 - accuracy: 0.8846\n",
      "Epoch 20: val_loss improved from 0.67859 to 0.67794, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3478 - accuracy: 0.8872 - val_loss: 0.6779 - val_accuracy: 0.6090\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3271 - accuracy: 0.8759\n",
      "Epoch 21: val_loss did not improve from 0.67794\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3271 - accuracy: 0.8759 - val_loss: 0.6788 - val_accuracy: 0.6090\n",
      "Epoch 22/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.3334 - accuracy: 0.8795\n",
      "Epoch 22: val_loss improved from 0.67794 to 0.67771, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3433 - accuracy: 0.8759 - val_loss: 0.6777 - val_accuracy: 0.6090\n",
      "Epoch 23/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2919 - accuracy: 0.8958\n",
      "Epoch 23: val_loss did not improve from 0.67771\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3105 - accuracy: 0.8929 - val_loss: 0.6787 - val_accuracy: 0.5940\n",
      "Epoch 24/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3250 - accuracy: 0.8894\n",
      "Epoch 24: val_loss did not improve from 0.67771\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3303 - accuracy: 0.8722 - val_loss: 0.6830 - val_accuracy: 0.5865\n",
      "Epoch 25/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2921 - accuracy: 0.9087\n",
      "Epoch 25: val_loss did not improve from 0.67771\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2960 - accuracy: 0.9041 - val_loss: 0.6872 - val_accuracy: 0.5789\n",
      "Epoch 26/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3156 - accuracy: 0.8894\n",
      "Epoch 26: val_loss did not improve from 0.67771\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3099 - accuracy: 0.8910 - val_loss: 0.6928 - val_accuracy: 0.5639\n",
      "Epoch 27/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2813 - accuracy: 0.9036\n",
      "Epoch 27: val_loss did not improve from 0.67771\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2823 - accuracy: 0.9004 - val_loss: 0.6937 - val_accuracy: 0.5639\n",
      "Epoch 28/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2821 - accuracy: 0.9062\n",
      "Epoch 28: val_loss did not improve from 0.67771\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2815 - accuracy: 0.9023 - val_loss: 0.6935 - val_accuracy: 0.5639\n",
      "Epoch 29/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2474 - accuracy: 0.9255\n",
      "Epoch 29: val_loss did not improve from 0.67771\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2457 - accuracy: 0.9192 - val_loss: 0.6980 - val_accuracy: 0.5789\n",
      "Epoch 30/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2781 - accuracy: 0.8990\n",
      "Epoch 30: val_loss did not improve from 0.67771\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2689 - accuracy: 0.9004 - val_loss: 0.7077 - val_accuracy: 0.5639\n",
      "Epoch 31/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.2426 - accuracy: 0.9152\n",
      "Epoch 31: val_loss did not improve from 0.67771\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.2539 - accuracy: 0.9098 - val_loss: 0.7133 - val_accuracy: 0.5564\n",
      "Epoch 32/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2596 - accuracy: 0.9089\n",
      "Epoch 32: val_loss did not improve from 0.67771\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2485 - accuracy: 0.9079 - val_loss: 0.7246 - val_accuracy: 0.5414\n",
      "Epoch 33/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.2313 - accuracy: 0.9263\n",
      "Epoch 33: val_loss did not improve from 0.67771\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.2260 - accuracy: 0.9323 - val_loss: 0.7340 - val_accuracy: 0.5414\n",
      "Epoch 34/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2229 - accuracy: 0.9303\n",
      "Epoch 34: val_loss did not improve from 0.67771\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2208 - accuracy: 0.9305 - val_loss: 0.7393 - val_accuracy: 0.5564\n",
      "Epoch 35/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2181 - accuracy: 0.9327\n",
      "Epoch 35: val_loss did not improve from 0.67771\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2193 - accuracy: 0.9286 - val_loss: 0.7490 - val_accuracy: 0.5489\n",
      "Epoch 36/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.1961 - accuracy: 0.9351\n",
      "Epoch 36: val_loss did not improve from 0.67771\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.1960 - accuracy: 0.9398 - val_loss: 0.7644 - val_accuracy: 0.5489\n",
      "Epoch 37/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2101 - accuracy: 0.9401Restoring model weights from the end of the best epoch: 22.\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.67771\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2064 - accuracy: 0.9417 - val_loss: 0.7886 - val_accuracy: 0.5338\n",
      "Epoch 37: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4851\n",
      "Test AUC for Layer 2: 0.4779\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_32 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_24 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_25 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_26 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.8134 - accuracy: 0.5165\n",
      "Epoch 1: val_loss improved from inf to 0.69082, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 21ms/step - loss: 0.8129 - accuracy: 0.5165 - val_loss: 0.6908 - val_accuracy: 0.5347\n",
      "Epoch 2/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.7705 - accuracy: 0.5556\n",
      "Epoch 2: val_loss did not improve from 0.69082\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.7863 - accuracy: 0.5435 - val_loss: 0.6932 - val_accuracy: 0.5347\n",
      "Epoch 3/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.7211 - accuracy: 0.5691\n",
      "Epoch 3: val_loss did not improve from 0.69082\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.7175 - accuracy: 0.5736 - val_loss: 0.6980 - val_accuracy: 0.5347\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/21 [==========================>...] - ETA: 0s - loss: 0.6762 - accuracy: 0.6234\n",
      "Epoch 4: val_loss did not improve from 0.69082\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.6754 - accuracy: 0.6231 - val_loss: 0.7046 - val_accuracy: 0.5347\n",
      "Epoch 5/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.6712 - accuracy: 0.6250\n",
      "Epoch 5: val_loss did not improve from 0.69082\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6692 - accuracy: 0.6201 - val_loss: 0.7115 - val_accuracy: 0.5347\n",
      "Epoch 6/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5979 - accuracy: 0.6530\n",
      "Epoch 6: val_loss did not improve from 0.69082\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5979 - accuracy: 0.6562 - val_loss: 0.7168 - val_accuracy: 0.5347\n",
      "Epoch 7/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5922 - accuracy: 0.6924\n",
      "Epoch 7: val_loss did not improve from 0.69082\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5913 - accuracy: 0.6847 - val_loss: 0.7237 - val_accuracy: 0.5347\n",
      "Epoch 8/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.6101 - accuracy: 0.6711\n",
      "Epoch 8: val_loss did not improve from 0.69082\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.6065 - accuracy: 0.6742 - val_loss: 0.7298 - val_accuracy: 0.5347\n",
      "Epoch 9/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5671 - accuracy: 0.7072\n",
      "Epoch 9: val_loss did not improve from 0.69082\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5568 - accuracy: 0.7192 - val_loss: 0.7339 - val_accuracy: 0.5347\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5040 - accuracy: 0.7538\n",
      "Epoch 10: val_loss did not improve from 0.69082\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5040 - accuracy: 0.7538 - val_loss: 0.7375 - val_accuracy: 0.5347\n",
      "Epoch 11/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5070 - accuracy: 0.7697\n",
      "Epoch 11: val_loss did not improve from 0.69082\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5133 - accuracy: 0.7673 - val_loss: 0.7417 - val_accuracy: 0.5347\n",
      "Epoch 12/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4812 - accuracy: 0.7625\n",
      "Epoch 12: val_loss did not improve from 0.69082\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4888 - accuracy: 0.7598 - val_loss: 0.7458 - val_accuracy: 0.5347\n",
      "Epoch 13/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4795 - accuracy: 0.7875\n",
      "Epoch 13: val_loss did not improve from 0.69082\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4855 - accuracy: 0.7838 - val_loss: 0.7481 - val_accuracy: 0.5347\n",
      "Epoch 14/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4761 - accuracy: 0.7845\n",
      "Epoch 14: val_loss did not improve from 0.69082\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4788 - accuracy: 0.7793 - val_loss: 0.7488 - val_accuracy: 0.5347\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4356 - accuracy: 0.7913\n",
      "Epoch 15: val_loss did not improve from 0.69082\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4356 - accuracy: 0.7913 - val_loss: 0.7516 - val_accuracy: 0.5347\n",
      "Epoch 16/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4282 - accuracy: 0.8076Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69082\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4273 - accuracy: 0.8033 - val_loss: 0.7551 - val_accuracy: 0.5347\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5143\n",
      "Test AUC for Layer 3: 0.3725\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4635\n",
      "Average Test AUC across all layers: 0.4323\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_36 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_27 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_28 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_29 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8423 - accuracy: 0.5256\n",
      "Epoch 1: val_loss improved from inf to 0.68620, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 29ms/step - loss: 0.8295 - accuracy: 0.5411 - val_loss: 0.6862 - val_accuracy: 0.6947\n",
      "Epoch 2/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.7607 - accuracy: 0.5594\n",
      "Epoch 2: val_loss improved from 0.68620 to 0.67901, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.7754 - accuracy: 0.5561 - val_loss: 0.6790 - val_accuracy: 0.6794\n",
      "Epoch 3/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.7522 - accuracy: 0.5833\n",
      "Epoch 3: val_loss improved from 0.67901 to 0.67384, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.7485 - accuracy: 0.5860 - val_loss: 0.6738 - val_accuracy: 0.6794\n",
      "Epoch 4/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.7304 - accuracy: 0.5859\n",
      "Epoch 4: val_loss improved from 0.67384 to 0.66859, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.7258 - accuracy: 0.5910 - val_loss: 0.6686 - val_accuracy: 0.6794\n",
      "Epoch 5/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.6595 - accuracy: 0.6927\n",
      "Epoch 5: val_loss improved from 0.66859 to 0.66484, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6639 - accuracy: 0.6858 - val_loss: 0.6648 - val_accuracy: 0.6794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6426 - accuracy: 0.6648\n",
      "Epoch 6: val_loss improved from 0.66484 to 0.66120, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6344 - accuracy: 0.6633 - val_loss: 0.6612 - val_accuracy: 0.6794\n",
      "Epoch 7/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5981 - accuracy: 0.6693\n",
      "Epoch 7: val_loss improved from 0.66120 to 0.65778, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6031 - accuracy: 0.6658 - val_loss: 0.6578 - val_accuracy: 0.6794\n",
      "Epoch 8/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5902 - accuracy: 0.6979\n",
      "Epoch 8: val_loss improved from 0.65778 to 0.65470, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.5864 - accuracy: 0.7032 - val_loss: 0.6547 - val_accuracy: 0.6794\n",
      "Epoch 9/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5807 - accuracy: 0.6849\n",
      "Epoch 9: val_loss improved from 0.65470 to 0.65201, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.5850 - accuracy: 0.6808 - val_loss: 0.6520 - val_accuracy: 0.6794\n",
      "Epoch 10/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4820 - accuracy: 0.7500\n",
      "Epoch 10: val_loss improved from 0.65201 to 0.64994, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.4848 - accuracy: 0.7506 - val_loss: 0.6499 - val_accuracy: 0.6794\n",
      "Epoch 11/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5189 - accuracy: 0.7526\n",
      "Epoch 11: val_loss improved from 0.64994 to 0.64807, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.5176 - accuracy: 0.7531 - val_loss: 0.6481 - val_accuracy: 0.6794\n",
      "Epoch 12/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4370 - accuracy: 0.8047\n",
      "Epoch 12: val_loss improved from 0.64807 to 0.64642, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.4355 - accuracy: 0.8055 - val_loss: 0.6464 - val_accuracy: 0.6794\n",
      "Epoch 13/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4321 - accuracy: 0.7891\n",
      "Epoch 13: val_loss improved from 0.64642 to 0.64623, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.4286 - accuracy: 0.7905 - val_loss: 0.6462 - val_accuracy: 0.6794\n",
      "Epoch 14/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4596 - accuracy: 0.7917\n",
      "Epoch 14: val_loss improved from 0.64623 to 0.64528, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.4614 - accuracy: 0.7905 - val_loss: 0.6453 - val_accuracy: 0.6794\n",
      "Epoch 15/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4355 - accuracy: 0.7943\n",
      "Epoch 15: val_loss did not improve from 0.64528\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4384 - accuracy: 0.7930 - val_loss: 0.6456 - val_accuracy: 0.6794\n",
      "Epoch 16/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4113 - accuracy: 0.8177\n",
      "Epoch 16: val_loss did not improve from 0.64528\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4162 - accuracy: 0.8204 - val_loss: 0.6466 - val_accuracy: 0.6794\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3804 - accuracy: 0.8404\n",
      "Epoch 17: val_loss did not improve from 0.64528\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3804 - accuracy: 0.8404 - val_loss: 0.6476 - val_accuracy: 0.6794\n",
      "Epoch 18/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3672 - accuracy: 0.8464\n",
      "Epoch 18: val_loss did not improve from 0.64528\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3683 - accuracy: 0.8479 - val_loss: 0.6485 - val_accuracy: 0.6794\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3819 - accuracy: 0.8379\n",
      "Epoch 19: val_loss did not improve from 0.64528\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3819 - accuracy: 0.8379 - val_loss: 0.6493 - val_accuracy: 0.6794\n",
      "Epoch 20/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3219 - accuracy: 0.8854\n",
      "Epoch 20: val_loss did not improve from 0.64528\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3239 - accuracy: 0.8853 - val_loss: 0.6514 - val_accuracy: 0.6794\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3575 - accuracy: 0.8504\n",
      "Epoch 21: val_loss did not improve from 0.64528\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3575 - accuracy: 0.8504 - val_loss: 0.6508 - val_accuracy: 0.6794\n",
      "Epoch 22/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3127 - accuracy: 0.9036\n",
      "Epoch 22: val_loss did not improve from 0.64528\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3054 - accuracy: 0.9077 - val_loss: 0.6513 - val_accuracy: 0.6718\n",
      "Epoch 23/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3063 - accuracy: 0.8949\n",
      "Epoch 23: val_loss did not improve from 0.64528\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3116 - accuracy: 0.8903 - val_loss: 0.6532 - val_accuracy: 0.6641\n",
      "Epoch 24/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3191 - accuracy: 0.8776\n",
      "Epoch 24: val_loss did not improve from 0.64528\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3235 - accuracy: 0.8728 - val_loss: 0.6534 - val_accuracy: 0.6718\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.9202\n",
      "Epoch 25: val_loss did not improve from 0.64528\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.2691 - accuracy: 0.9202 - val_loss: 0.6527 - val_accuracy: 0.6794\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.9127\n",
      "Epoch 26: val_loss did not improve from 0.64528\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.2699 - accuracy: 0.9127 - val_loss: 0.6547 - val_accuracy: 0.6718\n",
      "Epoch 27/100\n",
      " 7/13 [===============>..............] - ETA: 0s - loss: 0.2427 - accuracy: 0.9152\n",
      "Epoch 27: val_loss did not improve from 0.64528\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.2518 - accuracy: 0.9127 - val_loss: 0.6588 - val_accuracy: 0.6565\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2375 - accuracy: 0.9277\n",
      "Epoch 28: val_loss did not improve from 0.64528\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.2375 - accuracy: 0.9277 - val_loss: 0.6645 - val_accuracy: 0.6489\n",
      "Epoch 29/100\n",
      " 7/13 [===============>..............] - ETA: 0s - loss: 0.2274 - accuracy: 0.9152Restoring model weights from the end of the best epoch: 14.\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.64528\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.2237 - accuracy: 0.9401 - val_loss: 0.6689 - val_accuracy: 0.6260\n",
      "Epoch 29: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.7368\n",
      "Test AUC for Layer 1: 0.3924\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_10\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_40 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_30 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_31 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_32 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.8594 - accuracy: 0.5000\n",
      "Epoch 1: val_loss improved from inf to 0.71423, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 23ms/step - loss: 0.8587 - accuracy: 0.5113 - val_loss: 0.7142 - val_accuracy: 0.2632\n",
      "Epoch 2/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.8144 - accuracy: 0.5234\n",
      "Epoch 2: val_loss did not improve from 0.71423\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7878 - accuracy: 0.5489 - val_loss: 0.7295 - val_accuracy: 0.2632\n",
      "Epoch 3/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.7348 - accuracy: 0.6058\n",
      "Epoch 3: val_loss did not improve from 0.71423\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.7300 - accuracy: 0.6015 - val_loss: 0.7468 - val_accuracy: 0.2632\n",
      "Epoch 4/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6890 - accuracy: 0.6094\n",
      "Epoch 4: val_loss did not improve from 0.71423\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6643 - accuracy: 0.6241 - val_loss: 0.7591 - val_accuracy: 0.2632\n",
      "Epoch 5/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6304 - accuracy: 0.6587\n",
      "Epoch 5: val_loss did not improve from 0.71423\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6297 - accuracy: 0.6617 - val_loss: 0.7691 - val_accuracy: 0.2632\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5963 - accuracy: 0.6755\n",
      "Epoch 6: val_loss did not improve from 0.71423\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5971 - accuracy: 0.6748 - val_loss: 0.7766 - val_accuracy: 0.2632\n",
      "Epoch 7/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5450 - accuracy: 0.7284\n",
      "Epoch 7: val_loss did not improve from 0.71423\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5612 - accuracy: 0.7105 - val_loss: 0.7851 - val_accuracy: 0.2632\n",
      "Epoch 8/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5469 - accuracy: 0.7332\n",
      "Epoch 8: val_loss did not improve from 0.71423\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5565 - accuracy: 0.7256 - val_loss: 0.7951 - val_accuracy: 0.2632\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5069 - accuracy: 0.7668\n",
      "Epoch 9: val_loss did not improve from 0.71423\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5090 - accuracy: 0.7632 - val_loss: 0.8030 - val_accuracy: 0.2632\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4843 - accuracy: 0.7548\n",
      "Epoch 10: val_loss did not improve from 0.71423\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5027 - accuracy: 0.7538 - val_loss: 0.8063 - val_accuracy: 0.2632\n",
      "Epoch 11/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4679 - accuracy: 0.7933\n",
      "Epoch 11: val_loss did not improve from 0.71423\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4703 - accuracy: 0.7932 - val_loss: 0.8126 - val_accuracy: 0.2632\n",
      "Epoch 12/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4515 - accuracy: 0.7668\n",
      "Epoch 12: val_loss did not improve from 0.71423\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4579 - accuracy: 0.7650 - val_loss: 0.8183 - val_accuracy: 0.2632\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4308 - accuracy: 0.8005\n",
      "Epoch 13: val_loss did not improve from 0.71423\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4182 - accuracy: 0.8102 - val_loss: 0.8234 - val_accuracy: 0.2632\n",
      "Epoch 14/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3851 - accuracy: 0.8630\n",
      "Epoch 14: val_loss did not improve from 0.71423\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3901 - accuracy: 0.8590 - val_loss: 0.8259 - val_accuracy: 0.2632\n",
      "Epoch 15/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4059 - accuracy: 0.7995\n",
      "Epoch 15: val_loss did not improve from 0.71423\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4188 - accuracy: 0.7970 - val_loss: 0.8238 - val_accuracy: 0.2632\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4002 - accuracy: 0.8221Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.71423\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3896 - accuracy: 0.8289 - val_loss: 0.8185 - val_accuracy: 0.2632\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5842\n",
      "Test AUC for Layer 2: 0.5230\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_44 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_33 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_34 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_35 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.9425 - accuracy: 0.5039\n",
      "Epoch 1: val_loss improved from inf to 0.70287, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 21ms/step - loss: 0.9135 - accuracy: 0.5090 - val_loss: 0.7029 - val_accuracy: 0.4158\n",
      "Epoch 2/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.8475 - accuracy: 0.5202\n",
      "Epoch 2: val_loss did not improve from 0.70287\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.8389 - accuracy: 0.5330 - val_loss: 0.7158 - val_accuracy: 0.4158\n",
      "Epoch 3/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.7799 - accuracy: 0.5833\n",
      "Epoch 3: val_loss did not improve from 0.70287\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.7809 - accuracy: 0.5811 - val_loss: 0.7310 - val_accuracy: 0.4158\n",
      "Epoch 4/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.6913 - accuracy: 0.6217\n",
      "Epoch 4: val_loss did not improve from 0.70287\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6912 - accuracy: 0.6291 - val_loss: 0.7444 - val_accuracy: 0.4158\n",
      "Epoch 5/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.6655 - accuracy: 0.6645\n",
      "Epoch 5: val_loss did not improve from 0.70287\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6696 - accuracy: 0.6562 - val_loss: 0.7597 - val_accuracy: 0.4158\n",
      "Epoch 6/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5949 - accuracy: 0.6974\n",
      "Epoch 6: val_loss did not improve from 0.70287\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6019 - accuracy: 0.6832 - val_loss: 0.7772 - val_accuracy: 0.4158\n",
      "Epoch 7/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.6080 - accuracy: 0.6661\n",
      "Epoch 7: val_loss did not improve from 0.70287\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6051 - accuracy: 0.6742 - val_loss: 0.7889 - val_accuracy: 0.4158\n",
      "Epoch 8/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5558 - accuracy: 0.7188\n",
      "Epoch 8: val_loss did not improve from 0.70287\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5496 - accuracy: 0.7237 - val_loss: 0.8036 - val_accuracy: 0.4158\n",
      "Epoch 9/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5561 - accuracy: 0.7125\n",
      "Epoch 9: val_loss did not improve from 0.70287\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5604 - accuracy: 0.7072 - val_loss: 0.8237 - val_accuracy: 0.4158\n",
      "Epoch 10/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4920 - accuracy: 0.7547\n",
      "Epoch 10: val_loss did not improve from 0.70287\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4964 - accuracy: 0.7523 - val_loss: 0.8347 - val_accuracy: 0.4158\n",
      "Epoch 11/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5095 - accuracy: 0.7500\n",
      "Epoch 11: val_loss did not improve from 0.70287\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5080 - accuracy: 0.7508 - val_loss: 0.8562 - val_accuracy: 0.4158\n",
      "Epoch 12/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4871 - accuracy: 0.7730\n",
      "Epoch 12: val_loss did not improve from 0.70287\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4878 - accuracy: 0.7748 - val_loss: 0.8725 - val_accuracy: 0.4158\n",
      "Epoch 13/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4477 - accuracy: 0.7697\n",
      "Epoch 13: val_loss did not improve from 0.70287\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4424 - accuracy: 0.7748 - val_loss: 0.8885 - val_accuracy: 0.4158\n",
      "Epoch 14/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4274 - accuracy: 0.8076\n",
      "Epoch 14: val_loss did not improve from 0.70287\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4247 - accuracy: 0.8093 - val_loss: 0.8999 - val_accuracy: 0.4158\n",
      "Epoch 15/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4168 - accuracy: 0.8156\n",
      "Epoch 15: val_loss did not improve from 0.70287\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4166 - accuracy: 0.8153 - val_loss: 0.9053 - val_accuracy: 0.4158\n",
      "Epoch 16/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4059 - accuracy: 0.8289Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70287\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4085 - accuracy: 0.8288 - val_loss: 0.9200 - val_accuracy: 0.4158\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5571\n",
      "Test AUC for Layer 3: 0.5261\n",
      "\n",
      "Average Test Accuracy across all layers: 0.6260\n",
      "Average Test AUC across all layers: 0.4805\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS (COP)\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Merged + S_label\n",
      "Average Accuracy: 0.4635\n",
      "Average AUC: 0.4323\n",
      "  Layer 1 - Accuracy: 0.3910, AUC: 0.4463\n",
      "  Layer 2 - Accuracy: 0.4851, AUC: 0.4779\n",
      "  Layer 3 - Accuracy: 0.5143, AUC: 0.3725\n",
      "\n",
      "Combination: MLP with Merged + L_label\n",
      "Average Accuracy: 0.6260\n",
      "Average AUC: 0.4805\n",
      "  Layer 1 - Accuracy: 0.7368, AUC: 0.3924\n",
      "  Layer 2 - Accuracy: 0.5842, AUC: 0.5230\n",
      "  Layer 3 - Accuracy: 0.5571, AUC: 0.5261\n",
      "MLP summary comparison visualization saved as: Merged_OpenAI_MLP/visualizations_summary/CVX\\mlp_merged_performance_comparison.png\n",
      "MLP layer performance visualization saved as: Merged_OpenAI_MLP/visualizations_summary/CVX\\mlp_merged_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class MergedEmbeddingMLPPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using merged OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with merged OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'Merged_OpenAI_MLP/visualizations_mlp/CVX'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs('Merged_OpenAI_MLP/visualizations_summary/CVX', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with merged OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing merged OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert merged embeddings to numpy arrays\n",
    "        self.data['Merged_embedding'] = self.data['Merged_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_merged_embedding = self.data['Merged_embedding'].iloc[0]\n",
    "        print(f\"Sample Merged embedding shape: {sample_merged_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2014-2020), validation (2020-2021), test (2021-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2020-10-31'),\n",
    "            'val_start': pd.Timestamp('2020-11-01'),\n",
    "            'val_end': pd.Timestamp('2021-10-31'),\n",
    "            'test_start': pd.Timestamp('2021-11-01'),\n",
    "            'test_end': pd.Timestamp('2022-10-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2014-2021), validation (2021-2022), test (2022-2023)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2021-10-31'),\n",
    "            'val_start': pd.Timestamp('2021-11-01'),\n",
    "            'val_end': pd.Timestamp('2022-10-31'),\n",
    "            'test_start': pd.Timestamp('2022-11-01'),\n",
    "            'test_end': pd.Timestamp('2023-10-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2014-2022), validation (2022-2023), test (2023-2024)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2022-10-31'),\n",
    "            'val_start': pd.Timestamp('2022-11-01'),\n",
    "            'val_end': pd.Timestamp('2023-10-31'),\n",
    "            'test_start': pd.Timestamp('2023-11-01'),\n",
    "            'test_end': pd.Timestamp('2024-11-01')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data['Merged_embedding'].values)\n",
    "        X_val = np.stack(val_data['Merged_embedding'].values)\n",
    "        X_test = np.stack(test_data['Merged_embedding'].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_mlp_merged_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, label_col):\n",
    "        \"\"\"\n",
    "        Train and evaluate an MLP model for merged embeddings and a specific label column.\n",
    "        \n",
    "        Args:\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        combination_key = f\"MLP|Merged|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training MLP model for Merged Embeddings and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            model = self.create_mlp_model((1536,))\n",
    "            print(f\"Created MLP model for Merged Embeddings ({label_col})\")\n",
    "            model.summary()\n",
    "            \n",
    "            # Setup callbacks\n",
    "            plot_callback = PlotLearningCallback('Merged', label_col, i+1, self.mlp_viz_dir)\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=15,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            model_checkpoint = ModelCheckpoint(\n",
    "                filepath=f\"{self.mlp_viz_dir}/best_mlp_merged_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                monitor='val_loss',\n",
    "                save_weights_only=True,\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            print(f\"Training MLP model...\")\n",
    "            batch_size = 32  # Larger batch size for embeddings\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred_proba = model.predict(X_test)\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "            \n",
    "            # Create final learning curve visualization\n",
    "            self.plot_final_learning_curves(history, label_col, i+1, self.mlp_viz_dir)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': history.history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for merged embeddings with both short-term and long-term labels.\"\"\"\n",
    "        # Define label columns\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        \n",
    "        # Run analysis for each label column\n",
    "        for label_col in label_cols:\n",
    "            self.train_and_evaluate_model(label_col)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS (CVX)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Performance comparison for MLP with merged embeddings\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Plot accuracy and AUC bars\n",
    "        x = np.arange(len(df))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, df['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, df['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with Merged OpenAI Embeddings')\n",
    "        labels = [f\"Merged + {row['Label']}\" for _, row in df.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(df['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(df['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(df['Avg Accuracy'].max(), df['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/CVX', \"mlp_merged_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                layer_data.append(layer_info)\n",
    "        \n",
    "        if not layer_data:\n",
    "            print(\"No layer data available\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title('MLP Accuracy by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title('MLP AUC by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/CVX', \"mlp_merged_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['Merged_OpenAI_MLP/visualizations_mlp/CVX', 'Merged_OpenAI_MLP/visualizations_summary/CVX']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/wall_street_news_title_full_text_SP500_database/semantic/wall_street_news_semantics_CVX_completed_openai_Merged.csv'\n",
    "    \n",
    "    # Initialize the predictor with merged OpenAI embeddings\n",
    "    predictor = MergedEmbeddingMLPPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing merged OpenAI embedding vectors from string format...\n",
      "Sample Merged embedding shape: (1536,)\n",
      "Loaded 838 climate change news articles spanning from 07/11/2014 to 24/09/2024\n",
      "Class distribution for short-term prediction: {1: 460, 0: 378}\n",
      "Class distribution for long-term prediction: {1: 503, 0: 335}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_48 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_36 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_37 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_38 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8691 - accuracy: 0.5256\n",
      "Epoch 1: val_loss improved from inf to 0.69306, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 28ms/step - loss: 0.8687 - accuracy: 0.5162 - val_loss: 0.6931 - val_accuracy: 0.5038\n",
      "Epoch 2/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.8268 - accuracy: 0.5260\n",
      "Epoch 2: val_loss improved from 0.69306 to 0.69214, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.8401 - accuracy: 0.5212 - val_loss: 0.6921 - val_accuracy: 0.5878\n",
      "Epoch 3/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.7470 - accuracy: 0.5573\n",
      "Epoch 3: val_loss improved from 0.69214 to 0.69111, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.7416 - accuracy: 0.5586 - val_loss: 0.6911 - val_accuracy: 0.5649\n",
      "Epoch 4/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6867 - accuracy: 0.6108\n",
      "Epoch 4: val_loss improved from 0.69111 to 0.69101, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6815 - accuracy: 0.6185 - val_loss: 0.6910 - val_accuracy: 0.5802\n",
      "Epoch 5/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.6316 - accuracy: 0.6591\n",
      "Epoch 5: val_loss improved from 0.69101 to 0.69053, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6241 - accuracy: 0.6584 - val_loss: 0.6905 - val_accuracy: 0.5725\n",
      "Epoch 6/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.6838 - accuracy: 0.6042\n",
      "Epoch 6: val_loss improved from 0.69053 to 0.68943, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6874 - accuracy: 0.6035 - val_loss: 0.6894 - val_accuracy: 0.5878\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5913 - accuracy: 0.6676\n",
      "Epoch 7: val_loss improved from 0.68943 to 0.68873, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.5927 - accuracy: 0.6584 - val_loss: 0.6887 - val_accuracy: 0.5954\n",
      "Epoch 8/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5673 - accuracy: 0.7135\n",
      "Epoch 8: val_loss improved from 0.68873 to 0.68775, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.5698 - accuracy: 0.7107 - val_loss: 0.6878 - val_accuracy: 0.6031\n",
      "Epoch 9/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5805 - accuracy: 0.6979\n",
      "Epoch 9: val_loss improved from 0.68775 to 0.68679, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.5804 - accuracy: 0.6983 - val_loss: 0.6868 - val_accuracy: 0.5954\n",
      "Epoch 10/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4999 - accuracy: 0.7474\n",
      "Epoch 10: val_loss improved from 0.68679 to 0.68620, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.4990 - accuracy: 0.7481 - val_loss: 0.6862 - val_accuracy: 0.6031\n",
      "Epoch 11/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5108 - accuracy: 0.7448\n",
      "Epoch 11: val_loss improved from 0.68620 to 0.68529, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.5091 - accuracy: 0.7431 - val_loss: 0.6853 - val_accuracy: 0.6031\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5068 - accuracy: 0.7557\n",
      "Epoch 12: val_loss improved from 0.68529 to 0.68412, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.4972 - accuracy: 0.7706 - val_loss: 0.6841 - val_accuracy: 0.6031\n",
      "Epoch 13/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4302 - accuracy: 0.8021\n",
      "Epoch 13: val_loss improved from 0.68412 to 0.68377, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.4396 - accuracy: 0.7930 - val_loss: 0.6838 - val_accuracy: 0.6031\n",
      "Epoch 14/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4719 - accuracy: 0.7670\n",
      "Epoch 14: val_loss improved from 0.68377 to 0.68341, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.4683 - accuracy: 0.7681 - val_loss: 0.6834 - val_accuracy: 0.6031\n",
      "Epoch 15/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4397 - accuracy: 0.7865\n",
      "Epoch 15: val_loss improved from 0.68341 to 0.68276, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.4393 - accuracy: 0.7905 - val_loss: 0.6828 - val_accuracy: 0.6031\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4596 - accuracy: 0.7855\n",
      "Epoch 16: val_loss improved from 0.68276 to 0.68183, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.4596 - accuracy: 0.7855 - val_loss: 0.6818 - val_accuracy: 0.6031\n",
      "Epoch 17/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3685 - accuracy: 0.8646\n",
      "Epoch 17: val_loss improved from 0.68183 to 0.68129, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.3705 - accuracy: 0.8653 - val_loss: 0.6813 - val_accuracy: 0.6031\n",
      "Epoch 18/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3618 - accuracy: 0.8568\n",
      "Epoch 18: val_loss did not improve from 0.68129\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3660 - accuracy: 0.8529 - val_loss: 0.6818 - val_accuracy: 0.6031\n",
      "Epoch 19/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3427 - accuracy: 0.8568\n",
      "Epoch 19: val_loss did not improve from 0.68129\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3495 - accuracy: 0.8479 - val_loss: 0.6820 - val_accuracy: 0.5802\n",
      "Epoch 20/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3785 - accuracy: 0.8385\n",
      "Epoch 20: val_loss did not improve from 0.68129\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3747 - accuracy: 0.8379 - val_loss: 0.6823 - val_accuracy: 0.5802\n",
      "Epoch 21/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3110 - accuracy: 0.8880\n",
      "Epoch 21: val_loss did not improve from 0.68129\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3059 - accuracy: 0.8928 - val_loss: 0.6835 - val_accuracy: 0.5802\n",
      "Epoch 22/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.3371 - accuracy: 0.8636\n",
      "Epoch 22: val_loss did not improve from 0.68129\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3449 - accuracy: 0.8529 - val_loss: 0.6835 - val_accuracy: 0.5802\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3237 - accuracy: 0.8778\n",
      "Epoch 23: val_loss did not improve from 0.68129\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3237 - accuracy: 0.8778 - val_loss: 0.6844 - val_accuracy: 0.5725\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3377 - accuracy: 0.8878\n",
      "Epoch 24: val_loss did not improve from 0.68129\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.3377 - accuracy: 0.8878 - val_loss: 0.6856 - val_accuracy: 0.5802\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3195 - accuracy: 0.8778\n",
      "Epoch 25: val_loss did not improve from 0.68129\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3195 - accuracy: 0.8778 - val_loss: 0.6866 - val_accuracy: 0.5725\n",
      "Epoch 26/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3054 - accuracy: 0.8932\n",
      "Epoch 26: val_loss did not improve from 0.68129\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3056 - accuracy: 0.8953 - val_loss: 0.6876 - val_accuracy: 0.5725\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3178 - accuracy: 0.8778\n",
      "Epoch 27: val_loss did not improve from 0.68129\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3178 - accuracy: 0.8778 - val_loss: 0.6877 - val_accuracy: 0.5725\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2821 - accuracy: 0.9077\n",
      "Epoch 28: val_loss did not improve from 0.68129\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.2821 - accuracy: 0.9077 - val_loss: 0.6886 - val_accuracy: 0.5802\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.9077\n",
      "Epoch 29: val_loss did not improve from 0.68129\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.2579 - accuracy: 0.9077 - val_loss: 0.6908 - val_accuracy: 0.5649\n",
      "Epoch 30/100\n",
      " 7/13 [===============>..............] - ETA: 0s - loss: 0.2719 - accuracy: 0.9018\n",
      "Epoch 30: val_loss did not improve from 0.68129\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.2753 - accuracy: 0.9052 - val_loss: 0.6924 - val_accuracy: 0.5649\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2585 - accuracy: 0.9202\n",
      "Epoch 31: val_loss did not improve from 0.68129\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.2585 - accuracy: 0.9202 - val_loss: 0.6946 - val_accuracy: 0.5649\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2317 - accuracy: 0.9227Restoring model weights from the end of the best epoch: 17.\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.68129\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.2317 - accuracy: 0.9227 - val_loss: 0.6973 - val_accuracy: 0.5420\n",
      "Epoch 32: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.5564\n",
      "Test AUC for Layer 1: 0.4603\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_52 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_39 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_40 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_41 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.9333 - accuracy: 0.4883\n",
      "Epoch 1: val_loss improved from inf to 0.69361, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 24ms/step - loss: 0.9327 - accuracy: 0.4887 - val_loss: 0.6936 - val_accuracy: 0.4887\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 0.8002 - accuracy: 0.5320\n",
      "Epoch 2: val_loss did not improve from 0.69361\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.8002 - accuracy: 0.5320 - val_loss: 0.6953 - val_accuracy: 0.4586\n",
      "Epoch 3/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.7642 - accuracy: 0.5703\n",
      "Epoch 3: val_loss did not improve from 0.69361\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7766 - accuracy: 0.5602 - val_loss: 0.6967 - val_accuracy: 0.4511\n",
      "Epoch 4/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.7096 - accuracy: 0.6106\n",
      "Epoch 4: val_loss did not improve from 0.69361\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.7302 - accuracy: 0.6034 - val_loss: 0.6969 - val_accuracy: 0.4511\n",
      "Epoch 5/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.7138 - accuracy: 0.6354\n",
      "Epoch 5: val_loss did not improve from 0.69361\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6861 - accuracy: 0.6485 - val_loss: 0.6982 - val_accuracy: 0.4511\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6271 - accuracy: 0.6659\n",
      "Epoch 6: val_loss did not improve from 0.69361\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6142 - accuracy: 0.6579 - val_loss: 0.6991 - val_accuracy: 0.4511\n",
      "Epoch 7/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5422 - accuracy: 0.7236\n",
      "Epoch 7: val_loss did not improve from 0.69361\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5657 - accuracy: 0.7011 - val_loss: 0.6999 - val_accuracy: 0.4586\n",
      "Epoch 8/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6045 - accuracy: 0.6875\n",
      "Epoch 8: val_loss did not improve from 0.69361\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6043 - accuracy: 0.6823 - val_loss: 0.7009 - val_accuracy: 0.4586\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5107 - accuracy: 0.7548\n",
      "Epoch 9: val_loss did not improve from 0.69361\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4951 - accuracy: 0.7632 - val_loss: 0.7014 - val_accuracy: 0.4586\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5177 - accuracy: 0.7476\n",
      "Epoch 10: val_loss did not improve from 0.69361\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5081 - accuracy: 0.7481 - val_loss: 0.7020 - val_accuracy: 0.4586\n",
      "Epoch 11/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5599 - accuracy: 0.7284\n",
      "Epoch 11: val_loss did not improve from 0.69361\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5755 - accuracy: 0.7030 - val_loss: 0.7031 - val_accuracy: 0.4586\n",
      "Epoch 12/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4956 - accuracy: 0.7656\n",
      "Epoch 12: val_loss did not improve from 0.69361\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4858 - accuracy: 0.7632 - val_loss: 0.7039 - val_accuracy: 0.4662\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4221 - accuracy: 0.8245\n",
      "Epoch 13: val_loss did not improve from 0.69361\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4433 - accuracy: 0.8064 - val_loss: 0.7064 - val_accuracy: 0.4586\n",
      "Epoch 14/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4242 - accuracy: 0.8101\n",
      "Epoch 14: val_loss did not improve from 0.69361\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4299 - accuracy: 0.8102 - val_loss: 0.7073 - val_accuracy: 0.4737\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4263 - accuracy: 0.8064\n",
      "Epoch 15: val_loss did not improve from 0.69361\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4263 - accuracy: 0.8064 - val_loss: 0.7099 - val_accuracy: 0.4737\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4000 - accuracy: 0.8317Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69361\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4083 - accuracy: 0.8271 - val_loss: 0.7082 - val_accuracy: 0.4662\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5347\n",
      "Test AUC for Layer 2: 0.5359\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_56 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_42 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_43 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_44 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.8936 - accuracy: 0.5156\n",
      "Epoch 1: val_loss improved from inf to 0.69154, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 21ms/step - loss: 0.9001 - accuracy: 0.5135 - val_loss: 0.6915 - val_accuracy: 0.5842\n",
      "Epoch 2/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.8120 - accuracy: 0.5478\n",
      "Epoch 2: val_loss improved from 0.69154 to 0.69068, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.8258 - accuracy: 0.5526 - val_loss: 0.6907 - val_accuracy: 0.5347\n",
      "Epoch 3/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.7682 - accuracy: 0.5938\n",
      "Epoch 3: val_loss improved from 0.69068 to 0.68954, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7619 - accuracy: 0.5931 - val_loss: 0.6895 - val_accuracy: 0.5347\n",
      "Epoch 4/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.6611 - accuracy: 0.6528\n",
      "Epoch 4: val_loss improved from 0.68954 to 0.68887, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6679 - accuracy: 0.6396 - val_loss: 0.6889 - val_accuracy: 0.5347\n",
      "Epoch 5/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.6481 - accuracy: 0.6562\n",
      "Epoch 5: val_loss improved from 0.68887 to 0.68817, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6423 - accuracy: 0.6517 - val_loss: 0.6882 - val_accuracy: 0.5347\n",
      "Epoch 6/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.6165 - accuracy: 0.6746\n",
      "Epoch 6: val_loss improved from 0.68817 to 0.68756, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6114 - accuracy: 0.6742 - val_loss: 0.6876 - val_accuracy: 0.5347\n",
      "Epoch 7/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.6219 - accuracy: 0.6581\n",
      "Epoch 7: val_loss did not improve from 0.68756\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6056 - accuracy: 0.6772 - val_loss: 0.6876 - val_accuracy: 0.5347\n",
      "Epoch 8/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5895 - accuracy: 0.6938\n",
      "Epoch 8: val_loss improved from 0.68756 to 0.68721, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5900 - accuracy: 0.6937 - val_loss: 0.6872 - val_accuracy: 0.5347\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5582 - accuracy: 0.7252\n",
      "Epoch 9: val_loss improved from 0.68721 to 0.68657, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5582 - accuracy: 0.7252 - val_loss: 0.6866 - val_accuracy: 0.5347\n",
      "Epoch 10/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.5463 - accuracy: 0.7066\n",
      "Epoch 10: val_loss improved from 0.68657 to 0.68651, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5379 - accuracy: 0.7177 - val_loss: 0.6865 - val_accuracy: 0.5347\n",
      "Epoch 11/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.5046 - accuracy: 0.7537\n",
      "Epoch 11: val_loss improved from 0.68651 to 0.68610, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5053 - accuracy: 0.7523 - val_loss: 0.6861 - val_accuracy: 0.5347\n",
      "Epoch 12/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.5084 - accuracy: 0.7309\n",
      "Epoch 12: val_loss did not improve from 0.68610\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5200 - accuracy: 0.7327 - val_loss: 0.6868 - val_accuracy: 0.5347\n",
      "Epoch 13/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4638 - accuracy: 0.7928\n",
      "Epoch 13: val_loss did not improve from 0.68610\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4661 - accuracy: 0.7958 - val_loss: 0.6872 - val_accuracy: 0.5347\n",
      "Epoch 14/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4376 - accuracy: 0.8109\n",
      "Epoch 14: val_loss did not improve from 0.68610\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4416 - accuracy: 0.8078 - val_loss: 0.6863 - val_accuracy: 0.5347\n",
      "Epoch 15/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4141 - accuracy: 0.8207\n",
      "Epoch 15: val_loss improved from 0.68610 to 0.68545, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4190 - accuracy: 0.8183 - val_loss: 0.6855 - val_accuracy: 0.5347\n",
      "Epoch 16/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4218 - accuracy: 0.8038\n",
      "Epoch 16: val_loss improved from 0.68545 to 0.68407, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4201 - accuracy: 0.8138 - val_loss: 0.6841 - val_accuracy: 0.5347\n",
      "Epoch 17/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4035 - accuracy: 0.8177\n",
      "Epoch 17: val_loss improved from 0.68407 to 0.68231, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3990 - accuracy: 0.8213 - val_loss: 0.6823 - val_accuracy: 0.5545\n",
      "Epoch 18/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.3858 - accuracy: 0.8327\n",
      "Epoch 18: val_loss improved from 0.68231 to 0.68055, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.3964 - accuracy: 0.8273 - val_loss: 0.6806 - val_accuracy: 0.5743\n",
      "Epoch 19/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.3855 - accuracy: 0.8235\n",
      "Epoch 19: val_loss did not improve from 0.68055\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3896 - accuracy: 0.8243 - val_loss: 0.6834 - val_accuracy: 0.5842\n",
      "Epoch 20/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3527 - accuracy: 0.8484\n",
      "Epoch 20: val_loss did not improve from 0.68055\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3514 - accuracy: 0.8514 - val_loss: 0.6817 - val_accuracy: 0.5842\n",
      "Epoch 21/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3678 - accuracy: 0.8453\n",
      "Epoch 21: val_loss did not improve from 0.68055\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3667 - accuracy: 0.8453 - val_loss: 0.6810 - val_accuracy: 0.5941\n",
      "Epoch 22/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3515 - accuracy: 0.8717\n",
      "Epoch 22: val_loss improved from 0.68055 to 0.67870, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3512 - accuracy: 0.8694 - val_loss: 0.6787 - val_accuracy: 0.6139\n",
      "Epoch 23/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.3222 - accuracy: 0.8733\n",
      "Epoch 23: val_loss did not improve from 0.67870\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3279 - accuracy: 0.8724 - val_loss: 0.6806 - val_accuracy: 0.6139\n",
      "Epoch 24/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3345 - accuracy: 0.8717\n",
      "Epoch 24: val_loss did not improve from 0.67870\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3288 - accuracy: 0.8769 - val_loss: 0.6818 - val_accuracy: 0.6139\n",
      "Epoch 25/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3162 - accuracy: 0.8953\n",
      "Epoch 25: val_loss improved from 0.67870 to 0.67798, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3143 - accuracy: 0.8949 - val_loss: 0.6780 - val_accuracy: 0.6238\n",
      "Epoch 26/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2911 - accuracy: 0.8766\n",
      "Epoch 26: val_loss did not improve from 0.67798\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2991 - accuracy: 0.8739 - val_loss: 0.6843 - val_accuracy: 0.6139\n",
      "Epoch 27/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2838 - accuracy: 0.8964\n",
      "Epoch 27: val_loss did not improve from 0.67798\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2775 - accuracy: 0.9024 - val_loss: 0.6886 - val_accuracy: 0.5941\n",
      "Epoch 28/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2798 - accuracy: 0.9030\n",
      "Epoch 28: val_loss did not improve from 0.67798\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2853 - accuracy: 0.9024 - val_loss: 0.7001 - val_accuracy: 0.5743\n",
      "Epoch 29/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2905 - accuracy: 0.9013\n",
      "Epoch 29: val_loss did not improve from 0.67798\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2900 - accuracy: 0.8994 - val_loss: 0.7092 - val_accuracy: 0.5446\n",
      "Epoch 30/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2670 - accuracy: 0.8953\n",
      "Epoch 30: val_loss did not improve from 0.67798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2679 - accuracy: 0.8934 - val_loss: 0.7114 - val_accuracy: 0.5545\n",
      "Epoch 31/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2552 - accuracy: 0.9211\n",
      "Epoch 31: val_loss did not improve from 0.67798\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2560 - accuracy: 0.9174 - val_loss: 0.7144 - val_accuracy: 0.5446\n",
      "Epoch 32/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2491 - accuracy: 0.9045\n",
      "Epoch 32: val_loss did not improve from 0.67798\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2487 - accuracy: 0.9099 - val_loss: 0.7234 - val_accuracy: 0.5842\n",
      "Epoch 33/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2471 - accuracy: 0.9128\n",
      "Epoch 33: val_loss did not improve from 0.67798\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2485 - accuracy: 0.9099 - val_loss: 0.7335 - val_accuracy: 0.5446\n",
      "Epoch 34/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2517 - accuracy: 0.9211\n",
      "Epoch 34: val_loss did not improve from 0.67798\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2559 - accuracy: 0.9189 - val_loss: 0.7562 - val_accuracy: 0.5644\n",
      "Epoch 35/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2217 - accuracy: 0.9375\n",
      "Epoch 35: val_loss did not improve from 0.67798\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.2242 - accuracy: 0.9354 - val_loss: 0.7744 - val_accuracy: 0.5842\n",
      "Epoch 36/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2146 - accuracy: 0.9328\n",
      "Epoch 36: val_loss did not improve from 0.67798\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.2141 - accuracy: 0.9324 - val_loss: 0.7885 - val_accuracy: 0.5941\n",
      "Epoch 37/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.1905 - accuracy: 0.9408\n",
      "Epoch 37: val_loss did not improve from 0.67798\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.1890 - accuracy: 0.9429 - val_loss: 0.7941 - val_accuracy: 0.6139\n",
      "Epoch 38/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.1878 - accuracy: 0.9457\n",
      "Epoch 38: val_loss did not improve from 0.67798\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.1914 - accuracy: 0.9444 - val_loss: 0.7851 - val_accuracy: 0.6139\n",
      "Epoch 39/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.1808 - accuracy: 0.9474\n",
      "Epoch 39: val_loss did not improve from 0.67798\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.1868 - accuracy: 0.9474 - val_loss: 0.8029 - val_accuracy: 0.5644\n",
      "Epoch 40/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.1891 - accuracy: 0.9408Restoring model weights from the end of the best epoch: 25.\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.67798\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.1918 - accuracy: 0.9399 - val_loss: 0.8228 - val_accuracy: 0.5743\n",
      "Epoch 40: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5000\n",
      "Test AUC for Layer 3: 0.4292\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5303\n",
      "Average Test AUC across all layers: 0.4751\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_60 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_45 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_46 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_47 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8647 - accuracy: 0.4830\n",
      "Epoch 1: val_loss improved from inf to 0.67812, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 30ms/step - loss: 0.8625 - accuracy: 0.4888 - val_loss: 0.6781 - val_accuracy: 0.6947\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7571 - accuracy: 0.5682\n",
      "Epoch 2: val_loss improved from 0.67812 to 0.66961, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.7659 - accuracy: 0.5711 - val_loss: 0.6696 - val_accuracy: 0.6947\n",
      "Epoch 3/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.7532 - accuracy: 0.5833\n",
      "Epoch 3: val_loss improved from 0.66961 to 0.66420, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.7579 - accuracy: 0.5736 - val_loss: 0.6642 - val_accuracy: 0.6947\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7037 - accuracy: 0.5935\n",
      "Epoch 4: val_loss improved from 0.66420 to 0.66003, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.7037 - accuracy: 0.5935 - val_loss: 0.6600 - val_accuracy: 0.6947\n",
      "Epoch 5/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.6311 - accuracy: 0.6641\n",
      "Epoch 5: val_loss improved from 0.66003 to 0.65754, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6320 - accuracy: 0.6633 - val_loss: 0.6575 - val_accuracy: 0.6947\n",
      "Epoch 6/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5923 - accuracy: 0.7102\n",
      "Epoch 6: val_loss improved from 0.65754 to 0.65586, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.5817 - accuracy: 0.7257 - val_loss: 0.6559 - val_accuracy: 0.6947\n",
      "Epoch 7/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5813 - accuracy: 0.6960\n",
      "Epoch 7: val_loss did not improve from 0.65586\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6031 - accuracy: 0.6883 - val_loss: 0.6565 - val_accuracy: 0.6947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5429 - accuracy: 0.7161\n",
      "Epoch 8: val_loss did not improve from 0.65586\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5389 - accuracy: 0.7182 - val_loss: 0.6561 - val_accuracy: 0.6947\n",
      "Epoch 9/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5433 - accuracy: 0.7159\n",
      "Epoch 9: val_loss did not improve from 0.65586\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5524 - accuracy: 0.7082 - val_loss: 0.6566 - val_accuracy: 0.6947\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5108 - accuracy: 0.7481\n",
      "Epoch 10: val_loss did not improve from 0.65586\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5108 - accuracy: 0.7481 - val_loss: 0.6580 - val_accuracy: 0.6947\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4978 - accuracy: 0.7830\n",
      "Epoch 11: val_loss did not improve from 0.65586\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4978 - accuracy: 0.7830 - val_loss: 0.6589 - val_accuracy: 0.6947\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4956 - accuracy: 0.7681\n",
      "Epoch 12: val_loss did not improve from 0.65586\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4956 - accuracy: 0.7681 - val_loss: 0.6603 - val_accuracy: 0.6947\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4300 - accuracy: 0.8130\n",
      "Epoch 13: val_loss did not improve from 0.65586\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4300 - accuracy: 0.8130 - val_loss: 0.6591 - val_accuracy: 0.6947\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4317 - accuracy: 0.8105\n",
      "Epoch 14: val_loss did not improve from 0.65586\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4317 - accuracy: 0.8105 - val_loss: 0.6628 - val_accuracy: 0.6870\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4084 - accuracy: 0.8229\n",
      "Epoch 15: val_loss did not improve from 0.65586\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4084 - accuracy: 0.8229 - val_loss: 0.6659 - val_accuracy: 0.6870\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3777 - accuracy: 0.8229\n",
      "Epoch 16: val_loss did not improve from 0.65586\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3777 - accuracy: 0.8229 - val_loss: 0.6675 - val_accuracy: 0.6565\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4014 - accuracy: 0.8155\n",
      "Epoch 17: val_loss did not improve from 0.65586\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4014 - accuracy: 0.8155 - val_loss: 0.6711 - val_accuracy: 0.6412\n",
      "Epoch 18/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3491 - accuracy: 0.8620\n",
      "Epoch 18: val_loss did not improve from 0.65586\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3568 - accuracy: 0.8529 - val_loss: 0.6750 - val_accuracy: 0.6336\n",
      "Epoch 19/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3319 - accuracy: 0.8620\n",
      "Epoch 19: val_loss did not improve from 0.65586\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3304 - accuracy: 0.8628 - val_loss: 0.6755 - val_accuracy: 0.6336\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3642 - accuracy: 0.8379\n",
      "Epoch 20: val_loss did not improve from 0.65586\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3642 - accuracy: 0.8379 - val_loss: 0.6780 - val_accuracy: 0.6183\n",
      "Epoch 21/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.3275 - accuracy: 0.8672Restoring model weights from the end of the best epoch: 6.\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.65586\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3294 - accuracy: 0.8653 - val_loss: 0.6811 - val_accuracy: 0.6031\n",
      "Epoch 21: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.7368\n",
      "Test AUC for Layer 1: 0.6496\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_64 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_48 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_49 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_50 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.9166 - accuracy: 0.4980\n",
      "Epoch 1: val_loss improved from inf to 0.67422, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 24ms/step - loss: 0.9189 - accuracy: 0.4906 - val_loss: 0.6742 - val_accuracy: 0.7368\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8063 - accuracy: 0.5195\n",
      "Epoch 2: val_loss improved from 0.67422 to 0.66041, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.7984 - accuracy: 0.5244 - val_loss: 0.6604 - val_accuracy: 0.7368\n",
      "Epoch 3/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.7412 - accuracy: 0.6094\n",
      "Epoch 3: val_loss improved from 0.66041 to 0.64903, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7412 - accuracy: 0.6147 - val_loss: 0.6490 - val_accuracy: 0.7368\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7431 - accuracy: 0.6055\n",
      "Epoch 4: val_loss improved from 0.64903 to 0.64019, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.7369 - accuracy: 0.6090 - val_loss: 0.6402 - val_accuracy: 0.7368\n",
      "Epoch 5/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.7332 - accuracy: 0.6016\n",
      "Epoch 5: val_loss improved from 0.64019 to 0.63305, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.7029 - accuracy: 0.6297 - val_loss: 0.6330 - val_accuracy: 0.7368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6173 - accuracy: 0.6372\n",
      "Epoch 6: val_loss improved from 0.63305 to 0.62730, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6173 - accuracy: 0.6372 - val_loss: 0.6273 - val_accuracy: 0.7368\n",
      "Epoch 7/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5855 - accuracy: 0.6849\n",
      "Epoch 7: val_loss improved from 0.62730 to 0.62366, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5790 - accuracy: 0.6823 - val_loss: 0.6237 - val_accuracy: 0.7368\n",
      "Epoch 8/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5800 - accuracy: 0.6901\n",
      "Epoch 8: val_loss improved from 0.62366 to 0.62210, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.5960 - accuracy: 0.6767 - val_loss: 0.6221 - val_accuracy: 0.7368\n",
      "Epoch 9/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5273 - accuracy: 0.7422\n",
      "Epoch 9: val_loss improved from 0.62210 to 0.62068, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5307 - accuracy: 0.7312 - val_loss: 0.6207 - val_accuracy: 0.7368\n",
      "Epoch 10/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4926 - accuracy: 0.7578\n",
      "Epoch 10: val_loss improved from 0.62068 to 0.61974, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4857 - accuracy: 0.7594 - val_loss: 0.6197 - val_accuracy: 0.7368\n",
      "Epoch 11/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4772 - accuracy: 0.7812\n",
      "Epoch 11: val_loss improved from 0.61974 to 0.61966, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4926 - accuracy: 0.7650 - val_loss: 0.6197 - val_accuracy: 0.7368\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4454 - accuracy: 0.7838\n",
      "Epoch 12: val_loss improved from 0.61966 to 0.61715, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4454 - accuracy: 0.7838 - val_loss: 0.6172 - val_accuracy: 0.7368\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4397 - accuracy: 0.7812\n",
      "Epoch 13: val_loss improved from 0.61715 to 0.61675, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4426 - accuracy: 0.7782 - val_loss: 0.6168 - val_accuracy: 0.7368\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4518 - accuracy: 0.7857\n",
      "Epoch 14: val_loss improved from 0.61675 to 0.61477, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4518 - accuracy: 0.7857 - val_loss: 0.6148 - val_accuracy: 0.7368\n",
      "Epoch 15/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3826 - accuracy: 0.8568\n",
      "Epoch 15: val_loss improved from 0.61477 to 0.61351, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.3962 - accuracy: 0.8365 - val_loss: 0.6135 - val_accuracy: 0.7368\n",
      "Epoch 16/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4035 - accuracy: 0.8203\n",
      "Epoch 16: val_loss did not improve from 0.61351\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3916 - accuracy: 0.8271 - val_loss: 0.6136 - val_accuracy: 0.7293\n",
      "Epoch 17/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3679 - accuracy: 0.8534\n",
      "Epoch 17: val_loss did not improve from 0.61351\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3659 - accuracy: 0.8553 - val_loss: 0.6143 - val_accuracy: 0.7143\n",
      "Epoch 18/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3761 - accuracy: 0.8365\n",
      "Epoch 18: val_loss did not improve from 0.61351\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3913 - accuracy: 0.8327 - val_loss: 0.6139 - val_accuracy: 0.7218\n",
      "Epoch 19/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3466 - accuracy: 0.8413\n",
      "Epoch 19: val_loss did not improve from 0.61351\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3395 - accuracy: 0.8515 - val_loss: 0.6150 - val_accuracy: 0.7143\n",
      "Epoch 20/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3526 - accuracy: 0.8389\n",
      "Epoch 20: val_loss did not improve from 0.61351\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3508 - accuracy: 0.8346 - val_loss: 0.6168 - val_accuracy: 0.7143\n",
      "Epoch 21/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3286 - accuracy: 0.8750\n",
      "Epoch 21: val_loss did not improve from 0.61351\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3279 - accuracy: 0.8703 - val_loss: 0.6157 - val_accuracy: 0.7143\n",
      "Epoch 22/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2918 - accuracy: 0.8918\n",
      "Epoch 22: val_loss did not improve from 0.61351\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3075 - accuracy: 0.8872 - val_loss: 0.6174 - val_accuracy: 0.6917\n",
      "Epoch 23/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2893 - accuracy: 0.8918\n",
      "Epoch 23: val_loss did not improve from 0.61351\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2982 - accuracy: 0.8872 - val_loss: 0.6196 - val_accuracy: 0.6767\n",
      "Epoch 24/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3082 - accuracy: 0.8906\n",
      "Epoch 24: val_loss did not improve from 0.61351\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3056 - accuracy: 0.8891 - val_loss: 0.6202 - val_accuracy: 0.6466\n",
      "Epoch 25/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2609 - accuracy: 0.9135\n",
      "Epoch 25: val_loss did not improve from 0.61351\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2732 - accuracy: 0.9098 - val_loss: 0.6210 - val_accuracy: 0.6466\n",
      "Epoch 26/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2628 - accuracy: 0.9014\n",
      "Epoch 26: val_loss did not improve from 0.61351\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2645 - accuracy: 0.9004 - val_loss: 0.6218 - val_accuracy: 0.6466\n",
      "Epoch 27/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2633 - accuracy: 0.9062\n",
      "Epoch 27: val_loss did not improve from 0.61351\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2575 - accuracy: 0.9154 - val_loss: 0.6243 - val_accuracy: 0.6541\n",
      "Epoch 28/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2550 - accuracy: 0.9183\n",
      "Epoch 28: val_loss did not improve from 0.61351\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2590 - accuracy: 0.9117 - val_loss: 0.6251 - val_accuracy: 0.6391\n",
      "Epoch 29/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2434 - accuracy: 0.9279\n",
      "Epoch 29: val_loss did not improve from 0.61351\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2380 - accuracy: 0.9380 - val_loss: 0.6194 - val_accuracy: 0.6541\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2389 - accuracy: 0.9173Restoring model weights from the end of the best epoch: 15.\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.61351\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2389 - accuracy: 0.9173 - val_loss: 0.6166 - val_accuracy: 0.6617\n",
      "Epoch 30: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.6139\n",
      "Test AUC for Layer 2: 0.6261\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_17\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_68 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_51 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_52 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_53 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.8346 - accuracy: 0.5404\n",
      "Epoch 1: val_loss improved from inf to 0.70406, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 22ms/step - loss: 0.8451 - accuracy: 0.5300 - val_loss: 0.7041 - val_accuracy: 0.3861\n",
      "Epoch 2/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.8353 - accuracy: 0.5147\n",
      "Epoch 2: val_loss did not improve from 0.70406\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.8099 - accuracy: 0.5315 - val_loss: 0.7196 - val_accuracy: 0.3861\n",
      "Epoch 3/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.7304 - accuracy: 0.6069\n",
      "Epoch 3: val_loss did not improve from 0.70406\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.7316 - accuracy: 0.6066 - val_loss: 0.7355 - val_accuracy: 0.3861\n",
      "Epoch 4/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.6623 - accuracy: 0.6332\n",
      "Epoch 4: val_loss did not improve from 0.70406\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6635 - accuracy: 0.6366 - val_loss: 0.7537 - val_accuracy: 0.3861\n",
      "Epoch 5/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.5881 - accuracy: 0.6892\n",
      "Epoch 5: val_loss did not improve from 0.70406\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6055 - accuracy: 0.6802 - val_loss: 0.7748 - val_accuracy: 0.3861\n",
      "Epoch 6/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6083 - accuracy: 0.6641\n",
      "Epoch 6: val_loss did not improve from 0.70406\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.6027 - accuracy: 0.6712 - val_loss: 0.7939 - val_accuracy: 0.3861\n",
      "Epoch 7/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5967 - accuracy: 0.6875\n",
      "Epoch 7: val_loss did not improve from 0.70406\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.6016 - accuracy: 0.6862 - val_loss: 0.8146 - val_accuracy: 0.3861\n",
      "Epoch 8/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5458 - accuracy: 0.7401\n",
      "Epoch 8: val_loss did not improve from 0.70406\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5359 - accuracy: 0.7477 - val_loss: 0.8346 - val_accuracy: 0.3861\n",
      "Epoch 9/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5306 - accuracy: 0.7418\n",
      "Epoch 9: val_loss did not improve from 0.70406\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5254 - accuracy: 0.7447 - val_loss: 0.8588 - val_accuracy: 0.3861\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5260 - accuracy: 0.7297\n",
      "Epoch 10: val_loss did not improve from 0.70406\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5260 - accuracy: 0.7297 - val_loss: 0.8735 - val_accuracy: 0.3861\n",
      "Epoch 11/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4611 - accuracy: 0.7747\n",
      "Epoch 11: val_loss did not improve from 0.70406\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4658 - accuracy: 0.7658 - val_loss: 0.8881 - val_accuracy: 0.3861\n",
      "Epoch 12/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4416 - accuracy: 0.7951\n",
      "Epoch 12: val_loss did not improve from 0.70406\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4353 - accuracy: 0.8018 - val_loss: 0.9087 - val_accuracy: 0.3861\n",
      "Epoch 13/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4385 - accuracy: 0.8010\n",
      "Epoch 13: val_loss did not improve from 0.70406\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4336 - accuracy: 0.8033 - val_loss: 0.9201 - val_accuracy: 0.3861\n",
      "Epoch 14/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4125 - accuracy: 0.8141\n",
      "Epoch 14: val_loss did not improve from 0.70406\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4121 - accuracy: 0.8123 - val_loss: 0.9341 - val_accuracy: 0.3861\n",
      "Epoch 15/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4287 - accuracy: 0.7928\n",
      "Epoch 15: val_loss did not improve from 0.70406\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4204 - accuracy: 0.7943 - val_loss: 0.9389 - val_accuracy: 0.3861\n",
      "Epoch 16/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3867 - accuracy: 0.8372Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70406\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3894 - accuracy: 0.8348 - val_loss: 0.9426 - val_accuracy: 0.3861\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5143\n",
      "Test AUC for Layer 3: 0.4984\n",
      "\n",
      "Average Test Accuracy across all layers: 0.6217\n",
      "Average Test AUC across all layers: 0.5914\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS (MPC)\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Merged + S_label\n",
      "Average Accuracy: 0.5303\n",
      "Average AUC: 0.4751\n",
      "  Layer 1 - Accuracy: 0.5564, AUC: 0.4603\n",
      "  Layer 2 - Accuracy: 0.5347, AUC: 0.5359\n",
      "  Layer 3 - Accuracy: 0.5000, AUC: 0.4292\n",
      "\n",
      "Combination: MLP with Merged + L_label\n",
      "Average Accuracy: 0.6217\n",
      "Average AUC: 0.5914\n",
      "  Layer 1 - Accuracy: 0.7368, AUC: 0.6496\n",
      "  Layer 2 - Accuracy: 0.6139, AUC: 0.6261\n",
      "  Layer 3 - Accuracy: 0.5143, AUC: 0.4984\n",
      "MLP summary comparison visualization saved as: Merged_OpenAI_MLP/visualizations_summary/MPC\\mlp_merged_performance_comparison.png\n",
      "MLP layer performance visualization saved as: Merged_OpenAI_MLP/visualizations_summary/MPC\\mlp_merged_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class MergedEmbeddingMLPPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using merged OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with merged OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'Merged_OpenAI_MLP/visualizations_mlp/MPC'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs('Merged_OpenAI_MLP/visualizations_summary/MPC', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with merged OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing merged OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert merged embeddings to numpy arrays\n",
    "        self.data['Merged_embedding'] = self.data['Merged_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_merged_embedding = self.data['Merged_embedding'].iloc[0]\n",
    "        print(f\"Sample Merged embedding shape: {sample_merged_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2014-2020), validation (2020-2021), test (2021-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2020-10-31'),\n",
    "            'val_start': pd.Timestamp('2020-11-01'),\n",
    "            'val_end': pd.Timestamp('2021-10-31'),\n",
    "            'test_start': pd.Timestamp('2021-11-01'),\n",
    "            'test_end': pd.Timestamp('2022-10-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2014-2021), validation (2021-2022), test (2022-2023)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2021-10-31'),\n",
    "            'val_start': pd.Timestamp('2021-11-01'),\n",
    "            'val_end': pd.Timestamp('2022-10-31'),\n",
    "            'test_start': pd.Timestamp('2022-11-01'),\n",
    "            'test_end': pd.Timestamp('2023-10-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2014-2022), validation (2022-2023), test (2023-2024)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2022-10-31'),\n",
    "            'val_start': pd.Timestamp('2022-11-01'),\n",
    "            'val_end': pd.Timestamp('2023-10-31'),\n",
    "            'test_start': pd.Timestamp('2023-11-01'),\n",
    "            'test_end': pd.Timestamp('2024-11-01')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data['Merged_embedding'].values)\n",
    "        X_val = np.stack(val_data['Merged_embedding'].values)\n",
    "        X_test = np.stack(test_data['Merged_embedding'].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_mlp_merged_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, label_col):\n",
    "        \"\"\"\n",
    "        Train and evaluate an MLP model for merged embeddings and a specific label column.\n",
    "        \n",
    "        Args:\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        combination_key = f\"MLP|Merged|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training MLP model for Merged Embeddings and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            model = self.create_mlp_model((1536,))\n",
    "            print(f\"Created MLP model for Merged Embeddings ({label_col})\")\n",
    "            model.summary()\n",
    "            \n",
    "            # Setup callbacks\n",
    "            plot_callback = PlotLearningCallback('Merged', label_col, i+1, self.mlp_viz_dir)\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=15,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            model_checkpoint = ModelCheckpoint(\n",
    "                filepath=f\"{self.mlp_viz_dir}/best_mlp_merged_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                monitor='val_loss',\n",
    "                save_weights_only=True,\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            print(f\"Training MLP model...\")\n",
    "            batch_size = 32  # Larger batch size for embeddings\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred_proba = model.predict(X_test)\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "            \n",
    "            # Create final learning curve visualization\n",
    "            self.plot_final_learning_curves(history, label_col, i+1, self.mlp_viz_dir)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': history.history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for merged embeddings with both short-term and long-term labels.\"\"\"\n",
    "        # Define label columns\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        \n",
    "        # Run analysis for each label column\n",
    "        for label_col in label_cols:\n",
    "            self.train_and_evaluate_model(label_col)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS (MPC)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Performance comparison for MLP with merged embeddings\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Plot accuracy and AUC bars\n",
    "        x = np.arange(len(df))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, df['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, df['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with Merged OpenAI Embeddings')\n",
    "        labels = [f\"Merged + {row['Label']}\" for _, row in df.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(df['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(df['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(df['Avg Accuracy'].max(), df['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/MPC', \"mlp_merged_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                layer_data.append(layer_info)\n",
    "        \n",
    "        if not layer_data:\n",
    "            print(\"No layer data available\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title('MLP Accuracy by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title('MLP AUC by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/MPC', \"mlp_merged_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['Merged_OpenAI_MLP/visualizations_mlp/MPC', 'Merged_OpenAI_MLP/visualizations_summary/MPC']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/wall_street_news_title_full_text_SP500_database/semantic/wall_street_news_semantics_MPC_completed_openai_Merged.csv'\n",
    "    \n",
    "    # Initialize the predictor with merged OpenAI embeddings\n",
    "    predictor = MergedEmbeddingMLPPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing merged OpenAI embedding vectors from string format...\n",
      "Sample Merged embedding shape: (1536,)\n",
      "Loaded 838 climate change news articles spanning from 07/11/2014 to 24/09/2024\n",
      "Class distribution for short-term prediction: {0: 433, 1: 405}\n",
      "Class distribution for long-term prediction: {0: 437, 1: 401}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_72 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_54 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_55 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_56 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8621 - accuracy: 0.5028\n",
      "Epoch 1: val_loss improved from inf to 0.69247, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 29ms/step - loss: 0.8618 - accuracy: 0.5037 - val_loss: 0.6925 - val_accuracy: 0.5191\n",
      "Epoch 2/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.8075 - accuracy: 0.5469\n",
      "Epoch 2: val_loss did not improve from 0.69247\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.8067 - accuracy: 0.5461 - val_loss: 0.6944 - val_accuracy: 0.5191\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7612 - accuracy: 0.5795\n",
      "Epoch 3: val_loss did not improve from 0.69247\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.7579 - accuracy: 0.5810 - val_loss: 0.6985 - val_accuracy: 0.5191\n",
      "Epoch 4/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.6938 - accuracy: 0.6510\n",
      "Epoch 4: val_loss did not improve from 0.69247\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6905 - accuracy: 0.6459 - val_loss: 0.7036 - val_accuracy: 0.5191\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6668 - accuracy: 0.6534\n",
      "Epoch 5: val_loss did not improve from 0.69247\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6668 - accuracy: 0.6534 - val_loss: 0.7096 - val_accuracy: 0.5191\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6551 - accuracy: 0.6334\n",
      "Epoch 6: val_loss did not improve from 0.69247\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6551 - accuracy: 0.6334 - val_loss: 0.7152 - val_accuracy: 0.5191\n",
      "Epoch 7/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5833 - accuracy: 0.6901\n",
      "Epoch 7: val_loss did not improve from 0.69247\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5858 - accuracy: 0.6958 - val_loss: 0.7208 - val_accuracy: 0.5191\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5383 - accuracy: 0.7431\n",
      "Epoch 8: val_loss did not improve from 0.69247\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5383 - accuracy: 0.7431 - val_loss: 0.7260 - val_accuracy: 0.5191\n",
      "Epoch 9/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5228 - accuracy: 0.7578\n",
      "Epoch 9: val_loss did not improve from 0.69247\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5263 - accuracy: 0.7531 - val_loss: 0.7306 - val_accuracy: 0.5191\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5449 - accuracy: 0.7132\n",
      "Epoch 10: val_loss did not improve from 0.69247\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5449 - accuracy: 0.7132 - val_loss: 0.7338 - val_accuracy: 0.5191\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4916 - accuracy: 0.7706\n",
      "Epoch 11: val_loss did not improve from 0.69247\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4916 - accuracy: 0.7706 - val_loss: 0.7369 - val_accuracy: 0.5191\n",
      "Epoch 12/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4945 - accuracy: 0.7474\n",
      "Epoch 12: val_loss did not improve from 0.69247\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4989 - accuracy: 0.7431 - val_loss: 0.7400 - val_accuracy: 0.5191\n",
      "Epoch 13/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4707 - accuracy: 0.7656\n",
      "Epoch 13: val_loss did not improve from 0.69247\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4743 - accuracy: 0.7656 - val_loss: 0.7418 - val_accuracy: 0.5191\n",
      "Epoch 14/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.4396 - accuracy: 0.7875\n",
      "Epoch 14: val_loss did not improve from 0.69247\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.4458 - accuracy: 0.7781 - val_loss: 0.7435 - val_accuracy: 0.5191\n",
      "Epoch 15/100\n",
      " 7/13 [===============>..............] - ETA: 0s - loss: 0.4202 - accuracy: 0.8080\n",
      "Epoch 15: val_loss did not improve from 0.69247\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.4294 - accuracy: 0.7955 - val_loss: 0.7460 - val_accuracy: 0.5191\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4521 - accuracy: 0.7905Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69247\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4521 - accuracy: 0.7905 - val_loss: 0.7489 - val_accuracy: 0.5191\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.5113\n",
      "Test AUC for Layer 1: 0.5403\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_76 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_57 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_58 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_59 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.9072 - accuracy: 0.5078\n",
      "Epoch 1: val_loss improved from inf to 0.69250, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 24ms/step - loss: 0.9059 - accuracy: 0.5075 - val_loss: 0.6925 - val_accuracy: 0.5113\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8341 - accuracy: 0.5391\n",
      "Epoch 2: val_loss did not improve from 0.69250\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.8317 - accuracy: 0.5395 - val_loss: 0.6937 - val_accuracy: 0.5113\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7477 - accuracy: 0.5752\n",
      "Epoch 3: val_loss did not improve from 0.69250\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7477 - accuracy: 0.5752 - val_loss: 0.6947 - val_accuracy: 0.5113\n",
      "Epoch 4/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.7343 - accuracy: 0.5889\n",
      "Epoch 4: val_loss did not improve from 0.69250\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.7238 - accuracy: 0.6015 - val_loss: 0.6951 - val_accuracy: 0.5113\n",
      "Epoch 5/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6685 - accuracy: 0.6250\n",
      "Epoch 5: val_loss did not improve from 0.69250\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6832 - accuracy: 0.6147 - val_loss: 0.6945 - val_accuracy: 0.5113\n",
      "Epoch 6/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6454 - accuracy: 0.6615\n",
      "Epoch 6: val_loss did not improve from 0.69250\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6305 - accuracy: 0.6692 - val_loss: 0.6938 - val_accuracy: 0.5113\n",
      "Epoch 7/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6012 - accuracy: 0.6971\n",
      "Epoch 7: val_loss did not improve from 0.69250\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6110 - accuracy: 0.6842 - val_loss: 0.6930 - val_accuracy: 0.5113\n",
      "Epoch 8/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5645 - accuracy: 0.7031\n",
      "Epoch 8: val_loss did not improve from 0.69250\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5631 - accuracy: 0.7068 - val_loss: 0.6925 - val_accuracy: 0.5038\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5494 - accuracy: 0.7236\n",
      "Epoch 9: val_loss did not improve from 0.69250\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5519 - accuracy: 0.7162 - val_loss: 0.6927 - val_accuracy: 0.4812\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4891 - accuracy: 0.7644\n",
      "Epoch 10: val_loss improved from 0.69250 to 0.69249, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5022 - accuracy: 0.7538 - val_loss: 0.6925 - val_accuracy: 0.5038\n",
      "Epoch 11/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4813 - accuracy: 0.7604\n",
      "Epoch 11: val_loss did not improve from 0.69249\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4668 - accuracy: 0.7763 - val_loss: 0.6931 - val_accuracy: 0.5263\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4895 - accuracy: 0.7838\n",
      "Epoch 12: val_loss did not improve from 0.69249\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4895 - accuracy: 0.7838 - val_loss: 0.6935 - val_accuracy: 0.5338\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4790 - accuracy: 0.7788\n",
      "Epoch 13: val_loss did not improve from 0.69249\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4807 - accuracy: 0.7726 - val_loss: 0.6956 - val_accuracy: 0.5113\n",
      "Epoch 14/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.4270 - accuracy: 0.8170\n",
      "Epoch 14: val_loss did not improve from 0.69249\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4281 - accuracy: 0.8158 - val_loss: 0.6964 - val_accuracy: 0.5038\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4533 - accuracy: 0.7793\n",
      "Epoch 15: val_loss did not improve from 0.69249\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4490 - accuracy: 0.7838 - val_loss: 0.6979 - val_accuracy: 0.4962\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4170 - accuracy: 0.8269\n",
      "Epoch 16: val_loss did not improve from 0.69249\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4264 - accuracy: 0.8139 - val_loss: 0.7023 - val_accuracy: 0.4887\n",
      "Epoch 17/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4026 - accuracy: 0.8221\n",
      "Epoch 17: val_loss did not improve from 0.69249\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4041 - accuracy: 0.8158 - val_loss: 0.7051 - val_accuracy: 0.4962\n",
      "Epoch 18/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3797 - accuracy: 0.8293\n",
      "Epoch 18: val_loss did not improve from 0.69249\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3755 - accuracy: 0.8289 - val_loss: 0.7069 - val_accuracy: 0.4887\n",
      "Epoch 19/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3724 - accuracy: 0.8365\n",
      "Epoch 19: val_loss did not improve from 0.69249\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3742 - accuracy: 0.8346 - val_loss: 0.7090 - val_accuracy: 0.4962\n",
      "Epoch 20/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3611 - accuracy: 0.8462\n",
      "Epoch 20: val_loss did not improve from 0.69249\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3625 - accuracy: 0.8459 - val_loss: 0.7096 - val_accuracy: 0.4887\n",
      "Epoch 21/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3474 - accuracy: 0.8606\n",
      "Epoch 21: val_loss did not improve from 0.69249\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3362 - accuracy: 0.8609 - val_loss: 0.7127 - val_accuracy: 0.5038\n",
      "Epoch 22/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.3397 - accuracy: 0.8527\n",
      "Epoch 22: val_loss did not improve from 0.69249\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3433 - accuracy: 0.8515 - val_loss: 0.7117 - val_accuracy: 0.5188\n",
      "Epoch 23/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3412 - accuracy: 0.8558\n",
      "Epoch 23: val_loss did not improve from 0.69249\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3468 - accuracy: 0.8496 - val_loss: 0.7105 - val_accuracy: 0.5188\n",
      "Epoch 24/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3455 - accuracy: 0.8654\n",
      "Epoch 24: val_loss did not improve from 0.69249\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3357 - accuracy: 0.8684 - val_loss: 0.7089 - val_accuracy: 0.5263\n",
      "Epoch 25/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.3336 - accuracy: 0.8571Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.69249\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3434 - accuracy: 0.8515 - val_loss: 0.7067 - val_accuracy: 0.5263\n",
      "Epoch 25: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5149\n",
      "Test AUC for Layer 2: 0.4467\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_80 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_60 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_61 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_62 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.8689 - accuracy: 0.5331\n",
      "Epoch 1: val_loss improved from inf to 0.69191, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 21ms/step - loss: 0.8805 - accuracy: 0.5270 - val_loss: 0.6919 - val_accuracy: 0.5644\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7837 - accuracy: 0.5571\n",
      "Epoch 2: val_loss improved from 0.69191 to 0.69122, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7837 - accuracy: 0.5571 - val_loss: 0.6912 - val_accuracy: 0.5545\n",
      "Epoch 3/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.7157 - accuracy: 0.6146\n",
      "Epoch 3: val_loss did not improve from 0.69122\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.7253 - accuracy: 0.6051 - val_loss: 0.6914 - val_accuracy: 0.5545\n",
      "Epoch 4/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.7305 - accuracy: 0.6016\n",
      "Epoch 4: val_loss improved from 0.69122 to 0.69090, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.7256 - accuracy: 0.6066 - val_loss: 0.6909 - val_accuracy: 0.5644\n",
      "Epoch 5/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.6870 - accuracy: 0.6360\n",
      "Epoch 5: val_loss improved from 0.69090 to 0.69083, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6789 - accuracy: 0.6381 - val_loss: 0.6908 - val_accuracy: 0.5644\n",
      "Epoch 6/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.6031 - accuracy: 0.6728\n",
      "Epoch 6: val_loss improved from 0.69083 to 0.69058, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6224 - accuracy: 0.6622 - val_loss: 0.6906 - val_accuracy: 0.5743\n",
      "Epoch 7/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.6103 - accuracy: 0.6562\n",
      "Epoch 7: val_loss did not improve from 0.69058\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5963 - accuracy: 0.6727 - val_loss: 0.6909 - val_accuracy: 0.5644\n",
      "Epoch 8/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5973 - accuracy: 0.6711\n",
      "Epoch 8: val_loss did not improve from 0.69058\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5985 - accuracy: 0.6682 - val_loss: 0.6917 - val_accuracy: 0.5545\n",
      "Epoch 9/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5719 - accuracy: 0.7007\n",
      "Epoch 9: val_loss did not improve from 0.69058\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.5700 - accuracy: 0.7027 - val_loss: 0.6943 - val_accuracy: 0.5644\n",
      "Epoch 10/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5579 - accuracy: 0.6957\n",
      "Epoch 10: val_loss did not improve from 0.69058\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5526 - accuracy: 0.7027 - val_loss: 0.6963 - val_accuracy: 0.5545\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4854 - accuracy: 0.7583\n",
      "Epoch 11: val_loss did not improve from 0.69058\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4854 - accuracy: 0.7583 - val_loss: 0.6988 - val_accuracy: 0.5347\n",
      "Epoch 12/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4750 - accuracy: 0.7674\n",
      "Epoch 12: val_loss did not improve from 0.69058\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4911 - accuracy: 0.7553 - val_loss: 0.7007 - val_accuracy: 0.5248\n",
      "Epoch 13/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4822 - accuracy: 0.7763\n",
      "Epoch 13: val_loss did not improve from 0.69058\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4864 - accuracy: 0.7688 - val_loss: 0.7029 - val_accuracy: 0.5446\n",
      "Epoch 14/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4634 - accuracy: 0.7648\n",
      "Epoch 14: val_loss did not improve from 0.69058\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4615 - accuracy: 0.7703 - val_loss: 0.7061 - val_accuracy: 0.5545\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4801 - accuracy: 0.7568\n",
      "Epoch 15: val_loss did not improve from 0.69058\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.4801 - accuracy: 0.7568 - val_loss: 0.7111 - val_accuracy: 0.5050\n",
      "Epoch 16/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4357 - accuracy: 0.8141\n",
      "Epoch 16: val_loss did not improve from 0.69058\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4415 - accuracy: 0.8108 - val_loss: 0.7139 - val_accuracy: 0.4950\n",
      "Epoch 17/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3848 - accuracy: 0.8438\n",
      "Epoch 17: val_loss did not improve from 0.69058\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3842 - accuracy: 0.8483 - val_loss: 0.7191 - val_accuracy: 0.4851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.3953 - accuracy: 0.8385\n",
      "Epoch 18: val_loss did not improve from 0.69058\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4017 - accuracy: 0.8348 - val_loss: 0.7249 - val_accuracy: 0.4554\n",
      "Epoch 19/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.3838 - accuracy: 0.8420\n",
      "Epoch 19: val_loss did not improve from 0.69058\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.3836 - accuracy: 0.8393 - val_loss: 0.7312 - val_accuracy: 0.4158\n",
      "Epoch 20/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3496 - accuracy: 0.8553\n",
      "Epoch 20: val_loss did not improve from 0.69058\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3456 - accuracy: 0.8619 - val_loss: 0.7415 - val_accuracy: 0.4059\n",
      "Epoch 21/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3643 - accuracy: 0.8553Restoring model weights from the end of the best epoch: 6.\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.69058\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3660 - accuracy: 0.8498 - val_loss: 0.7509 - val_accuracy: 0.3960\n",
      "Epoch 21: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4429\n",
      "Test AUC for Layer 3: 0.5724\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4897\n",
      "Average Test AUC across all layers: 0.5198\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_84 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_63 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_64 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_65 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8753 - accuracy: 0.4915\n",
      "Epoch 1: val_loss improved from inf to 0.70525, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 30ms/step - loss: 0.8909 - accuracy: 0.4838 - val_loss: 0.7052 - val_accuracy: 0.3740\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7807 - accuracy: 0.5426\n",
      "Epoch 2: val_loss did not improve from 0.70525\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.7888 - accuracy: 0.5411 - val_loss: 0.7181 - val_accuracy: 0.3740\n",
      "Epoch 3/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.7065 - accuracy: 0.5911\n",
      "Epoch 3: val_loss did not improve from 0.70525\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6987 - accuracy: 0.5960 - val_loss: 0.7295 - val_accuracy: 0.3740\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6844 - accuracy: 0.6284\n",
      "Epoch 4: val_loss did not improve from 0.70525\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6844 - accuracy: 0.6284 - val_loss: 0.7408 - val_accuracy: 0.3740\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6225 - accuracy: 0.6983\n",
      "Epoch 5: val_loss did not improve from 0.70525\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6225 - accuracy: 0.6983 - val_loss: 0.7525 - val_accuracy: 0.3740\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6060 - accuracy: 0.7082\n",
      "Epoch 6: val_loss did not improve from 0.70525\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6060 - accuracy: 0.7082 - val_loss: 0.7612 - val_accuracy: 0.3740\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5999 - accuracy: 0.6933\n",
      "Epoch 7: val_loss did not improve from 0.70525\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5999 - accuracy: 0.6933 - val_loss: 0.7651 - val_accuracy: 0.3740\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5515 - accuracy: 0.7082\n",
      "Epoch 8: val_loss did not improve from 0.70525\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5515 - accuracy: 0.7082 - val_loss: 0.7697 - val_accuracy: 0.3740\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5189 - accuracy: 0.7581\n",
      "Epoch 9: val_loss did not improve from 0.70525\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5189 - accuracy: 0.7581 - val_loss: 0.7735 - val_accuracy: 0.3740\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5035 - accuracy: 0.7207\n",
      "Epoch 10: val_loss did not improve from 0.70525\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5035 - accuracy: 0.7207 - val_loss: 0.7790 - val_accuracy: 0.3740\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4787 - accuracy: 0.7606\n",
      "Epoch 11: val_loss did not improve from 0.70525\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4787 - accuracy: 0.7606 - val_loss: 0.7838 - val_accuracy: 0.3740\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4494 - accuracy: 0.7980\n",
      "Epoch 12: val_loss did not improve from 0.70525\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4494 - accuracy: 0.7980 - val_loss: 0.7833 - val_accuracy: 0.3740\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4279 - accuracy: 0.8005\n",
      "Epoch 13: val_loss did not improve from 0.70525\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4279 - accuracy: 0.8005 - val_loss: 0.7839 - val_accuracy: 0.3740\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3877 - accuracy: 0.8429\n",
      "Epoch 14: val_loss did not improve from 0.70525\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3877 - accuracy: 0.8429 - val_loss: 0.7840 - val_accuracy: 0.3740\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4074 - accuracy: 0.8354\n",
      "Epoch 15: val_loss did not improve from 0.70525\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4074 - accuracy: 0.8354 - val_loss: 0.7826 - val_accuracy: 0.3740\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3756 - accuracy: 0.8454Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70525\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3756 - accuracy: 0.8454 - val_loss: 0.7829 - val_accuracy: 0.3740\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.3684\n",
      "Test AUC for Layer 1: 0.6086\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_88 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_66 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_67 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_68 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8873 - accuracy: 0.4981\n",
      "Epoch 1: val_loss improved from inf to 0.70875, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 24ms/step - loss: 0.8873 - accuracy: 0.4981 - val_loss: 0.7087 - val_accuracy: 0.3684\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8233 - accuracy: 0.5226\n",
      "Epoch 2: val_loss did not improve from 0.70875\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.8233 - accuracy: 0.5226 - val_loss: 0.7207 - val_accuracy: 0.3684\n",
      "Epoch 3/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.8098 - accuracy: 0.5457\n",
      "Epoch 3: val_loss did not improve from 0.70875\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.7952 - accuracy: 0.5602 - val_loss: 0.7334 - val_accuracy: 0.3684\n",
      "Epoch 4/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6654 - accuracy: 0.6484\n",
      "Epoch 4: val_loss did not improve from 0.70875\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6562 - accuracy: 0.6635 - val_loss: 0.7401 - val_accuracy: 0.3684\n",
      "Epoch 5/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6330 - accuracy: 0.6923\n",
      "Epoch 5: val_loss did not improve from 0.70875\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6479 - accuracy: 0.6786 - val_loss: 0.7463 - val_accuracy: 0.3684\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5659 - accuracy: 0.7043\n",
      "Epoch 6: val_loss did not improve from 0.70875\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6012 - accuracy: 0.6786 - val_loss: 0.7502 - val_accuracy: 0.3684\n",
      "Epoch 7/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5832 - accuracy: 0.6615\n",
      "Epoch 7: val_loss did not improve from 0.70875\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5964 - accuracy: 0.6654 - val_loss: 0.7540 - val_accuracy: 0.3684\n",
      "Epoch 8/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6011 - accuracy: 0.6953\n",
      "Epoch 8: val_loss did not improve from 0.70875\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5876 - accuracy: 0.6992 - val_loss: 0.7539 - val_accuracy: 0.3684\n",
      "Epoch 9/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5655 - accuracy: 0.6979\n",
      "Epoch 9: val_loss did not improve from 0.70875\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5675 - accuracy: 0.6936 - val_loss: 0.7525 - val_accuracy: 0.3684\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5139 - accuracy: 0.7404\n",
      "Epoch 10: val_loss did not improve from 0.70875\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5127 - accuracy: 0.7406 - val_loss: 0.7499 - val_accuracy: 0.3684\n",
      "Epoch 11/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4635 - accuracy: 0.7995\n",
      "Epoch 11: val_loss did not improve from 0.70875\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4674 - accuracy: 0.7857 - val_loss: 0.7471 - val_accuracy: 0.3684\n",
      "Epoch 12/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4726 - accuracy: 0.7812\n",
      "Epoch 12: val_loss did not improve from 0.70875\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4747 - accuracy: 0.7782 - val_loss: 0.7410 - val_accuracy: 0.3684\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4329 - accuracy: 0.8125\n",
      "Epoch 13: val_loss did not improve from 0.70875\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4288 - accuracy: 0.8177 - val_loss: 0.7342 - val_accuracy: 0.3684\n",
      "Epoch 14/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4471 - accuracy: 0.7909\n",
      "Epoch 14: val_loss did not improve from 0.70875\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4520 - accuracy: 0.7801 - val_loss: 0.7286 - val_accuracy: 0.3910\n",
      "Epoch 15/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3984 - accuracy: 0.8151\n",
      "Epoch 15: val_loss did not improve from 0.70875\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4058 - accuracy: 0.8139 - val_loss: 0.7248 - val_accuracy: 0.4060\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3929 - accuracy: 0.8197Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70875\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3929 - accuracy: 0.8233 - val_loss: 0.7233 - val_accuracy: 0.4361\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5149\n",
      "Test AUC for Layer 2: 0.5341\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_92 (Dense)            (None, 512)               786944    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " batch_normalization_69 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_70 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_71 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9006 - accuracy: 0.4790\n",
      "Epoch 1: val_loss improved from inf to 0.69261, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 21ms/step - loss: 0.9006 - accuracy: 0.4790 - val_loss: 0.6926 - val_accuracy: 0.5149\n",
      "Epoch 2/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.7847 - accuracy: 0.5586\n",
      "Epoch 2: val_loss did not improve from 0.69261\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.7711 - accuracy: 0.5721 - val_loss: 0.6937 - val_accuracy: 0.5149\n",
      "Epoch 3/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.6966 - accuracy: 0.6354\n",
      "Epoch 3: val_loss did not improve from 0.69261\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.7066 - accuracy: 0.6201 - val_loss: 0.6969 - val_accuracy: 0.5149\n",
      "Epoch 4/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.6661 - accuracy: 0.6414\n",
      "Epoch 4: val_loss did not improve from 0.69261\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.6770 - accuracy: 0.6336 - val_loss: 0.7007 - val_accuracy: 0.5149\n",
      "Epoch 5/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6176 - accuracy: 0.6641\n",
      "Epoch 5: val_loss did not improve from 0.69261\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.6181 - accuracy: 0.6652 - val_loss: 0.7062 - val_accuracy: 0.5149\n",
      "Epoch 6/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.5532 - accuracy: 0.7096\n",
      "Epoch 6: val_loss did not improve from 0.69261\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5716 - accuracy: 0.6937 - val_loss: 0.7137 - val_accuracy: 0.5149\n",
      "Epoch 7/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5791 - accuracy: 0.6859\n",
      "Epoch 7: val_loss did not improve from 0.69261\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5733 - accuracy: 0.6952 - val_loss: 0.7223 - val_accuracy: 0.5149\n",
      "Epoch 8/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.5424 - accuracy: 0.7222\n",
      "Epoch 8: val_loss did not improve from 0.69261\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5331 - accuracy: 0.7342 - val_loss: 0.7282 - val_accuracy: 0.5149\n",
      "Epoch 9/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4961 - accuracy: 0.7453\n",
      "Epoch 9: val_loss did not improve from 0.69261\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4919 - accuracy: 0.7477 - val_loss: 0.7356 - val_accuracy: 0.5149\n",
      "Epoch 10/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4792 - accuracy: 0.7812\n",
      "Epoch 10: val_loss did not improve from 0.69261\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4828 - accuracy: 0.7778 - val_loss: 0.7403 - val_accuracy: 0.5149\n",
      "Epoch 11/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4692 - accuracy: 0.7743\n",
      "Epoch 11: val_loss did not improve from 0.69261\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4731 - accuracy: 0.7613 - val_loss: 0.7457 - val_accuracy: 0.5149\n",
      "Epoch 12/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4560 - accuracy: 0.7899\n",
      "Epoch 12: val_loss did not improve from 0.69261\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4652 - accuracy: 0.7838 - val_loss: 0.7530 - val_accuracy: 0.5149\n",
      "Epoch 13/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4316 - accuracy: 0.8125\n",
      "Epoch 13: val_loss did not improve from 0.69261\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4334 - accuracy: 0.8093 - val_loss: 0.7603 - val_accuracy: 0.5149\n",
      "Epoch 14/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4082 - accuracy: 0.8191\n",
      "Epoch 14: val_loss did not improve from 0.69261\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4101 - accuracy: 0.8198 - val_loss: 0.7674 - val_accuracy: 0.5149\n",
      "Epoch 15/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3886 - accuracy: 0.8454\n",
      "Epoch 15: val_loss did not improve from 0.69261\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.3842 - accuracy: 0.8483 - val_loss: 0.7696 - val_accuracy: 0.5149\n",
      "Epoch 16/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3811 - accuracy: 0.8322Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69261\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3999 - accuracy: 0.8213 - val_loss: 0.7721 - val_accuracy: 0.5149\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.6571\n",
      "Test AUC for Layer 3: 0.4511\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5135\n",
      "Average Test AUC across all layers: 0.5313\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS (SLB)\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Merged + S_label\n",
      "Average Accuracy: 0.4897\n",
      "Average AUC: 0.5198\n",
      "  Layer 1 - Accuracy: 0.5113, AUC: 0.5403\n",
      "  Layer 2 - Accuracy: 0.5149, AUC: 0.4467\n",
      "  Layer 3 - Accuracy: 0.4429, AUC: 0.5724\n",
      "\n",
      "Combination: MLP with Merged + L_label\n",
      "Average Accuracy: 0.5135\n",
      "Average AUC: 0.5313\n",
      "  Layer 1 - Accuracy: 0.3684, AUC: 0.6086\n",
      "  Layer 2 - Accuracy: 0.5149, AUC: 0.5341\n",
      "  Layer 3 - Accuracy: 0.6571, AUC: 0.4511\n",
      "MLP summary comparison visualization saved as: Merged_OpenAI_MLP/visualizations_summary/SLB\\mlp_merged_performance_comparison.png\n",
      "MLP layer performance visualization saved as: Merged_OpenAI_MLP/visualizations_summary/SLB\\mlp_merged_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class MergedEmbeddingMLPPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using merged OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with merged OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'Merged_OpenAI_MLP/visualizations_mlp/SLB'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs('Merged_OpenAI_MLP/visualizations_summary/SLB', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with merged OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing merged OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert merged embeddings to numpy arrays\n",
    "        self.data['Merged_embedding'] = self.data['Merged_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_merged_embedding = self.data['Merged_embedding'].iloc[0]\n",
    "        print(f\"Sample Merged embedding shape: {sample_merged_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2014-2020), validation (2020-2021), test (2021-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2020-10-31'),\n",
    "            'val_start': pd.Timestamp('2020-11-01'),\n",
    "            'val_end': pd.Timestamp('2021-10-31'),\n",
    "            'test_start': pd.Timestamp('2021-11-01'),\n",
    "            'test_end': pd.Timestamp('2022-10-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2014-2021), validation (2021-2022), test (2022-2023)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2021-10-31'),\n",
    "            'val_start': pd.Timestamp('2021-11-01'),\n",
    "            'val_end': pd.Timestamp('2022-10-31'),\n",
    "            'test_start': pd.Timestamp('2022-11-01'),\n",
    "            'test_end': pd.Timestamp('2023-10-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2014-2022), validation (2022-2023), test (2023-2024)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2022-10-31'),\n",
    "            'val_start': pd.Timestamp('2022-11-01'),\n",
    "            'val_end': pd.Timestamp('2023-10-31'),\n",
    "            'test_start': pd.Timestamp('2023-11-01'),\n",
    "            'test_end': pd.Timestamp('2024-11-01')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data['Merged_embedding'].values)\n",
    "        X_val = np.stack(val_data['Merged_embedding'].values)\n",
    "        X_test = np.stack(test_data['Merged_embedding'].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_mlp_merged_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, label_col):\n",
    "        \"\"\"\n",
    "        Train and evaluate an MLP model for merged embeddings and a specific label column.\n",
    "        \n",
    "        Args:\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        combination_key = f\"MLP|Merged|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training MLP model for Merged Embeddings and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            model = self.create_mlp_model((1536,))\n",
    "            print(f\"Created MLP model for Merged Embeddings ({label_col})\")\n",
    "            model.summary()\n",
    "            \n",
    "            # Setup callbacks\n",
    "            plot_callback = PlotLearningCallback('Merged', label_col, i+1, self.mlp_viz_dir)\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=15,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            model_checkpoint = ModelCheckpoint(\n",
    "                filepath=f\"{self.mlp_viz_dir}/best_mlp_merged_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                monitor='val_loss',\n",
    "                save_weights_only=True,\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            print(f\"Training MLP model...\")\n",
    "            batch_size = 32  # Larger batch size for embeddings\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred_proba = model.predict(X_test)\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "            \n",
    "            # Create final learning curve visualization\n",
    "            self.plot_final_learning_curves(history, label_col, i+1, self.mlp_viz_dir)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': history.history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for merged embeddings with both short-term and long-term labels.\"\"\"\n",
    "        # Define label columns\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        \n",
    "        # Run analysis for each label column\n",
    "        for label_col in label_cols:\n",
    "            self.train_and_evaluate_model(label_col)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS (SLB)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Performance comparison for MLP with merged embeddings\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Plot accuracy and AUC bars\n",
    "        x = np.arange(len(df))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, df['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, df['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with Merged OpenAI Embeddings')\n",
    "        labels = [f\"Merged + {row['Label']}\" for _, row in df.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(df['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(df['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(df['Avg Accuracy'].max(), df['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/SLB', \"mlp_merged_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                layer_data.append(layer_info)\n",
    "        \n",
    "        if not layer_data:\n",
    "            print(\"No layer data available\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title('MLP Accuracy by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title('MLP AUC by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/SLB', \"mlp_merged_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['Merged_OpenAI_MLP/visualizations_mlp/SLB', 'Merged_OpenAI_MLP/visualizations_summary/SLB']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/wall_street_news_title_full_text_SP500_database/semantic/wall_street_news_semantics_SLB_completed_openai_Merged.csv'\n",
    "    \n",
    "    # Initialize the predictor with merged OpenAI embeddings\n",
    "    predictor = MergedEmbeddingMLPPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing merged OpenAI embedding vectors from string format...\n",
      "Sample Merged embedding shape: (1536,)\n",
      "Loaded 838 climate change news articles spanning from 07/11/2014 to 24/09/2024\n",
      "Class distribution for short-term prediction: {1: 427, 0: 411}\n",
      "Class distribution for long-term prediction: {1: 428, 0: 410}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_96 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_72 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_73 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_74 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.9758 - accuracy: 0.4659\n",
      "Epoch 1: val_loss improved from inf to 0.68752, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 29ms/step - loss: 0.9579 - accuracy: 0.4713 - val_loss: 0.6875 - val_accuracy: 0.5573\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8071 - accuracy: 0.5795\n",
      "Epoch 2: val_loss did not improve from 0.68752\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.8051 - accuracy: 0.5786 - val_loss: 0.6880 - val_accuracy: 0.5573\n",
      "Epoch 3/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.7538 - accuracy: 0.5938\n",
      "Epoch 3: val_loss did not improve from 0.68752\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.7416 - accuracy: 0.6035 - val_loss: 0.6926 - val_accuracy: 0.5573\n",
      "Epoch 4/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.6591 - accuracy: 0.6276\n",
      "Epoch 4: val_loss did not improve from 0.68752\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6697 - accuracy: 0.6259 - val_loss: 0.7002 - val_accuracy: 0.5573\n",
      "Epoch 5/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.6736 - accuracy: 0.6458\n",
      "Epoch 5: val_loss did not improve from 0.68752\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6802 - accuracy: 0.6434 - val_loss: 0.7090 - val_accuracy: 0.5573\n",
      "Epoch 6/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5765 - accuracy: 0.7031\n",
      "Epoch 6: val_loss did not improve from 0.68752\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5840 - accuracy: 0.6958 - val_loss: 0.7186 - val_accuracy: 0.5573\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6110 - accuracy: 0.6708\n",
      "Epoch 7: val_loss did not improve from 0.68752\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6110 - accuracy: 0.6708 - val_loss: 0.7274 - val_accuracy: 0.5573\n",
      "Epoch 8/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.5573 - accuracy: 0.7074\n",
      "Epoch 8: val_loss did not improve from 0.68752\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5609 - accuracy: 0.6933 - val_loss: 0.7360 - val_accuracy: 0.5573\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5599 - accuracy: 0.7082\n",
      "Epoch 9: val_loss did not improve from 0.68752\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5599 - accuracy: 0.7082 - val_loss: 0.7421 - val_accuracy: 0.5573\n",
      "Epoch 10/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4791 - accuracy: 0.7682\n",
      "Epoch 10: val_loss did not improve from 0.68752\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4774 - accuracy: 0.7706 - val_loss: 0.7477 - val_accuracy: 0.5573\n",
      "Epoch 11/100\n",
      "10/13 [======================>.......] - ETA: 0s - loss: 0.4765 - accuracy: 0.7656\n",
      "Epoch 11: val_loss did not improve from 0.68752\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4724 - accuracy: 0.7681 - val_loss: 0.7554 - val_accuracy: 0.5573\n",
      "Epoch 12/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4457 - accuracy: 0.7727\n",
      "Epoch 12: val_loss did not improve from 0.68752\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.4593 - accuracy: 0.7681 - val_loss: 0.7606 - val_accuracy: 0.5573\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4384 - accuracy: 0.8080\n",
      "Epoch 13: val_loss did not improve from 0.68752\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4384 - accuracy: 0.8080 - val_loss: 0.7645 - val_accuracy: 0.5573\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4088 - accuracy: 0.8180\n",
      "Epoch 14: val_loss did not improve from 0.68752\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4088 - accuracy: 0.8180 - val_loss: 0.7680 - val_accuracy: 0.5573\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4201 - accuracy: 0.8155\n",
      "Epoch 15: val_loss did not improve from 0.68752\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4201 - accuracy: 0.8155 - val_loss: 0.7712 - val_accuracy: 0.5573\n",
      "Epoch 16/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.4010 - accuracy: 0.8099Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.68752\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.3955 - accuracy: 0.8180 - val_loss: 0.7742 - val_accuracy: 0.5573\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.5564\n",
      "Test AUC for Layer 1: 0.4716\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_100 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_75 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_76 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_77 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8796 - accuracy: 0.4941\n",
      "Epoch 1: val_loss improved from inf to 0.70171, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8748 - accuracy: 0.4925 - val_loss: 0.7017 - val_accuracy: 0.4436\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8407 - accuracy: 0.5117\n",
      "Epoch 2: val_loss did not improve from 0.70171\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.8415 - accuracy: 0.5150 - val_loss: 0.7130 - val_accuracy: 0.4436\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7388 - accuracy: 0.5752\n",
      "Epoch 3: val_loss did not improve from 0.70171\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7388 - accuracy: 0.5752 - val_loss: 0.7263 - val_accuracy: 0.4436\n",
      "Epoch 4/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.7524 - accuracy: 0.5599\n",
      "Epoch 4: val_loss did not improve from 0.70171\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7501 - accuracy: 0.5583 - val_loss: 0.7414 - val_accuracy: 0.4436\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6833 - accuracy: 0.6289\n",
      "Epoch 5: val_loss did not improve from 0.70171\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6847 - accuracy: 0.6278 - val_loss: 0.7542 - val_accuracy: 0.4436\n",
      "Epoch 6/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6472 - accuracy: 0.6458\n",
      "Epoch 6: val_loss did not improve from 0.70171\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6612 - accuracy: 0.6504 - val_loss: 0.7647 - val_accuracy: 0.4436\n",
      "Epoch 7/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6170 - accuracy: 0.6484\n",
      "Epoch 7: val_loss did not improve from 0.70171\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6187 - accuracy: 0.6523 - val_loss: 0.7740 - val_accuracy: 0.4436\n",
      "Epoch 8/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6242 - accuracy: 0.6823\n",
      "Epoch 8: val_loss did not improve from 0.70171\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6120 - accuracy: 0.6974 - val_loss: 0.7840 - val_accuracy: 0.4436\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5571 - accuracy: 0.7356\n",
      "Epoch 9: val_loss did not improve from 0.70171\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5575 - accuracy: 0.7274 - val_loss: 0.7899 - val_accuracy: 0.4436\n",
      "Epoch 10/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5161 - accuracy: 0.7448\n",
      "Epoch 10: val_loss did not improve from 0.70171\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5159 - accuracy: 0.7368 - val_loss: 0.7962 - val_accuracy: 0.4436\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5261 - accuracy: 0.7425\n",
      "Epoch 11: val_loss did not improve from 0.70171\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5261 - accuracy: 0.7425 - val_loss: 0.8042 - val_accuracy: 0.4436\n",
      "Epoch 12/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5203 - accuracy: 0.7524\n",
      "Epoch 12: val_loss did not improve from 0.70171\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5128 - accuracy: 0.7575 - val_loss: 0.8073 - val_accuracy: 0.4436\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4736 - accuracy: 0.7692\n",
      "Epoch 13: val_loss did not improve from 0.70171\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4775 - accuracy: 0.7632 - val_loss: 0.8066 - val_accuracy: 0.4436\n",
      "Epoch 14/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4693 - accuracy: 0.7943\n",
      "Epoch 14: val_loss did not improve from 0.70171\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4607 - accuracy: 0.8064 - val_loss: 0.8086 - val_accuracy: 0.4436\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4689 - accuracy: 0.7838\n",
      "Epoch 15: val_loss did not improve from 0.70171\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4689 - accuracy: 0.7838 - val_loss: 0.8085 - val_accuracy: 0.4436\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4410 - accuracy: 0.8029Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70171\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4346 - accuracy: 0.8177 - val_loss: 0.8120 - val_accuracy: 0.4436\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4554\n",
      "Test AUC for Layer 2: 0.4684\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_104 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_78 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_79 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_80 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_107 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.8824 - accuracy: 0.5000\n",
      "Epoch 1: val_loss improved from inf to 0.70349, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 22ms/step - loss: 0.8736 - accuracy: 0.5075 - val_loss: 0.7035 - val_accuracy: 0.4554\n",
      "Epoch 2/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.8463 - accuracy: 0.5347\n",
      "Epoch 2: val_loss did not improve from 0.70349\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.8487 - accuracy: 0.5285 - val_loss: 0.7183 - val_accuracy: 0.4554\n",
      "Epoch 3/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.7617 - accuracy: 0.5625\n",
      "Epoch 3: val_loss did not improve from 0.70349\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.7628 - accuracy: 0.5571 - val_loss: 0.7352 - val_accuracy: 0.4554\n",
      "Epoch 4/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.7088 - accuracy: 0.5954\n",
      "Epoch 4: val_loss did not improve from 0.70349\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.7056 - accuracy: 0.6006 - val_loss: 0.7511 - val_accuracy: 0.4554\n",
      "Epoch 5/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.6731 - accuracy: 0.6250\n",
      "Epoch 5: val_loss did not improve from 0.70349\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6762 - accuracy: 0.6276 - val_loss: 0.7643 - val_accuracy: 0.4554\n",
      "Epoch 6/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.6314 - accuracy: 0.6672\n",
      "Epoch 6: val_loss did not improve from 0.70349\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.6268 - accuracy: 0.6727 - val_loss: 0.7734 - val_accuracy: 0.4554\n",
      "Epoch 7/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.6326 - accuracy: 0.6414\n",
      "Epoch 7: val_loss did not improve from 0.70349\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6267 - accuracy: 0.6471 - val_loss: 0.7806 - val_accuracy: 0.4554\n",
      "Epoch 8/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.6116 - accuracy: 0.6632\n",
      "Epoch 8: val_loss did not improve from 0.70349\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6035 - accuracy: 0.6682 - val_loss: 0.7885 - val_accuracy: 0.4554\n",
      "Epoch 9/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5859 - accuracy: 0.7072\n",
      "Epoch 9: val_loss did not improve from 0.70349\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5799 - accuracy: 0.7072 - val_loss: 0.7938 - val_accuracy: 0.4554\n",
      "Epoch 10/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5427 - accuracy: 0.7220\n",
      "Epoch 10: val_loss did not improve from 0.70349\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5378 - accuracy: 0.7267 - val_loss: 0.8012 - val_accuracy: 0.4554\n",
      "Epoch 11/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.5196 - accuracy: 0.7279\n",
      "Epoch 11: val_loss did not improve from 0.70349\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5152 - accuracy: 0.7372 - val_loss: 0.8063 - val_accuracy: 0.4554\n",
      "Epoch 12/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.4982 - accuracy: 0.7794\n",
      "Epoch 12: val_loss did not improve from 0.70349\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5031 - accuracy: 0.7718 - val_loss: 0.8068 - val_accuracy: 0.4554\n",
      "Epoch 13/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5432 - accuracy: 0.7188\n",
      "Epoch 13: val_loss did not improve from 0.70349\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5401 - accuracy: 0.7222 - val_loss: 0.8127 - val_accuracy: 0.4554\n",
      "Epoch 14/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4684 - accuracy: 0.7899\n",
      "Epoch 14: val_loss did not improve from 0.70349\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4704 - accuracy: 0.7898 - val_loss: 0.8104 - val_accuracy: 0.4554\n",
      "Epoch 15/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4726 - accuracy: 0.7781\n",
      "Epoch 15: val_loss did not improve from 0.70349\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.4746 - accuracy: 0.7733 - val_loss: 0.8129 - val_accuracy: 0.4554\n",
      "Epoch 16/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4083 - accuracy: 0.8172Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70349\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4090 - accuracy: 0.8183 - val_loss: 0.8154 - val_accuracy: 0.4554\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5571\n",
      "Test AUC for Layer 3: 0.4657\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5230\n",
      "Average Test AUC across all layers: 0.4686\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/11/2014 - 31/10/2020\n",
      "Validation period: 01/11/2020 - 31/10/2021\n",
      "Testing period: 01/11/2021 - 31/10/2022\n",
      "Training data: 401 samples\n",
      "Validation data: 131 samples\n",
      "Test data: 133 samples\n",
      "Embeddings shapes - Train: (401, 1536), Val: (131, 1536), Test: (133, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_108 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_81 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_82 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_55 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_83 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.9093 - accuracy: 0.5000\n",
      "Epoch 1: val_loss improved from inf to 0.73475, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "13/13 [==============================] - 2s 28ms/step - loss: 0.9091 - accuracy: 0.4963 - val_loss: 0.7347 - val_accuracy: 0.3130\n",
      "Epoch 2/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.8909 - accuracy: 0.5028\n",
      "Epoch 2: val_loss did not improve from 0.73475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 14ms/step - loss: 0.8564 - accuracy: 0.5287 - val_loss: 0.7906 - val_accuracy: 0.3130\n",
      "Epoch 3/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.7567 - accuracy: 0.5703\n",
      "Epoch 3: val_loss did not improve from 0.73475\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.7519 - accuracy: 0.5761 - val_loss: 0.8484 - val_accuracy: 0.3130\n",
      "Epoch 4/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.7162 - accuracy: 0.5859\n",
      "Epoch 4: val_loss did not improve from 0.73475\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.7196 - accuracy: 0.5810 - val_loss: 0.9114 - val_accuracy: 0.3130\n",
      "Epoch 5/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.6228 - accuracy: 0.6562\n",
      "Epoch 5: val_loss did not improve from 0.73475\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6280 - accuracy: 0.6559 - val_loss: 0.9722 - val_accuracy: 0.3130\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6184 - accuracy: 0.6633\n",
      "Epoch 6: val_loss did not improve from 0.73475\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6184 - accuracy: 0.6633 - val_loss: 1.0340 - val_accuracy: 0.3130\n",
      "Epoch 7/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5875 - accuracy: 0.7083\n",
      "Epoch 7: val_loss did not improve from 0.73475\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5902 - accuracy: 0.7007 - val_loss: 1.0929 - val_accuracy: 0.3130\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5737 - accuracy: 0.7032\n",
      "Epoch 8: val_loss did not improve from 0.73475\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5737 - accuracy: 0.7032 - val_loss: 1.1441 - val_accuracy: 0.3130\n",
      "Epoch 9/100\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 0.5129 - accuracy: 0.7396\n",
      "Epoch 9: val_loss did not improve from 0.73475\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5121 - accuracy: 0.7456 - val_loss: 1.1979 - val_accuracy: 0.3130\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4941 - accuracy: 0.7581\n",
      "Epoch 10: val_loss did not improve from 0.73475\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4941 - accuracy: 0.7581 - val_loss: 1.2491 - val_accuracy: 0.3130\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5292 - accuracy: 0.7382\n",
      "Epoch 11: val_loss did not improve from 0.73475\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5292 - accuracy: 0.7382 - val_loss: 1.3022 - val_accuracy: 0.3130\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4522 - accuracy: 0.7731\n",
      "Epoch 12: val_loss did not improve from 0.73475\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4522 - accuracy: 0.7731 - val_loss: 1.3449 - val_accuracy: 0.3130\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4567 - accuracy: 0.7930\n",
      "Epoch 13: val_loss did not improve from 0.73475\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4567 - accuracy: 0.7930 - val_loss: 1.3856 - val_accuracy: 0.3130\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3952 - accuracy: 0.8279\n",
      "Epoch 14: val_loss did not improve from 0.73475\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3952 - accuracy: 0.8279 - val_loss: 1.4310 - val_accuracy: 0.3130\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4182 - accuracy: 0.8379\n",
      "Epoch 15: val_loss did not improve from 0.73475\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.4182 - accuracy: 0.8379 - val_loss: 1.4707 - val_accuracy: 0.3130\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3773 - accuracy: 0.8479Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.73475\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.3773 - accuracy: 0.8479 - val_loss: 1.5026 - val_accuracy: 0.3130\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.2707\n",
      "Test AUC for Layer 1: 0.4104\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/11/2014 - 31/10/2021\n",
      "Validation period: 01/11/2021 - 31/10/2022\n",
      "Testing period: 01/11/2022 - 31/10/2023\n",
      "Training data: 532 samples\n",
      "Validation data: 133 samples\n",
      "Test data: 101 samples\n",
      "Embeddings shapes - Train: (532, 1536), Val: (133, 1536), Test: (101, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_112 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_84 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_56 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_85 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_57 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_86 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_115 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8930 - accuracy: 0.4922\n",
      "Epoch 1: val_loss improved from inf to 0.68632, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 2s 24ms/step - loss: 0.8978 - accuracy: 0.4925 - val_loss: 0.6863 - val_accuracy: 0.7218\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8143 - accuracy: 0.5469\n",
      "Epoch 2: val_loss improved from 0.68632 to 0.67513, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.8252 - accuracy: 0.5395 - val_loss: 0.6751 - val_accuracy: 0.7293\n",
      "Epoch 3/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.8053 - accuracy: 0.5521\n",
      "Epoch 3: val_loss improved from 0.67513 to 0.66780, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.7849 - accuracy: 0.5677 - val_loss: 0.6678 - val_accuracy: 0.7293\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7110 - accuracy: 0.5827\n",
      "Epoch 4: val_loss improved from 0.66780 to 0.66114, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.7110 - accuracy: 0.5827 - val_loss: 0.6611 - val_accuracy: 0.7293\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6392 - accuracy: 0.6484\n",
      "Epoch 5: val_loss improved from 0.66114 to 0.65446, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 15ms/step - loss: 0.6457 - accuracy: 0.6429 - val_loss: 0.6545 - val_accuracy: 0.7293\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6516 - accuracy: 0.6152\n",
      "Epoch 6: val_loss improved from 0.65446 to 0.64766, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.6443 - accuracy: 0.6184 - val_loss: 0.6477 - val_accuracy: 0.7293\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6066 - accuracy: 0.6855\n",
      "Epoch 7: val_loss improved from 0.64766 to 0.64184, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6055 - accuracy: 0.6880 - val_loss: 0.6418 - val_accuracy: 0.7293\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5833 - accuracy: 0.6823\n",
      "Epoch 8: val_loss improved from 0.64184 to 0.63671, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.5833 - accuracy: 0.6823 - val_loss: 0.6367 - val_accuracy: 0.7293\n",
      "Epoch 9/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5450 - accuracy: 0.7161\n",
      "Epoch 9: val_loss improved from 0.63671 to 0.63242, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5445 - accuracy: 0.7218 - val_loss: 0.6324 - val_accuracy: 0.7293\n",
      "Epoch 10/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5409 - accuracy: 0.7188\n",
      "Epoch 10: val_loss improved from 0.63242 to 0.62978, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.5600 - accuracy: 0.7237 - val_loss: 0.6298 - val_accuracy: 0.7293\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5319 - accuracy: 0.7350\n",
      "Epoch 11: val_loss improved from 0.62978 to 0.62721, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.5319 - accuracy: 0.7350 - val_loss: 0.6272 - val_accuracy: 0.7293\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4624 - accuracy: 0.7852\n",
      "Epoch 12: val_loss improved from 0.62721 to 0.62572, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4716 - accuracy: 0.7820 - val_loss: 0.6257 - val_accuracy: 0.7293\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4411 - accuracy: 0.8008\n",
      "Epoch 13: val_loss improved from 0.62572 to 0.62438, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4423 - accuracy: 0.7970 - val_loss: 0.6244 - val_accuracy: 0.7293\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4230 - accuracy: 0.8105\n",
      "Epoch 14: val_loss improved from 0.62438 to 0.62276, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4200 - accuracy: 0.8139 - val_loss: 0.6228 - val_accuracy: 0.7293\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4319 - accuracy: 0.8026\n",
      "Epoch 15: val_loss improved from 0.62276 to 0.61999, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4319 - accuracy: 0.8026 - val_loss: 0.6200 - val_accuracy: 0.7293\n",
      "Epoch 16/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3998 - accuracy: 0.8099\n",
      "Epoch 16: val_loss improved from 0.61999 to 0.61850, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4005 - accuracy: 0.8120 - val_loss: 0.6185 - val_accuracy: 0.7293\n",
      "Epoch 17/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4038 - accuracy: 0.8438\n",
      "Epoch 17: val_loss improved from 0.61850 to 0.61525, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4006 - accuracy: 0.8459 - val_loss: 0.6153 - val_accuracy: 0.7293\n",
      "Epoch 18/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3883 - accuracy: 0.8262\n",
      "Epoch 18: val_loss improved from 0.61525 to 0.61312, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.3899 - accuracy: 0.8214 - val_loss: 0.6131 - val_accuracy: 0.7293\n",
      "Epoch 19/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4062 - accuracy: 0.8223\n",
      "Epoch 19: val_loss did not improve from 0.61312\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4067 - accuracy: 0.8214 - val_loss: 0.6139 - val_accuracy: 0.7293\n",
      "Epoch 20/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3602 - accuracy: 0.8359\n",
      "Epoch 20: val_loss did not improve from 0.61312\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3758 - accuracy: 0.8271 - val_loss: 0.6158 - val_accuracy: 0.7293\n",
      "Epoch 21/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3734 - accuracy: 0.8542\n",
      "Epoch 21: val_loss did not improve from 0.61312\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3566 - accuracy: 0.8665 - val_loss: 0.6161 - val_accuracy: 0.7218\n",
      "Epoch 22/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3234 - accuracy: 0.8750\n",
      "Epoch 22: val_loss did not improve from 0.61312\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3341 - accuracy: 0.8609 - val_loss: 0.6172 - val_accuracy: 0.7218\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3145 - accuracy: 0.8722\n",
      "Epoch 23: val_loss did not improve from 0.61312\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3145 - accuracy: 0.8722 - val_loss: 0.6225 - val_accuracy: 0.7218\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3006 - accuracy: 0.8741\n",
      "Epoch 24: val_loss did not improve from 0.61312\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3006 - accuracy: 0.8741 - val_loss: 0.6247 - val_accuracy: 0.6917\n",
      "Epoch 25/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2775 - accuracy: 0.9062\n",
      "Epoch 25: val_loss did not improve from 0.61312\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2724 - accuracy: 0.9023 - val_loss: 0.6280 - val_accuracy: 0.6917\n",
      "Epoch 26/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2830 - accuracy: 0.9010\n",
      "Epoch 26: val_loss did not improve from 0.61312\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2863 - accuracy: 0.8947 - val_loss: 0.6354 - val_accuracy: 0.6917\n",
      "Epoch 27/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2618 - accuracy: 0.9219\n",
      "Epoch 27: val_loss did not improve from 0.61312\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2818 - accuracy: 0.9098 - val_loss: 0.6407 - val_accuracy: 0.6466\n",
      "Epoch 28/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2665 - accuracy: 0.9062\n",
      "Epoch 28: val_loss did not improve from 0.61312\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2742 - accuracy: 0.9023 - val_loss: 0.6507 - val_accuracy: 0.6316\n",
      "Epoch 29/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2674 - accuracy: 0.9043\n",
      "Epoch 29: val_loss did not improve from 0.61312\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2716 - accuracy: 0.9004 - val_loss: 0.6590 - val_accuracy: 0.6015\n",
      "Epoch 30/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2407 - accuracy: 0.9245\n",
      "Epoch 30: val_loss did not improve from 0.61312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2504 - accuracy: 0.9248 - val_loss: 0.6716 - val_accuracy: 0.6015\n",
      "Epoch 31/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2343 - accuracy: 0.9303\n",
      "Epoch 31: val_loss did not improve from 0.61312\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2373 - accuracy: 0.9267 - val_loss: 0.6839 - val_accuracy: 0.5714\n",
      "Epoch 32/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2291 - accuracy: 0.9271\n",
      "Epoch 32: val_loss did not improve from 0.61312\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2260 - accuracy: 0.9323 - val_loss: 0.6945 - val_accuracy: 0.5714\n",
      "Epoch 33/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2239 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 18.\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.61312\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2410 - accuracy: 0.9023 - val_loss: 0.7056 - val_accuracy: 0.5865\n",
      "Epoch 33: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4158\n",
      "Test AUC for Layer 2: 0.4681\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/11/2014 - 31/10/2022\n",
      "Validation period: 01/11/2022 - 31/10/2023\n",
      "Testing period: 01/11/2023 - 01/11/2024\n",
      "Training data: 666 samples\n",
      "Validation data: 101 samples\n",
      "Test data: 70 samples\n",
      "Embeddings shapes - Train: (666, 1536), Val: (101, 1536), Test: (70, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_116 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_87 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_58 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_88 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_59 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_89 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9040 - accuracy: 0.4940\n",
      "Epoch 1: val_loss improved from inf to 0.70660, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "21/21 [==============================] - 2s 21ms/step - loss: 0.9040 - accuracy: 0.4940 - val_loss: 0.7066 - val_accuracy: 0.4158\n",
      "Epoch 2/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.7890 - accuracy: 0.5469\n",
      "Epoch 2: val_loss did not improve from 0.70660\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.7758 - accuracy: 0.5586 - val_loss: 0.7208 - val_accuracy: 0.4158\n",
      "Epoch 3/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.7445 - accuracy: 0.6121\n",
      "Epoch 3: val_loss did not improve from 0.70660\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.7451 - accuracy: 0.6111 - val_loss: 0.7341 - val_accuracy: 0.4158\n",
      "Epoch 4/100\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 0.6704 - accuracy: 0.6342\n",
      "Epoch 4: val_loss did not improve from 0.70660\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.6715 - accuracy: 0.6351 - val_loss: 0.7476 - val_accuracy: 0.4158\n",
      "Epoch 5/100\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 0.6449 - accuracy: 0.6270\n",
      "Epoch 5: val_loss did not improve from 0.70660\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.6528 - accuracy: 0.6351 - val_loss: 0.7593 - val_accuracy: 0.4158\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6243 - accuracy: 0.6562\n",
      "Epoch 6: val_loss did not improve from 0.70660\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6243 - accuracy: 0.6562 - val_loss: 0.7717 - val_accuracy: 0.4158\n",
      "Epoch 7/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5825 - accuracy: 0.6957\n",
      "Epoch 7: val_loss did not improve from 0.70660\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5795 - accuracy: 0.7012 - val_loss: 0.7824 - val_accuracy: 0.4158\n",
      "Epoch 8/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5424 - accuracy: 0.7171\n",
      "Epoch 8: val_loss did not improve from 0.70660\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5369 - accuracy: 0.7252 - val_loss: 0.7944 - val_accuracy: 0.4158\n",
      "Epoch 9/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5217 - accuracy: 0.7303\n",
      "Epoch 9: val_loss did not improve from 0.70660\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.5220 - accuracy: 0.7327 - val_loss: 0.8060 - val_accuracy: 0.4158\n",
      "Epoch 10/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.5428 - accuracy: 0.7257\n",
      "Epoch 10: val_loss did not improve from 0.70660\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.5277 - accuracy: 0.7417 - val_loss: 0.8122 - val_accuracy: 0.4158\n",
      "Epoch 11/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.4869 - accuracy: 0.7587\n",
      "Epoch 11: val_loss did not improve from 0.70660\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4787 - accuracy: 0.7643 - val_loss: 0.8213 - val_accuracy: 0.4158\n",
      "Epoch 12/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4845 - accuracy: 0.7615\n",
      "Epoch 12: val_loss did not improve from 0.70660\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4853 - accuracy: 0.7598 - val_loss: 0.8244 - val_accuracy: 0.4158\n",
      "Epoch 13/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4658 - accuracy: 0.7681\n",
      "Epoch 13: val_loss did not improve from 0.70660\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4553 - accuracy: 0.7763 - val_loss: 0.8303 - val_accuracy: 0.4158\n",
      "Epoch 14/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4452 - accuracy: 0.7928\n",
      "Epoch 14: val_loss did not improve from 0.70660\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.4482 - accuracy: 0.7913 - val_loss: 0.8321 - val_accuracy: 0.4158\n",
      "Epoch 15/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3897 - accuracy: 0.8224\n",
      "Epoch 15: val_loss did not improve from 0.70660\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3991 - accuracy: 0.8228 - val_loss: 0.8374 - val_accuracy: 0.4158\n",
      "Epoch 16/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.3803 - accuracy: 0.8351Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70660\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.3860 - accuracy: 0.8303 - val_loss: 0.8489 - val_accuracy: 0.4158\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5000\n",
      "Test AUC for Layer 3: 0.5706\n",
      "\n",
      "Average Test Accuracy across all layers: 0.3955\n",
      "Average Test AUC across all layers: 0.4830\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS (XOM)\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Merged + S_label\n",
      "Average Accuracy: 0.5230\n",
      "Average AUC: 0.4686\n",
      "  Layer 1 - Accuracy: 0.5564, AUC: 0.4716\n",
      "  Layer 2 - Accuracy: 0.4554, AUC: 0.4684\n",
      "  Layer 3 - Accuracy: 0.5571, AUC: 0.4657\n",
      "\n",
      "Combination: MLP with Merged + L_label\n",
      "Average Accuracy: 0.3955\n",
      "Average AUC: 0.4830\n",
      "  Layer 1 - Accuracy: 0.2707, AUC: 0.4104\n",
      "  Layer 2 - Accuracy: 0.4158, AUC: 0.4681\n",
      "  Layer 3 - Accuracy: 0.5000, AUC: 0.5706\n",
      "MLP summary comparison visualization saved as: Merged_OpenAI_MLP/visualizations_summary/XOM\\mlp_merged_performance_comparison.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP layer performance visualization saved as: Merged_OpenAI_MLP/visualizations_summary/XOM\\mlp_merged_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class MergedEmbeddingMLPPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using merged OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with merged OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'Merged_OpenAI_MLP/visualizations_mlp/XOM'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs('Merged_OpenAI_MLP/visualizations_summary/XOM', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with merged OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing merged OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert merged embeddings to numpy arrays\n",
    "        self.data['Merged_embedding'] = self.data['Merged_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_merged_embedding = self.data['Merged_embedding'].iloc[0]\n",
    "        print(f\"Sample Merged embedding shape: {sample_merged_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2014-2020), validation (2020-2021), test (2021-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2020-10-31'),\n",
    "            'val_start': pd.Timestamp('2020-11-01'),\n",
    "            'val_end': pd.Timestamp('2021-10-31'),\n",
    "            'test_start': pd.Timestamp('2021-11-01'),\n",
    "            'test_end': pd.Timestamp('2022-10-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2014-2021), validation (2021-2022), test (2022-2023)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2021-10-31'),\n",
    "            'val_start': pd.Timestamp('2021-11-01'),\n",
    "            'val_end': pd.Timestamp('2022-10-31'),\n",
    "            'test_start': pd.Timestamp('2022-11-01'),\n",
    "            'test_end': pd.Timestamp('2023-10-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2014-2022), validation (2022-2023), test (2023-2024)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2014-11-01'),\n",
    "            'train_end': pd.Timestamp('2022-10-31'),\n",
    "            'val_start': pd.Timestamp('2022-11-01'),\n",
    "            'val_end': pd.Timestamp('2023-10-31'),\n",
    "            'test_start': pd.Timestamp('2023-11-01'),\n",
    "            'test_end': pd.Timestamp('2024-11-01')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data['Merged_embedding'].values)\n",
    "        X_val = np.stack(val_data['Merged_embedding'].values)\n",
    "        X_test = np.stack(test_data['Merged_embedding'].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_mlp_merged_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, label_col):\n",
    "        \"\"\"\n",
    "        Train and evaluate an MLP model for merged embeddings and a specific label column.\n",
    "        \n",
    "        Args:\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        combination_key = f\"MLP|Merged|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training MLP model for Merged Embeddings and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            model = self.create_mlp_model((1536,))\n",
    "            print(f\"Created MLP model for Merged Embeddings ({label_col})\")\n",
    "            model.summary()\n",
    "            \n",
    "            # Setup callbacks\n",
    "            plot_callback = PlotLearningCallback('Merged', label_col, i+1, self.mlp_viz_dir)\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=15,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            model_checkpoint = ModelCheckpoint(\n",
    "                filepath=f\"{self.mlp_viz_dir}/best_mlp_merged_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                monitor='val_loss',\n",
    "                save_weights_only=True,\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            print(f\"Training MLP model...\")\n",
    "            batch_size = 32  # Larger batch size for embeddings\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred_proba = model.predict(X_test)\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "            \n",
    "            # Create final learning curve visualization\n",
    "            self.plot_final_learning_curves(history, label_col, i+1, self.mlp_viz_dir)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': history.history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for merged embeddings with both short-term and long-term labels.\"\"\"\n",
    "        # Define label columns\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        \n",
    "        # Run analysis for each label column\n",
    "        for label_col in label_cols:\n",
    "            self.train_and_evaluate_model(label_col)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS (XOM)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Performance comparison for MLP with merged embeddings\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Plot accuracy and AUC bars\n",
    "        x = np.arange(len(df))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, df['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, df['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with Merged OpenAI Embeddings')\n",
    "        labels = [f\"Merged + {row['Label']}\" for _, row in df.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(df['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(df['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(df['Avg Accuracy'].max(), df['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/XOM', \"mlp_merged_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                layer_data.append(layer_info)\n",
    "        \n",
    "        if not layer_data:\n",
    "            print(\"No layer data available\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title('MLP Accuracy by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title('MLP AUC by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/XOM', \"mlp_merged_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['Merged_OpenAI_MLP/visualizations_mlp/XOM', 'Merged_OpenAI_MLP/visualizations_summary/XOM']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/wall_street_news_title_full_text_SP500_database/semantic/wall_street_news_semantics_XOM_completed_openai_Merged.csv'\n",
    "    \n",
    "    # Initialize the predictor with merged OpenAI embeddings\n",
    "    predictor = MergedEmbeddingMLPPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing OpenAI embedding vectors from string format...\n",
      "Sample Title embedding shape: (1536,)\n",
      "Sample Fulltext embedding shape: (1536,)\n",
      "Loaded 929 climate change news articles spanning from 02/01/2019 to 07/05/2023\n",
      "Class distribution for short-term prediction: {0: 466, 1: 463}\n",
      "Class distribution for long-term prediction: {1: 507, 0: 422}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 512)               2048      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.8681 - accuracy: 0.5130\n",
      "Epoch 1: val_loss improved from inf to 0.69236, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 26ms/step - loss: 0.8658 - accuracy: 0.5249 - val_loss: 0.6924 - val_accuracy: 0.5031\n",
      "Epoch 2/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.8240 - accuracy: 0.5130\n",
      "Epoch 2: val_loss did not improve from 0.69236\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.8307 - accuracy: 0.5172 - val_loss: 0.6928 - val_accuracy: 0.5031\n",
      "Epoch 3/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.7604 - accuracy: 0.5673\n",
      "Epoch 3: val_loss did not improve from 0.69236\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.7532 - accuracy: 0.5709 - val_loss: 0.6930 - val_accuracy: 0.5031\n",
      "Epoch 4/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.7360 - accuracy: 0.5769\n",
      "Epoch 4: val_loss did not improve from 0.69236\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.7189 - accuracy: 0.5900 - val_loss: 0.6933 - val_accuracy: 0.5031\n",
      "Epoch 5/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6375 - accuracy: 0.6490\n",
      "Epoch 5: val_loss did not improve from 0.69236\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6253 - accuracy: 0.6590 - val_loss: 0.6933 - val_accuracy: 0.5031\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6232 - accuracy: 0.6659\n",
      "Epoch 6: val_loss did not improve from 0.69236\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6072 - accuracy: 0.6724 - val_loss: 0.6929 - val_accuracy: 0.5031\n",
      "Epoch 7/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5846 - accuracy: 0.6995\n",
      "Epoch 7: val_loss did not improve from 0.69236\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5818 - accuracy: 0.6973 - val_loss: 0.6929 - val_accuracy: 0.4969\n",
      "Epoch 8/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5391 - accuracy: 0.7260\n",
      "Epoch 8: val_loss did not improve from 0.69236\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5322 - accuracy: 0.7241 - val_loss: 0.6926 - val_accuracy: 0.4908\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5412 - accuracy: 0.7284\n",
      "Epoch 9: val_loss improved from 0.69236 to 0.69210, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5380 - accuracy: 0.7184 - val_loss: 0.6921 - val_accuracy: 0.5092\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5089 - accuracy: 0.7422\n",
      "Epoch 10: val_loss improved from 0.69210 to 0.69184, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5071 - accuracy: 0.7433 - val_loss: 0.6918 - val_accuracy: 0.5276\n",
      "Epoch 11/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4604 - accuracy: 0.7917\n",
      "Epoch 11: val_loss improved from 0.69184 to 0.69148, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4726 - accuracy: 0.7835 - val_loss: 0.6915 - val_accuracy: 0.5337\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4618 - accuracy: 0.7893\n",
      "Epoch 12: val_loss improved from 0.69148 to 0.69135, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4618 - accuracy: 0.7893 - val_loss: 0.6913 - val_accuracy: 0.5460\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4462 - accuracy: 0.8101\n",
      "Epoch 13: val_loss did not improve from 0.69135\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4615 - accuracy: 0.7912 - val_loss: 0.6917 - val_accuracy: 0.5460\n",
      "Epoch 14/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4388 - accuracy: 0.7957\n",
      "Epoch 14: val_loss did not improve from 0.69135\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4421 - accuracy: 0.7912 - val_loss: 0.6921 - val_accuracy: 0.5215\n",
      "Epoch 15/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4098 - accuracy: 0.8269\n",
      "Epoch 15: val_loss did not improve from 0.69135\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4228 - accuracy: 0.8180 - val_loss: 0.6923 - val_accuracy: 0.5337\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4131 - accuracy: 0.8125\n",
      "Epoch 16: val_loss did not improve from 0.69135\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4193 - accuracy: 0.8084 - val_loss: 0.6925 - val_accuracy: 0.5276\n",
      "Epoch 17/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3772 - accuracy: 0.8438\n",
      "Epoch 17: val_loss did not improve from 0.69135\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3740 - accuracy: 0.8429 - val_loss: 0.6930 - val_accuracy: 0.5337\n",
      "Epoch 18/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3766 - accuracy: 0.8486\n",
      "Epoch 18: val_loss did not improve from 0.69135\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3860 - accuracy: 0.8391 - val_loss: 0.6937 - val_accuracy: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3557 - accuracy: 0.8822\n",
      "Epoch 19: val_loss did not improve from 0.69135\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3567 - accuracy: 0.8793 - val_loss: 0.6934 - val_accuracy: 0.5153\n",
      "Epoch 20/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3444 - accuracy: 0.8726\n",
      "Epoch 20: val_loss did not improve from 0.69135\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3392 - accuracy: 0.8755 - val_loss: 0.6931 - val_accuracy: 0.5276\n",
      "Epoch 21/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3272 - accuracy: 0.8870\n",
      "Epoch 21: val_loss did not improve from 0.69135\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3176 - accuracy: 0.8870 - val_loss: 0.6926 - val_accuracy: 0.5460\n",
      "Epoch 22/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3080 - accuracy: 0.8894\n",
      "Epoch 22: val_loss improved from 0.69135 to 0.69077, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.3133 - accuracy: 0.8793 - val_loss: 0.6908 - val_accuracy: 0.5644\n",
      "Epoch 23/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2901 - accuracy: 0.8984\n",
      "Epoch 23: val_loss improved from 0.69077 to 0.68955, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.3031 - accuracy: 0.8870 - val_loss: 0.6895 - val_accuracy: 0.5644\n",
      "Epoch 24/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2788 - accuracy: 0.9207\n",
      "Epoch 24: val_loss did not improve from 0.68955\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2901 - accuracy: 0.9061 - val_loss: 0.6905 - val_accuracy: 0.5399\n",
      "Epoch 25/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2839 - accuracy: 0.9014\n",
      "Epoch 25: val_loss did not improve from 0.68955\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2772 - accuracy: 0.9100 - val_loss: 0.6905 - val_accuracy: 0.5337\n",
      "Epoch 26/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2510 - accuracy: 0.9159\n",
      "Epoch 26: val_loss did not improve from 0.68955\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2534 - accuracy: 0.9176 - val_loss: 0.6928 - val_accuracy: 0.5337\n",
      "Epoch 27/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2701 - accuracy: 0.9111\n",
      "Epoch 27: val_loss did not improve from 0.68955\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2743 - accuracy: 0.9061 - val_loss: 0.6967 - val_accuracy: 0.5521\n",
      "Epoch 28/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2262 - accuracy: 0.9399\n",
      "Epoch 28: val_loss did not improve from 0.68955\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2399 - accuracy: 0.9291 - val_loss: 0.7000 - val_accuracy: 0.5399\n",
      "Epoch 29/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2434 - accuracy: 0.9062\n",
      "Epoch 29: val_loss did not improve from 0.68955\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2372 - accuracy: 0.9157 - val_loss: 0.7028 - val_accuracy: 0.5276\n",
      "Epoch 30/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2209 - accuracy: 0.9495\n",
      "Epoch 30: val_loss did not improve from 0.68955\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2291 - accuracy: 0.9425 - val_loss: 0.7053 - val_accuracy: 0.5337\n",
      "Epoch 31/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2002 - accuracy: 0.9591\n",
      "Epoch 31: val_loss did not improve from 0.68955\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2057 - accuracy: 0.9579 - val_loss: 0.7091 - val_accuracy: 0.5276\n",
      "Epoch 32/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2218 - accuracy: 0.9375\n",
      "Epoch 32: val_loss did not improve from 0.68955\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2190 - accuracy: 0.9406 - val_loss: 0.7162 - val_accuracy: 0.5337\n",
      "Epoch 33/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2148 - accuracy: 0.9135\n",
      "Epoch 33: val_loss did not improve from 0.68955\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2137 - accuracy: 0.9176 - val_loss: 0.7225 - val_accuracy: 0.5399\n",
      "Epoch 34/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.1988 - accuracy: 0.9479\n",
      "Epoch 34: val_loss did not improve from 0.68955\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.1941 - accuracy: 0.9502 - val_loss: 0.7290 - val_accuracy: 0.5153\n",
      "Epoch 35/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.1730 - accuracy: 0.9688\n",
      "Epoch 35: val_loss did not improve from 0.68955\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.1726 - accuracy: 0.9693 - val_loss: 0.7391 - val_accuracy: 0.5276\n",
      "Epoch 36/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.1791 - accuracy: 0.9591\n",
      "Epoch 36: val_loss did not improve from 0.68955\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.1766 - accuracy: 0.9579 - val_loss: 0.7533 - val_accuracy: 0.5337\n",
      "Epoch 37/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.1902 - accuracy: 0.9399\n",
      "Epoch 37: val_loss did not improve from 0.68955\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.1971 - accuracy: 0.9349 - val_loss: 0.7629 - val_accuracy: 0.5276\n",
      "Epoch 38/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.1682 - accuracy: 0.9688Restoring model weights from the end of the best epoch: 23.\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.68955\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.1678 - accuracy: 0.9713 - val_loss: 0.7750 - val_accuracy: 0.5276\n",
      "Epoch 38: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.4848\n",
      "Test AUC for Layer 1: 0.4654\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8577 - accuracy: 0.5164\n",
      "Epoch 1: val_loss improved from inf to 0.67951, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 20ms/step - loss: 0.8621 - accuracy: 0.5124 - val_loss: 0.6795 - val_accuracy: 0.6212\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8016 - accuracy: 0.5504\n",
      "Epoch 2: val_loss improved from 0.67951 to 0.67366, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.8016 - accuracy: 0.5504 - val_loss: 0.6737 - val_accuracy: 0.6212\n",
      "Epoch 3/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6945 - accuracy: 0.6007\n",
      "Epoch 3: val_loss improved from 0.67366 to 0.67100, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6784 - accuracy: 0.6146 - val_loss: 0.6710 - val_accuracy: 0.6212\n",
      "Epoch 4/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6904 - accuracy: 0.6163\n",
      "Epoch 4: val_loss improved from 0.67100 to 0.66970, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6930 - accuracy: 0.6102 - val_loss: 0.6697 - val_accuracy: 0.6212\n",
      "Epoch 5/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6706 - accuracy: 0.6458\n",
      "Epoch 5: val_loss did not improve from 0.66970\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6622 - accuracy: 0.6453 - val_loss: 0.6697 - val_accuracy: 0.6212\n",
      "Epoch 6/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.6094 - accuracy: 0.6801\n",
      "Epoch 6: val_loss did not improve from 0.66970\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5998 - accuracy: 0.6832 - val_loss: 0.6704 - val_accuracy: 0.6212\n",
      "Epoch 7/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5581 - accuracy: 0.7105\n",
      "Epoch 7: val_loss did not improve from 0.66970\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5673 - accuracy: 0.7007 - val_loss: 0.6716 - val_accuracy: 0.6212\n",
      "Epoch 8/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5232 - accuracy: 0.7401\n",
      "Epoch 8: val_loss did not improve from 0.66970\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5270 - accuracy: 0.7372 - val_loss: 0.6721 - val_accuracy: 0.6212\n",
      "Epoch 9/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.5436 - accuracy: 0.7250\n",
      "Epoch 9: val_loss did not improve from 0.66970\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5477 - accuracy: 0.7241 - val_loss: 0.6741 - val_accuracy: 0.5909\n",
      "Epoch 10/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5059 - accuracy: 0.7484\n",
      "Epoch 10: val_loss did not improve from 0.66970\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5083 - accuracy: 0.7460 - val_loss: 0.6758 - val_accuracy: 0.6061\n",
      "Epoch 11/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4686 - accuracy: 0.7847\n",
      "Epoch 11: val_loss did not improve from 0.66970\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4653 - accuracy: 0.7839 - val_loss: 0.6776 - val_accuracy: 0.5758\n",
      "Epoch 12/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4667 - accuracy: 0.7862\n",
      "Epoch 12: val_loss did not improve from 0.66970\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4654 - accuracy: 0.7825 - val_loss: 0.6802 - val_accuracy: 0.5606\n",
      "Epoch 13/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4188 - accuracy: 0.8224\n",
      "Epoch 13: val_loss did not improve from 0.66970\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4224 - accuracy: 0.8204 - val_loss: 0.6807 - val_accuracy: 0.5455\n",
      "Epoch 14/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4210 - accuracy: 0.8388\n",
      "Epoch 14: val_loss did not improve from 0.66970\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4184 - accuracy: 0.8409 - val_loss: 0.6810 - val_accuracy: 0.5606\n",
      "Epoch 15/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3918 - accuracy: 0.8454\n",
      "Epoch 15: val_loss did not improve from 0.66970\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3878 - accuracy: 0.8482 - val_loss: 0.6826 - val_accuracy: 0.5606\n",
      "Epoch 16/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3806 - accuracy: 0.8454\n",
      "Epoch 16: val_loss did not improve from 0.66970\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3828 - accuracy: 0.8423 - val_loss: 0.6858 - val_accuracy: 0.5303\n",
      "Epoch 17/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3777 - accuracy: 0.8503\n",
      "Epoch 17: val_loss did not improve from 0.66970\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3750 - accuracy: 0.8511 - val_loss: 0.6891 - val_accuracy: 0.5455\n",
      "Epoch 18/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3296 - accuracy: 0.8717\n",
      "Epoch 18: val_loss did not improve from 0.66970\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3296 - accuracy: 0.8715 - val_loss: 0.6941 - val_accuracy: 0.5606\n",
      "Epoch 19/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3363 - accuracy: 0.8717Restoring model weights from the end of the best epoch: 4.\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.66970\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3307 - accuracy: 0.8788 - val_loss: 0.6992 - val_accuracy: 0.5303\n",
      "Epoch 19: early stopping\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5648\n",
      "Test AUC for Layer 2: 0.4855\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.8787 - accuracy: 0.5027\n",
      "Epoch 1: val_loss improved from inf to 0.70933, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_S_label_layer_3.weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 2s 20ms/step - loss: 0.8777 - accuracy: 0.5060 - val_loss: 0.7093 - val_accuracy: 0.4352\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.8336 - accuracy: 0.5166\n",
      "Epoch 2: val_loss did not improve from 0.70933\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.8336 - accuracy: 0.5166 - val_loss: 0.7306 - val_accuracy: 0.4352\n",
      "Epoch 3/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.7635 - accuracy: 0.5609\n",
      "Epoch 3: val_loss did not improve from 0.70933\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.7692 - accuracy: 0.5539 - val_loss: 0.7558 - val_accuracy: 0.4352\n",
      "Epoch 4/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6678 - accuracy: 0.6382\n",
      "Epoch 4: val_loss did not improve from 0.70933\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.6764 - accuracy: 0.6338 - val_loss: 0.7823 - val_accuracy: 0.4352\n",
      "Epoch 5/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6149 - accuracy: 0.6562\n",
      "Epoch 5: val_loss did not improve from 0.70933\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6274 - accuracy: 0.6525 - val_loss: 0.8016 - val_accuracy: 0.4352\n",
      "Epoch 6/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6115 - accuracy: 0.6711\n",
      "Epoch 6: val_loss did not improve from 0.70933\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.6219 - accuracy: 0.6684 - val_loss: 0.8184 - val_accuracy: 0.4352\n",
      "Epoch 7/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5759 - accuracy: 0.7072\n",
      "Epoch 7: val_loss did not improve from 0.70933\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5800 - accuracy: 0.7044 - val_loss: 0.8365 - val_accuracy: 0.4352\n",
      "Epoch 8/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5725 - accuracy: 0.6908\n",
      "Epoch 8: val_loss did not improve from 0.70933\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5658 - accuracy: 0.6937 - val_loss: 0.8518 - val_accuracy: 0.4352\n",
      "Epoch 9/100\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.5099 - accuracy: 0.7563\n",
      "Epoch 9: val_loss did not improve from 0.70933\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5206 - accuracy: 0.7457 - val_loss: 0.8608 - val_accuracy: 0.4352\n",
      "Epoch 10/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5109 - accuracy: 0.7467\n",
      "Epoch 10: val_loss did not improve from 0.70933\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5059 - accuracy: 0.7417 - val_loss: 0.8628 - val_accuracy: 0.4352\n",
      "Epoch 11/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4912 - accuracy: 0.7615\n",
      "Epoch 11: val_loss did not improve from 0.70933\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4771 - accuracy: 0.7710 - val_loss: 0.8636 - val_accuracy: 0.4352\n",
      "Epoch 12/100\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.5011 - accuracy: 0.7578\n",
      "Epoch 12: val_loss did not improve from 0.70933\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4976 - accuracy: 0.7577 - val_loss: 0.8639 - val_accuracy: 0.4352\n",
      "Epoch 13/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4389 - accuracy: 0.8257\n",
      "Epoch 13: val_loss did not improve from 0.70933\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4418 - accuracy: 0.8162 - val_loss: 0.8585 - val_accuracy: 0.4352\n",
      "Epoch 14/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4566 - accuracy: 0.7961\n",
      "Epoch 14: val_loss did not improve from 0.70933\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4555 - accuracy: 0.7909 - val_loss: 0.8594 - val_accuracy: 0.4352\n",
      "Epoch 15/100\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.4123 - accuracy: 0.8172\n",
      "Epoch 15: val_loss did not improve from 0.70933\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4029 - accuracy: 0.8242 - val_loss: 0.8637 - val_accuracy: 0.4352\n",
      "Epoch 16/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3889 - accuracy: 0.8536Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70933\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3920 - accuracy: 0.8495 - val_loss: 0.8603 - val_accuracy: 0.4352\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4928\n",
      "Test AUC for Layer 3: 0.5975\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5141\n",
      "Average Test AUC across all layers: 0.5161\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9679 - accuracy: 0.4866\n",
      "Epoch 1: val_loss improved from inf to 0.69669, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 24ms/step - loss: 0.9679 - accuracy: 0.4866 - val_loss: 0.6967 - val_accuracy: 0.5031\n",
      "Epoch 2/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.8810 - accuracy: 0.5208\n",
      "Epoch 2: val_loss did not improve from 0.69669\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.8728 - accuracy: 0.5326 - val_loss: 0.7041 - val_accuracy: 0.5031\n",
      "Epoch 3/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.7955 - accuracy: 0.5409\n",
      "Epoch 3: val_loss did not improve from 0.69669\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.8059 - accuracy: 0.5421 - val_loss: 0.7137 - val_accuracy: 0.5031\n",
      "Epoch 4/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.7993 - accuracy: 0.5649\n",
      "Epoch 4: val_loss did not improve from 0.69669\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.7875 - accuracy: 0.5766 - val_loss: 0.7241 - val_accuracy: 0.5031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.7181 - accuracy: 0.6042\n",
      "Epoch 5: val_loss did not improve from 0.69669\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7014 - accuracy: 0.6169 - val_loss: 0.7316 - val_accuracy: 0.5031\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5988 - accuracy: 0.6683\n",
      "Epoch 6: val_loss did not improve from 0.69669\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5995 - accuracy: 0.6762 - val_loss: 0.7389 - val_accuracy: 0.5031\n",
      "Epoch 7/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5914 - accuracy: 0.6923\n",
      "Epoch 7: val_loss did not improve from 0.69669\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5777 - accuracy: 0.7088 - val_loss: 0.7458 - val_accuracy: 0.5031\n",
      "Epoch 8/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5737 - accuracy: 0.7067\n",
      "Epoch 8: val_loss did not improve from 0.69669\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5807 - accuracy: 0.7031 - val_loss: 0.7506 - val_accuracy: 0.5031\n",
      "Epoch 9/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5335 - accuracy: 0.7396\n",
      "Epoch 9: val_loss did not improve from 0.69669\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5233 - accuracy: 0.7433 - val_loss: 0.7521 - val_accuracy: 0.5031\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5280 - accuracy: 0.7740\n",
      "Epoch 10: val_loss did not improve from 0.69669\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5081 - accuracy: 0.7797 - val_loss: 0.7564 - val_accuracy: 0.5031\n",
      "Epoch 11/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.4395 - accuracy: 0.7879\n",
      "Epoch 11: val_loss did not improve from 0.69669\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4590 - accuracy: 0.7720 - val_loss: 0.7585 - val_accuracy: 0.5031\n",
      "Epoch 12/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4789 - accuracy: 0.7356\n",
      "Epoch 12: val_loss did not improve from 0.69669\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4774 - accuracy: 0.7433 - val_loss: 0.7579 - val_accuracy: 0.5031\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4494 - accuracy: 0.7885\n",
      "Epoch 13: val_loss did not improve from 0.69669\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4478 - accuracy: 0.7893 - val_loss: 0.7602 - val_accuracy: 0.5031\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4236 - accuracy: 0.7910\n",
      "Epoch 14: val_loss did not improve from 0.69669\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4217 - accuracy: 0.7912 - val_loss: 0.7585 - val_accuracy: 0.5031\n",
      "Epoch 15/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3999 - accuracy: 0.8413\n",
      "Epoch 15: val_loss did not improve from 0.69669\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3817 - accuracy: 0.8467 - val_loss: 0.7619 - val_accuracy: 0.5031\n",
      "Epoch 16/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.3583 - accuracy: 0.8527Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69669\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3643 - accuracy: 0.8372 - val_loss: 0.7649 - val_accuracy: 0.5031\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.1364\n",
      "Test AUC for Layer 1: 0.4639\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8621 - accuracy: 0.5036\n",
      "Epoch 1: val_loss improved from inf to 0.63242, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 21ms/step - loss: 0.8621 - accuracy: 0.5036 - val_loss: 0.6324 - val_accuracy: 0.8636\n",
      "Epoch 2/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.7848 - accuracy: 0.5460\n",
      "Epoch 2: val_loss improved from 0.63242 to 0.59914, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.7783 - accuracy: 0.5401 - val_loss: 0.5991 - val_accuracy: 0.8636\n",
      "Epoch 3/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.7270 - accuracy: 0.6232\n",
      "Epoch 3: val_loss improved from 0.59914 to 0.57462, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.7238 - accuracy: 0.6175 - val_loss: 0.5746 - val_accuracy: 0.8636\n",
      "Epoch 4/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6903 - accuracy: 0.6250\n",
      "Epoch 4: val_loss improved from 0.57462 to 0.55813, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6771 - accuracy: 0.6380 - val_loss: 0.5581 - val_accuracy: 0.8636\n",
      "Epoch 5/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6988 - accuracy: 0.6146\n",
      "Epoch 5: val_loss improved from 0.55813 to 0.54702, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6912 - accuracy: 0.6161 - val_loss: 0.5470 - val_accuracy: 0.8636\n",
      "Epoch 6/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6047 - accuracy: 0.6910\n",
      "Epoch 6: val_loss improved from 0.54702 to 0.54132, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.5964 - accuracy: 0.6905 - val_loss: 0.5413 - val_accuracy: 0.8636\n",
      "Epoch 7/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5770 - accuracy: 0.7022\n",
      "Epoch 7: val_loss did not improve from 0.54132\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5824 - accuracy: 0.7022 - val_loss: 0.5414 - val_accuracy: 0.8636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5326 - accuracy: 0.7385\n",
      "Epoch 8: val_loss improved from 0.54132 to 0.53789, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.5523 - accuracy: 0.7270 - val_loss: 0.5379 - val_accuracy: 0.8636\n",
      "Epoch 9/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.5151 - accuracy: 0.7266\n",
      "Epoch 9: val_loss did not improve from 0.53789\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5159 - accuracy: 0.7241 - val_loss: 0.5390 - val_accuracy: 0.8636\n",
      "Epoch 10/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4819 - accuracy: 0.7747\n",
      "Epoch 10: val_loss did not improve from 0.53789\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4801 - accuracy: 0.7752 - val_loss: 0.5398 - val_accuracy: 0.8636\n",
      "Epoch 11/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.4655 - accuracy: 0.7688\n",
      "Epoch 11: val_loss did not improve from 0.53789\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4663 - accuracy: 0.7664 - val_loss: 0.5440 - val_accuracy: 0.8636\n",
      "Epoch 12/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4386 - accuracy: 0.8158\n",
      "Epoch 12: val_loss did not improve from 0.53789\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4390 - accuracy: 0.8161 - val_loss: 0.5485 - val_accuracy: 0.8636\n",
      "Epoch 13/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4447 - accuracy: 0.8038\n",
      "Epoch 13: val_loss did not improve from 0.53789\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4471 - accuracy: 0.8015 - val_loss: 0.5494 - val_accuracy: 0.8636\n",
      "Epoch 14/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.3957 - accuracy: 0.8313\n",
      "Epoch 14: val_loss did not improve from 0.53789\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3980 - accuracy: 0.8350 - val_loss: 0.5507 - val_accuracy: 0.8636\n",
      "Epoch 15/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3778 - accuracy: 0.8536\n",
      "Epoch 15: val_loss did not improve from 0.53789\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3739 - accuracy: 0.8569 - val_loss: 0.5471 - val_accuracy: 0.8636\n",
      "Epoch 16/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.3699 - accuracy: 0.8562\n",
      "Epoch 16: val_loss did not improve from 0.53789\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3703 - accuracy: 0.8555 - val_loss: 0.5446 - val_accuracy: 0.8485\n",
      "Epoch 17/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3518 - accuracy: 0.8524\n",
      "Epoch 17: val_loss did not improve from 0.53789\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3547 - accuracy: 0.8555 - val_loss: 0.5461 - val_accuracy: 0.8182\n",
      "Epoch 18/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3401 - accuracy: 0.8635\n",
      "Epoch 18: val_loss did not improve from 0.53789\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3422 - accuracy: 0.8584 - val_loss: 0.5457 - val_accuracy: 0.8333\n",
      "Epoch 19/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.3381 - accuracy: 0.8646\n",
      "Epoch 19: val_loss did not improve from 0.53789\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3392 - accuracy: 0.8613 - val_loss: 0.5452 - val_accuracy: 0.8333\n",
      "Epoch 20/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3122 - accuracy: 0.8717\n",
      "Epoch 20: val_loss did not improve from 0.53789\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3100 - accuracy: 0.8774 - val_loss: 0.5526 - val_accuracy: 0.7879\n",
      "Epoch 21/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2709 - accuracy: 0.8947\n",
      "Epoch 21: val_loss did not improve from 0.53789\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.2739 - accuracy: 0.8934 - val_loss: 0.5599 - val_accuracy: 0.8030\n",
      "Epoch 22/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.2533 - accuracy: 0.9219\n",
      "Epoch 22: val_loss did not improve from 0.53789\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2671 - accuracy: 0.9095 - val_loss: 0.5656 - val_accuracy: 0.7424\n",
      "Epoch 23/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.2731 - accuracy: 0.9080Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.53789\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2607 - accuracy: 0.9109 - val_loss: 0.5746 - val_accuracy: 0.7121\n",
      "Epoch 23: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.6667\n",
      "Test AUC for Layer 2: 0.4965\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.9009 - accuracy: 0.4792\n",
      "Epoch 1: val_loss improved from inf to 0.67977, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.8983 - accuracy: 0.4767 - val_loss: 0.6798 - val_accuracy: 0.6667\n",
      "Epoch 2/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.8323 - accuracy: 0.5156\n",
      "Epoch 2: val_loss improved from 0.67977 to 0.67045, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.8361 - accuracy: 0.5206 - val_loss: 0.6704 - val_accuracy: 0.6667\n",
      "Epoch 3/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.7141 - accuracy: 0.5965\n",
      "Epoch 3: val_loss improved from 0.67045 to 0.66285, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7094 - accuracy: 0.5992 - val_loss: 0.6629 - val_accuracy: 0.6667\n",
      "Epoch 4/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6531 - accuracy: 0.6386\n",
      "Epoch 4: val_loss improved from 0.66285 to 0.65793, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6529 - accuracy: 0.6391 - val_loss: 0.6579 - val_accuracy: 0.6667\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6524 - accuracy: 0.6498\n",
      "Epoch 5: val_loss improved from 0.65793 to 0.65341, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6524 - accuracy: 0.6498 - val_loss: 0.6534 - val_accuracy: 0.6667\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6147 - accuracy: 0.6658\n",
      "Epoch 6: val_loss improved from 0.65341 to 0.65039, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6147 - accuracy: 0.6658 - val_loss: 0.6504 - val_accuracy: 0.6667\n",
      "Epoch 7/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.5790 - accuracy: 0.6960\n",
      "Epoch 7: val_loss improved from 0.65039 to 0.64890, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.5851 - accuracy: 0.6897 - val_loss: 0.6489 - val_accuracy: 0.6667\n",
      "Epoch 8/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5627 - accuracy: 0.7158\n",
      "Epoch 8: val_loss improved from 0.64890 to 0.64716, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.5695 - accuracy: 0.7057 - val_loss: 0.6472 - val_accuracy: 0.6667\n",
      "Epoch 9/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5434 - accuracy: 0.7133\n",
      "Epoch 9: val_loss improved from 0.64716 to 0.64617, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.5450 - accuracy: 0.7097 - val_loss: 0.6462 - val_accuracy: 0.6667\n",
      "Epoch 10/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5145 - accuracy: 0.7446\n",
      "Epoch 10: val_loss improved from 0.64617 to 0.64548, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.5145 - accuracy: 0.7457 - val_loss: 0.6455 - val_accuracy: 0.6667\n",
      "Epoch 11/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4767 - accuracy: 0.7785\n",
      "Epoch 11: val_loss improved from 0.64548 to 0.64481, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.4782 - accuracy: 0.7776 - val_loss: 0.6448 - val_accuracy: 0.6667\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4582 - accuracy: 0.8029\n",
      "Epoch 12: val_loss did not improve from 0.64481\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4582 - accuracy: 0.8029 - val_loss: 0.6449 - val_accuracy: 0.6759\n",
      "Epoch 13/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4342 - accuracy: 0.8043\n",
      "Epoch 13: val_loss did not improve from 0.64481\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4318 - accuracy: 0.8069 - val_loss: 0.6453 - val_accuracy: 0.6759\n",
      "Epoch 14/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4164 - accuracy: 0.8240\n",
      "Epoch 14: val_loss did not improve from 0.64481\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4296 - accuracy: 0.8216 - val_loss: 0.6453 - val_accuracy: 0.6759\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3989 - accuracy: 0.8336\n",
      "Epoch 15: val_loss did not improve from 0.64481\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3989 - accuracy: 0.8336 - val_loss: 0.6461 - val_accuracy: 0.6759\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3775 - accuracy: 0.8628\n",
      "Epoch 16: val_loss did not improve from 0.64481\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3775 - accuracy: 0.8628 - val_loss: 0.6458 - val_accuracy: 0.6574\n",
      "Epoch 17/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3341 - accuracy: 0.8766\n",
      "Epoch 17: val_loss did not improve from 0.64481\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3378 - accuracy: 0.8708 - val_loss: 0.6457 - val_accuracy: 0.6389\n",
      "Epoch 18/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3225 - accuracy: 0.8865\n",
      "Epoch 18: val_loss did not improve from 0.64481\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3390 - accuracy: 0.8708 - val_loss: 0.6454 - val_accuracy: 0.6389\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3330 - accuracy: 0.8762\n",
      "Epoch 19: val_loss improved from 0.64481 to 0.64267, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.3330 - accuracy: 0.8762 - val_loss: 0.6427 - val_accuracy: 0.6204\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3136 - accuracy: 0.8868\n",
      "Epoch 20: val_loss did not improve from 0.64267\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3136 - accuracy: 0.8868 - val_loss: 0.6434 - val_accuracy: 0.6111\n",
      "Epoch 21/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3083 - accuracy: 0.8865\n",
      "Epoch 21: val_loss did not improve from 0.64267\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3132 - accuracy: 0.8815 - val_loss: 0.6432 - val_accuracy: 0.6296\n",
      "Epoch 22/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2732 - accuracy: 0.9030\n",
      "Epoch 22: val_loss did not improve from 0.64267\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2814 - accuracy: 0.8988 - val_loss: 0.6442 - val_accuracy: 0.6111\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2705 - accuracy: 0.9134\n",
      "Epoch 23: val_loss did not improve from 0.64267\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2705 - accuracy: 0.9134 - val_loss: 0.6496 - val_accuracy: 0.6019\n",
      "Epoch 24/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2737 - accuracy: 0.9194\n",
      "Epoch 24: val_loss did not improve from 0.64267\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2662 - accuracy: 0.9214 - val_loss: 0.6570 - val_accuracy: 0.5648\n",
      "Epoch 25/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2553 - accuracy: 0.9211\n",
      "Epoch 25: val_loss did not improve from 0.64267\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2698 - accuracy: 0.9134 - val_loss: 0.6640 - val_accuracy: 0.5741\n",
      "Epoch 26/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2613 - accuracy: 0.9145\n",
      "Epoch 26: val_loss did not improve from 0.64267\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2690 - accuracy: 0.9068 - val_loss: 0.6715 - val_accuracy: 0.5648\n",
      "Epoch 27/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.2078 - accuracy: 0.9449\n",
      "Epoch 27: val_loss did not improve from 0.64267\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.2122 - accuracy: 0.9414 - val_loss: 0.6867 - val_accuracy: 0.5556\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2176 - accuracy: 0.9414\n",
      "Epoch 28: val_loss did not improve from 0.64267\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2176 - accuracy: 0.9414 - val_loss: 0.7022 - val_accuracy: 0.5556\n",
      "Epoch 29/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.1922 - accuracy: 0.9605\n",
      "Epoch 29: val_loss did not improve from 0.64267\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2011 - accuracy: 0.9521 - val_loss: 0.7236 - val_accuracy: 0.5648\n",
      "Epoch 30/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.2014 - accuracy: 0.9474\n",
      "Epoch 30: val_loss did not improve from 0.64267\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2065 - accuracy: 0.9454 - val_loss: 0.7367 - val_accuracy: 0.5648\n",
      "Epoch 31/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.1956 - accuracy: 0.9517\n",
      "Epoch 31: val_loss did not improve from 0.64267\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1957 - accuracy: 0.9521 - val_loss: 0.7420 - val_accuracy: 0.5741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2034 - accuracy: 0.9457\n",
      "Epoch 32: val_loss did not improve from 0.64267\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2023 - accuracy: 0.9454 - val_loss: 0.7693 - val_accuracy: 0.5833\n",
      "Epoch 33/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.1886 - accuracy: 0.9523\n",
      "Epoch 33: val_loss did not improve from 0.64267\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1888 - accuracy: 0.9547 - val_loss: 0.7890 - val_accuracy: 0.5926\n",
      "Epoch 34/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.1645 - accuracy: 0.9671Restoring model weights from the end of the best epoch: 19.\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.64267\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1765 - accuracy: 0.9587 - val_loss: 0.8088 - val_accuracy: 0.5926\n",
      "Epoch 34: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.3768\n",
      "Test AUC for Layer 3: 0.4832\n",
      "\n",
      "Average Test Accuracy across all layers: 0.3933\n",
      "Average Test AUC across all layers: 0.4812\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_18 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_19 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_20 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.9080 - accuracy: 0.4479\n",
      "Epoch 1: val_loss improved from inf to 0.69404, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 23ms/step - loss: 0.8951 - accuracy: 0.4579 - val_loss: 0.6940 - val_accuracy: 0.4908\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8042 - accuracy: 0.5479\n",
      "Epoch 2: val_loss did not improve from 0.69404\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.8042 - accuracy: 0.5479 - val_loss: 0.6950 - val_accuracy: 0.4969\n",
      "Epoch 3/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.7454 - accuracy: 0.5553\n",
      "Epoch 3: val_loss did not improve from 0.69404\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7684 - accuracy: 0.5517 - val_loss: 0.6963 - val_accuracy: 0.4969\n",
      "Epoch 4/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.7075 - accuracy: 0.5703\n",
      "Epoch 4: val_loss did not improve from 0.69404\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7023 - accuracy: 0.5881 - val_loss: 0.6985 - val_accuracy: 0.4969\n",
      "Epoch 5/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6857 - accuracy: 0.6058\n",
      "Epoch 5: val_loss did not improve from 0.69404\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6796 - accuracy: 0.6073 - val_loss: 0.6999 - val_accuracy: 0.4969\n",
      "Epoch 6/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6630 - accuracy: 0.6589\n",
      "Epoch 6: val_loss did not improve from 0.69404\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6709 - accuracy: 0.6437 - val_loss: 0.7021 - val_accuracy: 0.4969\n",
      "Epoch 7/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6355 - accuracy: 0.6538\n",
      "Epoch 7: val_loss did not improve from 0.69404\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6224 - accuracy: 0.6686 - val_loss: 0.7049 - val_accuracy: 0.4969\n",
      "Epoch 8/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5763 - accuracy: 0.6947\n",
      "Epoch 8: val_loss did not improve from 0.69404\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5757 - accuracy: 0.6954 - val_loss: 0.7073 - val_accuracy: 0.4969\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5559 - accuracy: 0.7236\n",
      "Epoch 9: val_loss did not improve from 0.69404\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5508 - accuracy: 0.7261 - val_loss: 0.7096 - val_accuracy: 0.4969\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5131 - accuracy: 0.7596\n",
      "Epoch 10: val_loss did not improve from 0.69404\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5064 - accuracy: 0.7605 - val_loss: 0.7109 - val_accuracy: 0.4969\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5437 - accuracy: 0.7126\n",
      "Epoch 11: val_loss did not improve from 0.69404\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5437 - accuracy: 0.7126 - val_loss: 0.7132 - val_accuracy: 0.4969\n",
      "Epoch 12/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5170 - accuracy: 0.7604\n",
      "Epoch 12: val_loss did not improve from 0.69404\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5258 - accuracy: 0.7471 - val_loss: 0.7154 - val_accuracy: 0.4969\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5035 - accuracy: 0.7596\n",
      "Epoch 13: val_loss did not improve from 0.69404\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4942 - accuracy: 0.7682 - val_loss: 0.7174 - val_accuracy: 0.4969\n",
      "Epoch 14/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5017 - accuracy: 0.7526\n",
      "Epoch 14: val_loss did not improve from 0.69404\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4906 - accuracy: 0.7644 - val_loss: 0.7197 - val_accuracy: 0.4969\n",
      "Epoch 15/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4349 - accuracy: 0.7861\n",
      "Epoch 15: val_loss did not improve from 0.69404\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4283 - accuracy: 0.7931 - val_loss: 0.7225 - val_accuracy: 0.4969\n",
      "Epoch 16/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4383 - accuracy: 0.8203Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69404\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4295 - accuracy: 0.8218 - val_loss: 0.7244 - val_accuracy: 0.4969\n",
      "Epoch 16: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.3788\n",
      "Test AUC for Layer 1: 0.6283\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_28 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_21 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_22 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_23 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.8460 - accuracy: 0.5221\n",
      "Epoch 1: val_loss improved from inf to 0.70426, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 20ms/step - loss: 0.8879 - accuracy: 0.5036 - val_loss: 0.7043 - val_accuracy: 0.3788\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7821 - accuracy: 0.5328\n",
      "Epoch 2: val_loss did not improve from 0.70426\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7821 - accuracy: 0.5328 - val_loss: 0.7220 - val_accuracy: 0.3788\n",
      "Epoch 3/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.7744 - accuracy: 0.5691\n",
      "Epoch 3: val_loss did not improve from 0.70426\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.7655 - accuracy: 0.5679 - val_loss: 0.7381 - val_accuracy: 0.3788\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6821 - accuracy: 0.6248\n",
      "Epoch 4: val_loss did not improve from 0.70426\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6821 - accuracy: 0.6248 - val_loss: 0.7533 - val_accuracy: 0.3788\n",
      "Epoch 5/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6662 - accuracy: 0.6250\n",
      "Epoch 5: val_loss did not improve from 0.70426\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6639 - accuracy: 0.6248 - val_loss: 0.7708 - val_accuracy: 0.3788\n",
      "Epoch 6/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5924 - accuracy: 0.7007\n",
      "Epoch 6: val_loss did not improve from 0.70426\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6046 - accuracy: 0.6876 - val_loss: 0.7839 - val_accuracy: 0.3788\n",
      "Epoch 7/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6070 - accuracy: 0.6628\n",
      "Epoch 7: val_loss did not improve from 0.70426\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6052 - accuracy: 0.6715 - val_loss: 0.7998 - val_accuracy: 0.3788\n",
      "Epoch 8/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5701 - accuracy: 0.6974\n",
      "Epoch 8: val_loss did not improve from 0.70426\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5742 - accuracy: 0.6964 - val_loss: 0.8186 - val_accuracy: 0.3788\n",
      "Epoch 9/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5524 - accuracy: 0.7204\n",
      "Epoch 9: val_loss did not improve from 0.70426\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5362 - accuracy: 0.7343 - val_loss: 0.8326 - val_accuracy: 0.3788\n",
      "Epoch 10/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5369 - accuracy: 0.7240\n",
      "Epoch 10: val_loss did not improve from 0.70426\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5395 - accuracy: 0.7212 - val_loss: 0.8454 - val_accuracy: 0.3788\n",
      "Epoch 11/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.5151 - accuracy: 0.7375\n",
      "Epoch 11: val_loss did not improve from 0.70426\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5151 - accuracy: 0.7372 - val_loss: 0.8572 - val_accuracy: 0.3788\n",
      "Epoch 12/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.4868 - accuracy: 0.7531\n",
      "Epoch 12: val_loss did not improve from 0.70426\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4919 - accuracy: 0.7489 - val_loss: 0.8658 - val_accuracy: 0.3788\n",
      "Epoch 13/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4828 - accuracy: 0.7465\n",
      "Epoch 13: val_loss did not improve from 0.70426\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4901 - accuracy: 0.7474 - val_loss: 0.8715 - val_accuracy: 0.3788\n",
      "Epoch 14/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4639 - accuracy: 0.7986\n",
      "Epoch 14: val_loss did not improve from 0.70426\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4616 - accuracy: 0.8000 - val_loss: 0.8822 - val_accuracy: 0.3788\n",
      "Epoch 15/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4728 - accuracy: 0.7648\n",
      "Epoch 15: val_loss did not improve from 0.70426\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4791 - accuracy: 0.7547 - val_loss: 0.8819 - val_accuracy: 0.3788\n",
      "Epoch 16/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4440 - accuracy: 0.7986Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70426\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4508 - accuracy: 0.7927 - val_loss: 0.8856 - val_accuracy: 0.3788\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4352\n",
      "Test AUC for Layer 2: 0.4765\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_32 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_24 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_33 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_25 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_26 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.9247 - accuracy: 0.4905\n",
      "Epoch 1: val_loss improved from inf to 0.69581, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.9312 - accuracy: 0.4900 - val_loss: 0.6958 - val_accuracy: 0.4352\n",
      "Epoch 2/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8050 - accuracy: 0.5446\n",
      "Epoch 2: val_loss improved from 0.69581 to 0.69443, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.8059 - accuracy: 0.5419 - val_loss: 0.6944 - val_accuracy: 0.4444\n",
      "Epoch 3/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.8056 - accuracy: 0.5666\n",
      "Epoch 3: val_loss improved from 0.69443 to 0.69130, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.8071 - accuracy: 0.5606 - val_loss: 0.6913 - val_accuracy: 0.5463\n",
      "Epoch 4/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.7547 - accuracy: 0.5883\n",
      "Epoch 4: val_loss improved from 0.69130 to 0.68727, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7507 - accuracy: 0.5899 - val_loss: 0.6873 - val_accuracy: 0.6019\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6585 - accuracy: 0.6418\n",
      "Epoch 5: val_loss improved from 0.68727 to 0.68432, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.6585 - accuracy: 0.6418 - val_loss: 0.6843 - val_accuracy: 0.5741\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6603 - accuracy: 0.6232\n",
      "Epoch 6: val_loss improved from 0.68432 to 0.68100, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.6603 - accuracy: 0.6232 - val_loss: 0.6810 - val_accuracy: 0.5648\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6339 - accuracy: 0.6538\n",
      "Epoch 7: val_loss improved from 0.68100 to 0.67921, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.6339 - accuracy: 0.6538 - val_loss: 0.6792 - val_accuracy: 0.5648\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5628 - accuracy: 0.7057\n",
      "Epoch 8: val_loss improved from 0.67921 to 0.67909, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.5628 - accuracy: 0.7057 - val_loss: 0.6791 - val_accuracy: 0.5648\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5806 - accuracy: 0.6897\n",
      "Epoch 9: val_loss improved from 0.67909 to 0.67902, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5806 - accuracy: 0.6897 - val_loss: 0.6790 - val_accuracy: 0.5648\n",
      "Epoch 10/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5364 - accuracy: 0.7296\n",
      "Epoch 10: val_loss did not improve from 0.67902\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5352 - accuracy: 0.7297 - val_loss: 0.6800 - val_accuracy: 0.5648\n",
      "Epoch 11/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5229 - accuracy: 0.7418\n",
      "Epoch 11: val_loss did not improve from 0.67902\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5179 - accuracy: 0.7497 - val_loss: 0.6825 - val_accuracy: 0.5648\n",
      "Epoch 12/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4963 - accuracy: 0.7648\n",
      "Epoch 12: val_loss did not improve from 0.67902\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5003 - accuracy: 0.7577 - val_loss: 0.6868 - val_accuracy: 0.5648\n",
      "Epoch 13/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4712 - accuracy: 0.7582\n",
      "Epoch 13: val_loss did not improve from 0.67902\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4902 - accuracy: 0.7510 - val_loss: 0.6884 - val_accuracy: 0.5648\n",
      "Epoch 14/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4808 - accuracy: 0.7599\n",
      "Epoch 14: val_loss did not improve from 0.67902\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4830 - accuracy: 0.7617 - val_loss: 0.6901 - val_accuracy: 0.5648\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4589 - accuracy: 0.7723\n",
      "Epoch 15: val_loss did not improve from 0.67902\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4589 - accuracy: 0.7723 - val_loss: 0.6929 - val_accuracy: 0.5648\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4264 - accuracy: 0.8123\n",
      "Epoch 16: val_loss did not improve from 0.67902\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4264 - accuracy: 0.8123 - val_loss: 0.6917 - val_accuracy: 0.5648\n",
      "Epoch 17/100\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.4134 - accuracy: 0.8031\n",
      "Epoch 17: val_loss did not improve from 0.67902\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4161 - accuracy: 0.8003 - val_loss: 0.6921 - val_accuracy: 0.5741\n",
      "Epoch 18/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3799 - accuracy: 0.8356\n",
      "Epoch 18: val_loss did not improve from 0.67902\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3793 - accuracy: 0.8362 - val_loss: 0.6942 - val_accuracy: 0.5648\n",
      "Epoch 19/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4441 - accuracy: 0.7880\n",
      "Epoch 19: val_loss did not improve from 0.67902\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4443 - accuracy: 0.7843 - val_loss: 0.6993 - val_accuracy: 0.5741\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3764 - accuracy: 0.8482\n",
      "Epoch 20: val_loss did not improve from 0.67902\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3764 - accuracy: 0.8482 - val_loss: 0.7034 - val_accuracy: 0.5741\n",
      "Epoch 21/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.3943 - accuracy: 0.8214\n",
      "Epoch 21: val_loss did not improve from 0.67902\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4013 - accuracy: 0.8189 - val_loss: 0.7103 - val_accuracy: 0.5556\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3402 - accuracy: 0.8628\n",
      "Epoch 22: val_loss did not improve from 0.67902\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3402 - accuracy: 0.8628 - val_loss: 0.7158 - val_accuracy: 0.5463\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3250 - accuracy: 0.8828\n",
      "Epoch 23: val_loss did not improve from 0.67902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3250 - accuracy: 0.8828 - val_loss: 0.7186 - val_accuracy: 0.5370\n",
      "Epoch 24/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3598 - accuracy: 0.8372Restoring model weights from the end of the best epoch: 9.\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.67902\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3597 - accuracy: 0.8375 - val_loss: 0.7216 - val_accuracy: 0.5278\n",
      "Epoch 24: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5072\n",
      "Test AUC for Layer 3: 0.4613\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4404\n",
      "Average Test AUC across all layers: 0.5220\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_36 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_27 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_28 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_29 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9532 - accuracy: 0.5019\n",
      "Epoch 1: val_loss improved from inf to 0.69236, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.9532 - accuracy: 0.5019 - val_loss: 0.6924 - val_accuracy: 0.5276\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7945 - accuracy: 0.5690\n",
      "Epoch 2: val_loss did not improve from 0.69236\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7945 - accuracy: 0.5690 - val_loss: 0.6924 - val_accuracy: 0.5337\n",
      "Epoch 3/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.7433 - accuracy: 0.5807\n",
      "Epoch 3: val_loss did not improve from 0.69236\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7704 - accuracy: 0.5632 - val_loss: 0.6924 - val_accuracy: 0.5644\n",
      "Epoch 4/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6443 - accuracy: 0.6418\n",
      "Epoch 4: val_loss improved from 0.69236 to 0.69233, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6645 - accuracy: 0.6303 - val_loss: 0.6923 - val_accuracy: 0.5521\n",
      "Epoch 5/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6847 - accuracy: 0.6250\n",
      "Epoch 5: val_loss improved from 0.69233 to 0.69223, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6814 - accuracy: 0.6360 - val_loss: 0.6922 - val_accuracy: 0.5706\n",
      "Epoch 6/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6546 - accuracy: 0.6380\n",
      "Epoch 6: val_loss improved from 0.69223 to 0.69218, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6595 - accuracy: 0.6418 - val_loss: 0.6922 - val_accuracy: 0.5521\n",
      "Epoch 7/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5758 - accuracy: 0.7031\n",
      "Epoch 7: val_loss improved from 0.69218 to 0.69183, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.6004 - accuracy: 0.6877 - val_loss: 0.6918 - val_accuracy: 0.5644\n",
      "Epoch 8/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5402 - accuracy: 0.7031\n",
      "Epoch 8: val_loss did not improve from 0.69183\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5620 - accuracy: 0.6992 - val_loss: 0.6919 - val_accuracy: 0.5521\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5013 - accuracy: 0.7356\n",
      "Epoch 9: val_loss improved from 0.69183 to 0.69161, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5146 - accuracy: 0.7318 - val_loss: 0.6916 - val_accuracy: 0.5092\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5216 - accuracy: 0.7476\n",
      "Epoch 10: val_loss did not improve from 0.69161\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5211 - accuracy: 0.7356 - val_loss: 0.6917 - val_accuracy: 0.5092\n",
      "Epoch 11/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4625 - accuracy: 0.7812\n",
      "Epoch 11: val_loss did not improve from 0.69161\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4786 - accuracy: 0.7701 - val_loss: 0.6917 - val_accuracy: 0.5276\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4388 - accuracy: 0.7816\n",
      "Epoch 12: val_loss improved from 0.69161 to 0.69144, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4388 - accuracy: 0.7816 - val_loss: 0.6914 - val_accuracy: 0.5337\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4511 - accuracy: 0.7816\n",
      "Epoch 13: val_loss did not improve from 0.69144\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4511 - accuracy: 0.7816 - val_loss: 0.6917 - val_accuracy: 0.5706\n",
      "Epoch 14/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4557 - accuracy: 0.7885\n",
      "Epoch 14: val_loss did not improve from 0.69144\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4621 - accuracy: 0.7816 - val_loss: 0.6922 - val_accuracy: 0.5583\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.8103\n",
      "Epoch 15: val_loss did not improve from 0.69144\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3998 - accuracy: 0.8103 - val_loss: 0.6926 - val_accuracy: 0.5337\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4004 - accuracy: 0.8269\n",
      "Epoch 16: val_loss did not improve from 0.69144\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4001 - accuracy: 0.8276 - val_loss: 0.6920 - val_accuracy: 0.5521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4018 - accuracy: 0.8099\n",
      "Epoch 17: val_loss did not improve from 0.69144\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3969 - accuracy: 0.8180 - val_loss: 0.6924 - val_accuracy: 0.5460\n",
      "Epoch 18/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3686 - accuracy: 0.8413\n",
      "Epoch 18: val_loss did not improve from 0.69144\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3746 - accuracy: 0.8391 - val_loss: 0.6927 - val_accuracy: 0.5521\n",
      "Epoch 19/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.3572 - accuracy: 0.8549\n",
      "Epoch 19: val_loss did not improve from 0.69144\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3613 - accuracy: 0.8467 - val_loss: 0.6940 - val_accuracy: 0.5583\n",
      "Epoch 20/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3533 - accuracy: 0.8486\n",
      "Epoch 20: val_loss did not improve from 0.69144\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3633 - accuracy: 0.8429 - val_loss: 0.6933 - val_accuracy: 0.5583\n",
      "Epoch 21/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3377 - accuracy: 0.8654\n",
      "Epoch 21: val_loss did not improve from 0.69144\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3364 - accuracy: 0.8602 - val_loss: 0.6946 - val_accuracy: 0.5583\n",
      "Epoch 22/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3138 - accuracy: 0.8750\n",
      "Epoch 22: val_loss did not improve from 0.69144\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3107 - accuracy: 0.8697 - val_loss: 0.6938 - val_accuracy: 0.5644\n",
      "Epoch 23/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.2629 - accuracy: 0.9129\n",
      "Epoch 23: val_loss did not improve from 0.69144\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2713 - accuracy: 0.9080 - val_loss: 0.6954 - val_accuracy: 0.5706\n",
      "Epoch 24/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.3202 - accuracy: 0.8638\n",
      "Epoch 24: val_loss did not improve from 0.69144\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.3187 - accuracy: 0.8640 - val_loss: 0.7011 - val_accuracy: 0.5706\n",
      "Epoch 25/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2808 - accuracy: 0.8984\n",
      "Epoch 25: val_loss did not improve from 0.69144\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2897 - accuracy: 0.8889 - val_loss: 0.7043 - val_accuracy: 0.5644\n",
      "Epoch 26/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3040 - accuracy: 0.8932\n",
      "Epoch 26: val_loss did not improve from 0.69144\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2965 - accuracy: 0.8985 - val_loss: 0.7110 - val_accuracy: 0.5521\n",
      "Epoch 27/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2664 - accuracy: 0.9115Restoring model weights from the end of the best epoch: 12.\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.69144\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2650 - accuracy: 0.9061 - val_loss: 0.7173 - val_accuracy: 0.5521\n",
      "Epoch 27: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.6212\n",
      "Test AUC for Layer 1: 0.7193\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_40 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_30 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_31 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_32 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.9127 - accuracy: 0.4866\n",
      "Epoch 1: val_loss improved from inf to 0.64525, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 22ms/step - loss: 0.9085 - accuracy: 0.4905 - val_loss: 0.6453 - val_accuracy: 0.8636\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7866 - accuracy: 0.5461\n",
      "Epoch 2: val_loss improved from 0.64525 to 0.60894, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.7912 - accuracy: 0.5431 - val_loss: 0.6089 - val_accuracy: 0.8636\n",
      "Epoch 3/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7193 - accuracy: 0.5759\n",
      "Epoch 3: val_loss improved from 0.60894 to 0.58159, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.7178 - accuracy: 0.5781 - val_loss: 0.5816 - val_accuracy: 0.8636\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7255 - accuracy: 0.5971\n",
      "Epoch 4: val_loss improved from 0.58159 to 0.56120, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.7255 - accuracy: 0.5971 - val_loss: 0.5612 - val_accuracy: 0.8636\n",
      "Epoch 5/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.6727 - accuracy: 0.6379\n",
      "Epoch 5: val_loss improved from 0.56120 to 0.54443, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6724 - accuracy: 0.6409 - val_loss: 0.5444 - val_accuracy: 0.8636\n",
      "Epoch 6/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6473 - accuracy: 0.6580\n",
      "Epoch 6: val_loss improved from 0.54443 to 0.53720, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6210 - accuracy: 0.6657 - val_loss: 0.5372 - val_accuracy: 0.8636\n",
      "Epoch 7/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5999 - accuracy: 0.6858\n",
      "Epoch 7: val_loss improved from 0.53720 to 0.52873, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6096 - accuracy: 0.6788 - val_loss: 0.5287 - val_accuracy: 0.8636\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5908 - accuracy: 0.6830\n",
      "Epoch 8: val_loss improved from 0.52873 to 0.52668, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5894 - accuracy: 0.6818 - val_loss: 0.5267 - val_accuracy: 0.8636\n",
      "Epoch 9/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5443 - accuracy: 0.7077\n",
      "Epoch 9: val_loss improved from 0.52668 to 0.52137, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5480 - accuracy: 0.7066 - val_loss: 0.5214 - val_accuracy: 0.8636\n",
      "Epoch 10/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.5291 - accuracy: 0.7500\n",
      "Epoch 10: val_loss improved from 0.52137 to 0.52128, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.5323 - accuracy: 0.7489 - val_loss: 0.5213 - val_accuracy: 0.8636\n",
      "Epoch 11/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5124 - accuracy: 0.7411\n",
      "Epoch 11: val_loss improved from 0.52128 to 0.52056, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5135 - accuracy: 0.7416 - val_loss: 0.5206 - val_accuracy: 0.8636\n",
      "Epoch 12/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5266 - accuracy: 0.7366\n",
      "Epoch 12: val_loss improved from 0.52056 to 0.51774, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5281 - accuracy: 0.7328 - val_loss: 0.5177 - val_accuracy: 0.8636\n",
      "Epoch 13/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.4599 - accuracy: 0.7738\n",
      "Epoch 13: val_loss did not improve from 0.51774\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4570 - accuracy: 0.7766 - val_loss: 0.5219 - val_accuracy: 0.8636\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4635 - accuracy: 0.7839\n",
      "Epoch 14: val_loss did not improve from 0.51774\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4635 - accuracy: 0.7839 - val_loss: 0.5183 - val_accuracy: 0.8636\n",
      "Epoch 15/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4579 - accuracy: 0.7639\n",
      "Epoch 15: val_loss improved from 0.51774 to 0.51525, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.4560 - accuracy: 0.7723 - val_loss: 0.5152 - val_accuracy: 0.8636\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4480 - accuracy: 0.7985\n",
      "Epoch 16: val_loss did not improve from 0.51525\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4480 - accuracy: 0.7985 - val_loss: 0.5195 - val_accuracy: 0.8636\n",
      "Epoch 17/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4211 - accuracy: 0.7996\n",
      "Epoch 17: val_loss did not improve from 0.51525\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4253 - accuracy: 0.8044 - val_loss: 0.5328 - val_accuracy: 0.8788\n",
      "Epoch 18/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3913 - accuracy: 0.8229\n",
      "Epoch 18: val_loss did not improve from 0.51525\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3923 - accuracy: 0.8248 - val_loss: 0.5318 - val_accuracy: 0.8485\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3877 - accuracy: 0.8350\n",
      "Epoch 19: val_loss did not improve from 0.51525\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.3877 - accuracy: 0.8350 - val_loss: 0.5405 - val_accuracy: 0.8333\n",
      "Epoch 20/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.3546 - accuracy: 0.8467\n",
      "Epoch 20: val_loss did not improve from 0.51525\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3537 - accuracy: 0.8482 - val_loss: 0.5327 - val_accuracy: 0.8485\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3418 - accuracy: 0.8642\n",
      "Epoch 21: val_loss did not improve from 0.51525\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3418 - accuracy: 0.8642 - val_loss: 0.5251 - val_accuracy: 0.8333\n",
      "Epoch 22/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3541 - accuracy: 0.8503\n",
      "Epoch 22: val_loss did not improve from 0.51525\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3689 - accuracy: 0.8394 - val_loss: 0.5438 - val_accuracy: 0.8030\n",
      "Epoch 23/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.3217 - accuracy: 0.8839\n",
      "Epoch 23: val_loss did not improve from 0.51525\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3205 - accuracy: 0.8847 - val_loss: 0.5580 - val_accuracy: 0.7424\n",
      "Epoch 24/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3183 - accuracy: 0.8889\n",
      "Epoch 24: val_loss did not improve from 0.51525\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3122 - accuracy: 0.8934 - val_loss: 0.5625 - val_accuracy: 0.7576\n",
      "Epoch 25/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.2922 - accuracy: 0.9099\n",
      "Epoch 25: val_loss did not improve from 0.51525\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2972 - accuracy: 0.9022 - val_loss: 0.5989 - val_accuracy: 0.7121\n",
      "Epoch 26/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.3188 - accuracy: 0.8601\n",
      "Epoch 26: val_loss did not improve from 0.51525\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3175 - accuracy: 0.8628 - val_loss: 0.5793 - val_accuracy: 0.7121\n",
      "Epoch 27/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3127 - accuracy: 0.8924\n",
      "Epoch 27: val_loss did not improve from 0.51525\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3100 - accuracy: 0.8905 - val_loss: 0.5856 - val_accuracy: 0.6970\n",
      "Epoch 28/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.2728 - accuracy: 0.9115\n",
      "Epoch 28: val_loss did not improve from 0.51525\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.2830 - accuracy: 0.9036 - val_loss: 0.5999 - val_accuracy: 0.7121\n",
      "Epoch 29/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.2719 - accuracy: 0.9211\n",
      "Epoch 29: val_loss did not improve from 0.51525\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.2754 - accuracy: 0.9168 - val_loss: 0.6418 - val_accuracy: 0.6818\n",
      "Epoch 30/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.2662 - accuracy: 0.9062Restoring model weights from the end of the best epoch: 15.\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.51525\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.2739 - accuracy: 0.9007 - val_loss: 0.6799 - val_accuracy: 0.6364\n",
      "Epoch 30: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.6759\n",
      "Test AUC for Layer 2: 0.5421\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_44 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_33 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_45 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_34 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_35 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8742 - accuracy: 0.5119\n",
      "Epoch 1: val_loss improved from inf to 0.66154, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.8817 - accuracy: 0.5033 - val_loss: 0.6615 - val_accuracy: 0.6667\n",
      "Epoch 2/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8077 - accuracy: 0.5402\n",
      "Epoch 2: val_loss improved from 0.66154 to 0.64618, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.7822 - accuracy: 0.5513 - val_loss: 0.6462 - val_accuracy: 0.6667\n",
      "Epoch 3/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.7397 - accuracy: 0.6009\n",
      "Epoch 3: val_loss improved from 0.64618 to 0.63969, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7350 - accuracy: 0.6085 - val_loss: 0.6397 - val_accuracy: 0.6667\n",
      "Epoch 4/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.6951 - accuracy: 0.6146\n",
      "Epoch 4: val_loss improved from 0.63969 to 0.63756, saving model to OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.7076 - accuracy: 0.6112 - val_loss: 0.6376 - val_accuracy: 0.6667\n",
      "Epoch 5/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6607 - accuracy: 0.6413\n",
      "Epoch 5: val_loss did not improve from 0.63756\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.6596 - accuracy: 0.6418 - val_loss: 0.6377 - val_accuracy: 0.6667\n",
      "Epoch 6/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6178 - accuracy: 0.6711\n",
      "Epoch 6: val_loss did not improve from 0.63756\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6088 - accuracy: 0.6738 - val_loss: 0.6390 - val_accuracy: 0.6667\n",
      "Epoch 7/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5992 - accuracy: 0.6974\n",
      "Epoch 7: val_loss did not improve from 0.63756\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6068 - accuracy: 0.6844 - val_loss: 0.6397 - val_accuracy: 0.6667\n",
      "Epoch 8/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.5654 - accuracy: 0.7017\n",
      "Epoch 8: val_loss did not improve from 0.63756\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5622 - accuracy: 0.7071 - val_loss: 0.6408 - val_accuracy: 0.6667\n",
      "Epoch 9/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5392 - accuracy: 0.7385\n",
      "Epoch 9: val_loss did not improve from 0.63756\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5575 - accuracy: 0.7164 - val_loss: 0.6419 - val_accuracy: 0.6667\n",
      "Epoch 10/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.5280 - accuracy: 0.7344\n",
      "Epoch 10: val_loss did not improve from 0.63756\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5247 - accuracy: 0.7364 - val_loss: 0.6428 - val_accuracy: 0.6667\n",
      "Epoch 11/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.4958 - accuracy: 0.7604\n",
      "Epoch 11: val_loss did not improve from 0.63756\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4969 - accuracy: 0.7563 - val_loss: 0.6436 - val_accuracy: 0.6667\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4620 - accuracy: 0.7803\n",
      "Epoch 12: val_loss did not improve from 0.63756\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4620 - accuracy: 0.7803 - val_loss: 0.6425 - val_accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4574 - accuracy: 0.7926\n",
      "Epoch 13: val_loss did not improve from 0.63756\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4550 - accuracy: 0.7949 - val_loss: 0.6431 - val_accuracy: 0.6667\n",
      "Epoch 14/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4118 - accuracy: 0.8125\n",
      "Epoch 14: val_loss did not improve from 0.63756\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4098 - accuracy: 0.8136 - val_loss: 0.6442 - val_accuracy: 0.6667\n",
      "Epoch 15/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.3871 - accuracy: 0.8289\n",
      "Epoch 15: val_loss did not improve from 0.63756\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.3990 - accuracy: 0.8256 - val_loss: 0.6454 - val_accuracy: 0.6667\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4029 - accuracy: 0.8189\n",
      "Epoch 16: val_loss did not improve from 0.63756\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4029 - accuracy: 0.8189 - val_loss: 0.6473 - val_accuracy: 0.6667\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3957 - accuracy: 0.8336\n",
      "Epoch 17: val_loss did not improve from 0.63756\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3957 - accuracy: 0.8336 - val_loss: 0.6506 - val_accuracy: 0.6667\n",
      "Epoch 18/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3719 - accuracy: 0.8454\n",
      "Epoch 18: val_loss did not improve from 0.63756\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3702 - accuracy: 0.8495 - val_loss: 0.6538 - val_accuracy: 0.6667\n",
      "Epoch 19/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.3675 - accuracy: 0.8363Restoring model weights from the end of the best epoch: 4.\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.63756\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.3711 - accuracy: 0.8349 - val_loss: 0.6584 - val_accuracy: 0.6667\n",
      "Epoch 19: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.3913\n",
      "Test AUC for Layer 3: 0.5326\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5628\n",
      "Average Test AUC across all layers: 0.5980\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS XOM)\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Title + S_label\n",
      "Average Accuracy: 0.5141\n",
      "Average AUC: 0.5161\n",
      "  Layer 1 - Accuracy: 0.4848, AUC: 0.4654\n",
      "  Layer 2 - Accuracy: 0.5648, AUC: 0.4855\n",
      "  Layer 3 - Accuracy: 0.4928, AUC: 0.5975\n",
      "\n",
      "Combination: MLP with Title + L_label\n",
      "Average Accuracy: 0.3933\n",
      "Average AUC: 0.4812\n",
      "  Layer 1 - Accuracy: 0.1364, AUC: 0.4639\n",
      "  Layer 2 - Accuracy: 0.6667, AUC: 0.4965\n",
      "  Layer 3 - Accuracy: 0.3768, AUC: 0.4832\n",
      "\n",
      "Combination: MLP with Full text + S_label\n",
      "Average Accuracy: 0.4404\n",
      "Average AUC: 0.5220\n",
      "  Layer 1 - Accuracy: 0.3788, AUC: 0.6283\n",
      "  Layer 2 - Accuracy: 0.4352, AUC: 0.4765\n",
      "  Layer 3 - Accuracy: 0.5072, AUC: 0.4613\n",
      "\n",
      "Combination: MLP with Full text + L_label\n",
      "Average Accuracy: 0.5628\n",
      "Average AUC: 0.5980\n",
      "  Layer 1 - Accuracy: 0.6212, AUC: 0.7193\n",
      "  Layer 2 - Accuracy: 0.6759, AUC: 0.5421\n",
      "  Layer 3 - Accuracy: 0.3913, AUC: 0.5326\n",
      "MLP summary comparison visualization saved as: OpenAI_MLP/visualizations_summary/XOM\\mlp_performance_comparison.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP layer performance visualization saved as: OpenAI_MLP/visualizations_summary/XOM\\mlp_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'OpenAI_MLP' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "from xgboost import callback\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, model_type, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class ClimateNewsOpenAIPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'OpenAI_MLP/visualizations_mlp/XOM'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs('OpenAI_MLP/visualizations_summary/XOM', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert embeddings to numpy arrays\n",
    "        self.data['Title_embedding'] = self.data['Title_embedding_vector'].apply(parse_embedding)\n",
    "        self.data['Fulltext_embedding'] = self.data['Full_text_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_title_embedding = self.data['Title_embedding'].iloc[0]\n",
    "        sample_fulltext_embedding = self.data['Fulltext_embedding'].iloc[0]\n",
    "        \n",
    "        print(f\"Sample Title embedding shape: {sample_title_embedding.shape}\")\n",
    "        print(f\"Sample Fulltext embedding shape: {sample_fulltext_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2019-2021), validation (2021-2021), test (2022-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-05-31'),\n",
    "            'val_start': pd.Timestamp('2021-06-01'),\n",
    "            'val_end': pd.Timestamp('2021-12-31'),\n",
    "            'test_start': pd.Timestamp('2022-01-01'),\n",
    "            'test_end': pd.Timestamp('2022-05-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2019-2021), validation (2022-2022), test (2022-2022)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-12-31'),\n",
    "            'val_start': pd.Timestamp('2022-01-01'),\n",
    "            'val_end': pd.Timestamp('2022-05-31'),\n",
    "            'test_start': pd.Timestamp('2022-06-01'),\n",
    "            'test_end': pd.Timestamp('2022-12-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2019-2022), validation (2022-2022), test (2023-2023)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2022-05-31'),\n",
    "            'val_start': pd.Timestamp('2022-06-01'),\n",
    "            'val_end': pd.Timestamp('2022-12-31'),\n",
    "            'test_start': pd.Timestamp('2023-01-01'),\n",
    "            'test_end': pd.Timestamp('2023-05-31')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, text_col, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            text_col: The column containing the embeddings ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data[text_col].values)\n",
    "        X_val = np.stack(val_data[text_col].values)\n",
    "        X_test = np.stack(test_data[text_col].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, model_type, text_col, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            model_type: Model type (MLP)\n",
    "            text_col: Text column used\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_{model_type.lower()}_{display_text.replace(' ', '_')}_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, text_col, label_col, model_type):\n",
    "        \"\"\"\n",
    "        Train and evaluate a model for a specific text column and label column.\n",
    "        \n",
    "        Args:\n",
    "            text_col: The embedding column to use ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "            model_type: The model type to use ('MLP')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        combination_key = f\"{model_type}|{display_text}|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training {model_type} model for {display_text} and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        visualization_dir = self.mlp_viz_dir if model_type == 'MLP' else self.xgb_viz_dir\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, text_col, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            if model_type == 'MLP':\n",
    "                # MLP model training\n",
    "                model = self.create_mlp_model((1536,))\n",
    "                print(f\"Created MLP model for {display_text} ({label_col})\")\n",
    "                model.summary()\n",
    "                \n",
    "                # Setup callbacks\n",
    "                plot_callback = PlotLearningCallback(model_type, display_text, label_col, i+1, visualization_dir)\n",
    "                early_stopping = EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=15,\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                model_checkpoint = ModelCheckpoint(\n",
    "                    filepath=f\"{visualization_dir}/best_{model_type.lower()}_{display_text.lower().replace(' ', '_')}_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                    monitor='val_loss',\n",
    "                    save_weights_only=True,\n",
    "                    save_best_only=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Train model\n",
    "                print(f\"Training MLP model...\")\n",
    "                batch_size = 32  # Larger batch size for embeddings\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                y_pred_proba = model.predict(X_test)\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                # Create final learning curve visualization\n",
    "                self.plot_final_learning_curves(history, model_type, text_col, label_col, i+1, visualization_dir)\n",
    "                \n",
    "                # Store training history\n",
    "                training_history = history.history\n",
    "                \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'model_type': model_type,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': training_history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for all combinations of text inputs, label columns, and model types.\"\"\"\n",
    "        # Define all combinations\n",
    "        embedding_cols = ['Title_embedding', 'Fulltext_embedding']\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        model_types = ['MLP']\n",
    "        \n",
    "        # Run analysis for each combination\n",
    "        for model_type in model_types:\n",
    "            for embedding_col in embedding_cols:\n",
    "                for label_col in label_cols:\n",
    "                    self.train_and_evaluate_model(embedding_col, label_col, model_type)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS XOM)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Model': model_type,\n",
    "                'Text': text_col,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing all model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Split by model type\n",
    "        mlp_data = df[df['Model'] == 'MLP']\n",
    "        \n",
    "        # 1. Performance comparison for MLP\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot accuracy and AUC bars for MLP\n",
    "        x = np.arange(len(mlp_data))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, mlp_data['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, mlp_data['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('MLP Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with OpenAI Embeddings')\n",
    "        labels = [f\"{row['Text']} + {row['Label']}\" for _, row in mlp_data.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(mlp_data['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(mlp_data['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(mlp_data['Avg Accuracy'].max(), mlp_data['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP/visualizations_summary/XOM', \"mlp_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        mlp_layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Model': model_type,\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                if model_type == 'MLP':\n",
    "                    mlp_layer_data.append(layer_info)\n",
    "        \n",
    "        # Create visualizations for each model type\n",
    "        self._create_model_layer_visualization(mlp_layer_data, 'MLP')\n",
    "    \n",
    "    def _create_model_layer_visualization(self, layer_data, model_type):\n",
    "        \"\"\"Create layer-specific visualizations for a given model type.\"\"\"\n",
    "        if not layer_data:\n",
    "            print(f\"No layer data available for {model_type}\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} Accuracy by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} AUC by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP/visualizations_summary/XOM', f\"{model_type.lower()}_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"{model_type} layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['OpenAI_MLP/visualizations_mlp/XOM',\n",
    "                      'OpenAI_MLP/visualizations_summary/XOM']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/US_news/SP500_semantic/us_news_semantics_XOM_completed_openai.csv'\n",
    "    \n",
    "    # Initialize the predictor with OpenAI embeddings\n",
    "    predictor = ClimateNewsOpenAIPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'OpenAI_MLP' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing merged OpenAI embedding vectors from string format...\n",
      "Sample Merged embedding shape: (1536,)\n",
      "Loaded 929 climate change news articles spanning from 02/01/2019 to 07/05/2023\n",
      "Class distribution for short-term prediction: {0: 466, 1: 463}\n",
      "Class distribution for long-term prediction: {1: 507, 0: 422}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_48 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_36 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_37 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_38 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8830 - accuracy: 0.5536\n",
      "Epoch 1: val_loss improved from inf to 0.69296, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 24ms/step - loss: 0.8830 - accuracy: 0.5536 - val_loss: 0.6930 - val_accuracy: 0.5031\n",
      "Epoch 2/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.8478 - accuracy: 0.5391\n",
      "Epoch 2: val_loss did not improve from 0.69296\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.8153 - accuracy: 0.5536 - val_loss: 0.6942 - val_accuracy: 0.5031\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7660 - accuracy: 0.5900\n",
      "Epoch 3: val_loss did not improve from 0.69296\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7660 - accuracy: 0.5900 - val_loss: 0.6970 - val_accuracy: 0.5031\n",
      "Epoch 4/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6665 - accuracy: 0.6635\n",
      "Epoch 4: val_loss did not improve from 0.69296\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6717 - accuracy: 0.6609 - val_loss: 0.7002 - val_accuracy: 0.5031\n",
      "Epoch 5/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6606 - accuracy: 0.6226\n",
      "Epoch 5: val_loss did not improve from 0.69296\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6716 - accuracy: 0.6245 - val_loss: 0.7029 - val_accuracy: 0.5031\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5708 - accuracy: 0.7139\n",
      "Epoch 6: val_loss did not improve from 0.69296\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5679 - accuracy: 0.7088 - val_loss: 0.7060 - val_accuracy: 0.5031\n",
      "Epoch 7/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5985 - accuracy: 0.6659\n",
      "Epoch 7: val_loss did not improve from 0.69296\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6114 - accuracy: 0.6686 - val_loss: 0.7090 - val_accuracy: 0.5031\n",
      "Epoch 8/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5347 - accuracy: 0.7308\n",
      "Epoch 8: val_loss did not improve from 0.69296\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5432 - accuracy: 0.7318 - val_loss: 0.7136 - val_accuracy: 0.5031\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4952 - accuracy: 0.7404\n",
      "Epoch 9: val_loss did not improve from 0.69296\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5129 - accuracy: 0.7337 - val_loss: 0.7178 - val_accuracy: 0.5031\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5215 - accuracy: 0.7452\n",
      "Epoch 10: val_loss did not improve from 0.69296\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5201 - accuracy: 0.7414 - val_loss: 0.7209 - val_accuracy: 0.5031\n",
      "Epoch 11/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4841 - accuracy: 0.7500\n",
      "Epoch 11: val_loss did not improve from 0.69296\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4773 - accuracy: 0.7663 - val_loss: 0.7239 - val_accuracy: 0.5031\n",
      "Epoch 12/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4725 - accuracy: 0.7620\n",
      "Epoch 12: val_loss did not improve from 0.69296\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4877 - accuracy: 0.7548 - val_loss: 0.7279 - val_accuracy: 0.5031\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4233 - accuracy: 0.8269\n",
      "Epoch 13: val_loss did not improve from 0.69296\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4252 - accuracy: 0.8103 - val_loss: 0.7288 - val_accuracy: 0.5031\n",
      "Epoch 14/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4486 - accuracy: 0.8029\n",
      "Epoch 14: val_loss did not improve from 0.69296\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4334 - accuracy: 0.8161 - val_loss: 0.7295 - val_accuracy: 0.5031\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4103 - accuracy: 0.8142\n",
      "Epoch 15: val_loss did not improve from 0.69296\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4103 - accuracy: 0.8142 - val_loss: 0.7339 - val_accuracy: 0.5031\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4160 - accuracy: 0.8164Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69296\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4175 - accuracy: 0.8161 - val_loss: 0.7342 - val_accuracy: 0.5031\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.6212\n",
      "Test AUC for Layer 1: 0.4868\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_52 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_39 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_40 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_41 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.8770 - accuracy: 0.5018\n",
      "Epoch 1: val_loss improved from inf to 0.73058, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 20ms/step - loss: 0.8911 - accuracy: 0.4949 - val_loss: 0.7306 - val_accuracy: 0.3788\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8088 - accuracy: 0.5476\n",
      "Epoch 2: val_loss did not improve from 0.73058\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.8138 - accuracy: 0.5474 - val_loss: 0.7681 - val_accuracy: 0.3788\n",
      "Epoch 3/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.7398 - accuracy: 0.5864\n",
      "Epoch 3: val_loss did not improve from 0.73058\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7313 - accuracy: 0.5956 - val_loss: 0.8039 - val_accuracy: 0.3788\n",
      "Epoch 4/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6833 - accuracy: 0.6316\n",
      "Epoch 4: val_loss did not improve from 0.73058\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6880 - accuracy: 0.6248 - val_loss: 0.8352 - val_accuracy: 0.3788\n",
      "Epoch 5/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.6490 - accuracy: 0.6324\n",
      "Epoch 5: val_loss did not improve from 0.73058\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6577 - accuracy: 0.6292 - val_loss: 0.8593 - val_accuracy: 0.3788\n",
      "Epoch 6/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6352 - accuracy: 0.6513\n",
      "Epoch 6: val_loss did not improve from 0.73058\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6318 - accuracy: 0.6569 - val_loss: 0.8760 - val_accuracy: 0.3788\n",
      "Epoch 7/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5810 - accuracy: 0.7004\n",
      "Epoch 7: val_loss did not improve from 0.73058\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6135 - accuracy: 0.6818 - val_loss: 0.8865 - val_accuracy: 0.3788\n",
      "Epoch 8/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6018 - accuracy: 0.6645\n",
      "Epoch 8: val_loss did not improve from 0.73058\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5980 - accuracy: 0.6701 - val_loss: 0.8931 - val_accuracy: 0.3788\n",
      "Epoch 9/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5660 - accuracy: 0.7105\n",
      "Epoch 9: val_loss did not improve from 0.73058\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5702 - accuracy: 0.7124 - val_loss: 0.8956 - val_accuracy: 0.3788\n",
      "Epoch 10/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5449 - accuracy: 0.7039\n",
      "Epoch 10: val_loss did not improve from 0.73058\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5470 - accuracy: 0.7066 - val_loss: 0.8911 - val_accuracy: 0.3788\n",
      "Epoch 11/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5248 - accuracy: 0.7253\n",
      "Epoch 11: val_loss did not improve from 0.73058\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5281 - accuracy: 0.7285 - val_loss: 0.8873 - val_accuracy: 0.3788\n",
      "Epoch 12/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4963 - accuracy: 0.7566\n",
      "Epoch 12: val_loss did not improve from 0.73058\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4991 - accuracy: 0.7547 - val_loss: 0.8896 - val_accuracy: 0.3788\n",
      "Epoch 13/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4637 - accuracy: 0.7847\n",
      "Epoch 13: val_loss did not improve from 0.73058\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4713 - accuracy: 0.7810 - val_loss: 0.8867 - val_accuracy: 0.3788\n",
      "Epoch 14/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4555 - accuracy: 0.7780\n",
      "Epoch 14: val_loss did not improve from 0.73058\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4564 - accuracy: 0.7796 - val_loss: 0.8919 - val_accuracy: 0.3788\n",
      "Epoch 15/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4414 - accuracy: 0.8003\n",
      "Epoch 15: val_loss did not improve from 0.73058\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4465 - accuracy: 0.7956 - val_loss: 0.8931 - val_accuracy: 0.3788\n",
      "Epoch 16/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4423 - accuracy: 0.7944Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.73058\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4455 - accuracy: 0.7912 - val_loss: 0.8916 - val_accuracy: 0.3788\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4352\n",
      "Test AUC for Layer 2: 0.5075\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_56 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_42 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_43 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_44 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_59 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.9525 - accuracy: 0.4531\n",
      "Epoch 1: val_loss improved from inf to 0.69657, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.9525 - accuracy: 0.4607 - val_loss: 0.6966 - val_accuracy: 0.4352\n",
      "Epoch 2/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.8446 - accuracy: 0.5462\n",
      "Epoch 2: val_loss did not improve from 0.69657\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.8484 - accuracy: 0.5433 - val_loss: 0.7011 - val_accuracy: 0.4352\n",
      "Epoch 3/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.7462 - accuracy: 0.5679\n",
      "Epoch 3: val_loss did not improve from 0.69657\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.7413 - accuracy: 0.5712 - val_loss: 0.7057 - val_accuracy: 0.4352\n",
      "Epoch 4/100\n",
      "18/24 [=====================>........] - ETA: 0s - loss: 0.6937 - accuracy: 0.6181\n",
      "Epoch 4: val_loss did not improve from 0.69657\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.7073 - accuracy: 0.6138 - val_loss: 0.7087 - val_accuracy: 0.4352\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6907 - accuracy: 0.6258\n",
      "Epoch 5: val_loss did not improve from 0.69657\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6907 - accuracy: 0.6258 - val_loss: 0.7120 - val_accuracy: 0.4352\n",
      "Epoch 6/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6813 - accuracy: 0.6209\n",
      "Epoch 6: val_loss did not improve from 0.69657\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6881 - accuracy: 0.6138 - val_loss: 0.7131 - val_accuracy: 0.4352\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6553 - accuracy: 0.6538\n",
      "Epoch 7: val_loss did not improve from 0.69657\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6553 - accuracy: 0.6538 - val_loss: 0.7154 - val_accuracy: 0.4352\n",
      "Epoch 8/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5840 - accuracy: 0.7056\n",
      "Epoch 8: val_loss did not improve from 0.69657\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5799 - accuracy: 0.7057 - val_loss: 0.7196 - val_accuracy: 0.4259\n",
      "Epoch 9/100\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.5920 - accuracy: 0.6734\n",
      "Epoch 9: val_loss did not improve from 0.69657\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5996 - accuracy: 0.6698 - val_loss: 0.7201 - val_accuracy: 0.4259\n",
      "Epoch 10/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5696 - accuracy: 0.7105\n",
      "Epoch 10: val_loss did not improve from 0.69657\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5680 - accuracy: 0.7031 - val_loss: 0.7250 - val_accuracy: 0.4259\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4909 - accuracy: 0.7723\n",
      "Epoch 11: val_loss did not improve from 0.69657\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4909 - accuracy: 0.7723 - val_loss: 0.7270 - val_accuracy: 0.4167\n",
      "Epoch 12/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4935 - accuracy: 0.7500\n",
      "Epoch 12: val_loss did not improve from 0.69657\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5103 - accuracy: 0.7443 - val_loss: 0.7305 - val_accuracy: 0.4074\n",
      "Epoch 13/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4915 - accuracy: 0.7664\n",
      "Epoch 13: val_loss did not improve from 0.69657\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4923 - accuracy: 0.7630 - val_loss: 0.7306 - val_accuracy: 0.4444\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4920 - accuracy: 0.7630\n",
      "Epoch 14: val_loss did not improve from 0.69657\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4920 - accuracy: 0.7630 - val_loss: 0.7364 - val_accuracy: 0.4352\n",
      "Epoch 15/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4445 - accuracy: 0.8054\n",
      "Epoch 15: val_loss did not improve from 0.69657\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4426 - accuracy: 0.8056 - val_loss: 0.7390 - val_accuracy: 0.4259\n",
      "Epoch 16/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4482 - accuracy: 0.7961Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69657\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4436 - accuracy: 0.7989 - val_loss: 0.7457 - val_accuracy: 0.4444\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4928\n",
      "Test AUC for Layer 3: 0.5067\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5164\n",
      "Average Test AUC across all layers: 0.5004\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_60 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_45 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_46 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_47 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8870 - accuracy: 0.5211\n",
      "Epoch 1: val_loss improved from inf to 0.69267, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 24ms/step - loss: 0.8870 - accuracy: 0.5211 - val_loss: 0.6927 - val_accuracy: 0.5031\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7766 - accuracy: 0.5460\n",
      "Epoch 2: val_loss did not improve from 0.69267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7766 - accuracy: 0.5460 - val_loss: 0.6928 - val_accuracy: 0.5031\n",
      "Epoch 3/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.7562 - accuracy: 0.5673\n",
      "Epoch 3: val_loss did not improve from 0.69267\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7424 - accuracy: 0.5690 - val_loss: 0.6932 - val_accuracy: 0.5031\n",
      "Epoch 4/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6721 - accuracy: 0.6394\n",
      "Epoch 4: val_loss did not improve from 0.69267\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6779 - accuracy: 0.6360 - val_loss: 0.6938 - val_accuracy: 0.5031\n",
      "Epoch 5/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.6425 - accuracy: 0.6562\n",
      "Epoch 5: val_loss did not improve from 0.69267\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6501 - accuracy: 0.6475 - val_loss: 0.6951 - val_accuracy: 0.5031\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6237 - accuracy: 0.6418\n",
      "Epoch 6: val_loss did not improve from 0.69267\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5994 - accuracy: 0.6648 - val_loss: 0.6964 - val_accuracy: 0.5031\n",
      "Epoch 7/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5841 - accuracy: 0.6899\n",
      "Epoch 7: val_loss did not improve from 0.69267\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5785 - accuracy: 0.6973 - val_loss: 0.6981 - val_accuracy: 0.5031\n",
      "Epoch 8/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.5236 - accuracy: 0.7500\n",
      "Epoch 8: val_loss did not improve from 0.69267\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.5079 - accuracy: 0.7644 - val_loss: 0.7013 - val_accuracy: 0.5031\n",
      "Epoch 9/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5573 - accuracy: 0.7266\n",
      "Epoch 9: val_loss did not improve from 0.69267\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5356 - accuracy: 0.7318 - val_loss: 0.7040 - val_accuracy: 0.5031\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5141 - accuracy: 0.7500\n",
      "Epoch 10: val_loss did not improve from 0.69267\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5146 - accuracy: 0.7452 - val_loss: 0.7065 - val_accuracy: 0.5031\n",
      "Epoch 11/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5179 - accuracy: 0.7708\n",
      "Epoch 11: val_loss did not improve from 0.69267\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5193 - accuracy: 0.7586 - val_loss: 0.7102 - val_accuracy: 0.5031\n",
      "Epoch 12/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4954 - accuracy: 0.7837\n",
      "Epoch 12: val_loss did not improve from 0.69267\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.4839 - accuracy: 0.7969 - val_loss: 0.7127 - val_accuracy: 0.5031\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4342 - accuracy: 0.8149\n",
      "Epoch 13: val_loss did not improve from 0.69267\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4433 - accuracy: 0.8065 - val_loss: 0.7173 - val_accuracy: 0.5031\n",
      "Epoch 14/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3915 - accuracy: 0.8411\n",
      "Epoch 14: val_loss did not improve from 0.69267\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3985 - accuracy: 0.8467 - val_loss: 0.7193 - val_accuracy: 0.5031\n",
      "Epoch 15/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3776 - accuracy: 0.8516\n",
      "Epoch 15: val_loss did not improve from 0.69267\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3942 - accuracy: 0.8448 - val_loss: 0.7237 - val_accuracy: 0.5031\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4168 - accuracy: 0.8077Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69267\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4074 - accuracy: 0.8142 - val_loss: 0.7282 - val_accuracy: 0.5031\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.2727\n",
      "Test AUC for Layer 1: 0.4561\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_64 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_48 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_49 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_50 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8714 - accuracy: 0.5051\n",
      "Epoch 1: val_loss improved from inf to 0.70577, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 20ms/step - loss: 0.8714 - accuracy: 0.5051 - val_loss: 0.7058 - val_accuracy: 0.1515\n",
      "Epoch 2/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.7774 - accuracy: 0.5694\n",
      "Epoch 2: val_loss improved from 0.70577 to 0.70462, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.7748 - accuracy: 0.5679 - val_loss: 0.7046 - val_accuracy: 0.1364\n",
      "Epoch 3/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.7366 - accuracy: 0.5809\n",
      "Epoch 3: val_loss improved from 0.70462 to 0.70186, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.7259 - accuracy: 0.5912 - val_loss: 0.7019 - val_accuracy: 0.2121\n",
      "Epoch 4/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.6692 - accuracy: 0.6011\n",
      "Epoch 4: val_loss did not improve from 0.70186\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6624 - accuracy: 0.6044 - val_loss: 0.7043 - val_accuracy: 0.1970\n",
      "Epoch 5/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.6582 - accuracy: 0.6297\n",
      "Epoch 5: val_loss did not improve from 0.70186\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6688 - accuracy: 0.6234 - val_loss: 0.7061 - val_accuracy: 0.1970\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6285 - accuracy: 0.6510\n",
      "Epoch 6: val_loss did not improve from 0.70186\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6347 - accuracy: 0.6540 - val_loss: 0.7032 - val_accuracy: 0.2879\n",
      "Epoch 7/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6072 - accuracy: 0.6645\n",
      "Epoch 7: val_loss improved from 0.70186 to 0.70101, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6017 - accuracy: 0.6642 - val_loss: 0.7010 - val_accuracy: 0.3333\n",
      "Epoch 8/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5438 - accuracy: 0.7361\n",
      "Epoch 8: val_loss improved from 0.70101 to 0.69660, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5615 - accuracy: 0.7212 - val_loss: 0.6966 - val_accuracy: 0.4394\n",
      "Epoch 9/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5454 - accuracy: 0.7261\n",
      "Epoch 9: val_loss improved from 0.69660 to 0.69514, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.5430 - accuracy: 0.7197 - val_loss: 0.6951 - val_accuracy: 0.5000\n",
      "Epoch 10/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5419 - accuracy: 0.7309\n",
      "Epoch 10: val_loss improved from 0.69514 to 0.68933, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.5249 - accuracy: 0.7431 - val_loss: 0.6893 - val_accuracy: 0.5758\n",
      "Epoch 11/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4962 - accuracy: 0.7582\n",
      "Epoch 11: val_loss improved from 0.68933 to 0.68623, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4850 - accuracy: 0.7679 - val_loss: 0.6862 - val_accuracy: 0.5909\n",
      "Epoch 12/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4902 - accuracy: 0.7413\n",
      "Epoch 12: val_loss did not improve from 0.68623\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4957 - accuracy: 0.7372 - val_loss: 0.6886 - val_accuracy: 0.5152\n",
      "Epoch 13/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4763 - accuracy: 0.7780\n",
      "Epoch 13: val_loss improved from 0.68623 to 0.67535, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.4800 - accuracy: 0.7752 - val_loss: 0.6754 - val_accuracy: 0.5909\n",
      "Epoch 14/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4756 - accuracy: 0.7886\n",
      "Epoch 14: val_loss did not improve from 0.67535\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4699 - accuracy: 0.7927 - val_loss: 0.6755 - val_accuracy: 0.5758\n",
      "Epoch 15/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.4471 - accuracy: 0.7984\n",
      "Epoch 15: val_loss improved from 0.67535 to 0.67121, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4463 - accuracy: 0.7971 - val_loss: 0.6712 - val_accuracy: 0.5909\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4217 - accuracy: 0.8102\n",
      "Epoch 16: val_loss improved from 0.67121 to 0.66115, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.4217 - accuracy: 0.8102 - val_loss: 0.6612 - val_accuracy: 0.6364\n",
      "Epoch 17/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3880 - accuracy: 0.8472\n",
      "Epoch 17: val_loss did not improve from 0.66115\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3879 - accuracy: 0.8467 - val_loss: 0.6628 - val_accuracy: 0.6212\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4072 - accuracy: 0.8146\n",
      "Epoch 18: val_loss did not improve from 0.66115\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4072 - accuracy: 0.8146 - val_loss: 0.6694 - val_accuracy: 0.5909\n",
      "Epoch 19/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3771 - accuracy: 0.8536\n",
      "Epoch 19: val_loss did not improve from 0.66115\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3817 - accuracy: 0.8482 - val_loss: 0.6625 - val_accuracy: 0.5909\n",
      "Epoch 20/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.3647 - accuracy: 0.8512\n",
      "Epoch 20: val_loss improved from 0.66115 to 0.65792, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3634 - accuracy: 0.8526 - val_loss: 0.6579 - val_accuracy: 0.6061\n",
      "Epoch 21/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3433 - accuracy: 0.8594\n",
      "Epoch 21: val_loss did not improve from 0.65792\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3613 - accuracy: 0.8423 - val_loss: 0.6705 - val_accuracy: 0.5606\n",
      "Epoch 22/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3238 - accuracy: 0.8819\n",
      "Epoch 22: val_loss did not improve from 0.65792\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3301 - accuracy: 0.8788 - val_loss: 0.6761 - val_accuracy: 0.5606\n",
      "Epoch 23/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3348 - accuracy: 0.8503\n",
      "Epoch 23: val_loss did not improve from 0.65792\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3321 - accuracy: 0.8555 - val_loss: 0.6729 - val_accuracy: 0.6061\n",
      "Epoch 24/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3268 - accuracy: 0.8832\n",
      "Epoch 24: val_loss did not improve from 0.65792\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3195 - accuracy: 0.8905 - val_loss: 0.6632 - val_accuracy: 0.6818\n",
      "Epoch 25/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2860 - accuracy: 0.9128\n",
      "Epoch 25: val_loss did not improve from 0.65792\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.2957 - accuracy: 0.9036 - val_loss: 0.6805 - val_accuracy: 0.5909\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2936 - accuracy: 0.8978\n",
      "Epoch 26: val_loss did not improve from 0.65792\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.2936 - accuracy: 0.8978 - val_loss: 0.6752 - val_accuracy: 0.6212\n",
      "Epoch 27/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2982 - accuracy: 0.8898\n",
      "Epoch 27: val_loss did not improve from 0.65792\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2966 - accuracy: 0.8920 - val_loss: 0.6724 - val_accuracy: 0.6364\n",
      "Epoch 28/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2728 - accuracy: 0.9062\n",
      "Epoch 28: val_loss did not improve from 0.65792\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.2709 - accuracy: 0.9109 - val_loss: 0.6989 - val_accuracy: 0.6061\n",
      "Epoch 29/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2655 - accuracy: 0.9062\n",
      "Epoch 29: val_loss did not improve from 0.65792\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.2629 - accuracy: 0.9109 - val_loss: 0.7079 - val_accuracy: 0.5909\n",
      "Epoch 30/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.2396 - accuracy: 0.9154\n",
      "Epoch 30: val_loss did not improve from 0.65792\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2489 - accuracy: 0.9139 - val_loss: 0.7255 - val_accuracy: 0.5758\n",
      "Epoch 31/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2371 - accuracy: 0.9145\n",
      "Epoch 31: val_loss did not improve from 0.65792\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2337 - accuracy: 0.9197 - val_loss: 0.7250 - val_accuracy: 0.5909\n",
      "Epoch 32/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.2410 - accuracy: 0.9132\n",
      "Epoch 32: val_loss did not improve from 0.65792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2525 - accuracy: 0.9051 - val_loss: 0.7570 - val_accuracy: 0.5606\n",
      "Epoch 33/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.2271 - accuracy: 0.9266\n",
      "Epoch 33: val_loss did not improve from 0.65792\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.2233 - accuracy: 0.9285 - val_loss: 0.7635 - val_accuracy: 0.5758\n",
      "Epoch 34/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2142 - accuracy: 0.9457\n",
      "Epoch 34: val_loss did not improve from 0.65792\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2197 - accuracy: 0.9387 - val_loss: 0.8093 - val_accuracy: 0.5758\n",
      "Epoch 35/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2193 - accuracy: 0.9408Restoring model weights from the end of the best epoch: 20.\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.65792\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2151 - accuracy: 0.9416 - val_loss: 0.7850 - val_accuracy: 0.5909\n",
      "Epoch 35: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5648\n",
      "Test AUC for Layer 2: 0.5575\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_68 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_51 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_52 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_53 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.8745 - accuracy: 0.5043\n",
      "Epoch 1: val_loss improved from inf to 0.66189, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 19ms/step - loss: 0.8741 - accuracy: 0.5033 - val_loss: 0.6619 - val_accuracy: 0.6667\n",
      "Epoch 2/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.7556 - accuracy: 0.5789\n",
      "Epoch 2: val_loss improved from 0.66189 to 0.64649, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.7445 - accuracy: 0.5859 - val_loss: 0.6465 - val_accuracy: 0.6667\n",
      "Epoch 3/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.7475 - accuracy: 0.5910\n",
      "Epoch 3: val_loss improved from 0.64649 to 0.64072, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.7485 - accuracy: 0.5912 - val_loss: 0.6407 - val_accuracy: 0.6667\n",
      "Epoch 4/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.7291 - accuracy: 0.5855\n",
      "Epoch 4: val_loss improved from 0.64072 to 0.63881, saving model to Merged_OpenAI_MLP/visualizations_mlp/XOM\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.7364 - accuracy: 0.5819 - val_loss: 0.6388 - val_accuracy: 0.6667\n",
      "Epoch 5/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6762 - accuracy: 0.6316\n",
      "Epoch 5: val_loss did not improve from 0.63881\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6735 - accuracy: 0.6378 - val_loss: 0.6391 - val_accuracy: 0.6667\n",
      "Epoch 6/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6664 - accuracy: 0.6118\n",
      "Epoch 6: val_loss did not improve from 0.63881\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.6594 - accuracy: 0.6325 - val_loss: 0.6396 - val_accuracy: 0.6667\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6202 - accuracy: 0.6591\n",
      "Epoch 7: val_loss did not improve from 0.63881\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6202 - accuracy: 0.6591 - val_loss: 0.6406 - val_accuracy: 0.6667\n",
      "Epoch 8/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5853 - accuracy: 0.6990\n",
      "Epoch 8: val_loss did not improve from 0.63881\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5936 - accuracy: 0.6924 - val_loss: 0.6423 - val_accuracy: 0.6667\n",
      "Epoch 9/100\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.5075 - accuracy: 0.7359\n",
      "Epoch 9: val_loss did not improve from 0.63881\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5331 - accuracy: 0.7177 - val_loss: 0.6436 - val_accuracy: 0.6667\n",
      "Epoch 10/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5352 - accuracy: 0.7089\n",
      "Epoch 10: val_loss did not improve from 0.63881\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5389 - accuracy: 0.7111 - val_loss: 0.6473 - val_accuracy: 0.6667\n",
      "Epoch 11/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.5182 - accuracy: 0.7358\n",
      "Epoch 11: val_loss did not improve from 0.63881\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5200 - accuracy: 0.7350 - val_loss: 0.6509 - val_accuracy: 0.6667\n",
      "Epoch 12/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4807 - accuracy: 0.7582\n",
      "Epoch 12: val_loss did not improve from 0.63881\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4741 - accuracy: 0.7683 - val_loss: 0.6515 - val_accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4586 - accuracy: 0.7648\n",
      "Epoch 13: val_loss did not improve from 0.63881\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4608 - accuracy: 0.7683 - val_loss: 0.6528 - val_accuracy: 0.6667\n",
      "Epoch 14/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4464 - accuracy: 0.8026\n",
      "Epoch 14: val_loss did not improve from 0.63881\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4555 - accuracy: 0.7936 - val_loss: 0.6540 - val_accuracy: 0.6667\n",
      "Epoch 15/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3985 - accuracy: 0.8315\n",
      "Epoch 15: val_loss did not improve from 0.63881\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3992 - accuracy: 0.8282 - val_loss: 0.6601 - val_accuracy: 0.6667\n",
      "Epoch 16/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.4131 - accuracy: 0.8065\n",
      "Epoch 16: val_loss did not improve from 0.63881\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 0.4168 - accuracy: 0.8003 - val_loss: 0.6640 - val_accuracy: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3739 - accuracy: 0.8336\n",
      "Epoch 17: val_loss did not improve from 0.63881\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3739 - accuracy: 0.8336 - val_loss: 0.6629 - val_accuracy: 0.6667\n",
      "Epoch 18/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.3738 - accuracy: 0.8438\n",
      "Epoch 18: val_loss did not improve from 0.63881\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.3678 - accuracy: 0.8495 - val_loss: 0.6724 - val_accuracy: 0.6574\n",
      "Epoch 19/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3701 - accuracy: 0.8454Restoring model weights from the end of the best epoch: 4.\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.63881\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3715 - accuracy: 0.8362 - val_loss: 0.6705 - val_accuracy: 0.6481\n",
      "Epoch 19: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.3913\n",
      "Test AUC for Layer 3: 0.5053\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4096\n",
      "Average Test AUC across all layers: 0.5063\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS (XOM)\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Merged + S_label\n",
      "Average Accuracy: 0.5164\n",
      "Average AUC: 0.5004\n",
      "  Layer 1 - Accuracy: 0.6212, AUC: 0.4868\n",
      "  Layer 2 - Accuracy: 0.4352, AUC: 0.5075\n",
      "  Layer 3 - Accuracy: 0.4928, AUC: 0.5067\n",
      "\n",
      "Combination: MLP with Merged + L_label\n",
      "Average Accuracy: 0.4096\n",
      "Average AUC: 0.5063\n",
      "  Layer 1 - Accuracy: 0.2727, AUC: 0.4561\n",
      "  Layer 2 - Accuracy: 0.5648, AUC: 0.5575\n",
      "  Layer 3 - Accuracy: 0.3913, AUC: 0.5053\n",
      "MLP summary comparison visualization saved as: Merged_OpenAI_MLP/visualizations_summary/XOM\\mlp_merged_performance_comparison.png\n",
      "MLP layer performance visualization saved as: Merged_OpenAI_MLP/visualizations_summary/XOM\\mlp_merged_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class MergedEmbeddingMLPPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using merged OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with merged OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'Merged_OpenAI_MLP/visualizations_mlp/XOM'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs('Merged_OpenAI_MLP/visualizations_summary/XOM', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with merged OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing merged OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert merged embeddings to numpy arrays\n",
    "        self.data['Merged_embedding'] = self.data['Merged_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_merged_embedding = self.data['Merged_embedding'].iloc[0]\n",
    "        print(f\"Sample Merged embedding shape: {sample_merged_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2019-2021), validation (2021-2021), test (2022-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-05-31'),\n",
    "            'val_start': pd.Timestamp('2021-06-01'),\n",
    "            'val_end': pd.Timestamp('2021-12-31'),\n",
    "            'test_start': pd.Timestamp('2022-01-01'),\n",
    "            'test_end': pd.Timestamp('2022-05-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2019-2021), validation (2022-2022), test (2022-2022)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-12-31'),\n",
    "            'val_start': pd.Timestamp('2022-01-01'),\n",
    "            'val_end': pd.Timestamp('2022-05-31'),\n",
    "            'test_start': pd.Timestamp('2022-06-01'),\n",
    "            'test_end': pd.Timestamp('2022-12-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2019-2022), validation (2022-2022), test (2023-2023)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2022-05-31'),\n",
    "            'val_start': pd.Timestamp('2022-06-01'),\n",
    "            'val_end': pd.Timestamp('2022-12-31'),\n",
    "            'test_start': pd.Timestamp('2023-01-01'),\n",
    "            'test_end': pd.Timestamp('2023-05-31')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data['Merged_embedding'].values)\n",
    "        X_val = np.stack(val_data['Merged_embedding'].values)\n",
    "        X_test = np.stack(test_data['Merged_embedding'].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_mlp_merged_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, label_col):\n",
    "        \"\"\"\n",
    "        Train and evaluate an MLP model for merged embeddings and a specific label column.\n",
    "        \n",
    "        Args:\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        combination_key = f\"MLP|Merged|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training MLP model for Merged Embeddings and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            model = self.create_mlp_model((1536,))\n",
    "            print(f\"Created MLP model for Merged Embeddings ({label_col})\")\n",
    "            model.summary()\n",
    "            \n",
    "            # Setup callbacks\n",
    "            plot_callback = PlotLearningCallback('Merged', label_col, i+1, self.mlp_viz_dir)\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=15,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            model_checkpoint = ModelCheckpoint(\n",
    "                filepath=f\"{self.mlp_viz_dir}/best_mlp_merged_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                monitor='val_loss',\n",
    "                save_weights_only=True,\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            print(f\"Training MLP model...\")\n",
    "            batch_size = 32  # Larger batch size for embeddings\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred_proba = model.predict(X_test)\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "            \n",
    "            # Create final learning curve visualization\n",
    "            self.plot_final_learning_curves(history, label_col, i+1, self.mlp_viz_dir)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': history.history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for merged embeddings with both short-term and long-term labels.\"\"\"\n",
    "        # Define label columns\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        \n",
    "        # Run analysis for each label column\n",
    "        for label_col in label_cols:\n",
    "            self.train_and_evaluate_model(label_col)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS (XOM)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Performance comparison for MLP with merged embeddings\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Plot accuracy and AUC bars\n",
    "        x = np.arange(len(df))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, df['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, df['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with Merged OpenAI Embeddings')\n",
    "        labels = [f\"Merged + {row['Label']}\" for _, row in df.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(df['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(df['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(df['Avg Accuracy'].max(), df['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/XOM', \"mlp_merged_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                layer_data.append(layer_info)\n",
    "        \n",
    "        if not layer_data:\n",
    "            print(\"No layer data available\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title('MLP Accuracy by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title('MLP AUC by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/XOM', \"mlp_merged_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['Merged_OpenAI_MLP/visualizations_mlp/XOM', 'Merged_OpenAI_MLP/visualizations_summary/XOM']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/US_news/SP500_semantic/us_news_semantics_XOM_completed_openai.csv'\n",
    "    \n",
    "    # Initialize the predictor with merged OpenAI embeddings\n",
    "    predictor = MergedEmbeddingMLPPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing OpenAI embedding vectors from string format...\n",
      "Sample Title embedding shape: (1536,)\n",
      "Sample Fulltext embedding shape: (1536,)\n",
      "Loaded 929 climate change news articles spanning from 02/01/2019 to 07/05/2023\n",
      "Class distribution for short-term prediction: {1: 472, 0: 457}\n",
      "Class distribution for long-term prediction: {1: 494, 0: 435}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_72 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_54 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_55 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_56 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.9140 - accuracy: 0.4746\n",
      "Epoch 1: val_loss improved from inf to 0.69396, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 24ms/step - loss: 0.9173 - accuracy: 0.4693 - val_loss: 0.6940 - val_accuracy: 0.4601\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8128 - accuracy: 0.5488\n",
      "Epoch 2: val_loss did not improve from 0.69396\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.8102 - accuracy: 0.5517 - val_loss: 0.6949 - val_accuracy: 0.4601\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7428 - accuracy: 0.5820\n",
      "Epoch 3: val_loss did not improve from 0.69396\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7420 - accuracy: 0.5805 - val_loss: 0.6959 - val_accuracy: 0.4601\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7025 - accuracy: 0.6092\n",
      "Epoch 4: val_loss did not improve from 0.69396\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7025 - accuracy: 0.6092 - val_loss: 0.6962 - val_accuracy: 0.4601\n",
      "Epoch 5/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6659 - accuracy: 0.6406\n",
      "Epoch 5: val_loss did not improve from 0.69396\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6752 - accuracy: 0.6264 - val_loss: 0.6962 - val_accuracy: 0.4601\n",
      "Epoch 6/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6127 - accuracy: 0.6823\n",
      "Epoch 6: val_loss did not improve from 0.69396\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6137 - accuracy: 0.6877 - val_loss: 0.6959 - val_accuracy: 0.4908\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6062 - accuracy: 0.6954\n",
      "Epoch 7: val_loss did not improve from 0.69396\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6062 - accuracy: 0.6954 - val_loss: 0.6957 - val_accuracy: 0.4785\n",
      "Epoch 8/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5705 - accuracy: 0.6899\n",
      "Epoch 8: val_loss did not improve from 0.69396\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5844 - accuracy: 0.6839 - val_loss: 0.6949 - val_accuracy: 0.4969\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5733 - accuracy: 0.6923\n",
      "Epoch 9: val_loss did not improve from 0.69396\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5603 - accuracy: 0.7031 - val_loss: 0.6947 - val_accuracy: 0.5153\n",
      "Epoch 10/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5252 - accuracy: 0.7448\n",
      "Epoch 10: val_loss did not improve from 0.69396\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5262 - accuracy: 0.7433 - val_loss: 0.6945 - val_accuracy: 0.5276\n",
      "Epoch 11/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5019 - accuracy: 0.7500\n",
      "Epoch 11: val_loss did not improve from 0.69396\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5055 - accuracy: 0.7510 - val_loss: 0.6942 - val_accuracy: 0.4847\n",
      "Epoch 12/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4632 - accuracy: 0.7760\n",
      "Epoch 12: val_loss improved from 0.69396 to 0.69372, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4622 - accuracy: 0.7701 - val_loss: 0.6937 - val_accuracy: 0.5092\n",
      "Epoch 13/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4631 - accuracy: 0.7839\n",
      "Epoch 13: val_loss did not improve from 0.69372\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4633 - accuracy: 0.7912 - val_loss: 0.6940 - val_accuracy: 0.5215\n",
      "Epoch 14/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4209 - accuracy: 0.8047\n",
      "Epoch 14: val_loss did not improve from 0.69372\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4262 - accuracy: 0.8161 - val_loss: 0.6940 - val_accuracy: 0.5031\n",
      "Epoch 15/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4300 - accuracy: 0.7943\n",
      "Epoch 15: val_loss did not improve from 0.69372\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4215 - accuracy: 0.8103 - val_loss: 0.6941 - val_accuracy: 0.4969\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4190 - accuracy: 0.8125\n",
      "Epoch 16: val_loss did not improve from 0.69372\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4199 - accuracy: 0.8142 - val_loss: 0.6945 - val_accuracy: 0.4969\n",
      "Epoch 17/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4318 - accuracy: 0.7933\n",
      "Epoch 17: val_loss did not improve from 0.69372\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4184 - accuracy: 0.8142 - val_loss: 0.6941 - val_accuracy: 0.4785\n",
      "Epoch 18/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3471 - accuracy: 0.8894\n",
      "Epoch 18: val_loss did not improve from 0.69372\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3630 - accuracy: 0.8736 - val_loss: 0.6940 - val_accuracy: 0.4847\n",
      "Epoch 19/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3697 - accuracy: 0.8389\n",
      "Epoch 19: val_loss did not improve from 0.69372\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3628 - accuracy: 0.8487 - val_loss: 0.6944 - val_accuracy: 0.4601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3572 - accuracy: 0.8702\n",
      "Epoch 20: val_loss did not improve from 0.69372\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3602 - accuracy: 0.8716 - val_loss: 0.6951 - val_accuracy: 0.4908\n",
      "Epoch 21/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3257 - accuracy: 0.9036\n",
      "Epoch 21: val_loss did not improve from 0.69372\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3333 - accuracy: 0.8851 - val_loss: 0.6958 - val_accuracy: 0.4969\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3281 - accuracy: 0.8697\n",
      "Epoch 22: val_loss did not improve from 0.69372\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3281 - accuracy: 0.8697 - val_loss: 0.6973 - val_accuracy: 0.5031\n",
      "Epoch 23/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2978 - accuracy: 0.8966\n",
      "Epoch 23: val_loss did not improve from 0.69372\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3052 - accuracy: 0.8908 - val_loss: 0.6986 - val_accuracy: 0.5092\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2909 - accuracy: 0.8870\n",
      "Epoch 24: val_loss did not improve from 0.69372\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2909 - accuracy: 0.8870 - val_loss: 0.7013 - val_accuracy: 0.4785\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2820 - accuracy: 0.9061\n",
      "Epoch 25: val_loss did not improve from 0.69372\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2820 - accuracy: 0.9061 - val_loss: 0.7034 - val_accuracy: 0.4785\n",
      "Epoch 26/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2622 - accuracy: 0.9219\n",
      "Epoch 26: val_loss did not improve from 0.69372\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2575 - accuracy: 0.9291 - val_loss: 0.7061 - val_accuracy: 0.4908\n",
      "Epoch 27/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2486 - accuracy: 0.9349Restoring model weights from the end of the best epoch: 12.\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.69372\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2401 - accuracy: 0.9368 - val_loss: 0.7094 - val_accuracy: 0.5092\n",
      "Epoch 27: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.4394\n",
      "Test AUC for Layer 1: 0.4430\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_76 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_57 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_58 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_59 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9167 - accuracy: 0.5007\n",
      "Epoch 1: val_loss improved from inf to 0.69961, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 21ms/step - loss: 0.9167 - accuracy: 0.5007 - val_loss: 0.6996 - val_accuracy: 0.5152\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8805 - accuracy: 0.5208\n",
      "Epoch 2: val_loss did not improve from 0.69961\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.8741 - accuracy: 0.5241 - val_loss: 0.7244 - val_accuracy: 0.5152\n",
      "Epoch 3/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.7598 - accuracy: 0.5592\n",
      "Epoch 3: val_loss did not improve from 0.69961\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7588 - accuracy: 0.5664 - val_loss: 0.7565 - val_accuracy: 0.5152\n",
      "Epoch 4/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.7147 - accuracy: 0.5920\n",
      "Epoch 4: val_loss did not improve from 0.69961\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7022 - accuracy: 0.6044 - val_loss: 0.7884 - val_accuracy: 0.5152\n",
      "Epoch 5/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6263 - accuracy: 0.6684\n",
      "Epoch 5: val_loss did not improve from 0.69961\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6382 - accuracy: 0.6584 - val_loss: 0.8166 - val_accuracy: 0.5152\n",
      "Epoch 6/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6236 - accuracy: 0.6859\n",
      "Epoch 6: val_loss did not improve from 0.69961\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6274 - accuracy: 0.6803 - val_loss: 0.8408 - val_accuracy: 0.5152\n",
      "Epoch 7/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5981 - accuracy: 0.7135\n",
      "Epoch 7: val_loss did not improve from 0.69961\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5939 - accuracy: 0.7124 - val_loss: 0.8595 - val_accuracy: 0.5152\n",
      "Epoch 8/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5503 - accuracy: 0.7056\n",
      "Epoch 8: val_loss did not improve from 0.69961\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5544 - accuracy: 0.7007 - val_loss: 0.8724 - val_accuracy: 0.5152\n",
      "Epoch 9/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5479 - accuracy: 0.7040\n",
      "Epoch 9: val_loss did not improve from 0.69961\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5383 - accuracy: 0.7139 - val_loss: 0.8834 - val_accuracy: 0.5152\n",
      "Epoch 10/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4750 - accuracy: 0.7780\n",
      "Epoch 10: val_loss did not improve from 0.69961\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4678 - accuracy: 0.7810 - val_loss: 0.8892 - val_accuracy: 0.5152\n",
      "Epoch 11/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5080 - accuracy: 0.7632\n",
      "Epoch 11: val_loss did not improve from 0.69961\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5048 - accuracy: 0.7577 - val_loss: 0.8938 - val_accuracy: 0.5152\n",
      "Epoch 12/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4660 - accuracy: 0.7986\n",
      "Epoch 12: val_loss did not improve from 0.69961\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4842 - accuracy: 0.7869 - val_loss: 0.8994 - val_accuracy: 0.5152\n",
      "Epoch 13/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4353 - accuracy: 0.7899\n",
      "Epoch 13: val_loss did not improve from 0.69961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4320 - accuracy: 0.7942 - val_loss: 0.9004 - val_accuracy: 0.5152\n",
      "Epoch 14/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4165 - accuracy: 0.8109\n",
      "Epoch 14: val_loss did not improve from 0.69961\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4220 - accuracy: 0.8058 - val_loss: 0.9082 - val_accuracy: 0.5152\n",
      "Epoch 15/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3907 - accuracy: 0.8405\n",
      "Epoch 15: val_loss did not improve from 0.69961\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3916 - accuracy: 0.8365 - val_loss: 0.9122 - val_accuracy: 0.5152\n",
      "Epoch 16/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4007 - accuracy: 0.8174Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69961\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4007 - accuracy: 0.8146 - val_loss: 0.9072 - val_accuracy: 0.5152\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5741\n",
      "Test AUC for Layer 2: 0.4972\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_80 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_60 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_61 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_62 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.9133 - accuracy: 0.5085\n",
      "Epoch 1: val_loss improved from inf to 0.68685, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.9048 - accuracy: 0.5140 - val_loss: 0.6868 - val_accuracy: 0.5741\n",
      "Epoch 2/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.8634 - accuracy: 0.5270\n",
      "Epoch 2: val_loss improved from 0.68685 to 0.68388, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.8632 - accuracy: 0.5326 - val_loss: 0.6839 - val_accuracy: 0.5741\n",
      "Epoch 3/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.7585 - accuracy: 0.5978\n",
      "Epoch 3: val_loss did not improve from 0.68388\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.7615 - accuracy: 0.5965 - val_loss: 0.6844 - val_accuracy: 0.5741\n",
      "Epoch 4/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.7187 - accuracy: 0.6151\n",
      "Epoch 4: val_loss did not improve from 0.68388\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.7235 - accuracy: 0.6112 - val_loss: 0.6866 - val_accuracy: 0.5741\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6881 - accuracy: 0.6338\n",
      "Epoch 5: val_loss did not improve from 0.68388\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6881 - accuracy: 0.6338 - val_loss: 0.6895 - val_accuracy: 0.5741\n",
      "Epoch 6/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5930 - accuracy: 0.6760\n",
      "Epoch 6: val_loss did not improve from 0.68388\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5980 - accuracy: 0.6738 - val_loss: 0.6925 - val_accuracy: 0.5741\n",
      "Epoch 7/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5763 - accuracy: 0.7122\n",
      "Epoch 7: val_loss did not improve from 0.68388\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5803 - accuracy: 0.7004 - val_loss: 0.6950 - val_accuracy: 0.5741\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5526 - accuracy: 0.7364\n",
      "Epoch 8: val_loss did not improve from 0.68388\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5526 - accuracy: 0.7364 - val_loss: 0.6968 - val_accuracy: 0.5741\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5228 - accuracy: 0.7377\n",
      "Epoch 9: val_loss did not improve from 0.68388\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5228 - accuracy: 0.7377 - val_loss: 0.6987 - val_accuracy: 0.5741\n",
      "Epoch 10/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5159 - accuracy: 0.7385\n",
      "Epoch 10: val_loss did not improve from 0.68388\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5038 - accuracy: 0.7523 - val_loss: 0.6991 - val_accuracy: 0.5741\n",
      "Epoch 11/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4688 - accuracy: 0.7862\n",
      "Epoch 11: val_loss did not improve from 0.68388\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4706 - accuracy: 0.7790 - val_loss: 0.7006 - val_accuracy: 0.5741\n",
      "Epoch 12/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4742 - accuracy: 0.7713\n",
      "Epoch 12: val_loss did not improve from 0.68388\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4744 - accuracy: 0.7696 - val_loss: 0.7001 - val_accuracy: 0.5741\n",
      "Epoch 13/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4320 - accuracy: 0.8082\n",
      "Epoch 13: val_loss did not improve from 0.68388\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4313 - accuracy: 0.8083 - val_loss: 0.7017 - val_accuracy: 0.5833\n",
      "Epoch 14/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4327 - accuracy: 0.7961\n",
      "Epoch 14: val_loss did not improve from 0.68388\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4354 - accuracy: 0.7909 - val_loss: 0.7048 - val_accuracy: 0.5833\n",
      "Epoch 15/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3849 - accuracy: 0.8342\n",
      "Epoch 15: val_loss did not improve from 0.68388\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.3871 - accuracy: 0.8309 - val_loss: 0.7093 - val_accuracy: 0.5741\n",
      "Epoch 16/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3716 - accuracy: 0.8492\n",
      "Epoch 16: val_loss did not improve from 0.68388\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.3753 - accuracy: 0.8429 - val_loss: 0.7181 - val_accuracy: 0.5370\n",
      "Epoch 17/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3872 - accuracy: 0.8536Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.68388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3781 - accuracy: 0.8628 - val_loss: 0.7256 - val_accuracy: 0.5278\n",
      "Epoch 17: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5652\n",
      "Test AUC for Layer 3: 0.4726\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5262\n",
      "Average Test AUC across all layers: 0.4710\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_84 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_63 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_64 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_65 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8926 - accuracy: 0.4727\n",
      "Epoch 1: val_loss improved from inf to 0.69842, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8977 - accuracy: 0.4693 - val_loss: 0.6984 - val_accuracy: 0.4724\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7870 - accuracy: 0.5371\n",
      "Epoch 2: val_loss did not improve from 0.69842\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7867 - accuracy: 0.5383 - val_loss: 0.7055 - val_accuracy: 0.4724\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7417 - accuracy: 0.5785\n",
      "Epoch 3: val_loss did not improve from 0.69842\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7417 - accuracy: 0.5785 - val_loss: 0.7126 - val_accuracy: 0.4724\n",
      "Epoch 4/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6632 - accuracy: 0.6514\n",
      "Epoch 4: val_loss did not improve from 0.69842\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6609 - accuracy: 0.6552 - val_loss: 0.7175 - val_accuracy: 0.4724\n",
      "Epoch 5/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6626 - accuracy: 0.6394\n",
      "Epoch 5: val_loss did not improve from 0.69842\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6581 - accuracy: 0.6360 - val_loss: 0.7215 - val_accuracy: 0.4724\n",
      "Epoch 6/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6057 - accuracy: 0.6615\n",
      "Epoch 6: val_loss did not improve from 0.69842\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5921 - accuracy: 0.6667 - val_loss: 0.7236 - val_accuracy: 0.4724\n",
      "Epoch 7/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5958 - accuracy: 0.6927\n",
      "Epoch 7: val_loss did not improve from 0.69842\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5956 - accuracy: 0.6877 - val_loss: 0.7234 - val_accuracy: 0.4724\n",
      "Epoch 8/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5355 - accuracy: 0.7332\n",
      "Epoch 8: val_loss did not improve from 0.69842\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5493 - accuracy: 0.7241 - val_loss: 0.7224 - val_accuracy: 0.4724\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5044 - accuracy: 0.7668\n",
      "Epoch 9: val_loss did not improve from 0.69842\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5302 - accuracy: 0.7395 - val_loss: 0.7194 - val_accuracy: 0.4724\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5180 - accuracy: 0.7452\n",
      "Epoch 10: val_loss did not improve from 0.69842\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5104 - accuracy: 0.7567 - val_loss: 0.7157 - val_accuracy: 0.4724\n",
      "Epoch 11/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4520 - accuracy: 0.7861\n",
      "Epoch 11: val_loss did not improve from 0.69842\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4635 - accuracy: 0.7759 - val_loss: 0.7117 - val_accuracy: 0.4724\n",
      "Epoch 12/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4154 - accuracy: 0.8255\n",
      "Epoch 12: val_loss did not improve from 0.69842\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4312 - accuracy: 0.8046 - val_loss: 0.7072 - val_accuracy: 0.4724\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4287 - accuracy: 0.8103\n",
      "Epoch 13: val_loss did not improve from 0.69842\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4287 - accuracy: 0.8103 - val_loss: 0.7030 - val_accuracy: 0.4785\n",
      "Epoch 14/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3995 - accuracy: 0.8413\n",
      "Epoch 14: val_loss did not improve from 0.69842\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3914 - accuracy: 0.8467 - val_loss: 0.6994 - val_accuracy: 0.4908\n",
      "Epoch 15/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3845 - accuracy: 0.8385\n",
      "Epoch 15: val_loss improved from 0.69842 to 0.69626, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3813 - accuracy: 0.8467 - val_loss: 0.6963 - val_accuracy: 0.4785\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3902 - accuracy: 0.8574\n",
      "Epoch 16: val_loss improved from 0.69626 to 0.69341, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.3876 - accuracy: 0.8602 - val_loss: 0.6934 - val_accuracy: 0.4908\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3421 - accuracy: 0.8889\n",
      "Epoch 17: val_loss improved from 0.69341 to 0.69104, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.3421 - accuracy: 0.8889 - val_loss: 0.6910 - val_accuracy: 0.4785\n",
      "Epoch 18/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3360 - accuracy: 0.8770\n",
      "Epoch 18: val_loss improved from 0.69104 to 0.68804, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.3370 - accuracy: 0.8774 - val_loss: 0.6880 - val_accuracy: 0.4847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3193 - accuracy: 0.8906\n",
      "Epoch 19: val_loss did not improve from 0.68804\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3271 - accuracy: 0.8851 - val_loss: 0.6882 - val_accuracy: 0.5092\n",
      "Epoch 20/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3006 - accuracy: 0.9004\n",
      "Epoch 20: val_loss did not improve from 0.68804\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3008 - accuracy: 0.9023 - val_loss: 0.6888 - val_accuracy: 0.5215\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2916 - accuracy: 0.8985\n",
      "Epoch 21: val_loss did not improve from 0.68804\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2916 - accuracy: 0.8985 - val_loss: 0.6913 - val_accuracy: 0.5276\n",
      "Epoch 22/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2745 - accuracy: 0.9014\n",
      "Epoch 22: val_loss did not improve from 0.68804\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2865 - accuracy: 0.8946 - val_loss: 0.6924 - val_accuracy: 0.5153\n",
      "Epoch 23/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2761 - accuracy: 0.9183\n",
      "Epoch 23: val_loss did not improve from 0.68804\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2777 - accuracy: 0.9215 - val_loss: 0.6933 - val_accuracy: 0.5153\n",
      "Epoch 24/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2393 - accuracy: 0.9375\n",
      "Epoch 24: val_loss did not improve from 0.68804\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2417 - accuracy: 0.9387 - val_loss: 0.6944 - val_accuracy: 0.5031\n",
      "Epoch 25/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2538 - accuracy: 0.9323\n",
      "Epoch 25: val_loss did not improve from 0.68804\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2477 - accuracy: 0.9349 - val_loss: 0.6970 - val_accuracy: 0.5215\n",
      "Epoch 26/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2465 - accuracy: 0.9303\n",
      "Epoch 26: val_loss did not improve from 0.68804\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2524 - accuracy: 0.9195 - val_loss: 0.7009 - val_accuracy: 0.5092\n",
      "Epoch 27/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2244 - accuracy: 0.9297\n",
      "Epoch 27: val_loss did not improve from 0.68804\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2285 - accuracy: 0.9291 - val_loss: 0.7026 - val_accuracy: 0.5337\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2293 - accuracy: 0.9387\n",
      "Epoch 28: val_loss did not improve from 0.68804\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2293 - accuracy: 0.9387 - val_loss: 0.7089 - val_accuracy: 0.5276\n",
      "Epoch 29/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.1948 - accuracy: 0.9583\n",
      "Epoch 29: val_loss did not improve from 0.68804\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.1988 - accuracy: 0.9540 - val_loss: 0.7147 - val_accuracy: 0.5153\n",
      "Epoch 30/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2140 - accuracy: 0.9427\n",
      "Epoch 30: val_loss did not improve from 0.68804\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2012 - accuracy: 0.9540 - val_loss: 0.7197 - val_accuracy: 0.5276\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2062 - accuracy: 0.9464\n",
      "Epoch 31: val_loss did not improve from 0.68804\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2062 - accuracy: 0.9464 - val_loss: 0.7242 - val_accuracy: 0.5276\n",
      "Epoch 32/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2032 - accuracy: 0.9231\n",
      "Epoch 32: val_loss did not improve from 0.68804\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2028 - accuracy: 0.9253 - val_loss: 0.7306 - val_accuracy: 0.5337\n",
      "Epoch 33/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.1664 - accuracy: 0.9663Restoring model weights from the end of the best epoch: 18.\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.68804\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.1686 - accuracy: 0.9655 - val_loss: 0.7371 - val_accuracy: 0.5337\n",
      "Epoch 33: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.5455\n",
      "Test AUC for Layer 1: 0.4135\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_88 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_66 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_67 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_68 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.8924 - accuracy: 0.4844\n",
      "Epoch 1: val_loss improved from inf to 0.68380, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 21ms/step - loss: 0.8930 - accuracy: 0.4876 - val_loss: 0.6838 - val_accuracy: 0.6515\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7938 - accuracy: 0.5268\n",
      "Epoch 2: val_loss improved from 0.68380 to 0.67973, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.7912 - accuracy: 0.5299 - val_loss: 0.6797 - val_accuracy: 0.6515\n",
      "Epoch 3/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7690 - accuracy: 0.5446\n",
      "Epoch 3: val_loss improved from 0.67973 to 0.67710, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.7695 - accuracy: 0.5445 - val_loss: 0.6771 - val_accuracy: 0.6515\n",
      "Epoch 4/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6461 - accuracy: 0.6118\n",
      "Epoch 4: val_loss improved from 0.67710 to 0.67639, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6492 - accuracy: 0.6102 - val_loss: 0.6764 - val_accuracy: 0.6515\n",
      "Epoch 5/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6751 - accuracy: 0.6069\n",
      "Epoch 5: val_loss did not improve from 0.67639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6779 - accuracy: 0.6088 - val_loss: 0.6783 - val_accuracy: 0.6667\n",
      "Epoch 6/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6306 - accuracy: 0.6480\n",
      "Epoch 6: val_loss did not improve from 0.67639\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6258 - accuracy: 0.6526 - val_loss: 0.6787 - val_accuracy: 0.6667\n",
      "Epoch 7/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5884 - accuracy: 0.6967\n",
      "Epoch 7: val_loss did not improve from 0.67639\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5946 - accuracy: 0.6964 - val_loss: 0.6785 - val_accuracy: 0.6667\n",
      "Epoch 8/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5561 - accuracy: 0.7039\n",
      "Epoch 8: val_loss did not improve from 0.67639\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5594 - accuracy: 0.7066 - val_loss: 0.6807 - val_accuracy: 0.6667\n",
      "Epoch 9/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5388 - accuracy: 0.7336\n",
      "Epoch 9: val_loss did not improve from 0.67639\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5370 - accuracy: 0.7328 - val_loss: 0.6826 - val_accuracy: 0.5909\n",
      "Epoch 10/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5166 - accuracy: 0.7385\n",
      "Epoch 10: val_loss did not improve from 0.67639\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5271 - accuracy: 0.7285 - val_loss: 0.6857 - val_accuracy: 0.5606\n",
      "Epoch 11/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4945 - accuracy: 0.7622\n",
      "Epoch 11: val_loss did not improve from 0.67639\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4962 - accuracy: 0.7591 - val_loss: 0.6920 - val_accuracy: 0.5152\n",
      "Epoch 12/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4713 - accuracy: 0.7862\n",
      "Epoch 12: val_loss did not improve from 0.67639\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4759 - accuracy: 0.7781 - val_loss: 0.6961 - val_accuracy: 0.4394\n",
      "Epoch 13/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4652 - accuracy: 0.7763\n",
      "Epoch 13: val_loss did not improve from 0.67639\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4664 - accuracy: 0.7737 - val_loss: 0.6996 - val_accuracy: 0.4394\n",
      "Epoch 14/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4109 - accuracy: 0.8405\n",
      "Epoch 14: val_loss did not improve from 0.67639\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4059 - accuracy: 0.8453 - val_loss: 0.7044 - val_accuracy: 0.4242\n",
      "Epoch 15/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4017 - accuracy: 0.8316\n",
      "Epoch 15: val_loss did not improve from 0.67639\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4168 - accuracy: 0.8234 - val_loss: 0.7072 - val_accuracy: 0.4242\n",
      "Epoch 16/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3713 - accuracy: 0.8507\n",
      "Epoch 16: val_loss did not improve from 0.67639\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3653 - accuracy: 0.8584 - val_loss: 0.7097 - val_accuracy: 0.4242\n",
      "Epoch 17/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3709 - accuracy: 0.8470\n",
      "Epoch 17: val_loss did not improve from 0.67639\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3635 - accuracy: 0.8555 - val_loss: 0.7104 - val_accuracy: 0.4545\n",
      "Epoch 18/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3326 - accuracy: 0.8783\n",
      "Epoch 18: val_loss did not improve from 0.67639\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3275 - accuracy: 0.8920 - val_loss: 0.7134 - val_accuracy: 0.4242\n",
      "Epoch 19/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3582 - accuracy: 0.8698Restoring model weights from the end of the best epoch: 4.\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.67639\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3575 - accuracy: 0.8672 - val_loss: 0.7189 - val_accuracy: 0.4697\n",
      "Epoch 19: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.6667\n",
      "Test AUC for Layer 2: 0.4907\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_92 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_69 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_70 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_71 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8847 - accuracy: 0.5312\n",
      "Epoch 1: val_loss improved from inf to 0.65290, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.8896 - accuracy: 0.5246 - val_loss: 0.6529 - val_accuracy: 0.6667\n",
      "Epoch 2/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.8231 - accuracy: 0.5367\n",
      "Epoch 2: val_loss improved from 0.65290 to 0.63880, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.8203 - accuracy: 0.5366 - val_loss: 0.6388 - val_accuracy: 0.6667\n",
      "Epoch 3/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6871 - accuracy: 0.6080\n",
      "Epoch 3: val_loss improved from 0.63880 to 0.63563, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.6929 - accuracy: 0.6099 - val_loss: 0.6356 - val_accuracy: 0.6667\n",
      "Epoch 4/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6997 - accuracy: 0.6168\n",
      "Epoch 4: val_loss did not improve from 0.63563\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.7036 - accuracy: 0.6138 - val_loss: 0.6369 - val_accuracy: 0.6667\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6150 - accuracy: 0.6858\n",
      "Epoch 5: val_loss did not improve from 0.63563\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6150 - accuracy: 0.6858 - val_loss: 0.6393 - val_accuracy: 0.6667\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6225 - accuracy: 0.6678\n",
      "Epoch 6: val_loss did not improve from 0.63563\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.6198 - accuracy: 0.6698 - val_loss: 0.6402 - val_accuracy: 0.6667\n",
      "Epoch 7/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6063 - accuracy: 0.6793\n",
      "Epoch 7: val_loss did not improve from 0.63563\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5951 - accuracy: 0.6844 - val_loss: 0.6407 - val_accuracy: 0.6667\n",
      "Epoch 8/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5168 - accuracy: 0.7484\n",
      "Epoch 8: val_loss did not improve from 0.63563\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5266 - accuracy: 0.7390 - val_loss: 0.6404 - val_accuracy: 0.6667\n",
      "Epoch 9/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5241 - accuracy: 0.7368\n",
      "Epoch 9: val_loss did not improve from 0.63563\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5294 - accuracy: 0.7364 - val_loss: 0.6404 - val_accuracy: 0.6667\n",
      "Epoch 10/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4774 - accuracy: 0.7796\n",
      "Epoch 10: val_loss did not improve from 0.63563\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4772 - accuracy: 0.7830 - val_loss: 0.6402 - val_accuracy: 0.6667\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4857 - accuracy: 0.7483\n",
      "Epoch 11: val_loss did not improve from 0.63563\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4857 - accuracy: 0.7483 - val_loss: 0.6412 - val_accuracy: 0.6667\n",
      "Epoch 12/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4464 - accuracy: 0.7895\n",
      "Epoch 12: val_loss did not improve from 0.63563\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4450 - accuracy: 0.7896 - val_loss: 0.6406 - val_accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4213 - accuracy: 0.8191\n",
      "Epoch 13: val_loss did not improve from 0.63563\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4295 - accuracy: 0.8189 - val_loss: 0.6433 - val_accuracy: 0.6667\n",
      "Epoch 14/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4033 - accuracy: 0.8111\n",
      "Epoch 14: val_loss did not improve from 0.63563\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4031 - accuracy: 0.8109 - val_loss: 0.6466 - val_accuracy: 0.6667\n",
      "Epoch 15/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4227 - accuracy: 0.8174\n",
      "Epoch 15: val_loss did not improve from 0.63563\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4153 - accuracy: 0.8229 - val_loss: 0.6476 - val_accuracy: 0.6667\n",
      "Epoch 16/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3466 - accuracy: 0.8586\n",
      "Epoch 16: val_loss did not improve from 0.63563\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3521 - accuracy: 0.8482 - val_loss: 0.6502 - val_accuracy: 0.6667\n",
      "Epoch 17/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3216 - accuracy: 0.8717\n",
      "Epoch 17: val_loss did not improve from 0.63563\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3343 - accuracy: 0.8642 - val_loss: 0.6583 - val_accuracy: 0.6667\n",
      "Epoch 18/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3204 - accuracy: 0.8717Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.63563\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3372 - accuracy: 0.8655 - val_loss: 0.6617 - val_accuracy: 0.6667\n",
      "Epoch 18: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.3188\n",
      "Test AUC for Layer 3: 0.3085\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5103\n",
      "Average Test AUC across all layers: 0.4043\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_96 (Dense)            (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_72 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_73 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_74 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8688 - accuracy: 0.5117\n",
      "Epoch 1: val_loss improved from inf to 0.69068, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8674 - accuracy: 0.5077 - val_loss: 0.6907 - val_accuracy: 0.5399\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8045 - accuracy: 0.5364\n",
      "Epoch 2: val_loss improved from 0.69068 to 0.69047, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.8045 - accuracy: 0.5364 - val_loss: 0.6905 - val_accuracy: 0.5399\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7596 - accuracy: 0.5517\n",
      "Epoch 3: val_loss did not improve from 0.69047\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7596 - accuracy: 0.5517 - val_loss: 0.6927 - val_accuracy: 0.5399\n",
      "Epoch 4/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.7491 - accuracy: 0.5769\n",
      "Epoch 4: val_loss did not improve from 0.69047\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7469 - accuracy: 0.5824 - val_loss: 0.6963 - val_accuracy: 0.5399\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6594 - accuracy: 0.6418\n",
      "Epoch 5: val_loss did not improve from 0.69047\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6594 - accuracy: 0.6418 - val_loss: 0.7008 - val_accuracy: 0.5399\n",
      "Epoch 6/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6532 - accuracy: 0.6484\n",
      "Epoch 6: val_loss did not improve from 0.69047\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6490 - accuracy: 0.6590 - val_loss: 0.7056 - val_accuracy: 0.5399\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6393 - accuracy: 0.6328\n",
      "Epoch 7: val_loss did not improve from 0.69047\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6226 - accuracy: 0.6571 - val_loss: 0.7110 - val_accuracy: 0.5399\n",
      "Epoch 8/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6115 - accuracy: 0.6562\n",
      "Epoch 8: val_loss did not improve from 0.69047\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6036 - accuracy: 0.6609 - val_loss: 0.7159 - val_accuracy: 0.5399\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6132 - accuracy: 0.6935\n",
      "Epoch 9: val_loss did not improve from 0.69047\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6132 - accuracy: 0.6935 - val_loss: 0.7210 - val_accuracy: 0.5399\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5465 - accuracy: 0.7266\n",
      "Epoch 10: val_loss did not improve from 0.69047\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5481 - accuracy: 0.7261 - val_loss: 0.7253 - val_accuracy: 0.5399\n",
      "Epoch 11/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5389 - accuracy: 0.7139\n",
      "Epoch 11: val_loss did not improve from 0.69047\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5378 - accuracy: 0.7069 - val_loss: 0.7325 - val_accuracy: 0.5399\n",
      "Epoch 12/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5213 - accuracy: 0.7344\n",
      "Epoch 12: val_loss did not improve from 0.69047\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5214 - accuracy: 0.7395 - val_loss: 0.7359 - val_accuracy: 0.5399\n",
      "Epoch 13/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4923 - accuracy: 0.7266\n",
      "Epoch 13: val_loss did not improve from 0.69047\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4937 - accuracy: 0.7299 - val_loss: 0.7408 - val_accuracy: 0.5399\n",
      "Epoch 14/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4599 - accuracy: 0.7716\n",
      "Epoch 14: val_loss did not improve from 0.69047\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4503 - accuracy: 0.7854 - val_loss: 0.7431 - val_accuracy: 0.5399\n",
      "Epoch 15/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4462 - accuracy: 0.7969\n",
      "Epoch 15: val_loss did not improve from 0.69047\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4463 - accuracy: 0.8008 - val_loss: 0.7448 - val_accuracy: 0.5399\n",
      "Epoch 16/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4726 - accuracy: 0.7865\n",
      "Epoch 16: val_loss did not improve from 0.69047\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4713 - accuracy: 0.7854 - val_loss: 0.7500 - val_accuracy: 0.5399\n",
      "Epoch 17/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4338 - accuracy: 0.7885Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.69047\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4342 - accuracy: 0.7989 - val_loss: 0.7554 - val_accuracy: 0.5399\n",
      "Epoch 17: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.4848\n",
      "Test AUC for Layer 1: 0.4816\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_100 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_75 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_76 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_77 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9161 - accuracy: 0.4745\n",
      "Epoch 1: val_loss improved from inf to 0.70295, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 21ms/step - loss: 0.9161 - accuracy: 0.4745 - val_loss: 0.7029 - val_accuracy: 0.5152\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8124 - accuracy: 0.5387\n",
      "Epoch 2: val_loss did not improve from 0.70295\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.8124 - accuracy: 0.5387 - val_loss: 0.7400 - val_accuracy: 0.5152\n",
      "Epoch 3/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.7842 - accuracy: 0.5486\n",
      "Epoch 3: val_loss did not improve from 0.70295\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7814 - accuracy: 0.5606 - val_loss: 0.7849 - val_accuracy: 0.5152\n",
      "Epoch 4/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.7059 - accuracy: 0.6250\n",
      "Epoch 4: val_loss did not improve from 0.70295\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6960 - accuracy: 0.6350 - val_loss: 0.8377 - val_accuracy: 0.5152\n",
      "Epoch 5/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6461 - accuracy: 0.6458\n",
      "Epoch 5: val_loss did not improve from 0.70295\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6434 - accuracy: 0.6540 - val_loss: 0.8844 - val_accuracy: 0.5152\n",
      "Epoch 6/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6461 - accuracy: 0.6399\n",
      "Epoch 6: val_loss did not improve from 0.70295\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6450 - accuracy: 0.6409 - val_loss: 0.9249 - val_accuracy: 0.5152\n",
      "Epoch 7/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5877 - accuracy: 0.6912\n",
      "Epoch 7: val_loss did not improve from 0.70295\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5861 - accuracy: 0.6905 - val_loss: 0.9589 - val_accuracy: 0.5152\n",
      "Epoch 8/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5663 - accuracy: 0.7039\n",
      "Epoch 8: val_loss did not improve from 0.70295\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5608 - accuracy: 0.7080 - val_loss: 0.9952 - val_accuracy: 0.5152\n",
      "Epoch 9/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5527 - accuracy: 0.7039\n",
      "Epoch 9: val_loss did not improve from 0.70295\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5528 - accuracy: 0.7036 - val_loss: 1.0207 - val_accuracy: 0.5152\n",
      "Epoch 10/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5412 - accuracy: 0.7270\n",
      "Epoch 10: val_loss did not improve from 0.70295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5416 - accuracy: 0.7226 - val_loss: 1.0350 - val_accuracy: 0.5152\n",
      "Epoch 11/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5339 - accuracy: 0.7599\n",
      "Epoch 11: val_loss did not improve from 0.70295\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5368 - accuracy: 0.7562 - val_loss: 1.0455 - val_accuracy: 0.5152\n",
      "Epoch 12/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4995 - accuracy: 0.7500\n",
      "Epoch 12: val_loss did not improve from 0.70295\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4991 - accuracy: 0.7489 - val_loss: 1.0465 - val_accuracy: 0.5152\n",
      "Epoch 13/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5234 - accuracy: 0.7319\n",
      "Epoch 13: val_loss did not improve from 0.70295\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5210 - accuracy: 0.7299 - val_loss: 1.0457 - val_accuracy: 0.5152\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4735 - accuracy: 0.7839\n",
      "Epoch 14: val_loss did not improve from 0.70295\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4735 - accuracy: 0.7839 - val_loss: 1.0518 - val_accuracy: 0.5152\n",
      "Epoch 15/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4469 - accuracy: 0.7934\n",
      "Epoch 15: val_loss did not improve from 0.70295\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4569 - accuracy: 0.7869 - val_loss: 1.0485 - val_accuracy: 0.5152\n",
      "Epoch 16/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4493 - accuracy: 0.8010Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70295\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4478 - accuracy: 0.8000 - val_loss: 1.0422 - val_accuracy: 0.5152\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5741\n",
      "Test AUC for Layer 2: 0.5712\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_104 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_78 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_79 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_80 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8887 - accuracy: 0.5149\n",
      "Epoch 1: val_loss improved from inf to 0.70991, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 21ms/step - loss: 0.8815 - accuracy: 0.5126 - val_loss: 0.7099 - val_accuracy: 0.4259\n",
      "Epoch 2/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8087 - accuracy: 0.5461\n",
      "Epoch 2: val_loss did not improve from 0.70991\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.8057 - accuracy: 0.5553 - val_loss: 0.7297 - val_accuracy: 0.4259\n",
      "Epoch 3/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.7729 - accuracy: 0.5822\n",
      "Epoch 3: val_loss did not improve from 0.70991\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.7753 - accuracy: 0.5766 - val_loss: 0.7522 - val_accuracy: 0.4259\n",
      "Epoch 4/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.7348 - accuracy: 0.5829\n",
      "Epoch 4: val_loss did not improve from 0.70991\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.7316 - accuracy: 0.5832 - val_loss: 0.7716 - val_accuracy: 0.4259\n",
      "Epoch 5/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.7173 - accuracy: 0.6042\n",
      "Epoch 5: val_loss did not improve from 0.70991\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.7073 - accuracy: 0.6138 - val_loss: 0.7896 - val_accuracy: 0.4259\n",
      "Epoch 6/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6148 - accuracy: 0.6747\n",
      "Epoch 6: val_loss did not improve from 0.70991\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6194 - accuracy: 0.6711 - val_loss: 0.8065 - val_accuracy: 0.4259\n",
      "Epoch 7/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5886 - accuracy: 0.6771\n",
      "Epoch 7: val_loss did not improve from 0.70991\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6082 - accuracy: 0.6738 - val_loss: 0.8215 - val_accuracy: 0.4259\n",
      "Epoch 8/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5835 - accuracy: 0.6793\n",
      "Epoch 8: val_loss did not improve from 0.70991\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5835 - accuracy: 0.6791 - val_loss: 0.8285 - val_accuracy: 0.4259\n",
      "Epoch 9/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.5571 - accuracy: 0.7045\n",
      "Epoch 9: val_loss did not improve from 0.70991\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5559 - accuracy: 0.7097 - val_loss: 0.8390 - val_accuracy: 0.4259\n",
      "Epoch 10/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5722 - accuracy: 0.7007\n",
      "Epoch 10: val_loss did not improve from 0.70991\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5746 - accuracy: 0.6951 - val_loss: 0.8356 - val_accuracy: 0.4259\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4990 - accuracy: 0.7723\n",
      "Epoch 11: val_loss did not improve from 0.70991\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4990 - accuracy: 0.7723 - val_loss: 0.8408 - val_accuracy: 0.4259\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4795 - accuracy: 0.7790\n",
      "Epoch 12: val_loss did not improve from 0.70991\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4795 - accuracy: 0.7790 - val_loss: 0.8464 - val_accuracy: 0.4259\n",
      "Epoch 13/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4874 - accuracy: 0.7582\n",
      "Epoch 13: val_loss did not improve from 0.70991\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4897 - accuracy: 0.7577 - val_loss: 0.8530 - val_accuracy: 0.4259\n",
      "Epoch 14/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4866 - accuracy: 0.7500\n",
      "Epoch 14: val_loss did not improve from 0.70991\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4876 - accuracy: 0.7537 - val_loss: 0.8495 - val_accuracy: 0.4259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4545 - accuracy: 0.8043\n",
      "Epoch 15: val_loss did not improve from 0.70991\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4519 - accuracy: 0.8016 - val_loss: 0.8502 - val_accuracy: 0.4259\n",
      "Epoch 16/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4166 - accuracy: 0.8191Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70991\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4318 - accuracy: 0.8083 - val_loss: 0.8450 - val_accuracy: 0.4259\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4348\n",
      "Test AUC for Layer 3: 0.4248\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4979\n",
      "Average Test AUC across all layers: 0.4925\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_108 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_81 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_82 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_55 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_83 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.9888 - accuracy: 0.4746\n",
      "Epoch 1: val_loss improved from inf to 0.70029, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 24ms/step - loss: 0.9889 - accuracy: 0.4751 - val_loss: 0.7003 - val_accuracy: 0.4724\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7953 - accuracy: 0.5430\n",
      "Epoch 2: val_loss did not improve from 0.70029\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7994 - accuracy: 0.5421 - val_loss: 0.7058 - val_accuracy: 0.4724\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6945 - accuracy: 0.6445\n",
      "Epoch 3: val_loss did not improve from 0.70029\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6908 - accuracy: 0.6456 - val_loss: 0.7090 - val_accuracy: 0.4724\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7353 - accuracy: 0.5898\n",
      "Epoch 4: val_loss did not improve from 0.70029\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7323 - accuracy: 0.5920 - val_loss: 0.7104 - val_accuracy: 0.4724\n",
      "Epoch 5/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6699 - accuracy: 0.6322\n",
      "Epoch 5: val_loss did not improve from 0.70029\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6754 - accuracy: 0.6226 - val_loss: 0.7094 - val_accuracy: 0.4724\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6281 - accuracy: 0.6803\n",
      "Epoch 6: val_loss did not improve from 0.70029\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6248 - accuracy: 0.6762 - val_loss: 0.7076 - val_accuracy: 0.4724\n",
      "Epoch 7/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5827 - accuracy: 0.6851\n",
      "Epoch 7: val_loss did not improve from 0.70029\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5907 - accuracy: 0.6839 - val_loss: 0.7064 - val_accuracy: 0.4724\n",
      "Epoch 8/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5773 - accuracy: 0.6927\n",
      "Epoch 8: val_loss did not improve from 0.70029\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5808 - accuracy: 0.6877 - val_loss: 0.7041 - val_accuracy: 0.4724\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5573 - accuracy: 0.7067\n",
      "Epoch 9: val_loss did not improve from 0.70029\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5559 - accuracy: 0.7203 - val_loss: 0.7012 - val_accuracy: 0.4785\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5503 - accuracy: 0.7337\n",
      "Epoch 10: val_loss improved from 0.70029 to 0.69904, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.5503 - accuracy: 0.7337 - val_loss: 0.6990 - val_accuracy: 0.4785\n",
      "Epoch 11/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5178 - accuracy: 0.7318\n",
      "Epoch 11: val_loss improved from 0.69904 to 0.69772, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5118 - accuracy: 0.7433 - val_loss: 0.6977 - val_accuracy: 0.4601\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4767 - accuracy: 0.7832\n",
      "Epoch 12: val_loss improved from 0.69772 to 0.69594, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4776 - accuracy: 0.7835 - val_loss: 0.6959 - val_accuracy: 0.4663\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4717 - accuracy: 0.7852\n",
      "Epoch 13: val_loss improved from 0.69594 to 0.69532, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4679 - accuracy: 0.7874 - val_loss: 0.6953 - val_accuracy: 0.5215\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4354 - accuracy: 0.8103\n",
      "Epoch 14: val_loss improved from 0.69532 to 0.69501, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4354 - accuracy: 0.8103 - val_loss: 0.6950 - val_accuracy: 0.5399\n",
      "Epoch 15/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4099 - accuracy: 0.7969\n",
      "Epoch 15: val_loss did not improve from 0.69501\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4144 - accuracy: 0.8065 - val_loss: 0.6957 - val_accuracy: 0.5153\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4650 - accuracy: 0.7740\n",
      "Epoch 16: val_loss did not improve from 0.69501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4391 - accuracy: 0.7989 - val_loss: 0.6959 - val_accuracy: 0.4908\n",
      "Epoch 17/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3956 - accuracy: 0.8005\n",
      "Epoch 17: val_loss did not improve from 0.69501\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3849 - accuracy: 0.8142 - val_loss: 0.6966 - val_accuracy: 0.5092\n",
      "Epoch 18/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3888 - accuracy: 0.8125\n",
      "Epoch 18: val_loss did not improve from 0.69501\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3905 - accuracy: 0.8142 - val_loss: 0.6980 - val_accuracy: 0.5276\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3427 - accuracy: 0.8602\n",
      "Epoch 19: val_loss did not improve from 0.69501\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3427 - accuracy: 0.8602 - val_loss: 0.7005 - val_accuracy: 0.5153\n",
      "Epoch 20/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3670 - accuracy: 0.8385\n",
      "Epoch 20: val_loss did not improve from 0.69501\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3689 - accuracy: 0.8372 - val_loss: 0.7039 - val_accuracy: 0.5399\n",
      "Epoch 21/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3358 - accuracy: 0.8510\n",
      "Epoch 21: val_loss did not improve from 0.69501\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3404 - accuracy: 0.8544 - val_loss: 0.7062 - val_accuracy: 0.5399\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3646 - accuracy: 0.8506\n",
      "Epoch 22: val_loss did not improve from 0.69501\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3646 - accuracy: 0.8506 - val_loss: 0.7093 - val_accuracy: 0.5399\n",
      "Epoch 23/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2947 - accuracy: 0.9036\n",
      "Epoch 23: val_loss did not improve from 0.69501\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2929 - accuracy: 0.9080 - val_loss: 0.7129 - val_accuracy: 0.5276\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3143 - accuracy: 0.8736\n",
      "Epoch 24: val_loss did not improve from 0.69501\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3143 - accuracy: 0.8736 - val_loss: 0.7177 - val_accuracy: 0.5215\n",
      "Epoch 25/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3056 - accuracy: 0.8750\n",
      "Epoch 25: val_loss did not improve from 0.69501\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3004 - accuracy: 0.8831 - val_loss: 0.7249 - val_accuracy: 0.4969\n",
      "Epoch 26/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3085 - accuracy: 0.8828\n",
      "Epoch 26: val_loss did not improve from 0.69501\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2933 - accuracy: 0.8927 - val_loss: 0.7351 - val_accuracy: 0.4724\n",
      "Epoch 27/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2870 - accuracy: 0.9010\n",
      "Epoch 27: val_loss did not improve from 0.69501\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2779 - accuracy: 0.9023 - val_loss: 0.7431 - val_accuracy: 0.4601\n",
      "Epoch 28/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2845 - accuracy: 0.8958\n",
      "Epoch 28: val_loss did not improve from 0.69501\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2785 - accuracy: 0.9042 - val_loss: 0.7520 - val_accuracy: 0.4540\n",
      "Epoch 29/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2953 - accuracy: 0.8724Restoring model weights from the end of the best epoch: 14.\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.69501\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2826 - accuracy: 0.8870 - val_loss: 0.7676 - val_accuracy: 0.4540\n",
      "Epoch 29: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.4242\n",
      "Test AUC for Layer 1: 0.5531\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_112 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_84 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_56 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_85 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_57 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_86 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_115 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8938 - accuracy: 0.5179\n",
      "Epoch 1: val_loss improved from inf to 0.68510, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 21ms/step - loss: 0.8974 - accuracy: 0.5153 - val_loss: 0.6851 - val_accuracy: 0.6515\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8035 - accuracy: 0.5342\n",
      "Epoch 2: val_loss improved from 0.68510 to 0.68173, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.8100 - accuracy: 0.5314 - val_loss: 0.6817 - val_accuracy: 0.6515\n",
      "Epoch 3/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7189 - accuracy: 0.6057\n",
      "Epoch 3: val_loss improved from 0.68173 to 0.67869, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.7171 - accuracy: 0.6073 - val_loss: 0.6787 - val_accuracy: 0.6515\n",
      "Epoch 4/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7402 - accuracy: 0.5833\n",
      "Epoch 4: val_loss improved from 0.67869 to 0.67707, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.7399 - accuracy: 0.5825 - val_loss: 0.6771 - val_accuracy: 0.6515\n",
      "Epoch 5/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6518 - accuracy: 0.6464\n",
      "Epoch 5: val_loss did not improve from 0.67707\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6601 - accuracy: 0.6380 - val_loss: 0.6782 - val_accuracy: 0.6515\n",
      "Epoch 6/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6730 - accuracy: 0.6053\n",
      "Epoch 6: val_loss did not improve from 0.67707\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6663 - accuracy: 0.6088 - val_loss: 0.6779 - val_accuracy: 0.6515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5994 - accuracy: 0.6788\n",
      "Epoch 7: val_loss did not improve from 0.67707\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5987 - accuracy: 0.6788 - val_loss: 0.6794 - val_accuracy: 0.6364\n",
      "Epoch 8/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6146 - accuracy: 0.6528\n",
      "Epoch 8: val_loss did not improve from 0.67707\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6131 - accuracy: 0.6540 - val_loss: 0.6781 - val_accuracy: 0.6515\n",
      "Epoch 9/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5989 - accuracy: 0.6678\n",
      "Epoch 9: val_loss did not improve from 0.67707\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5995 - accuracy: 0.6715 - val_loss: 0.6780 - val_accuracy: 0.6667\n",
      "Epoch 10/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5406 - accuracy: 0.7648\n",
      "Epoch 10: val_loss improved from 0.67707 to 0.67638, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.5450 - accuracy: 0.7562 - val_loss: 0.6764 - val_accuracy: 0.6667\n",
      "Epoch 11/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5199 - accuracy: 0.7445\n",
      "Epoch 11: val_loss improved from 0.67638 to 0.67450, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5130 - accuracy: 0.7474 - val_loss: 0.6745 - val_accuracy: 0.6212\n",
      "Epoch 12/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5152 - accuracy: 0.7335\n",
      "Epoch 12: val_loss did not improve from 0.67450\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5032 - accuracy: 0.7401 - val_loss: 0.6768 - val_accuracy: 0.5909\n",
      "Epoch 13/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4576 - accuracy: 0.7681\n",
      "Epoch 13: val_loss did not improve from 0.67450\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4678 - accuracy: 0.7606 - val_loss: 0.6772 - val_accuracy: 0.5758\n",
      "Epoch 14/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4571 - accuracy: 0.7780\n",
      "Epoch 14: val_loss improved from 0.67450 to 0.67164, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4484 - accuracy: 0.7883 - val_loss: 0.6716 - val_accuracy: 0.5606\n",
      "Epoch 15/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4356 - accuracy: 0.8073\n",
      "Epoch 15: val_loss improved from 0.67164 to 0.67098, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.4465 - accuracy: 0.8088 - val_loss: 0.6710 - val_accuracy: 0.5758\n",
      "Epoch 16/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4279 - accuracy: 0.8109\n",
      "Epoch 16: val_loss did not improve from 0.67098\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4335 - accuracy: 0.8073 - val_loss: 0.6764 - val_accuracy: 0.5758\n",
      "Epoch 17/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4541 - accuracy: 0.8026\n",
      "Epoch 17: val_loss did not improve from 0.67098\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4542 - accuracy: 0.8015 - val_loss: 0.6770 - val_accuracy: 0.5758\n",
      "Epoch 18/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4227 - accuracy: 0.8090\n",
      "Epoch 18: val_loss did not improve from 0.67098\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4240 - accuracy: 0.8088 - val_loss: 0.6797 - val_accuracy: 0.5758\n",
      "Epoch 19/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3748 - accuracy: 0.8611\n",
      "Epoch 19: val_loss did not improve from 0.67098\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3911 - accuracy: 0.8482 - val_loss: 0.6796 - val_accuracy: 0.5606\n",
      "Epoch 20/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3700 - accuracy: 0.8333\n",
      "Epoch 20: val_loss did not improve from 0.67098\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3710 - accuracy: 0.8307 - val_loss: 0.6883 - val_accuracy: 0.5909\n",
      "Epoch 21/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3736 - accuracy: 0.8388\n",
      "Epoch 21: val_loss did not improve from 0.67098\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3750 - accuracy: 0.8438 - val_loss: 0.6892 - val_accuracy: 0.5455\n",
      "Epoch 22/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3551 - accuracy: 0.8403\n",
      "Epoch 22: val_loss did not improve from 0.67098\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3558 - accuracy: 0.8453 - val_loss: 0.6860 - val_accuracy: 0.5606\n",
      "Epoch 23/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3424 - accuracy: 0.8715\n",
      "Epoch 23: val_loss did not improve from 0.67098\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3413 - accuracy: 0.8715 - val_loss: 0.6933 - val_accuracy: 0.5909\n",
      "Epoch 24/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3593 - accuracy: 0.8503\n",
      "Epoch 24: val_loss did not improve from 0.67098\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3518 - accuracy: 0.8569 - val_loss: 0.7070 - val_accuracy: 0.5909\n",
      "Epoch 25/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2911 - accuracy: 0.9145\n",
      "Epoch 25: val_loss did not improve from 0.67098\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2933 - accuracy: 0.9139 - val_loss: 0.7261 - val_accuracy: 0.5455\n",
      "Epoch 26/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2941 - accuracy: 0.9013\n",
      "Epoch 26: val_loss did not improve from 0.67098\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.2946 - accuracy: 0.8978 - val_loss: 0.7399 - val_accuracy: 0.5303\n",
      "Epoch 27/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.2914 - accuracy: 0.8976\n",
      "Epoch 27: val_loss did not improve from 0.67098\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3020 - accuracy: 0.8964 - val_loss: 0.7453 - val_accuracy: 0.5152\n",
      "Epoch 28/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2916 - accuracy: 0.8997\n",
      "Epoch 28: val_loss did not improve from 0.67098\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2933 - accuracy: 0.8993 - val_loss: 0.7393 - val_accuracy: 0.5455\n",
      "Epoch 29/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3064 - accuracy: 0.8898\n",
      "Epoch 29: val_loss did not improve from 0.67098\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3048 - accuracy: 0.8891 - val_loss: 0.7425 - val_accuracy: 0.5606\n",
      "Epoch 30/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2652 - accuracy: 0.9062Restoring model weights from the end of the best epoch: 15.\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.67098\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2585 - accuracy: 0.9124 - val_loss: 0.7723 - val_accuracy: 0.5758\n",
      "Epoch 30: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.6111\n",
      "Test AUC for Layer 2: 0.6022\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_116 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_87 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_58 (Dropout)        (None, 512)               0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " dense_117 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_88 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_59 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_89 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8938 - accuracy: 0.4940\n",
      "Epoch 1: val_loss improved from inf to 0.67798, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.8991 - accuracy: 0.4887 - val_loss: 0.6780 - val_accuracy: 0.6667\n",
      "Epoch 2/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.7892 - accuracy: 0.5506\n",
      "Epoch 2: val_loss improved from 0.67798 to 0.66656, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.7975 - accuracy: 0.5459 - val_loss: 0.6666 - val_accuracy: 0.6667\n",
      "Epoch 3/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.7292 - accuracy: 0.5856\n",
      "Epoch 3: val_loss improved from 0.66656 to 0.65908, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7310 - accuracy: 0.5846 - val_loss: 0.6591 - val_accuracy: 0.6667\n",
      "Epoch 4/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.7201 - accuracy: 0.5895\n",
      "Epoch 4: val_loss improved from 0.65908 to 0.65218, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7197 - accuracy: 0.5912 - val_loss: 0.6522 - val_accuracy: 0.6667\n",
      "Epoch 5/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.6388 - accuracy: 0.6488\n",
      "Epoch 5: val_loss improved from 0.65218 to 0.64731, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6573 - accuracy: 0.6338 - val_loss: 0.6473 - val_accuracy: 0.6667\n",
      "Epoch 6/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6599 - accuracy: 0.6464\n",
      "Epoch 6: val_loss improved from 0.64731 to 0.64292, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6563 - accuracy: 0.6525 - val_loss: 0.6429 - val_accuracy: 0.6667\n",
      "Epoch 7/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5900 - accuracy: 0.6801\n",
      "Epoch 7: val_loss improved from 0.64292 to 0.64005, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.5905 - accuracy: 0.6804 - val_loss: 0.6401 - val_accuracy: 0.6667\n",
      "Epoch 8/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6226 - accuracy: 0.6454\n",
      "Epoch 8: val_loss improved from 0.64005 to 0.63722, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6203 - accuracy: 0.6485 - val_loss: 0.6372 - val_accuracy: 0.6667\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5718 - accuracy: 0.7084\n",
      "Epoch 9: val_loss improved from 0.63722 to 0.63577, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.5718 - accuracy: 0.7084 - val_loss: 0.6358 - val_accuracy: 0.6667\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5409 - accuracy: 0.7071\n",
      "Epoch 10: val_loss improved from 0.63577 to 0.63392, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.5409 - accuracy: 0.7071 - val_loss: 0.6339 - val_accuracy: 0.6667\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5456 - accuracy: 0.7044\n",
      "Epoch 11: val_loss improved from 0.63392 to 0.63183, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5456 - accuracy: 0.7044 - val_loss: 0.6318 - val_accuracy: 0.6667\n",
      "Epoch 12/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4796 - accuracy: 0.7649\n",
      "Epoch 12: val_loss improved from 0.63183 to 0.63019, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.4769 - accuracy: 0.7670 - val_loss: 0.6302 - val_accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4996 - accuracy: 0.7582\n",
      "Epoch 13: val_loss improved from 0.63019 to 0.62809, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.4969 - accuracy: 0.7603 - val_loss: 0.6281 - val_accuracy: 0.6667\n",
      "Epoch 14/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.4259 - accuracy: 0.8140\n",
      "Epoch 14: val_loss improved from 0.62809 to 0.62587, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.4229 - accuracy: 0.8189 - val_loss: 0.6259 - val_accuracy: 0.6759\n",
      "Epoch 15/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4202 - accuracy: 0.8295\n",
      "Epoch 15: val_loss did not improve from 0.62587\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4238 - accuracy: 0.8216 - val_loss: 0.6259 - val_accuracy: 0.6667\n",
      "Epoch 16/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4503 - accuracy: 0.8010\n",
      "Epoch 16: val_loss improved from 0.62587 to 0.62444, saving model to OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4434 - accuracy: 0.8003 - val_loss: 0.6244 - val_accuracy: 0.6667\n",
      "Epoch 17/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4215 - accuracy: 0.8210\n",
      "Epoch 17: val_loss did not improve from 0.62444\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4309 - accuracy: 0.8162 - val_loss: 0.6259 - val_accuracy: 0.6759\n",
      "Epoch 18/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4225 - accuracy: 0.8040\n",
      "Epoch 18: val_loss did not improve from 0.62444\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4213 - accuracy: 0.8069 - val_loss: 0.6259 - val_accuracy: 0.6759\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3944 - accuracy: 0.8402\n",
      "Epoch 19: val_loss did not improve from 0.62444\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3944 - accuracy: 0.8402 - val_loss: 0.6298 - val_accuracy: 0.6389\n",
      "Epoch 20/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4101 - accuracy: 0.8092\n",
      "Epoch 20: val_loss did not improve from 0.62444\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4054 - accuracy: 0.8136 - val_loss: 0.6325 - val_accuracy: 0.6481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3477 - accuracy: 0.8535\n",
      "Epoch 21: val_loss did not improve from 0.62444\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3477 - accuracy: 0.8535 - val_loss: 0.6319 - val_accuracy: 0.6481\n",
      "Epoch 22/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3499 - accuracy: 0.8602\n",
      "Epoch 22: val_loss did not improve from 0.62444\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3502 - accuracy: 0.8628 - val_loss: 0.6391 - val_accuracy: 0.6296\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3695 - accuracy: 0.8469\n",
      "Epoch 23: val_loss did not improve from 0.62444\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3695 - accuracy: 0.8469 - val_loss: 0.6554 - val_accuracy: 0.6296\n",
      "Epoch 24/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3335 - accuracy: 0.8701\n",
      "Epoch 24: val_loss did not improve from 0.62444\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3363 - accuracy: 0.8682 - val_loss: 0.6591 - val_accuracy: 0.6574\n",
      "Epoch 25/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3277 - accuracy: 0.8762\n",
      "Epoch 25: val_loss did not improve from 0.62444\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3277 - accuracy: 0.8762 - val_loss: 0.6728 - val_accuracy: 0.6481\n",
      "Epoch 26/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2978 - accuracy: 0.8832\n",
      "Epoch 26: val_loss did not improve from 0.62444\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2928 - accuracy: 0.8908 - val_loss: 0.6978 - val_accuracy: 0.6111\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3118 - accuracy: 0.8788\n",
      "Epoch 27: val_loss did not improve from 0.62444\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3118 - accuracy: 0.8788 - val_loss: 0.7252 - val_accuracy: 0.6204\n",
      "Epoch 28/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2716 - accuracy: 0.9046\n",
      "Epoch 28: val_loss did not improve from 0.62444\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2715 - accuracy: 0.9041 - val_loss: 0.7338 - val_accuracy: 0.5833\n",
      "Epoch 29/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2993 - accuracy: 0.8865\n",
      "Epoch 29: val_loss did not improve from 0.62444\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2904 - accuracy: 0.8948 - val_loss: 0.7282 - val_accuracy: 0.6296\n",
      "Epoch 30/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2637 - accuracy: 0.9095\n",
      "Epoch 30: val_loss did not improve from 0.62444\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2659 - accuracy: 0.9108 - val_loss: 0.7544 - val_accuracy: 0.6111\n",
      "Epoch 31/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2730 - accuracy: 0.9030Restoring model weights from the end of the best epoch: 16.\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.62444\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2750 - accuracy: 0.9028 - val_loss: 0.7785 - val_accuracy: 0.6204\n",
      "Epoch 31: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.3188\n",
      "Test AUC for Layer 3: 0.4072\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4514\n",
      "Average Test AUC across all layers: 0.5208\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS SLB)\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Title + S_label\n",
      "Average Accuracy: 0.5262\n",
      "Average AUC: 0.4710\n",
      "  Layer 1 - Accuracy: 0.4394, AUC: 0.4430\n",
      "  Layer 2 - Accuracy: 0.5741, AUC: 0.4972\n",
      "  Layer 3 - Accuracy: 0.5652, AUC: 0.4726\n",
      "\n",
      "Combination: MLP with Title + L_label\n",
      "Average Accuracy: 0.5103\n",
      "Average AUC: 0.4043\n",
      "  Layer 1 - Accuracy: 0.5455, AUC: 0.4135\n",
      "  Layer 2 - Accuracy: 0.6667, AUC: 0.4907\n",
      "  Layer 3 - Accuracy: 0.3188, AUC: 0.3085\n",
      "\n",
      "Combination: MLP with Full text + S_label\n",
      "Average Accuracy: 0.4979\n",
      "Average AUC: 0.4925\n",
      "  Layer 1 - Accuracy: 0.4848, AUC: 0.4816\n",
      "  Layer 2 - Accuracy: 0.5741, AUC: 0.5712\n",
      "  Layer 3 - Accuracy: 0.4348, AUC: 0.4248\n",
      "\n",
      "Combination: MLP with Full text + L_label\n",
      "Average Accuracy: 0.4514\n",
      "Average AUC: 0.5208\n",
      "  Layer 1 - Accuracy: 0.4242, AUC: 0.5531\n",
      "  Layer 2 - Accuracy: 0.6111, AUC: 0.6022\n",
      "  Layer 3 - Accuracy: 0.3188, AUC: 0.4072\n",
      "MLP summary comparison visualization saved as: OpenAI_MLP/visualizations_summary/SLB\\mlp_performance_comparison.png\n",
      "MLP layer performance visualization saved as: OpenAI_MLP/visualizations_summary/SLB\\mlp_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'OpenAI_MLP' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "from xgboost import callback\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, model_type, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class ClimateNewsOpenAIPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'OpenAI_MLP/visualizations_mlp/SLB'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs('OpenAI_MLP/visualizations_summary/SLB', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert embeddings to numpy arrays\n",
    "        self.data['Title_embedding'] = self.data['Title_embedding_vector'].apply(parse_embedding)\n",
    "        self.data['Fulltext_embedding'] = self.data['Full_text_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_title_embedding = self.data['Title_embedding'].iloc[0]\n",
    "        sample_fulltext_embedding = self.data['Fulltext_embedding'].iloc[0]\n",
    "        \n",
    "        print(f\"Sample Title embedding shape: {sample_title_embedding.shape}\")\n",
    "        print(f\"Sample Fulltext embedding shape: {sample_fulltext_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2019-2021), validation (2021-2021), test (2022-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-05-31'),\n",
    "            'val_start': pd.Timestamp('2021-06-01'),\n",
    "            'val_end': pd.Timestamp('2021-12-31'),\n",
    "            'test_start': pd.Timestamp('2022-01-01'),\n",
    "            'test_end': pd.Timestamp('2022-05-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2019-2021), validation (2022-2022), test (2022-2022)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-12-31'),\n",
    "            'val_start': pd.Timestamp('2022-01-01'),\n",
    "            'val_end': pd.Timestamp('2022-05-31'),\n",
    "            'test_start': pd.Timestamp('2022-06-01'),\n",
    "            'test_end': pd.Timestamp('2022-12-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2019-2022), validation (2022-2022), test (2023-2023)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2022-05-31'),\n",
    "            'val_start': pd.Timestamp('2022-06-01'),\n",
    "            'val_end': pd.Timestamp('2022-12-31'),\n",
    "            'test_start': pd.Timestamp('2023-01-01'),\n",
    "            'test_end': pd.Timestamp('2023-05-31')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, text_col, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            text_col: The column containing the embeddings ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data[text_col].values)\n",
    "        X_val = np.stack(val_data[text_col].values)\n",
    "        X_test = np.stack(test_data[text_col].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, model_type, text_col, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            model_type: Model type (MLP)\n",
    "            text_col: Text column used\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_{model_type.lower()}_{display_text.replace(' ', '_')}_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, text_col, label_col, model_type):\n",
    "        \"\"\"\n",
    "        Train and evaluate a model for a specific text column and label column.\n",
    "        \n",
    "        Args:\n",
    "            text_col: The embedding column to use ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "            model_type: The model type to use ('MLP')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        combination_key = f\"{model_type}|{display_text}|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training {model_type} model for {display_text} and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        visualization_dir = self.mlp_viz_dir if model_type == 'MLP' else self.xgb_viz_dir\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, text_col, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            if model_type == 'MLP':\n",
    "                # MLP model training\n",
    "                model = self.create_mlp_model((1536,))\n",
    "                print(f\"Created MLP model for {display_text} ({label_col})\")\n",
    "                model.summary()\n",
    "                \n",
    "                # Setup callbacks\n",
    "                plot_callback = PlotLearningCallback(model_type, display_text, label_col, i+1, visualization_dir)\n",
    "                early_stopping = EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=15,\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                model_checkpoint = ModelCheckpoint(\n",
    "                    filepath=f\"{visualization_dir}/best_{model_type.lower()}_{display_text.lower().replace(' ', '_')}_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                    monitor='val_loss',\n",
    "                    save_weights_only=True,\n",
    "                    save_best_only=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Train model\n",
    "                print(f\"Training MLP model...\")\n",
    "                batch_size = 32  # Larger batch size for embeddings\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                y_pred_proba = model.predict(X_test)\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                # Create final learning curve visualization\n",
    "                self.plot_final_learning_curves(history, model_type, text_col, label_col, i+1, visualization_dir)\n",
    "                \n",
    "                # Store training history\n",
    "                training_history = history.history\n",
    "                \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'model_type': model_type,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': training_history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for all combinations of text inputs, label columns, and model types.\"\"\"\n",
    "        # Define all combinations\n",
    "        embedding_cols = ['Title_embedding', 'Fulltext_embedding']\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        model_types = ['MLP']\n",
    "        \n",
    "        # Run analysis for each combination\n",
    "        for model_type in model_types:\n",
    "            for embedding_col in embedding_cols:\n",
    "                for label_col in label_cols:\n",
    "                    self.train_and_evaluate_model(embedding_col, label_col, model_type)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS SLB)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Model': model_type,\n",
    "                'Text': text_col,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing all model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Split by model type\n",
    "        mlp_data = df[df['Model'] == 'MLP']\n",
    "        \n",
    "        # 1. Performance comparison for MLP\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot accuracy and AUC bars for MLP\n",
    "        x = np.arange(len(mlp_data))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, mlp_data['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, mlp_data['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('MLP Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with OpenAI Embeddings')\n",
    "        labels = [f\"{row['Text']} + {row['Label']}\" for _, row in mlp_data.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(mlp_data['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(mlp_data['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(mlp_data['Avg Accuracy'].max(), mlp_data['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP/visualizations_summary/SLB', \"mlp_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        mlp_layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Model': model_type,\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                if model_type == 'MLP':\n",
    "                    mlp_layer_data.append(layer_info)\n",
    "        \n",
    "        # Create visualizations for each model type\n",
    "        self._create_model_layer_visualization(mlp_layer_data, 'MLP')\n",
    "    \n",
    "    def _create_model_layer_visualization(self, layer_data, model_type):\n",
    "        \"\"\"Create layer-specific visualizations for a given model type.\"\"\"\n",
    "        if not layer_data:\n",
    "            print(f\"No layer data available for {model_type}\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} Accuracy by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} AUC by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP/visualizations_summary/SLB', f\"{model_type.lower()}_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"{model_type} layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['OpenAI_MLP/visualizations_mlp/SLB',\n",
    "                      'OpenAI_MLP/visualizations_summary/SLB']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/US_news/SP500_semantic/us_news_semantics_SLB_completed_openai.csv'\n",
    "    \n",
    "    # Initialize the predictor with OpenAI embeddings\n",
    "    predictor = ClimateNewsOpenAIPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'OpenAI_MLP' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing merged OpenAI embedding vectors from string format...\n",
      "Sample Merged embedding shape: (1536,)\n",
      "Loaded 929 climate change news articles spanning from 02/01/2019 to 07/05/2023\n",
      "Class distribution for short-term prediction: {1: 472, 0: 457}\n",
      "Class distribution for long-term prediction: {1: 494, 0: 435}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_120 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_90 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_60 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_121 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_91 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_61 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_92 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_123 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8688 - accuracy: 0.4904\n",
      "Epoch 1: val_loss improved from inf to 0.69049, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 24ms/step - loss: 0.8688 - accuracy: 0.4904 - val_loss: 0.6905 - val_accuracy: 0.5399\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7747 - accuracy: 0.5625\n",
      "Epoch 2: val_loss did not improve from 0.69049\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7744 - accuracy: 0.5651 - val_loss: 0.6905 - val_accuracy: 0.5399\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7450 - accuracy: 0.5859\n",
      "Epoch 3: val_loss did not improve from 0.69049\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7473 - accuracy: 0.5862 - val_loss: 0.6923 - val_accuracy: 0.5399\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7242 - accuracy: 0.5862\n",
      "Epoch 4: val_loss did not improve from 0.69049\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7242 - accuracy: 0.5862 - val_loss: 0.6952 - val_accuracy: 0.5399\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6690 - accuracy: 0.6456\n",
      "Epoch 5: val_loss did not improve from 0.69049\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6690 - accuracy: 0.6456 - val_loss: 0.6999 - val_accuracy: 0.5399\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6519 - accuracy: 0.6250\n",
      "Epoch 6: val_loss did not improve from 0.69049\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6575 - accuracy: 0.6169 - val_loss: 0.7043 - val_accuracy: 0.5399\n",
      "Epoch 7/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5817 - accuracy: 0.7091\n",
      "Epoch 7: val_loss did not improve from 0.69049\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5825 - accuracy: 0.6992 - val_loss: 0.7093 - val_accuracy: 0.5399\n",
      "Epoch 8/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6032 - accuracy: 0.6899\n",
      "Epoch 8: val_loss did not improve from 0.69049\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5986 - accuracy: 0.6858 - val_loss: 0.7151 - val_accuracy: 0.5399\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5746 - accuracy: 0.7043\n",
      "Epoch 9: val_loss did not improve from 0.69049\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.5579 - accuracy: 0.7146 - val_loss: 0.7204 - val_accuracy: 0.5399\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5737 - accuracy: 0.6699\n",
      "Epoch 10: val_loss did not improve from 0.69049\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5679 - accuracy: 0.6743 - val_loss: 0.7245 - val_accuracy: 0.5399\n",
      "Epoch 11/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5197 - accuracy: 0.7375\n",
      "Epoch 11: val_loss did not improve from 0.69049\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5159 - accuracy: 0.7433 - val_loss: 0.7290 - val_accuracy: 0.5399\n",
      "Epoch 12/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5322 - accuracy: 0.7163\n",
      "Epoch 12: val_loss did not improve from 0.69049\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5273 - accuracy: 0.7222 - val_loss: 0.7350 - val_accuracy: 0.5399\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5011 - accuracy: 0.7605\n",
      "Epoch 13: val_loss did not improve from 0.69049\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5011 - accuracy: 0.7605 - val_loss: 0.7396 - val_accuracy: 0.5399\n",
      "Epoch 14/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5011 - accuracy: 0.7604\n",
      "Epoch 14: val_loss did not improve from 0.69049\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4899 - accuracy: 0.7644 - val_loss: 0.7448 - val_accuracy: 0.5399\n",
      "Epoch 15/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4282 - accuracy: 0.8029\n",
      "Epoch 15: val_loss did not improve from 0.69049\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4382 - accuracy: 0.7931 - val_loss: 0.7489 - val_accuracy: 0.5399\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4369 - accuracy: 0.8008Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69049\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4376 - accuracy: 0.7989 - val_loss: 0.7575 - val_accuracy: 0.5399\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.4848\n",
      "Test AUC for Layer 1: 0.4173\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_124 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_93 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_62 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_125 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_94 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_63 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_126 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_95 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8655 - accuracy: 0.5164\n",
      "Epoch 1: val_loss improved from inf to 0.69156, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 21ms/step - loss: 0.8629 - accuracy: 0.5182 - val_loss: 0.6916 - val_accuracy: 0.5152\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7793 - accuracy: 0.5460\n",
      "Epoch 2: val_loss did not improve from 0.69156\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7793 - accuracy: 0.5460 - val_loss: 0.6926 - val_accuracy: 0.5152\n",
      "Epoch 3/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.7292 - accuracy: 0.6168\n",
      "Epoch 3: val_loss did not improve from 0.69156\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7337 - accuracy: 0.6058 - val_loss: 0.6953 - val_accuracy: 0.5152\n",
      "Epoch 4/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7134 - accuracy: 0.6071\n",
      "Epoch 4: val_loss did not improve from 0.69156\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7106 - accuracy: 0.6073 - val_loss: 0.6998 - val_accuracy: 0.5152\n",
      "Epoch 5/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6579 - accuracy: 0.6168\n",
      "Epoch 5: val_loss did not improve from 0.69156\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6503 - accuracy: 0.6234 - val_loss: 0.7050 - val_accuracy: 0.5152\n",
      "Epoch 6/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6060 - accuracy: 0.6809\n",
      "Epoch 6: val_loss did not improve from 0.69156\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6176 - accuracy: 0.6730 - val_loss: 0.7118 - val_accuracy: 0.5152\n",
      "Epoch 7/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5816 - accuracy: 0.7220\n",
      "Epoch 7: val_loss did not improve from 0.69156\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5845 - accuracy: 0.7153 - val_loss: 0.7174 - val_accuracy: 0.5152\n",
      "Epoch 8/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5956 - accuracy: 0.6622\n",
      "Epoch 8: val_loss did not improve from 0.69156\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5995 - accuracy: 0.6599 - val_loss: 0.7240 - val_accuracy: 0.5152\n",
      "Epoch 9/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5255 - accuracy: 0.7270\n",
      "Epoch 9: val_loss did not improve from 0.69156\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5328 - accuracy: 0.7182 - val_loss: 0.7336 - val_accuracy: 0.5152\n",
      "Epoch 10/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5593 - accuracy: 0.7143\n",
      "Epoch 10: val_loss did not improve from 0.69156\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5587 - accuracy: 0.7124 - val_loss: 0.7422 - val_accuracy: 0.5152\n",
      "Epoch 11/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.4991 - accuracy: 0.7649\n",
      "Epoch 11: val_loss did not improve from 0.69156\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5008 - accuracy: 0.7635 - val_loss: 0.7478 - val_accuracy: 0.5152\n",
      "Epoch 12/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.4847 - accuracy: 0.7679\n",
      "Epoch 12: val_loss did not improve from 0.69156\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4845 - accuracy: 0.7679 - val_loss: 0.7536 - val_accuracy: 0.5152\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4552 - accuracy: 0.7898\n",
      "Epoch 13: val_loss did not improve from 0.69156\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4552 - accuracy: 0.7898 - val_loss: 0.7623 - val_accuracy: 0.5152\n",
      "Epoch 14/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4418 - accuracy: 0.8092\n",
      "Epoch 14: val_loss did not improve from 0.69156\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4489 - accuracy: 0.8044 - val_loss: 0.7686 - val_accuracy: 0.5152\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4481 - accuracy: 0.8000\n",
      "Epoch 15: val_loss did not improve from 0.69156\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4481 - accuracy: 0.8000 - val_loss: 0.7695 - val_accuracy: 0.5152\n",
      "Epoch 16/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3909 - accuracy: 0.8333Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69156\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3925 - accuracy: 0.8380 - val_loss: 0.7822 - val_accuracy: 0.5152\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5741\n",
      "Test AUC for Layer 2: 0.5533\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_128 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_96 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_64 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_97 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_65 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_130 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_98 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_131 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8845 - accuracy: 0.5104\n",
      "Epoch 1: val_loss improved from inf to 0.69025, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.8863 - accuracy: 0.5087 - val_loss: 0.6902 - val_accuracy: 0.5741\n",
      "Epoch 2/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.7837 - accuracy: 0.5461\n",
      "Epoch 2: val_loss improved from 0.69025 to 0.68817, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.7895 - accuracy: 0.5393 - val_loss: 0.6882 - val_accuracy: 0.5741\n",
      "Epoch 3/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.7864 - accuracy: 0.5476\n",
      "Epoch 3: val_loss did not improve from 0.68817\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7854 - accuracy: 0.5446 - val_loss: 0.6884 - val_accuracy: 0.5741\n",
      "Epoch 4/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6949 - accuracy: 0.6037\n",
      "Epoch 4: val_loss improved from 0.68817 to 0.68774, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.6921 - accuracy: 0.6072 - val_loss: 0.6877 - val_accuracy: 0.5741\n",
      "Epoch 5/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6713 - accuracy: 0.6467\n",
      "Epoch 5: val_loss improved from 0.68774 to 0.68707, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6698 - accuracy: 0.6498 - val_loss: 0.6871 - val_accuracy: 0.5741\n",
      "Epoch 6/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6404 - accuracy: 0.6634\n",
      "Epoch 6: val_loss did not improve from 0.68707\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.6358 - accuracy: 0.6658 - val_loss: 0.6873 - val_accuracy: 0.5648\n",
      "Epoch 7/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.6365 - accuracy: 0.6518\n",
      "Epoch 7: val_loss did not improve from 0.68707\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.6329 - accuracy: 0.6551 - val_loss: 0.6872 - val_accuracy: 0.5648\n",
      "Epoch 8/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6035 - accuracy: 0.6809\n",
      "Epoch 8: val_loss did not improve from 0.68707\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.6005 - accuracy: 0.6844 - val_loss: 0.6874 - val_accuracy: 0.5648\n",
      "Epoch 9/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5780 - accuracy: 0.6875\n",
      "Epoch 9: val_loss did not improve from 0.68707\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5770 - accuracy: 0.6884 - val_loss: 0.6886 - val_accuracy: 0.5648\n",
      "Epoch 10/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5545 - accuracy: 0.7201\n",
      "Epoch 10: val_loss did not improve from 0.68707\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5583 - accuracy: 0.7177 - val_loss: 0.6895 - val_accuracy: 0.5463\n",
      "Epoch 11/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5339 - accuracy: 0.7615\n",
      "Epoch 11: val_loss did not improve from 0.68707\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5338 - accuracy: 0.7563 - val_loss: 0.6914 - val_accuracy: 0.5463\n",
      "Epoch 12/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5057 - accuracy: 0.7368\n",
      "Epoch 12: val_loss did not improve from 0.68707\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5112 - accuracy: 0.7457 - val_loss: 0.6910 - val_accuracy: 0.5741\n",
      "Epoch 13/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4917 - accuracy: 0.7862\n",
      "Epoch 13: val_loss did not improve from 0.68707\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4891 - accuracy: 0.7830 - val_loss: 0.6914 - val_accuracy: 0.5833\n",
      "Epoch 14/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5093 - accuracy: 0.7677\n",
      "Epoch 14: val_loss did not improve from 0.68707\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5071 - accuracy: 0.7696 - val_loss: 0.6925 - val_accuracy: 0.5741\n",
      "Epoch 15/100\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.4708 - accuracy: 0.7844\n",
      "Epoch 15: val_loss did not improve from 0.68707\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4736 - accuracy: 0.7750 - val_loss: 0.6942 - val_accuracy: 0.5741\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4514 - accuracy: 0.8096\n",
      "Epoch 16: val_loss did not improve from 0.68707\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4514 - accuracy: 0.8096 - val_loss: 0.6960 - val_accuracy: 0.5741\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4377 - accuracy: 0.8123\n",
      "Epoch 17: val_loss did not improve from 0.68707\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4377 - accuracy: 0.8123 - val_loss: 0.7016 - val_accuracy: 0.5926\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4062 - accuracy: 0.8229\n",
      "Epoch 18: val_loss did not improve from 0.68707\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4062 - accuracy: 0.8229 - val_loss: 0.7094 - val_accuracy: 0.5833\n",
      "Epoch 19/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4022 - accuracy: 0.8322\n",
      "Epoch 19: val_loss did not improve from 0.68707\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4061 - accuracy: 0.8349 - val_loss: 0.7119 - val_accuracy: 0.5556\n",
      "Epoch 20/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4021 - accuracy: 0.8397Restoring model weights from the end of the best epoch: 5.\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.68707\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4047 - accuracy: 0.8375 - val_loss: 0.7127 - val_accuracy: 0.5463\n",
      "Epoch 20: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5652\n",
      "Test AUC for Layer 3: 0.4641\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5414\n",
      "Average Test AUC across all layers: 0.4782\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_132 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_99 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_66 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_100 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_67 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_101 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_135 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.9931 - accuracy: 0.4492\n",
      "Epoch 1: val_loss improved from inf to 0.69863, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.9907 - accuracy: 0.4483 - val_loss: 0.6986 - val_accuracy: 0.4724\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8334 - accuracy: 0.5234\n",
      "Epoch 2: val_loss did not improve from 0.69863\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.8296 - accuracy: 0.5268 - val_loss: 0.7078 - val_accuracy: 0.4724\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7675 - accuracy: 0.5881\n",
      "Epoch 3: val_loss did not improve from 0.69863\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7675 - accuracy: 0.5881 - val_loss: 0.7192 - val_accuracy: 0.4724\n",
      "Epoch 4/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.7124 - accuracy: 0.6354\n",
      "Epoch 4: val_loss did not improve from 0.69863\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.7362 - accuracy: 0.6092 - val_loss: 0.7294 - val_accuracy: 0.4724\n",
      "Epoch 5/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6949 - accuracy: 0.6224\n",
      "Epoch 5: val_loss did not improve from 0.69863\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6915 - accuracy: 0.6264 - val_loss: 0.7362 - val_accuracy: 0.4724\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6362 - accuracy: 0.6418\n",
      "Epoch 6: val_loss did not improve from 0.69863\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6362 - accuracy: 0.6418 - val_loss: 0.7419 - val_accuracy: 0.4724\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5942 - accuracy: 0.6762\n",
      "Epoch 7: val_loss did not improve from 0.69863\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5942 - accuracy: 0.6762 - val_loss: 0.7473 - val_accuracy: 0.4724\n",
      "Epoch 8/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5843 - accuracy: 0.7109\n",
      "Epoch 8: val_loss did not improve from 0.69863\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5982 - accuracy: 0.7088 - val_loss: 0.7503 - val_accuracy: 0.4724\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5710 - accuracy: 0.6971\n",
      "Epoch 9: val_loss did not improve from 0.69863\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5569 - accuracy: 0.7126 - val_loss: 0.7510 - val_accuracy: 0.4724\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4999 - accuracy: 0.7356\n",
      "Epoch 10: val_loss did not improve from 0.69863\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5115 - accuracy: 0.7356 - val_loss: 0.7539 - val_accuracy: 0.4724\n",
      "Epoch 11/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4928 - accuracy: 0.7644\n",
      "Epoch 11: val_loss did not improve from 0.69863\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4918 - accuracy: 0.7701 - val_loss: 0.7558 - val_accuracy: 0.4724\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4898 - accuracy: 0.7539\n",
      "Epoch 12: val_loss did not improve from 0.69863\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4884 - accuracy: 0.7567 - val_loss: 0.7545 - val_accuracy: 0.4724\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4607 - accuracy: 0.7739\n",
      "Epoch 13: val_loss did not improve from 0.69863\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4607 - accuracy: 0.7739 - val_loss: 0.7522 - val_accuracy: 0.4724\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4711 - accuracy: 0.7734\n",
      "Epoch 14: val_loss did not improve from 0.69863\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4724 - accuracy: 0.7720 - val_loss: 0.7527 - val_accuracy: 0.4724\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4449 - accuracy: 0.7793\n",
      "Epoch 15: val_loss did not improve from 0.69863\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4486 - accuracy: 0.7759 - val_loss: 0.7557 - val_accuracy: 0.4724\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4208 - accuracy: 0.8066Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69863\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4215 - accuracy: 0.8065 - val_loss: 0.7569 - val_accuracy: 0.4724\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.6515\n",
      "Test AUC for Layer 1: 0.6006\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_136 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_102 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_68 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_137 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_103 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_69 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_104 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.9069 - accuracy: 0.4926\n",
      "Epoch 1: val_loss improved from inf to 0.69897, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 20ms/step - loss: 0.9020 - accuracy: 0.4949 - val_loss: 0.6990 - val_accuracy: 0.3485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7706 - accuracy: 0.5708\n",
      "Epoch 2: val_loss did not improve from 0.69897\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7706 - accuracy: 0.5708 - val_loss: 0.7000 - val_accuracy: 0.3485\n",
      "Epoch 3/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7373 - accuracy: 0.5863\n",
      "Epoch 3: val_loss did not improve from 0.69897\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7389 - accuracy: 0.5854 - val_loss: 0.6992 - val_accuracy: 0.3485\n",
      "Epoch 4/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7327 - accuracy: 0.5893\n",
      "Epoch 4: val_loss improved from 0.69897 to 0.69893, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.7286 - accuracy: 0.5912 - val_loss: 0.6989 - val_accuracy: 0.3333\n",
      "Epoch 5/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6716 - accuracy: 0.6354\n",
      "Epoch 5: val_loss improved from 0.69893 to 0.69730, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6674 - accuracy: 0.6394 - val_loss: 0.6973 - val_accuracy: 0.3788\n",
      "Epoch 6/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6538 - accuracy: 0.6399\n",
      "Epoch 6: val_loss did not improve from 0.69730\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6536 - accuracy: 0.6409 - val_loss: 0.6985 - val_accuracy: 0.3788\n",
      "Epoch 7/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5752 - accuracy: 0.7004\n",
      "Epoch 7: val_loss did not improve from 0.69730\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5887 - accuracy: 0.6832 - val_loss: 0.6993 - val_accuracy: 0.3636\n",
      "Epoch 8/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5934 - accuracy: 0.6820\n",
      "Epoch 8: val_loss improved from 0.69730 to 0.69700, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.5876 - accuracy: 0.6920 - val_loss: 0.6970 - val_accuracy: 0.4091\n",
      "Epoch 9/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5625 - accuracy: 0.6830\n",
      "Epoch 9: val_loss did not improve from 0.69700\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5624 - accuracy: 0.6861 - val_loss: 0.6998 - val_accuracy: 0.3939\n",
      "Epoch 10/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5311 - accuracy: 0.7401\n",
      "Epoch 10: val_loss did not improve from 0.69700\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5394 - accuracy: 0.7328 - val_loss: 0.7009 - val_accuracy: 0.4091\n",
      "Epoch 11/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5003 - accuracy: 0.7418\n",
      "Epoch 11: val_loss did not improve from 0.69700\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4930 - accuracy: 0.7504 - val_loss: 0.6981 - val_accuracy: 0.4091\n",
      "Epoch 12/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.4865 - accuracy: 0.7828\n",
      "Epoch 12: val_loss improved from 0.69700 to 0.69666, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4904 - accuracy: 0.7796 - val_loss: 0.6967 - val_accuracy: 0.5000\n",
      "Epoch 13/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4517 - accuracy: 0.7812\n",
      "Epoch 13: val_loss did not improve from 0.69666\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4712 - accuracy: 0.7723 - val_loss: 0.7017 - val_accuracy: 0.4091\n",
      "Epoch 14/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4353 - accuracy: 0.8043\n",
      "Epoch 14: val_loss did not improve from 0.69666\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4447 - accuracy: 0.8015 - val_loss: 0.7073 - val_accuracy: 0.4242\n",
      "Epoch 15/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4753 - accuracy: 0.7697\n",
      "Epoch 15: val_loss did not improve from 0.69666\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4786 - accuracy: 0.7679 - val_loss: 0.7089 - val_accuracy: 0.4394\n",
      "Epoch 16/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.4562 - accuracy: 0.7872\n",
      "Epoch 16: val_loss did not improve from 0.69666\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4593 - accuracy: 0.7839 - val_loss: 0.7183 - val_accuracy: 0.3788\n",
      "Epoch 17/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.4197 - accuracy: 0.8141\n",
      "Epoch 17: val_loss did not improve from 0.69666\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4147 - accuracy: 0.8204 - val_loss: 0.7088 - val_accuracy: 0.4697\n",
      "Epoch 18/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3693 - accuracy: 0.8487\n",
      "Epoch 18: val_loss did not improve from 0.69666\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3704 - accuracy: 0.8482 - val_loss: 0.7175 - val_accuracy: 0.4545\n",
      "Epoch 19/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3566 - accuracy: 0.8663\n",
      "Epoch 19: val_loss did not improve from 0.69666\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3605 - accuracy: 0.8642 - val_loss: 0.7113 - val_accuracy: 0.4848\n",
      "Epoch 20/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.3924 - accuracy: 0.8382\n",
      "Epoch 20: val_loss did not improve from 0.69666\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4078 - accuracy: 0.8321 - val_loss: 0.7068 - val_accuracy: 0.5455\n",
      "Epoch 21/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3661 - accuracy: 0.8542\n",
      "Epoch 21: val_loss did not improve from 0.69666\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3651 - accuracy: 0.8584 - val_loss: 0.7170 - val_accuracy: 0.5000\n",
      "Epoch 22/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3459 - accuracy: 0.8715\n",
      "Epoch 22: val_loss did not improve from 0.69666\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3467 - accuracy: 0.8686 - val_loss: 0.7284 - val_accuracy: 0.4848\n",
      "Epoch 23/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3041 - accuracy: 0.9062\n",
      "Epoch 23: val_loss did not improve from 0.69666\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3054 - accuracy: 0.9022 - val_loss: 0.7438 - val_accuracy: 0.4545\n",
      "Epoch 24/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3430 - accuracy: 0.8388\n",
      "Epoch 24: val_loss did not improve from 0.69666\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3373 - accuracy: 0.8438 - val_loss: 0.7402 - val_accuracy: 0.4697\n",
      "Epoch 25/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2893 - accuracy: 0.8964\n",
      "Epoch 25: val_loss did not improve from 0.69666\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2942 - accuracy: 0.8920 - val_loss: 0.7496 - val_accuracy: 0.4697\n",
      "Epoch 26/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.2934 - accuracy: 0.9062\n",
      "Epoch 26: val_loss did not improve from 0.69666\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2898 - accuracy: 0.9080 - val_loss: 0.7448 - val_accuracy: 0.4848\n",
      "Epoch 27/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2786 - accuracy: 0.9079Restoring model weights from the end of the best epoch: 12.\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.69666\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2777 - accuracy: 0.9139 - val_loss: 0.7575 - val_accuracy: 0.4697\n",
      "Epoch 27: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5093\n",
      "Test AUC for Layer 2: 0.5197\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_35\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_140 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_105 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_70 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_141 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_106 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_71 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_142 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_107 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_143 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8929 - accuracy: 0.4940\n",
      "Epoch 1: val_loss improved from inf to 0.74662, saving model to Merged_OpenAI_MLP/visualizations_mlp/SLB\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.8896 - accuracy: 0.4967 - val_loss: 0.7466 - val_accuracy: 0.3333\n",
      "Epoch 2/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8146 - accuracy: 0.5506\n",
      "Epoch 2: val_loss did not improve from 0.74662\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.7995 - accuracy: 0.5646 - val_loss: 0.8059 - val_accuracy: 0.3333\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.7800 - accuracy: 0.5646\n",
      "Epoch 3: val_loss did not improve from 0.74662\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.7800 - accuracy: 0.5646 - val_loss: 0.8603 - val_accuracy: 0.3333\n",
      "Epoch 4/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6961 - accuracy: 0.5909\n",
      "Epoch 4: val_loss did not improve from 0.74662\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6937 - accuracy: 0.5939 - val_loss: 0.8980 - val_accuracy: 0.3333\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6711 - accuracy: 0.6352\n",
      "Epoch 5: val_loss did not improve from 0.74662\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6711 - accuracy: 0.6352 - val_loss: 0.9334 - val_accuracy: 0.3333\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6514 - accuracy: 0.6618\n",
      "Epoch 6: val_loss did not improve from 0.74662\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6514 - accuracy: 0.6618 - val_loss: 0.9553 - val_accuracy: 0.3333\n",
      "Epoch 7/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6325 - accuracy: 0.6711\n",
      "Epoch 7: val_loss did not improve from 0.74662\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6209 - accuracy: 0.6831 - val_loss: 0.9752 - val_accuracy: 0.3333\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5882 - accuracy: 0.6804\n",
      "Epoch 8: val_loss did not improve from 0.74662\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5882 - accuracy: 0.6804 - val_loss: 0.9747 - val_accuracy: 0.3333\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5456 - accuracy: 0.7177\n",
      "Epoch 9: val_loss did not improve from 0.74662\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5456 - accuracy: 0.7177 - val_loss: 0.9759 - val_accuracy: 0.3333\n",
      "Epoch 10/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5196 - accuracy: 0.7485\n",
      "Epoch 10: val_loss did not improve from 0.74662\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5195 - accuracy: 0.7470 - val_loss: 0.9778 - val_accuracy: 0.3333\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5296 - accuracy: 0.7257\n",
      "Epoch 11: val_loss did not improve from 0.74662\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5296 - accuracy: 0.7257 - val_loss: 0.9824 - val_accuracy: 0.3333\n",
      "Epoch 12/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4864 - accuracy: 0.7714\n",
      "Epoch 12: val_loss did not improve from 0.74662\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4789 - accuracy: 0.7763 - val_loss: 0.9703 - val_accuracy: 0.3333\n",
      "Epoch 13/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4626 - accuracy: 0.7714\n",
      "Epoch 13: val_loss did not improve from 0.74662\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4630 - accuracy: 0.7710 - val_loss: 0.9656 - val_accuracy: 0.3333\n",
      "Epoch 14/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.4800 - accuracy: 0.7545\n",
      "Epoch 14: val_loss did not improve from 0.74662\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4784 - accuracy: 0.7603 - val_loss: 0.9673 - val_accuracy: 0.3333\n",
      "Epoch 15/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4656 - accuracy: 0.7690\n",
      "Epoch 15: val_loss did not improve from 0.74662\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4620 - accuracy: 0.7723 - val_loss: 0.9623 - val_accuracy: 0.3333\n",
      "Epoch 16/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4472 - accuracy: 0.7944Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.74662\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4460 - accuracy: 0.7949 - val_loss: 0.9579 - val_accuracy: 0.3333\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.6812\n",
      "Test AUC for Layer 3: 0.5638\n",
      "\n",
      "Average Test Accuracy across all layers: 0.6140\n",
      "Average Test AUC across all layers: 0.5614\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS (SLB)\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Merged + S_label\n",
      "Average Accuracy: 0.5414\n",
      "Average AUC: 0.4782\n",
      "  Layer 1 - Accuracy: 0.4848, AUC: 0.4173\n",
      "  Layer 2 - Accuracy: 0.5741, AUC: 0.5533\n",
      "  Layer 3 - Accuracy: 0.5652, AUC: 0.4641\n",
      "\n",
      "Combination: MLP with Merged + L_label\n",
      "Average Accuracy: 0.6140\n",
      "Average AUC: 0.5614\n",
      "  Layer 1 - Accuracy: 0.6515, AUC: 0.6006\n",
      "  Layer 2 - Accuracy: 0.5093, AUC: 0.5197\n",
      "  Layer 3 - Accuracy: 0.6812, AUC: 0.5638\n",
      "MLP summary comparison visualization saved as: Merged_OpenAI_MLP/visualizations_summary/SLB\\mlp_merged_performance_comparison.png\n",
      "MLP layer performance visualization saved as: Merged_OpenAI_MLP/visualizations_summary/SLB\\mlp_merged_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class MergedEmbeddingMLPPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using merged OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with merged OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'Merged_OpenAI_MLP/visualizations_mlp/SLB'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs('Merged_OpenAI_MLP/visualizations_summary/SLB', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with merged OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing merged OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert merged embeddings to numpy arrays\n",
    "        self.data['Merged_embedding'] = self.data['Merged_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_merged_embedding = self.data['Merged_embedding'].iloc[0]\n",
    "        print(f\"Sample Merged embedding shape: {sample_merged_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2019-2021), validation (2021-2021), test (2022-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-05-31'),\n",
    "            'val_start': pd.Timestamp('2021-06-01'),\n",
    "            'val_end': pd.Timestamp('2021-12-31'),\n",
    "            'test_start': pd.Timestamp('2022-01-01'),\n",
    "            'test_end': pd.Timestamp('2022-05-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2019-2021), validation (2022-2022), test (2022-2022)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-12-31'),\n",
    "            'val_start': pd.Timestamp('2022-01-01'),\n",
    "            'val_end': pd.Timestamp('2022-05-31'),\n",
    "            'test_start': pd.Timestamp('2022-06-01'),\n",
    "            'test_end': pd.Timestamp('2022-12-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2019-2022), validation (2022-2022), test (2023-2023)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2022-05-31'),\n",
    "            'val_start': pd.Timestamp('2022-06-01'),\n",
    "            'val_end': pd.Timestamp('2022-12-31'),\n",
    "            'test_start': pd.Timestamp('2023-01-01'),\n",
    "            'test_end': pd.Timestamp('2023-05-31')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data['Merged_embedding'].values)\n",
    "        X_val = np.stack(val_data['Merged_embedding'].values)\n",
    "        X_test = np.stack(test_data['Merged_embedding'].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_mlp_merged_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, label_col):\n",
    "        \"\"\"\n",
    "        Train and evaluate an MLP model for merged embeddings and a specific label column.\n",
    "        \n",
    "        Args:\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        combination_key = f\"MLP|Merged|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training MLP model for Merged Embeddings and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            model = self.create_mlp_model((1536,))\n",
    "            print(f\"Created MLP model for Merged Embeddings ({label_col})\")\n",
    "            model.summary()\n",
    "            \n",
    "            # Setup callbacks\n",
    "            plot_callback = PlotLearningCallback('Merged', label_col, i+1, self.mlp_viz_dir)\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=15,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            model_checkpoint = ModelCheckpoint(\n",
    "                filepath=f\"{self.mlp_viz_dir}/best_mlp_merged_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                monitor='val_loss',\n",
    "                save_weights_only=True,\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            print(f\"Training MLP model...\")\n",
    "            batch_size = 32  # Larger batch size for embeddings\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred_proba = model.predict(X_test)\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "            \n",
    "            # Create final learning curve visualization\n",
    "            self.plot_final_learning_curves(history, label_col, i+1, self.mlp_viz_dir)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': history.history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for merged embeddings with both short-term and long-term labels.\"\"\"\n",
    "        # Define label columns\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        \n",
    "        # Run analysis for each label column\n",
    "        for label_col in label_cols:\n",
    "            self.train_and_evaluate_model(label_col)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS (SLB)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Performance comparison for MLP with merged embeddings\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Plot accuracy and AUC bars\n",
    "        x = np.arange(len(df))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, df['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, df['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with Merged OpenAI Embeddings')\n",
    "        labels = [f\"Merged + {row['Label']}\" for _, row in df.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(df['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(df['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(df['Avg Accuracy'].max(), df['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/SLB', \"mlp_merged_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                layer_data.append(layer_info)\n",
    "        \n",
    "        if not layer_data:\n",
    "            print(\"No layer data available\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title('MLP Accuracy by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title('MLP AUC by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/SLB', \"mlp_merged_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['Merged_OpenAI_MLP/visualizations_mlp/SLB', 'Merged_OpenAI_MLP/visualizations_summary/SLB']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/US_news/SP500_semantic/us_news_semantics_SLB_completed_openai.csv'\n",
    "    \n",
    "    # Initialize the predictor with merged OpenAI embeddings\n",
    "    predictor = MergedEmbeddingMLPPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing OpenAI embedding vectors from string format...\n",
      "Sample Title embedding shape: (1536,)\n",
      "Sample Fulltext embedding shape: (1536,)\n",
      "Loaded 929 climate change news articles spanning from 02/01/2019 to 07/05/2023\n",
      "Class distribution for short-term prediction: {1: 482, 0: 447}\n",
      "Class distribution for long-term prediction: {1: 555, 0: 374}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_144 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_108 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_72 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_145 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_109 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_73 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_146 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_110 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_147 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8882 - accuracy: 0.4766\n",
      "Epoch 1: val_loss improved from inf to 0.69379, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8926 - accuracy: 0.4770 - val_loss: 0.6938 - val_accuracy: 0.4417\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8477 - accuracy: 0.5211\n",
      "Epoch 2: val_loss improved from 0.69379 to 0.69351, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.8477 - accuracy: 0.5211 - val_loss: 0.6935 - val_accuracy: 0.4847\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7718 - accuracy: 0.5723\n",
      "Epoch 3: val_loss did not improve from 0.69351\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7708 - accuracy: 0.5728 - val_loss: 0.6935 - val_accuracy: 0.5092\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7192 - accuracy: 0.5977\n",
      "Epoch 4: val_loss did not improve from 0.69351\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7192 - accuracy: 0.5977 - val_loss: 0.6937 - val_accuracy: 0.5153\n",
      "Epoch 5/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6800 - accuracy: 0.6130\n",
      "Epoch 5: val_loss did not improve from 0.69351\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6655 - accuracy: 0.6264 - val_loss: 0.6941 - val_accuracy: 0.5092\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6517 - accuracy: 0.6466\n",
      "Epoch 6: val_loss did not improve from 0.69351\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6350 - accuracy: 0.6628 - val_loss: 0.6947 - val_accuracy: 0.5092\n",
      "Epoch 7/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5975 - accuracy: 0.6851\n",
      "Epoch 7: val_loss did not improve from 0.69351\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5960 - accuracy: 0.6762 - val_loss: 0.6958 - val_accuracy: 0.5092\n",
      "Epoch 8/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5827 - accuracy: 0.6875\n",
      "Epoch 8: val_loss did not improve from 0.69351\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5828 - accuracy: 0.6916 - val_loss: 0.6979 - val_accuracy: 0.5092\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5436 - accuracy: 0.7404\n",
      "Epoch 9: val_loss did not improve from 0.69351\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5581 - accuracy: 0.7356 - val_loss: 0.6997 - val_accuracy: 0.5092\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5040 - accuracy: 0.7452\n",
      "Epoch 10: val_loss did not improve from 0.69351\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5040 - accuracy: 0.7452 - val_loss: 0.7021 - val_accuracy: 0.5092\n",
      "Epoch 11/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5007 - accuracy: 0.7596\n",
      "Epoch 11: val_loss did not improve from 0.69351\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5033 - accuracy: 0.7529 - val_loss: 0.7051 - val_accuracy: 0.5092\n",
      "Epoch 12/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4430 - accuracy: 0.8029\n",
      "Epoch 12: val_loss did not improve from 0.69351\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4353 - accuracy: 0.8103 - val_loss: 0.7086 - val_accuracy: 0.5092\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4539 - accuracy: 0.7885\n",
      "Epoch 13: val_loss did not improve from 0.69351\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4671 - accuracy: 0.7739 - val_loss: 0.7119 - val_accuracy: 0.5092\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4266 - accuracy: 0.8242\n",
      "Epoch 14: val_loss did not improve from 0.69351\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4261 - accuracy: 0.8218 - val_loss: 0.7149 - val_accuracy: 0.5092\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4320 - accuracy: 0.8123\n",
      "Epoch 15: val_loss did not improve from 0.69351\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4320 - accuracy: 0.8123 - val_loss: 0.7180 - val_accuracy: 0.5092\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4074 - accuracy: 0.8149\n",
      "Epoch 16: val_loss did not improve from 0.69351\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4192 - accuracy: 0.8027 - val_loss: 0.7225 - val_accuracy: 0.5153\n",
      "Epoch 17/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3812 - accuracy: 0.8281Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.69351\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3797 - accuracy: 0.8314 - val_loss: 0.7275 - val_accuracy: 0.5153\n",
      "Epoch 17: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.4697\n",
      "Test AUC for Layer 1: 0.5798\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_37\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_148 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_111 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_74 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_149 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_112 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_75 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_150 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_113 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_151 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.9013 - accuracy: 0.4762\n",
      "Epoch 1: val_loss improved from inf to 0.70142, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 21ms/step - loss: 0.8981 - accuracy: 0.4759 - val_loss: 0.7014 - val_accuracy: 0.3939\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8025 - accuracy: 0.5080\n",
      "Epoch 2: val_loss did not improve from 0.70142\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.8025 - accuracy: 0.5080 - val_loss: 0.7127 - val_accuracy: 0.3939\n",
      "Epoch 3/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.7618 - accuracy: 0.5556\n",
      "Epoch 3: val_loss did not improve from 0.70142\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7665 - accuracy: 0.5431 - val_loss: 0.7244 - val_accuracy: 0.3939\n",
      "Epoch 4/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.7250 - accuracy: 0.5972\n",
      "Epoch 4: val_loss did not improve from 0.70142\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7118 - accuracy: 0.5956 - val_loss: 0.7351 - val_accuracy: 0.3939\n",
      "Epoch 5/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6953 - accuracy: 0.6086\n",
      "Epoch 5: val_loss did not improve from 0.70142\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6926 - accuracy: 0.6190 - val_loss: 0.7461 - val_accuracy: 0.3939\n",
      "Epoch 6/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6076 - accuracy: 0.6678\n",
      "Epoch 6: val_loss did not improve from 0.70142\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6057 - accuracy: 0.6672 - val_loss: 0.7583 - val_accuracy: 0.3939\n",
      "Epoch 7/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6121 - accuracy: 0.6661\n",
      "Epoch 7: val_loss did not improve from 0.70142\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6077 - accuracy: 0.6715 - val_loss: 0.7657 - val_accuracy: 0.3939\n",
      "Epoch 8/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5692 - accuracy: 0.7188\n",
      "Epoch 8: val_loss did not improve from 0.70142\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5680 - accuracy: 0.7212 - val_loss: 0.7782 - val_accuracy: 0.3939\n",
      "Epoch 9/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5404 - accuracy: 0.7307\n",
      "Epoch 9: val_loss did not improve from 0.70142\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5415 - accuracy: 0.7314 - val_loss: 0.7887 - val_accuracy: 0.3939\n",
      "Epoch 10/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5145 - accuracy: 0.7467\n",
      "Epoch 10: val_loss did not improve from 0.70142\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5115 - accuracy: 0.7504 - val_loss: 0.7934 - val_accuracy: 0.3939\n",
      "Epoch 11/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4840 - accuracy: 0.7555\n",
      "Epoch 11: val_loss did not improve from 0.70142\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4847 - accuracy: 0.7518 - val_loss: 0.8019 - val_accuracy: 0.3939\n",
      "Epoch 12/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4619 - accuracy: 0.7743\n",
      "Epoch 12: val_loss did not improve from 0.70142\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4730 - accuracy: 0.7650 - val_loss: 0.8077 - val_accuracy: 0.3939\n",
      "Epoch 13/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4596 - accuracy: 0.7934\n",
      "Epoch 13: val_loss did not improve from 0.70142\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4674 - accuracy: 0.7810 - val_loss: 0.8058 - val_accuracy: 0.3939\n",
      "Epoch 14/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4470 - accuracy: 0.7977\n",
      "Epoch 14: val_loss did not improve from 0.70142\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4465 - accuracy: 0.7942 - val_loss: 0.8047 - val_accuracy: 0.3939\n",
      "Epoch 15/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4159 - accuracy: 0.8364\n",
      "Epoch 15: val_loss did not improve from 0.70142\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4083 - accuracy: 0.8453 - val_loss: 0.8048 - val_accuracy: 0.3939\n",
      "Epoch 16/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3749 - accuracy: 0.8520Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70142\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3771 - accuracy: 0.8540 - val_loss: 0.8100 - val_accuracy: 0.3939\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4167\n",
      "Test AUC for Layer 2: 0.5263\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_152 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_114 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_76 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_153 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_115 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_77 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_116 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_155 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.9116 - accuracy: 0.4613\n",
      "Epoch 1: val_loss improved from inf to 0.68974, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.9032 - accuracy: 0.4674 - val_loss: 0.6897 - val_accuracy: 0.5833\n",
      "Epoch 2/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.8405 - accuracy: 0.5298\n",
      "Epoch 2: val_loss improved from 0.68974 to 0.68878, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.8356 - accuracy: 0.5353 - val_loss: 0.6888 - val_accuracy: 0.5833\n",
      "Epoch 3/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.7629 - accuracy: 0.5699\n",
      "Epoch 3: val_loss improved from 0.68878 to 0.68875, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.7661 - accuracy: 0.5672 - val_loss: 0.6888 - val_accuracy: 0.5833\n",
      "Epoch 4/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.7532 - accuracy: 0.5503\n",
      "Epoch 4: val_loss improved from 0.68875 to 0.68844, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7518 - accuracy: 0.5499 - val_loss: 0.6884 - val_accuracy: 0.5741\n",
      "Epoch 5/100\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.6540 - accuracy: 0.6250\n",
      "Epoch 5: val_loss did not improve from 0.68844\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.6603 - accuracy: 0.6218 - val_loss: 0.6891 - val_accuracy: 0.5741\n",
      "Epoch 6/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6392 - accuracy: 0.6399\n",
      "Epoch 6: val_loss did not improve from 0.68844\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.6340 - accuracy: 0.6445 - val_loss: 0.6901 - val_accuracy: 0.5278\n",
      "Epoch 7/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.6498 - accuracy: 0.6369\n",
      "Epoch 7: val_loss did not improve from 0.68844\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.6404 - accuracy: 0.6391 - val_loss: 0.6907 - val_accuracy: 0.5463\n",
      "Epoch 8/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5820 - accuracy: 0.6924\n",
      "Epoch 8: val_loss did not improve from 0.68844\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5690 - accuracy: 0.7044 - val_loss: 0.6905 - val_accuracy: 0.5556\n",
      "Epoch 9/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5472 - accuracy: 0.7242\n",
      "Epoch 9: val_loss did not improve from 0.68844\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5502 - accuracy: 0.7217 - val_loss: 0.6915 - val_accuracy: 0.5463\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5352 - accuracy: 0.7377\n",
      "Epoch 10: val_loss did not improve from 0.68844\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5352 - accuracy: 0.7377 - val_loss: 0.6926 - val_accuracy: 0.5556\n",
      "Epoch 11/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5257 - accuracy: 0.7405\n",
      "Epoch 11: val_loss did not improve from 0.68844\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5242 - accuracy: 0.7390 - val_loss: 0.6934 - val_accuracy: 0.5463\n",
      "Epoch 12/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4510 - accuracy: 0.7895\n",
      "Epoch 12: val_loss did not improve from 0.68844\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4561 - accuracy: 0.7830 - val_loss: 0.6926 - val_accuracy: 0.5370\n",
      "Epoch 13/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4440 - accuracy: 0.7961\n",
      "Epoch 13: val_loss did not improve from 0.68844\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4389 - accuracy: 0.8056 - val_loss: 0.6915 - val_accuracy: 0.5370\n",
      "Epoch 14/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4444 - accuracy: 0.8043\n",
      "Epoch 14: val_loss improved from 0.68844 to 0.68829, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.4454 - accuracy: 0.8043 - val_loss: 0.6883 - val_accuracy: 0.5370\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4190 - accuracy: 0.8016\n",
      "Epoch 15: val_loss improved from 0.68829 to 0.68604, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.4190 - accuracy: 0.8016 - val_loss: 0.6860 - val_accuracy: 0.5556\n",
      "Epoch 16/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.3876 - accuracy: 0.8363\n",
      "Epoch 16: val_loss improved from 0.68604 to 0.68390, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.3948 - accuracy: 0.8269 - val_loss: 0.6839 - val_accuracy: 0.5556\n",
      "Epoch 17/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4062 - accuracy: 0.8174\n",
      "Epoch 17: val_loss improved from 0.68390 to 0.68253, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.4044 - accuracy: 0.8216 - val_loss: 0.6825 - val_accuracy: 0.5833\n",
      "Epoch 18/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.3762 - accuracy: 0.8438\n",
      "Epoch 18: val_loss improved from 0.68253 to 0.68061, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.3770 - accuracy: 0.8402 - val_loss: 0.6806 - val_accuracy: 0.5648\n",
      "Epoch 19/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.3526 - accuracy: 0.8565\n",
      "Epoch 19: val_loss did not improve from 0.68061\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.3440 - accuracy: 0.8628 - val_loss: 0.6827 - val_accuracy: 0.5648\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3476 - accuracy: 0.8735\n",
      "Epoch 20: val_loss did not improve from 0.68061\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3476 - accuracy: 0.8735 - val_loss: 0.6875 - val_accuracy: 0.5741\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3305 - accuracy: 0.8628\n",
      "Epoch 21: val_loss did not improve from 0.68061\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3305 - accuracy: 0.8628 - val_loss: 0.6901 - val_accuracy: 0.5463\n",
      "Epoch 22/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2956 - accuracy: 0.8931\n",
      "Epoch 22: val_loss did not improve from 0.68061\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2968 - accuracy: 0.8935 - val_loss: 0.6956 - val_accuracy: 0.5648\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2902 - accuracy: 0.8988\n",
      "Epoch 23: val_loss did not improve from 0.68061\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.2902 - accuracy: 0.8988 - val_loss: 0.6984 - val_accuracy: 0.5278\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2756 - accuracy: 0.9028\n",
      "Epoch 24: val_loss did not improve from 0.68061\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2756 - accuracy: 0.9028 - val_loss: 0.7031 - val_accuracy: 0.5556\n",
      "Epoch 25/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.2609 - accuracy: 0.9185\n",
      "Epoch 25: val_loss did not improve from 0.68061\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.2606 - accuracy: 0.9188 - val_loss: 0.7048 - val_accuracy: 0.5556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.9148\n",
      "Epoch 26: val_loss did not improve from 0.68061\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2499 - accuracy: 0.9148 - val_loss: 0.7167 - val_accuracy: 0.5556\n",
      "Epoch 27/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2584 - accuracy: 0.9128\n",
      "Epoch 27: val_loss did not improve from 0.68061\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2581 - accuracy: 0.9121 - val_loss: 0.7218 - val_accuracy: 0.5556\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2520 - accuracy: 0.9174\n",
      "Epoch 28: val_loss did not improve from 0.68061\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2520 - accuracy: 0.9174 - val_loss: 0.7362 - val_accuracy: 0.5370\n",
      "Epoch 29/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2317 - accuracy: 0.9194\n",
      "Epoch 29: val_loss did not improve from 0.68061\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2243 - accuracy: 0.9268 - val_loss: 0.7532 - val_accuracy: 0.5463\n",
      "Epoch 30/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2153 - accuracy: 0.9424\n",
      "Epoch 30: val_loss did not improve from 0.68061\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2252 - accuracy: 0.9374 - val_loss: 0.7673 - val_accuracy: 0.5370\n",
      "Epoch 31/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2007 - accuracy: 0.9375\n",
      "Epoch 31: val_loss did not improve from 0.68061\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2033 - accuracy: 0.9348 - val_loss: 0.7731 - val_accuracy: 0.5648\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1949 - accuracy: 0.9494\n",
      "Epoch 32: val_loss did not improve from 0.68061\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1949 - accuracy: 0.9494 - val_loss: 0.7867 - val_accuracy: 0.5648\n",
      "Epoch 33/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.1941 - accuracy: 0.9443Restoring model weights from the end of the best epoch: 18.\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.68061\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1924 - accuracy: 0.9454 - val_loss: 0.8088 - val_accuracy: 0.5741\n",
      "Epoch 33: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5072\n",
      "Test AUC for Layer 3: 0.4588\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4645\n",
      "Average Test AUC across all layers: 0.5216\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_156 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_117 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_78 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_157 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_118 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_79 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_158 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_119 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_159 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8851 - accuracy: 0.5215\n",
      "Epoch 1: val_loss improved from inf to 0.72510, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8848 - accuracy: 0.5192 - val_loss: 0.7251 - val_accuracy: 0.3926\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7692 - accuracy: 0.5840\n",
      "Epoch 2: val_loss did not improve from 0.72510\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7639 - accuracy: 0.5862 - val_loss: 0.7626 - val_accuracy: 0.3926\n",
      "Epoch 3/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.7482 - accuracy: 0.5938\n",
      "Epoch 3: val_loss did not improve from 0.72510\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7509 - accuracy: 0.5900 - val_loss: 0.8046 - val_accuracy: 0.3926\n",
      "Epoch 4/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6940 - accuracy: 0.5964\n",
      "Epoch 4: val_loss did not improve from 0.72510\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6987 - accuracy: 0.5958 - val_loss: 0.8440 - val_accuracy: 0.3926\n",
      "Epoch 5/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6305 - accuracy: 0.6635\n",
      "Epoch 5: val_loss did not improve from 0.72510\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6414 - accuracy: 0.6628 - val_loss: 0.8801 - val_accuracy: 0.3926\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5685 - accuracy: 0.6947\n",
      "Epoch 6: val_loss did not improve from 0.72510\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5997 - accuracy: 0.6762 - val_loss: 0.9094 - val_accuracy: 0.3926\n",
      "Epoch 7/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5302 - accuracy: 0.7396\n",
      "Epoch 7: val_loss did not improve from 0.72510\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5304 - accuracy: 0.7395 - val_loss: 0.9398 - val_accuracy: 0.3926\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5247 - accuracy: 0.7433\n",
      "Epoch 8: val_loss did not improve from 0.72510\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5247 - accuracy: 0.7433 - val_loss: 0.9614 - val_accuracy: 0.3926\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5123 - accuracy: 0.7461\n",
      "Epoch 9: val_loss did not improve from 0.72510\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5112 - accuracy: 0.7471 - val_loss: 0.9790 - val_accuracy: 0.3926\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4782 - accuracy: 0.7620\n",
      "Epoch 10: val_loss did not improve from 0.72510\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4797 - accuracy: 0.7663 - val_loss: 0.9979 - val_accuracy: 0.3926\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4436 - accuracy: 0.8008\n",
      "Epoch 11: val_loss did not improve from 0.72510\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4418 - accuracy: 0.8046 - val_loss: 1.0106 - val_accuracy: 0.3926\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4854 - accuracy: 0.7701\n",
      "Epoch 12: val_loss did not improve from 0.72510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4854 - accuracy: 0.7701 - val_loss: 1.0224 - val_accuracy: 0.3926\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4265 - accuracy: 0.8077\n",
      "Epoch 13: val_loss did not improve from 0.72510\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4178 - accuracy: 0.8180 - val_loss: 1.0254 - val_accuracy: 0.3926\n",
      "Epoch 14/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3977 - accuracy: 0.8125\n",
      "Epoch 14: val_loss did not improve from 0.72510\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3998 - accuracy: 0.8123 - val_loss: 1.0354 - val_accuracy: 0.3926\n",
      "Epoch 15/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3728 - accuracy: 0.8462\n",
      "Epoch 15: val_loss did not improve from 0.72510\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3830 - accuracy: 0.8352 - val_loss: 1.0353 - val_accuracy: 0.3926\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3553 - accuracy: 0.8659Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.72510\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3553 - accuracy: 0.8659 - val_loss: 1.0454 - val_accuracy: 0.3926\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.1061\n",
      "Test AUC for Layer 1: 0.6441\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_160 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_120 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_80 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_161 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_121 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_81 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_162 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_122 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.8993 - accuracy: 0.5368\n",
      "Epoch 1: val_loss improved from inf to 0.76880, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 20ms/step - loss: 0.9131 - accuracy: 0.5255 - val_loss: 0.7688 - val_accuracy: 0.1061\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8297 - accuracy: 0.5670\n",
      "Epoch 2: val_loss did not improve from 0.76880\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.8319 - accuracy: 0.5679 - val_loss: 0.8550 - val_accuracy: 0.1061\n",
      "Epoch 3/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.7863 - accuracy: 0.5729\n",
      "Epoch 3: val_loss did not improve from 0.76880\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7797 - accuracy: 0.5620 - val_loss: 0.9499 - val_accuracy: 0.1061\n",
      "Epoch 4/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.7261 - accuracy: 0.6135\n",
      "Epoch 4: val_loss did not improve from 0.76880\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7391 - accuracy: 0.6088 - val_loss: 1.0344 - val_accuracy: 0.1061\n",
      "Epoch 5/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6846 - accuracy: 0.6513\n",
      "Epoch 5: val_loss did not improve from 0.76880\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6694 - accuracy: 0.6599 - val_loss: 1.1123 - val_accuracy: 0.1061\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6210 - accuracy: 0.6803\n",
      "Epoch 6: val_loss did not improve from 0.76880\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6210 - accuracy: 0.6803 - val_loss: 1.1870 - val_accuracy: 0.1061\n",
      "Epoch 7/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.5445 - accuracy: 0.7063\n",
      "Epoch 7: val_loss did not improve from 0.76880\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.5426 - accuracy: 0.7051 - val_loss: 1.2719 - val_accuracy: 0.1061\n",
      "Epoch 8/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5876 - accuracy: 0.6823\n",
      "Epoch 8: val_loss did not improve from 0.76880\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5815 - accuracy: 0.6920 - val_loss: 1.3447 - val_accuracy: 0.1061\n",
      "Epoch 9/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4878 - accuracy: 0.7537\n",
      "Epoch 9: val_loss did not improve from 0.76880\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4979 - accuracy: 0.7445 - val_loss: 1.3937 - val_accuracy: 0.1061\n",
      "Epoch 10/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5121 - accuracy: 0.7484\n",
      "Epoch 10: val_loss did not improve from 0.76880\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.5072 - accuracy: 0.7489 - val_loss: 1.4352 - val_accuracy: 0.1061\n",
      "Epoch 11/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.4660 - accuracy: 0.7946\n",
      "Epoch 11: val_loss did not improve from 0.76880\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4700 - accuracy: 0.7942 - val_loss: 1.5035 - val_accuracy: 0.1061\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4481 - accuracy: 0.7883\n",
      "Epoch 12: val_loss did not improve from 0.76880\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4481 - accuracy: 0.7883 - val_loss: 1.5413 - val_accuracy: 0.1061\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3977 - accuracy: 0.8204\n",
      "Epoch 13: val_loss did not improve from 0.76880\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3977 - accuracy: 0.8204 - val_loss: 1.5854 - val_accuracy: 0.1061\n",
      "Epoch 14/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4093 - accuracy: 0.8372\n",
      "Epoch 14: val_loss did not improve from 0.76880\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4093 - accuracy: 0.8365 - val_loss: 1.6254 - val_accuracy: 0.1061\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3997 - accuracy: 0.8350\n",
      "Epoch 15: val_loss did not improve from 0.76880\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3997 - accuracy: 0.8350 - val_loss: 1.6534 - val_accuracy: 0.1061\n",
      "Epoch 16/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.3890 - accuracy: 0.8274Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.76880\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3916 - accuracy: 0.8263 - val_loss: 1.6533 - val_accuracy: 0.1061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.2685\n",
      "Test AUC for Layer 2: 0.5260\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_164 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_123 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_82 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_165 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_124 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_83 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_166 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_125 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_167 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.9626 - accuracy: 0.4762\n",
      "Epoch 1: val_loss improved from inf to 0.67722, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.9623 - accuracy: 0.4780 - val_loss: 0.6772 - val_accuracy: 0.7315\n",
      "Epoch 2/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.8311 - accuracy: 0.5327\n",
      "Epoch 2: val_loss improved from 0.67722 to 0.66466, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.8317 - accuracy: 0.5406 - val_loss: 0.6647 - val_accuracy: 0.7315\n",
      "Epoch 3/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.7631 - accuracy: 0.5938\n",
      "Epoch 3: val_loss improved from 0.66466 to 0.65420, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7638 - accuracy: 0.5899 - val_loss: 0.6542 - val_accuracy: 0.7315\n",
      "Epoch 4/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.7031 - accuracy: 0.6277\n",
      "Epoch 4: val_loss improved from 0.65420 to 0.64282, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7028 - accuracy: 0.6285 - val_loss: 0.6428 - val_accuracy: 0.7315\n",
      "Epoch 5/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.6796 - accuracy: 0.6324\n",
      "Epoch 5: val_loss improved from 0.64282 to 0.63830, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.6757 - accuracy: 0.6378 - val_loss: 0.6383 - val_accuracy: 0.7315\n",
      "Epoch 6/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6283 - accuracy: 0.6534\n",
      "Epoch 6: val_loss improved from 0.63830 to 0.63349, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6262 - accuracy: 0.6551 - val_loss: 0.6335 - val_accuracy: 0.7315\n",
      "Epoch 7/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6033 - accuracy: 0.6902\n",
      "Epoch 7: val_loss improved from 0.63349 to 0.63117, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6029 - accuracy: 0.6897 - val_loss: 0.6312 - val_accuracy: 0.7315\n",
      "Epoch 8/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.5897 - accuracy: 0.6790\n",
      "Epoch 8: val_loss improved from 0.63117 to 0.62750, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.5930 - accuracy: 0.6804 - val_loss: 0.6275 - val_accuracy: 0.7315\n",
      "Epoch 9/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5238 - accuracy: 0.7411\n",
      "Epoch 9: val_loss did not improve from 0.62750\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5236 - accuracy: 0.7430 - val_loss: 0.6286 - val_accuracy: 0.7315\n",
      "Epoch 10/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5010 - accuracy: 0.7582\n",
      "Epoch 10: val_loss did not improve from 0.62750\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5023 - accuracy: 0.7563 - val_loss: 0.6284 - val_accuracy: 0.7315\n",
      "Epoch 11/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4984 - accuracy: 0.7566\n",
      "Epoch 11: val_loss improved from 0.62750 to 0.62474, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4895 - accuracy: 0.7643 - val_loss: 0.6247 - val_accuracy: 0.7315\n",
      "Epoch 12/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4691 - accuracy: 0.7685\n",
      "Epoch 12: val_loss improved from 0.62474 to 0.62460, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.4778 - accuracy: 0.7656 - val_loss: 0.6246 - val_accuracy: 0.7315\n",
      "Epoch 13/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.4416 - accuracy: 0.7887\n",
      "Epoch 13: val_loss did not improve from 0.62460\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4482 - accuracy: 0.7816 - val_loss: 0.6259 - val_accuracy: 0.7315\n",
      "Epoch 14/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4290 - accuracy: 0.7796\n",
      "Epoch 14: val_loss did not improve from 0.62460\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4392 - accuracy: 0.7723 - val_loss: 0.6277 - val_accuracy: 0.7222\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3931 - accuracy: 0.8309\n",
      "Epoch 15: val_loss did not improve from 0.62460\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3931 - accuracy: 0.8309 - val_loss: 0.6272 - val_accuracy: 0.7222\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3752 - accuracy: 0.8482\n",
      "Epoch 16: val_loss did not improve from 0.62460\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3752 - accuracy: 0.8482 - val_loss: 0.6257 - val_accuracy: 0.7222\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3640 - accuracy: 0.8442\n",
      "Epoch 17: val_loss did not improve from 0.62460\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3640 - accuracy: 0.8442 - val_loss: 0.6277 - val_accuracy: 0.7222\n",
      "Epoch 18/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3476 - accuracy: 0.8668\n",
      "Epoch 18: val_loss did not improve from 0.62460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3435 - accuracy: 0.8628 - val_loss: 0.6324 - val_accuracy: 0.6667\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3441 - accuracy: 0.8589\n",
      "Epoch 19: val_loss did not improve from 0.62460\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3441 - accuracy: 0.8589 - val_loss: 0.6391 - val_accuracy: 0.6389\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3265 - accuracy: 0.8762\n",
      "Epoch 20: val_loss did not improve from 0.62460\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3265 - accuracy: 0.8762 - val_loss: 0.6452 - val_accuracy: 0.6481\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.8868\n",
      "Epoch 21: val_loss did not improve from 0.62460\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3008 - accuracy: 0.8868 - val_loss: 0.6528 - val_accuracy: 0.6111\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.9081\n",
      "Epoch 22: val_loss did not improve from 0.62460\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2905 - accuracy: 0.9081 - val_loss: 0.6595 - val_accuracy: 0.6111\n",
      "Epoch 23/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.2879 - accuracy: 0.8981\n",
      "Epoch 23: val_loss did not improve from 0.62460\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2847 - accuracy: 0.9001 - val_loss: 0.6701 - val_accuracy: 0.5926\n",
      "Epoch 24/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.2637 - accuracy: 0.9171\n",
      "Epoch 24: val_loss did not improve from 0.62460\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2642 - accuracy: 0.9161 - val_loss: 0.6770 - val_accuracy: 0.6019\n",
      "Epoch 25/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2729 - accuracy: 0.9095\n",
      "Epoch 25: val_loss did not improve from 0.62460\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2722 - accuracy: 0.9055 - val_loss: 0.6851 - val_accuracy: 0.5833\n",
      "Epoch 26/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2442 - accuracy: 0.9326\n",
      "Epoch 26: val_loss did not improve from 0.62460\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2438 - accuracy: 0.9348 - val_loss: 0.6954 - val_accuracy: 0.6111\n",
      "Epoch 27/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.2379 - accuracy: 0.9280Restoring model weights from the end of the best epoch: 12.\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.62460\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.2388 - accuracy: 0.9281 - val_loss: 0.7017 - val_accuracy: 0.5926\n",
      "Epoch 27: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5217\n",
      "Test AUC for Layer 3: 0.4571\n",
      "\n",
      "Average Test Accuracy across all layers: 0.2988\n",
      "Average Test AUC across all layers: 0.5424\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_168 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_126 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_84 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_169 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_127 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_85 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_170 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_128 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_171 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.9211 - accuracy: 0.4854\n",
      "Epoch 1: val_loss improved from inf to 0.69425, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 24ms/step - loss: 0.9158 - accuracy: 0.4789 - val_loss: 0.6943 - val_accuracy: 0.5092\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8160 - accuracy: 0.5254\n",
      "Epoch 2: val_loss did not improve from 0.69425\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.8149 - accuracy: 0.5249 - val_loss: 0.6950 - val_accuracy: 0.5092\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8033 - accuracy: 0.5536\n",
      "Epoch 3: val_loss did not improve from 0.69425\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.8033 - accuracy: 0.5536 - val_loss: 0.6963 - val_accuracy: 0.5092\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7577 - accuracy: 0.5664\n",
      "Epoch 4: val_loss did not improve from 0.69425\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7575 - accuracy: 0.5613 - val_loss: 0.6986 - val_accuracy: 0.5092\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6842 - accuracy: 0.6387\n",
      "Epoch 5: val_loss did not improve from 0.69425\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6892 - accuracy: 0.6360 - val_loss: 0.6996 - val_accuracy: 0.5092\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6666 - accuracy: 0.6367\n",
      "Epoch 6: val_loss did not improve from 0.69425\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6598 - accuracy: 0.6437 - val_loss: 0.7005 - val_accuracy: 0.5092\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6106 - accuracy: 0.6777\n",
      "Epoch 7: val_loss did not improve from 0.69425\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6090 - accuracy: 0.6762 - val_loss: 0.7007 - val_accuracy: 0.5092\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6115 - accuracy: 0.6897\n",
      "Epoch 8: val_loss did not improve from 0.69425\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6115 - accuracy: 0.6897 - val_loss: 0.7003 - val_accuracy: 0.5092\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5596 - accuracy: 0.7067\n",
      "Epoch 9: val_loss did not improve from 0.69425\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5626 - accuracy: 0.7031 - val_loss: 0.6996 - val_accuracy: 0.5092\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5853 - accuracy: 0.7107\n",
      "Epoch 10: val_loss did not improve from 0.69425\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5853 - accuracy: 0.7107 - val_loss: 0.6990 - val_accuracy: 0.5092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5174 - accuracy: 0.7441\n",
      "Epoch 11: val_loss did not improve from 0.69425\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5142 - accuracy: 0.7452 - val_loss: 0.6985 - val_accuracy: 0.5092\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4959 - accuracy: 0.7605\n",
      "Epoch 12: val_loss did not improve from 0.69425\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4959 - accuracy: 0.7605 - val_loss: 0.6979 - val_accuracy: 0.5092\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5161 - accuracy: 0.7476\n",
      "Epoch 13: val_loss did not improve from 0.69425\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5012 - accuracy: 0.7663 - val_loss: 0.6976 - val_accuracy: 0.5092\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4866 - accuracy: 0.7578\n",
      "Epoch 14: val_loss did not improve from 0.69425\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4846 - accuracy: 0.7586 - val_loss: 0.6973 - val_accuracy: 0.5092\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4270 - accuracy: 0.8047\n",
      "Epoch 15: val_loss did not improve from 0.69425\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4338 - accuracy: 0.7989 - val_loss: 0.6976 - val_accuracy: 0.5031\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4225 - accuracy: 0.8184Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69425\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4245 - accuracy: 0.8180 - val_loss: 0.6968 - val_accuracy: 0.5031\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.3939\n",
      "Test AUC for Layer 1: 0.5538\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_172 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_129 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_86 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_173 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_130 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_87 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_174 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_131 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_175 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8900 - accuracy: 0.4940\n",
      "Epoch 1: val_loss improved from inf to 0.70138, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 20ms/step - loss: 0.8896 - accuracy: 0.4949 - val_loss: 0.7014 - val_accuracy: 0.3939\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7587 - accuracy: 0.5595\n",
      "Epoch 2: val_loss did not improve from 0.70138\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7584 - accuracy: 0.5620 - val_loss: 0.7089 - val_accuracy: 0.3939\n",
      "Epoch 3/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7183 - accuracy: 0.6235\n",
      "Epoch 3: val_loss did not improve from 0.70138\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7202 - accuracy: 0.6219 - val_loss: 0.7157 - val_accuracy: 0.3939\n",
      "Epoch 4/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6852 - accuracy: 0.6071\n",
      "Epoch 4: val_loss did not improve from 0.70138\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6874 - accuracy: 0.6058 - val_loss: 0.7230 - val_accuracy: 0.3939\n",
      "Epoch 5/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.6887 - accuracy: 0.6544\n",
      "Epoch 5: val_loss did not improve from 0.70138\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6941 - accuracy: 0.6496 - val_loss: 0.7268 - val_accuracy: 0.3939\n",
      "Epoch 6/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6476 - accuracy: 0.6480\n",
      "Epoch 6: val_loss did not improve from 0.70138\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6522 - accuracy: 0.6453 - val_loss: 0.7322 - val_accuracy: 0.3939\n",
      "Epoch 7/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5944 - accuracy: 0.6793\n",
      "Epoch 7: val_loss did not improve from 0.70138\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5916 - accuracy: 0.6818 - val_loss: 0.7357 - val_accuracy: 0.3939\n",
      "Epoch 8/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5920 - accuracy: 0.6801\n",
      "Epoch 8: val_loss did not improve from 0.70138\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6074 - accuracy: 0.6730 - val_loss: 0.7380 - val_accuracy: 0.3939\n",
      "Epoch 9/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5305 - accuracy: 0.7316\n",
      "Epoch 9: val_loss did not improve from 0.70138\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5487 - accuracy: 0.7226 - val_loss: 0.7400 - val_accuracy: 0.3939\n",
      "Epoch 10/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5259 - accuracy: 0.7500\n",
      "Epoch 10: val_loss did not improve from 0.70138\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5147 - accuracy: 0.7474 - val_loss: 0.7409 - val_accuracy: 0.3939\n",
      "Epoch 11/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5259 - accuracy: 0.7500\n",
      "Epoch 11: val_loss did not improve from 0.70138\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5281 - accuracy: 0.7387 - val_loss: 0.7392 - val_accuracy: 0.3939\n",
      "Epoch 12/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5122 - accuracy: 0.7353\n",
      "Epoch 12: val_loss did not improve from 0.70138\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4890 - accuracy: 0.7562 - val_loss: 0.7401 - val_accuracy: 0.3788\n",
      "Epoch 13/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4614 - accuracy: 0.7743\n",
      "Epoch 13: val_loss did not improve from 0.70138\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4684 - accuracy: 0.7708 - val_loss: 0.7419 - val_accuracy: 0.3939\n",
      "Epoch 14/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4640 - accuracy: 0.7795\n",
      "Epoch 14: val_loss did not improve from 0.70138\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4620 - accuracy: 0.7810 - val_loss: 0.7427 - val_accuracy: 0.4091\n",
      "Epoch 15/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4430 - accuracy: 0.8092\n",
      "Epoch 15: val_loss did not improve from 0.70138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4414 - accuracy: 0.8117 - val_loss: 0.7406 - val_accuracy: 0.3939\n",
      "Epoch 16/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4430 - accuracy: 0.8056Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70138\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4512 - accuracy: 0.7956 - val_loss: 0.7411 - val_accuracy: 0.3939\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4167\n",
      "Test AUC for Layer 2: 0.4942\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_176 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_132 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_88 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_177 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_133 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_89 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_178 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_134 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_179 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8609 - accuracy: 0.5179\n",
      "Epoch 1: val_loss improved from inf to 0.68997, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.8727 - accuracy: 0.5113 - val_loss: 0.6900 - val_accuracy: 0.5926\n",
      "Epoch 2/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.8137 - accuracy: 0.5369\n",
      "Epoch 2: val_loss improved from 0.68997 to 0.68764, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.8134 - accuracy: 0.5393 - val_loss: 0.6876 - val_accuracy: 0.5833\n",
      "Epoch 3/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.7350 - accuracy: 0.5893\n",
      "Epoch 3: val_loss improved from 0.68764 to 0.68525, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7352 - accuracy: 0.5912 - val_loss: 0.6852 - val_accuracy: 0.5833\n",
      "Epoch 4/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6562 - accuracy: 0.6435\n",
      "Epoch 4: val_loss improved from 0.68525 to 0.68331, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6584 - accuracy: 0.6378 - val_loss: 0.6833 - val_accuracy: 0.5833\n",
      "Epoch 5/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6663 - accuracy: 0.6101\n",
      "Epoch 5: val_loss improved from 0.68331 to 0.68250, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6698 - accuracy: 0.6085 - val_loss: 0.6825 - val_accuracy: 0.5833\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6197 - accuracy: 0.6605\n",
      "Epoch 6: val_loss improved from 0.68250 to 0.68228, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6197 - accuracy: 0.6605 - val_loss: 0.6823 - val_accuracy: 0.5833\n",
      "Epoch 7/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6130 - accuracy: 0.6685\n",
      "Epoch 7: val_loss did not improve from 0.68228\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6169 - accuracy: 0.6618 - val_loss: 0.6823 - val_accuracy: 0.5833\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5545 - accuracy: 0.7177\n",
      "Epoch 8: val_loss improved from 0.68228 to 0.68188, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5545 - accuracy: 0.7177 - val_loss: 0.6819 - val_accuracy: 0.5833\n",
      "Epoch 9/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5322 - accuracy: 0.7147\n",
      "Epoch 9: val_loss did not improve from 0.68188\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5344 - accuracy: 0.7150 - val_loss: 0.6834 - val_accuracy: 0.5833\n",
      "Epoch 10/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5278 - accuracy: 0.7228\n",
      "Epoch 10: val_loss did not improve from 0.68188\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5263 - accuracy: 0.7244 - val_loss: 0.6847 - val_accuracy: 0.5833\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5239 - accuracy: 0.7550\n",
      "Epoch 11: val_loss did not improve from 0.68188\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5239 - accuracy: 0.7550 - val_loss: 0.6855 - val_accuracy: 0.5833\n",
      "Epoch 12/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4817 - accuracy: 0.7745\n",
      "Epoch 12: val_loss did not improve from 0.68188\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4878 - accuracy: 0.7710 - val_loss: 0.6865 - val_accuracy: 0.5741\n",
      "Epoch 13/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5127 - accuracy: 0.7323\n",
      "Epoch 13: val_loss did not improve from 0.68188\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5132 - accuracy: 0.7324 - val_loss: 0.6866 - val_accuracy: 0.5648\n",
      "Epoch 14/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4421 - accuracy: 0.7928\n",
      "Epoch 14: val_loss did not improve from 0.68188\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4554 - accuracy: 0.7816 - val_loss: 0.6861 - val_accuracy: 0.5648\n",
      "Epoch 15/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4452 - accuracy: 0.7926\n",
      "Epoch 15: val_loss did not improve from 0.68188\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4474 - accuracy: 0.7923 - val_loss: 0.6865 - val_accuracy: 0.5741\n",
      "Epoch 16/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4448 - accuracy: 0.7961\n",
      "Epoch 16: val_loss did not improve from 0.68188\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4392 - accuracy: 0.7989 - val_loss: 0.6893 - val_accuracy: 0.5833\n",
      "Epoch 17/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4030 - accuracy: 0.8257\n",
      "Epoch 17: val_loss did not improve from 0.68188\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4095 - accuracy: 0.8216 - val_loss: 0.6936 - val_accuracy: 0.5741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3898 - accuracy: 0.8273\n",
      "Epoch 18: val_loss did not improve from 0.68188\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3882 - accuracy: 0.8309 - val_loss: 0.6997 - val_accuracy: 0.5648\n",
      "Epoch 19/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4070 - accuracy: 0.8239\n",
      "Epoch 19: val_loss did not improve from 0.68188\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4074 - accuracy: 0.8256 - val_loss: 0.7019 - val_accuracy: 0.5370\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3745 - accuracy: 0.8469\n",
      "Epoch 20: val_loss did not improve from 0.68188\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3745 - accuracy: 0.8469 - val_loss: 0.7061 - val_accuracy: 0.5278\n",
      "Epoch 21/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3640 - accuracy: 0.8478\n",
      "Epoch 21: val_loss did not improve from 0.68188\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3638 - accuracy: 0.8482 - val_loss: 0.7122 - val_accuracy: 0.5278\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3464 - accuracy: 0.8482\n",
      "Epoch 22: val_loss did not improve from 0.68188\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3464 - accuracy: 0.8482 - val_loss: 0.7163 - val_accuracy: 0.5093\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3274 - accuracy: 0.8762Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.68188\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3274 - accuracy: 0.8762 - val_loss: 0.7169 - val_accuracy: 0.5000\n",
      "Epoch 23: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5072\n",
      "Test AUC for Layer 3: 0.4664\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4393\n",
      "Average Test AUC across all layers: 0.5048\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_180 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_135 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_90 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_181 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_136 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_91 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_182 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_137 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_183 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.9255 - accuracy: 0.4668\n",
      "Epoch 1: val_loss improved from inf to 0.69150, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 24ms/step - loss: 0.9352 - accuracy: 0.4655 - val_loss: 0.6915 - val_accuracy: 0.6135\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8002 - accuracy: 0.5430\n",
      "Epoch 2: val_loss improved from 0.69150 to 0.69083, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7978 - accuracy: 0.5460 - val_loss: 0.6908 - val_accuracy: 0.5890\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7414 - accuracy: 0.6113\n",
      "Epoch 3: val_loss did not improve from 0.69083\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7468 - accuracy: 0.6073 - val_loss: 0.6909 - val_accuracy: 0.6074\n",
      "Epoch 4/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.7519 - accuracy: 0.5990\n",
      "Epoch 4: val_loss improved from 0.69083 to 0.69074, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7409 - accuracy: 0.6034 - val_loss: 0.6907 - val_accuracy: 0.5951\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6814 - accuracy: 0.6328\n",
      "Epoch 5: val_loss did not improve from 0.69074\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6768 - accuracy: 0.6341 - val_loss: 0.6908 - val_accuracy: 0.5644\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6293 - accuracy: 0.6590\n",
      "Epoch 6: val_loss improved from 0.69074 to 0.68949, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6293 - accuracy: 0.6590 - val_loss: 0.6895 - val_accuracy: 0.6135\n",
      "Epoch 7/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6928 - accuracy: 0.6380\n",
      "Epoch 7: val_loss improved from 0.68949 to 0.68877, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.6668 - accuracy: 0.6456 - val_loss: 0.6888 - val_accuracy: 0.6135\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5831 - accuracy: 0.6973\n",
      "Epoch 8: val_loss improved from 0.68877 to 0.68800, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.5831 - accuracy: 0.6973 - val_loss: 0.6880 - val_accuracy: 0.5951\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5724 - accuracy: 0.7070\n",
      "Epoch 9: val_loss improved from 0.68800 to 0.68788, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5722 - accuracy: 0.7069 - val_loss: 0.6879 - val_accuracy: 0.5706\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5489 - accuracy: 0.7188\n",
      "Epoch 10: val_loss improved from 0.68788 to 0.68605, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.5470 - accuracy: 0.7203 - val_loss: 0.6860 - val_accuracy: 0.5828\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5009 - accuracy: 0.7676\n",
      "Epoch 11: val_loss improved from 0.68605 to 0.68463, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_1.weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 15ms/step - loss: 0.5036 - accuracy: 0.7625 - val_loss: 0.6846 - val_accuracy: 0.6012\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4923 - accuracy: 0.7695\n",
      "Epoch 12: val_loss improved from 0.68463 to 0.68367, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4905 - accuracy: 0.7682 - val_loss: 0.6837 - val_accuracy: 0.6074\n",
      "Epoch 13/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4731 - accuracy: 0.7839\n",
      "Epoch 13: val_loss did not improve from 0.68367\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4658 - accuracy: 0.7797 - val_loss: 0.6846 - val_accuracy: 0.5767\n",
      "Epoch 14/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4592 - accuracy: 0.7682\n",
      "Epoch 14: val_loss did not improve from 0.68367\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4551 - accuracy: 0.7835 - val_loss: 0.6852 - val_accuracy: 0.5521\n",
      "Epoch 15/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4970 - accuracy: 0.7578\n",
      "Epoch 15: val_loss did not improve from 0.68367\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4811 - accuracy: 0.7625 - val_loss: 0.6860 - val_accuracy: 0.5521\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4212 - accuracy: 0.8149\n",
      "Epoch 16: val_loss did not improve from 0.68367\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4413 - accuracy: 0.8027 - val_loss: 0.6870 - val_accuracy: 0.5460\n",
      "Epoch 17/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3737 - accuracy: 0.8558\n",
      "Epoch 17: val_loss did not improve from 0.68367\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3729 - accuracy: 0.8544 - val_loss: 0.6860 - val_accuracy: 0.5460\n",
      "Epoch 18/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3967 - accuracy: 0.8293\n",
      "Epoch 18: val_loss did not improve from 0.68367\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3978 - accuracy: 0.8314 - val_loss: 0.6860 - val_accuracy: 0.5583\n",
      "Epoch 19/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3916 - accuracy: 0.8389\n",
      "Epoch 19: val_loss did not improve from 0.68367\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3928 - accuracy: 0.8372 - val_loss: 0.6873 - val_accuracy: 0.5644\n",
      "Epoch 20/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3623 - accuracy: 0.8389\n",
      "Epoch 20: val_loss did not improve from 0.68367\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3754 - accuracy: 0.8257 - val_loss: 0.6880 - val_accuracy: 0.5644\n",
      "Epoch 21/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3533 - accuracy: 0.8582\n",
      "Epoch 21: val_loss did not improve from 0.68367\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3490 - accuracy: 0.8678 - val_loss: 0.6896 - val_accuracy: 0.5276\n",
      "Epoch 22/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3321 - accuracy: 0.8802\n",
      "Epoch 22: val_loss did not improve from 0.68367\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3428 - accuracy: 0.8755 - val_loss: 0.6932 - val_accuracy: 0.5337\n",
      "Epoch 23/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3571 - accuracy: 0.8438\n",
      "Epoch 23: val_loss did not improve from 0.68367\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3527 - accuracy: 0.8506 - val_loss: 0.6979 - val_accuracy: 0.5337\n",
      "Epoch 24/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3281 - accuracy: 0.8854\n",
      "Epoch 24: val_loss did not improve from 0.68367\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3330 - accuracy: 0.8755 - val_loss: 0.7009 - val_accuracy: 0.5153\n",
      "Epoch 25/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2968 - accuracy: 0.9014\n",
      "Epoch 25: val_loss did not improve from 0.68367\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2963 - accuracy: 0.8946 - val_loss: 0.7063 - val_accuracy: 0.5153\n",
      "Epoch 26/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3055 - accuracy: 0.8822\n",
      "Epoch 26: val_loss did not improve from 0.68367\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2974 - accuracy: 0.8927 - val_loss: 0.7139 - val_accuracy: 0.5092\n",
      "Epoch 27/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2890 - accuracy: 0.8966Restoring model weights from the end of the best epoch: 12.\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.68367\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2883 - accuracy: 0.9004 - val_loss: 0.7269 - val_accuracy: 0.5153\n",
      "Epoch 27: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.7727\n",
      "Test AUC for Layer 1: 0.3801\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_184 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_138 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_92 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_185 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_139 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_93 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_186 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_140 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_187 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.9135 - accuracy: 0.4792\n",
      "Epoch 1: val_loss improved from inf to 0.80384, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 22ms/step - loss: 0.9077 - accuracy: 0.4832 - val_loss: 0.8038 - val_accuracy: 0.1061\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8312 - accuracy: 0.5164\n",
      "Epoch 2: val_loss did not improve from 0.80384\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.8253 - accuracy: 0.5212 - val_loss: 0.9079 - val_accuracy: 0.1061\n",
      "Epoch 3/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.7602 - accuracy: 0.5543\n",
      "Epoch 3: val_loss did not improve from 0.80384\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7557 - accuracy: 0.5591 - val_loss: 0.9975 - val_accuracy: 0.1061\n",
      "Epoch 4/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.7107 - accuracy: 0.5954\n",
      "Epoch 4: val_loss did not improve from 0.80384\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.7067 - accuracy: 0.6000 - val_loss: 1.0779 - val_accuracy: 0.1061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6587 - accuracy: 0.6316\n",
      "Epoch 5: val_loss did not improve from 0.80384\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6628 - accuracy: 0.6234 - val_loss: 1.1372 - val_accuracy: 0.1061\n",
      "Epoch 6/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6377 - accuracy: 0.6382\n",
      "Epoch 6: val_loss did not improve from 0.80384\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6333 - accuracy: 0.6409 - val_loss: 1.1900 - val_accuracy: 0.1061\n",
      "Epoch 7/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6082 - accuracy: 0.6760\n",
      "Epoch 7: val_loss did not improve from 0.80384\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6112 - accuracy: 0.6759 - val_loss: 1.2226 - val_accuracy: 0.1061\n",
      "Epoch 8/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5853 - accuracy: 0.6944\n",
      "Epoch 8: val_loss did not improve from 0.80384\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5856 - accuracy: 0.6934 - val_loss: 1.2529 - val_accuracy: 0.1061\n",
      "Epoch 9/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5374 - accuracy: 0.7537\n",
      "Epoch 9: val_loss did not improve from 0.80384\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5346 - accuracy: 0.7577 - val_loss: 1.2752 - val_accuracy: 0.1061\n",
      "Epoch 10/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5382 - accuracy: 0.7089\n",
      "Epoch 10: val_loss did not improve from 0.80384\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5423 - accuracy: 0.7080 - val_loss: 1.2788 - val_accuracy: 0.1061\n",
      "Epoch 11/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5099 - accuracy: 0.7316\n",
      "Epoch 11: val_loss did not improve from 0.80384\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5114 - accuracy: 0.7372 - val_loss: 1.2741 - val_accuracy: 0.1061\n",
      "Epoch 12/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4987 - accuracy: 0.7615\n",
      "Epoch 12: val_loss did not improve from 0.80384\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4953 - accuracy: 0.7620 - val_loss: 1.2707 - val_accuracy: 0.1061\n",
      "Epoch 13/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4816 - accuracy: 0.7714\n",
      "Epoch 13: val_loss did not improve from 0.80384\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4887 - accuracy: 0.7620 - val_loss: 1.2855 - val_accuracy: 0.1061\n",
      "Epoch 14/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4455 - accuracy: 0.8090\n",
      "Epoch 14: val_loss did not improve from 0.80384\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4459 - accuracy: 0.8073 - val_loss: 1.2848 - val_accuracy: 0.1061\n",
      "Epoch 15/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4390 - accuracy: 0.8010\n",
      "Epoch 15: val_loss did not improve from 0.80384\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4360 - accuracy: 0.8000 - val_loss: 1.2755 - val_accuracy: 0.1061\n",
      "Epoch 16/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4074 - accuracy: 0.8180Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.80384\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4223 - accuracy: 0.8073 - val_loss: 1.2490 - val_accuracy: 0.1061\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.2685\n",
      "Test AUC for Layer 2: 0.4644\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_188 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_141 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_94 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_189 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_142 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_95 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_190 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_143 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_191 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.9126 - accuracy: 0.4836\n",
      "Epoch 1: val_loss improved from inf to 0.68595, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 21ms/step - loss: 0.9062 - accuracy: 0.4900 - val_loss: 0.6860 - val_accuracy: 0.7222\n",
      "Epoch 2/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8049 - accuracy: 0.5744\n",
      "Epoch 2: val_loss improved from 0.68595 to 0.67328, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.8046 - accuracy: 0.5699 - val_loss: 0.6733 - val_accuracy: 0.7315\n",
      "Epoch 3/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.7773 - accuracy: 0.5856\n",
      "Epoch 3: val_loss improved from 0.67328 to 0.66377, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7740 - accuracy: 0.5872 - val_loss: 0.6638 - val_accuracy: 0.7315\n",
      "Epoch 4/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.7424 - accuracy: 0.6033\n",
      "Epoch 4: val_loss improved from 0.66377 to 0.65061, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7543 - accuracy: 0.5979 - val_loss: 0.6506 - val_accuracy: 0.7315\n",
      "Epoch 5/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6443 - accuracy: 0.6619\n",
      "Epoch 5: val_loss improved from 0.65061 to 0.64169, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6518 - accuracy: 0.6618 - val_loss: 0.6417 - val_accuracy: 0.7315\n",
      "Epoch 6/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6374 - accuracy: 0.6662\n",
      "Epoch 6: val_loss improved from 0.64169 to 0.62807, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.6309 - accuracy: 0.6711 - val_loss: 0.6281 - val_accuracy: 0.7315\n",
      "Epoch 7/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6484 - accuracy: 0.6454\n",
      "Epoch 7: val_loss improved from 0.62807 to 0.61595, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6519 - accuracy: 0.6431 - val_loss: 0.6160 - val_accuracy: 0.7315\n",
      "Epoch 8/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.5556 - accuracy: 0.7287\n",
      "Epoch 8: val_loss improved from 0.61595 to 0.60772, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.5596 - accuracy: 0.7244 - val_loss: 0.6077 - val_accuracy: 0.7315\n",
      "Epoch 9/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.5736 - accuracy: 0.7045\n",
      "Epoch 9: val_loss improved from 0.60772 to 0.60137, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.5824 - accuracy: 0.6977 - val_loss: 0.6014 - val_accuracy: 0.7315\n",
      "Epoch 10/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5644 - accuracy: 0.7052\n",
      "Epoch 10: val_loss improved from 0.60137 to 0.59865, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.5598 - accuracy: 0.7097 - val_loss: 0.5986 - val_accuracy: 0.7315\n",
      "Epoch 11/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5049 - accuracy: 0.7337\n",
      "Epoch 11: val_loss improved from 0.59865 to 0.59475, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.5100 - accuracy: 0.7324 - val_loss: 0.5948 - val_accuracy: 0.7315\n",
      "Epoch 12/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4988 - accuracy: 0.7543\n",
      "Epoch 12: val_loss improved from 0.59475 to 0.58922, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.5096 - accuracy: 0.7470 - val_loss: 0.5892 - val_accuracy: 0.7315\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4622 - accuracy: 0.7830\n",
      "Epoch 13: val_loss improved from 0.58922 to 0.58697, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.4622 - accuracy: 0.7830 - val_loss: 0.5870 - val_accuracy: 0.7315\n",
      "Epoch 14/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4548 - accuracy: 0.7898\n",
      "Epoch 14: val_loss improved from 0.58697 to 0.58486, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.4602 - accuracy: 0.7883 - val_loss: 0.5849 - val_accuracy: 0.7315\n",
      "Epoch 15/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4286 - accuracy: 0.7962\n",
      "Epoch 15: val_loss did not improve from 0.58486\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4280 - accuracy: 0.7976 - val_loss: 0.5858 - val_accuracy: 0.7315\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4201 - accuracy: 0.8242\n",
      "Epoch 16: val_loss did not improve from 0.58486\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4201 - accuracy: 0.8242 - val_loss: 0.5853 - val_accuracy: 0.7222\n",
      "Epoch 17/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3978 - accuracy: 0.8125\n",
      "Epoch 17: val_loss improved from 0.58486 to 0.58290, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.3979 - accuracy: 0.8189 - val_loss: 0.5829 - val_accuracy: 0.7222\n",
      "Epoch 18/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3895 - accuracy: 0.8247\n",
      "Epoch 18: val_loss improved from 0.58290 to 0.58155, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.3880 - accuracy: 0.8256 - val_loss: 0.5815 - val_accuracy: 0.7130\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3641 - accuracy: 0.8442\n",
      "Epoch 19: val_loss did not improve from 0.58155\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3641 - accuracy: 0.8442 - val_loss: 0.5855 - val_accuracy: 0.7037\n",
      "Epoch 20/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3395 - accuracy: 0.8569\n",
      "Epoch 20: val_loss did not improve from 0.58155\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3522 - accuracy: 0.8455 - val_loss: 0.5852 - val_accuracy: 0.7037\n",
      "Epoch 21/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3788 - accuracy: 0.8302\n",
      "Epoch 21: val_loss improved from 0.58155 to 0.58151, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.3761 - accuracy: 0.8322 - val_loss: 0.5815 - val_accuracy: 0.7037\n",
      "Epoch 22/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3537 - accuracy: 0.8261\n",
      "Epoch 22: val_loss improved from 0.58151 to 0.57935, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.3520 - accuracy: 0.8282 - val_loss: 0.5794 - val_accuracy: 0.6944\n",
      "Epoch 23/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3319 - accuracy: 0.8587\n",
      "Epoch 23: val_loss did not improve from 0.57935\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3303 - accuracy: 0.8602 - val_loss: 0.5825 - val_accuracy: 0.6944\n",
      "Epoch 24/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3133 - accuracy: 0.8791\n",
      "Epoch 24: val_loss did not improve from 0.57935\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.3136 - accuracy: 0.8788 - val_loss: 0.5825 - val_accuracy: 0.6852\n",
      "Epoch 25/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3023 - accuracy: 0.8818\n",
      "Epoch 25: val_loss did not improve from 0.57935\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3055 - accuracy: 0.8775 - val_loss: 0.5856 - val_accuracy: 0.6667\n",
      "Epoch 26/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.2831 - accuracy: 0.8967\n",
      "Epoch 26: val_loss did not improve from 0.57935\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2868 - accuracy: 0.8921 - val_loss: 0.5864 - val_accuracy: 0.6759\n",
      "Epoch 27/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2615 - accuracy: 0.9128\n",
      "Epoch 27: val_loss did not improve from 0.57935\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2675 - accuracy: 0.9041 - val_loss: 0.5969 - val_accuracy: 0.7037\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2664 - accuracy: 0.9015\n",
      "Epoch 28: val_loss did not improve from 0.57935\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2664 - accuracy: 0.9015 - val_loss: 0.5812 - val_accuracy: 0.7037\n",
      "Epoch 29/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.2668 - accuracy: 0.9008\n",
      "Epoch 29: val_loss improved from 0.57935 to 0.57782, saving model to OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.2670 - accuracy: 0.9001 - val_loss: 0.5778 - val_accuracy: 0.6852\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2389 - accuracy: 0.9055\n",
      "Epoch 30: val_loss did not improve from 0.57782\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2389 - accuracy: 0.9055 - val_loss: 0.5883 - val_accuracy: 0.7222\n",
      "Epoch 31/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2623 - accuracy: 0.9095\n",
      "Epoch 31: val_loss did not improve from 0.57782\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2623 - accuracy: 0.9095 - val_loss: 0.5977 - val_accuracy: 0.7037\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2140 - accuracy: 0.9348\n",
      "Epoch 32: val_loss did not improve from 0.57782\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2140 - accuracy: 0.9348 - val_loss: 0.6112 - val_accuracy: 0.6944\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2301 - accuracy: 0.9293\n",
      "Epoch 33: val_loss did not improve from 0.57782\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2363 - accuracy: 0.9268 - val_loss: 0.6129 - val_accuracy: 0.6759\n",
      "Epoch 34/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2117 - accuracy: 0.9293\n",
      "Epoch 34: val_loss did not improve from 0.57782\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2089 - accuracy: 0.9294 - val_loss: 0.6374 - val_accuracy: 0.6944\n",
      "Epoch 35/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2015 - accuracy: 0.9361\n",
      "Epoch 35: val_loss did not improve from 0.57782\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2015 - accuracy: 0.9361 - val_loss: 0.6540 - val_accuracy: 0.7037\n",
      "Epoch 36/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1980 - accuracy: 0.9401\n",
      "Epoch 36: val_loss did not improve from 0.57782\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1980 - accuracy: 0.9401 - val_loss: 0.6674 - val_accuracy: 0.6944\n",
      "Epoch 37/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.1950 - accuracy: 0.9342\n",
      "Epoch 37: val_loss did not improve from 0.57782\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1902 - accuracy: 0.9374 - val_loss: 0.6489 - val_accuracy: 0.6944\n",
      "Epoch 38/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2040 - accuracy: 0.9375\n",
      "Epoch 38: val_loss did not improve from 0.57782\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1957 - accuracy: 0.9387 - val_loss: 0.6455 - val_accuracy: 0.6852\n",
      "Epoch 39/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.1806 - accuracy: 0.9539\n",
      "Epoch 39: val_loss did not improve from 0.57782\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1861 - accuracy: 0.9481 - val_loss: 0.6548 - val_accuracy: 0.6667\n",
      "Epoch 40/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1946 - accuracy: 0.9414\n",
      "Epoch 40: val_loss did not improve from 0.57782\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1946 - accuracy: 0.9414 - val_loss: 0.6587 - val_accuracy: 0.6667\n",
      "Epoch 41/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.1768 - accuracy: 0.9389\n",
      "Epoch 41: val_loss did not improve from 0.57782\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1786 - accuracy: 0.9387 - val_loss: 0.6650 - val_accuracy: 0.6574\n",
      "Epoch 42/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9387\n",
      "Epoch 42: val_loss did not improve from 0.57782\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1874 - accuracy: 0.9387 - val_loss: 0.6632 - val_accuracy: 0.6667\n",
      "Epoch 43/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.1491 - accuracy: 0.9507\n",
      "Epoch 43: val_loss did not improve from 0.57782\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1610 - accuracy: 0.9441 - val_loss: 0.6498 - val_accuracy: 0.6852\n",
      "Epoch 44/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.1629 - accuracy: 0.9538Restoring model weights from the end of the best epoch: 29.\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.57782\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1618 - accuracy: 0.9547 - val_loss: 0.6465 - val_accuracy: 0.6759\n",
      "Epoch 44: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5362\n",
      "Test AUC for Layer 3: 0.4773\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5258\n",
      "Average Test AUC across all layers: 0.4406\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS MPC)\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Title + S_label\n",
      "Average Accuracy: 0.4645\n",
      "Average AUC: 0.5216\n",
      "  Layer 1 - Accuracy: 0.4697, AUC: 0.5798\n",
      "  Layer 2 - Accuracy: 0.4167, AUC: 0.5263\n",
      "  Layer 3 - Accuracy: 0.5072, AUC: 0.4588\n",
      "\n",
      "Combination: MLP with Title + L_label\n",
      "Average Accuracy: 0.2988\n",
      "Average AUC: 0.5424\n",
      "  Layer 1 - Accuracy: 0.1061, AUC: 0.6441\n",
      "  Layer 2 - Accuracy: 0.2685, AUC: 0.5260\n",
      "  Layer 3 - Accuracy: 0.5217, AUC: 0.4571\n",
      "\n",
      "Combination: MLP with Full text + S_label\n",
      "Average Accuracy: 0.4393\n",
      "Average AUC: 0.5048\n",
      "  Layer 1 - Accuracy: 0.3939, AUC: 0.5538\n",
      "  Layer 2 - Accuracy: 0.4167, AUC: 0.4942\n",
      "  Layer 3 - Accuracy: 0.5072, AUC: 0.4664\n",
      "\n",
      "Combination: MLP with Full text + L_label\n",
      "Average Accuracy: 0.5258\n",
      "Average AUC: 0.4406\n",
      "  Layer 1 - Accuracy: 0.7727, AUC: 0.3801\n",
      "  Layer 2 - Accuracy: 0.2685, AUC: 0.4644\n",
      "  Layer 3 - Accuracy: 0.5362, AUC: 0.4773\n",
      "MLP summary comparison visualization saved as: OpenAI_MLP/visualizations_summary/MPC\\mlp_performance_comparison.png\n",
      "MLP layer performance visualization saved as: OpenAI_MLP/visualizations_summary/MPC\\mlp_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'OpenAI_MLP' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "from xgboost import callback\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, model_type, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class ClimateNewsOpenAIPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'OpenAI_MLP/visualizations_mlp/MPC'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs('OpenAI_MLP/visualizations_summary/MPC', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert embeddings to numpy arrays\n",
    "        self.data['Title_embedding'] = self.data['Title_embedding_vector'].apply(parse_embedding)\n",
    "        self.data['Fulltext_embedding'] = self.data['Full_text_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_title_embedding = self.data['Title_embedding'].iloc[0]\n",
    "        sample_fulltext_embedding = self.data['Fulltext_embedding'].iloc[0]\n",
    "        \n",
    "        print(f\"Sample Title embedding shape: {sample_title_embedding.shape}\")\n",
    "        print(f\"Sample Fulltext embedding shape: {sample_fulltext_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2019-2021), validation (2021-2021), test (2022-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-05-31'),\n",
    "            'val_start': pd.Timestamp('2021-06-01'),\n",
    "            'val_end': pd.Timestamp('2021-12-31'),\n",
    "            'test_start': pd.Timestamp('2022-01-01'),\n",
    "            'test_end': pd.Timestamp('2022-05-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2019-2021), validation (2022-2022), test (2022-2022)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-12-31'),\n",
    "            'val_start': pd.Timestamp('2022-01-01'),\n",
    "            'val_end': pd.Timestamp('2022-05-31'),\n",
    "            'test_start': pd.Timestamp('2022-06-01'),\n",
    "            'test_end': pd.Timestamp('2022-12-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2019-2022), validation (2022-2022), test (2023-2023)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2022-05-31'),\n",
    "            'val_start': pd.Timestamp('2022-06-01'),\n",
    "            'val_end': pd.Timestamp('2022-12-31'),\n",
    "            'test_start': pd.Timestamp('2023-01-01'),\n",
    "            'test_end': pd.Timestamp('2023-05-31')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, text_col, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            text_col: The column containing the embeddings ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data[text_col].values)\n",
    "        X_val = np.stack(val_data[text_col].values)\n",
    "        X_test = np.stack(test_data[text_col].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, model_type, text_col, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            model_type: Model type (MLP)\n",
    "            text_col: Text column used\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_{model_type.lower()}_{display_text.replace(' ', '_')}_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, text_col, label_col, model_type):\n",
    "        \"\"\"\n",
    "        Train and evaluate a model for a specific text column and label column.\n",
    "        \n",
    "        Args:\n",
    "            text_col: The embedding column to use ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "            model_type: The model type to use ('MLP')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        combination_key = f\"{model_type}|{display_text}|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training {model_type} model for {display_text} and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        visualization_dir = self.mlp_viz_dir if model_type == 'MLP' else self.xgb_viz_dir\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, text_col, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            if model_type == 'MLP':\n",
    "                # MLP model training\n",
    "                model = self.create_mlp_model((1536,))\n",
    "                print(f\"Created MLP model for {display_text} ({label_col})\")\n",
    "                model.summary()\n",
    "                \n",
    "                # Setup callbacks\n",
    "                plot_callback = PlotLearningCallback(model_type, display_text, label_col, i+1, visualization_dir)\n",
    "                early_stopping = EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=15,\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                model_checkpoint = ModelCheckpoint(\n",
    "                    filepath=f\"{visualization_dir}/best_{model_type.lower()}_{display_text.lower().replace(' ', '_')}_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                    monitor='val_loss',\n",
    "                    save_weights_only=True,\n",
    "                    save_best_only=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Train model\n",
    "                print(f\"Training MLP model...\")\n",
    "                batch_size = 32  # Larger batch size for embeddings\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                y_pred_proba = model.predict(X_test)\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                # Create final learning curve visualization\n",
    "                self.plot_final_learning_curves(history, model_type, text_col, label_col, i+1, visualization_dir)\n",
    "                \n",
    "                # Store training history\n",
    "                training_history = history.history\n",
    "                \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'model_type': model_type,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': training_history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for all combinations of text inputs, label columns, and model types.\"\"\"\n",
    "        # Define all combinations\n",
    "        embedding_cols = ['Title_embedding', 'Fulltext_embedding']\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        model_types = ['MLP']\n",
    "        \n",
    "        # Run analysis for each combination\n",
    "        for model_type in model_types:\n",
    "            for embedding_col in embedding_cols:\n",
    "                for label_col in label_cols:\n",
    "                    self.train_and_evaluate_model(embedding_col, label_col, model_type)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS MPC)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Model': model_type,\n",
    "                'Text': text_col,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing all model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Split by model type\n",
    "        mlp_data = df[df['Model'] == 'MLP']\n",
    "        \n",
    "        # 1. Performance comparison for MLP\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot accuracy and AUC bars for MLP\n",
    "        x = np.arange(len(mlp_data))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, mlp_data['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, mlp_data['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('MLP Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with OpenAI Embeddings')\n",
    "        labels = [f\"{row['Text']} + {row['Label']}\" for _, row in mlp_data.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(mlp_data['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(mlp_data['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(mlp_data['Avg Accuracy'].max(), mlp_data['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP/visualizations_summary/MPC', \"mlp_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        mlp_layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Model': model_type,\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                if model_type == 'MLP':\n",
    "                    mlp_layer_data.append(layer_info)\n",
    "        \n",
    "        # Create visualizations for each model type\n",
    "        self._create_model_layer_visualization(mlp_layer_data, 'MLP')\n",
    "    \n",
    "    def _create_model_layer_visualization(self, layer_data, model_type):\n",
    "        \"\"\"Create layer-specific visualizations for a given model type.\"\"\"\n",
    "        if not layer_data:\n",
    "            print(f\"No layer data available for {model_type}\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} Accuracy by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} AUC by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP/visualizations_summary/MPC', f\"{model_type.lower()}_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"{model_type} layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['OpenAI_MLP/visualizations_mlp/MPC',\n",
    "                      'OpenAI_MLP/visualizations_summary/MPC']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/US_news/SP500_semantic/us_news_semantics_MPC_completed_openai.csv'\n",
    "    \n",
    "    # Initialize the predictor with OpenAI embeddings\n",
    "    predictor = ClimateNewsOpenAIPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'OpenAI_MLP' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing merged OpenAI embedding vectors from string format...\n",
      "Sample Merged embedding shape: (1536,)\n",
      "Loaded 929 climate change news articles spanning from 02/01/2019 to 07/05/2023\n",
      "Class distribution for short-term prediction: {1: 482, 0: 447}\n",
      "Class distribution for long-term prediction: {1: 555, 0: 374}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_192 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_144 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_96 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_193 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_145 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_97 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_194 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_146 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_195 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.8394 - accuracy: 0.5104\n",
      "Epoch 1: val_loss improved from inf to 0.69365, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8384 - accuracy: 0.5096 - val_loss: 0.6936 - val_accuracy: 0.4908\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7970 - accuracy: 0.5332\n",
      "Epoch 2: val_loss did not improve from 0.69365\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7952 - accuracy: 0.5345 - val_loss: 0.6947 - val_accuracy: 0.4908\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7733 - accuracy: 0.5625\n",
      "Epoch 3: val_loss did not improve from 0.69365\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7692 - accuracy: 0.5670 - val_loss: 0.6964 - val_accuracy: 0.4908\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7156 - accuracy: 0.5957\n",
      "Epoch 4: val_loss did not improve from 0.69365\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7117 - accuracy: 0.5996 - val_loss: 0.6971 - val_accuracy: 0.4908\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6581 - accuracy: 0.6465\n",
      "Epoch 5: val_loss did not improve from 0.69365\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6573 - accuracy: 0.6494 - val_loss: 0.6985 - val_accuracy: 0.4908\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6707 - accuracy: 0.6370\n",
      "Epoch 6: val_loss did not improve from 0.69365\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6617 - accuracy: 0.6456 - val_loss: 0.6992 - val_accuracy: 0.4908\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6056 - accuracy: 0.7012\n",
      "Epoch 7: val_loss did not improve from 0.69365\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6056 - accuracy: 0.7011 - val_loss: 0.6994 - val_accuracy: 0.4908\n",
      "Epoch 8/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5622 - accuracy: 0.7135\n",
      "Epoch 8: val_loss did not improve from 0.69365\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5662 - accuracy: 0.7146 - val_loss: 0.6994 - val_accuracy: 0.4908\n",
      "Epoch 9/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5621 - accuracy: 0.7005\n",
      "Epoch 9: val_loss did not improve from 0.69365\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5738 - accuracy: 0.6877 - val_loss: 0.6989 - val_accuracy: 0.4908\n",
      "Epoch 10/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5239 - accuracy: 0.7370\n",
      "Epoch 10: val_loss did not improve from 0.69365\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5307 - accuracy: 0.7241 - val_loss: 0.6988 - val_accuracy: 0.4908\n",
      "Epoch 11/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4877 - accuracy: 0.7692\n",
      "Epoch 11: val_loss did not improve from 0.69365\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5024 - accuracy: 0.7605 - val_loss: 0.6985 - val_accuracy: 0.4908\n",
      "Epoch 12/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5504 - accuracy: 0.7188\n",
      "Epoch 12: val_loss did not improve from 0.69365\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5451 - accuracy: 0.7241 - val_loss: 0.6971 - val_accuracy: 0.4908\n",
      "Epoch 13/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4388 - accuracy: 0.8203\n",
      "Epoch 13: val_loss did not improve from 0.69365\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4550 - accuracy: 0.8027 - val_loss: 0.6973 - val_accuracy: 0.4908\n",
      "Epoch 14/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4296 - accuracy: 0.8385\n",
      "Epoch 14: val_loss did not improve from 0.69365\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4449 - accuracy: 0.8180 - val_loss: 0.6959 - val_accuracy: 0.4969\n",
      "Epoch 15/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4500 - accuracy: 0.7764\n",
      "Epoch 15: val_loss did not improve from 0.69365\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4461 - accuracy: 0.7854 - val_loss: 0.6952 - val_accuracy: 0.4969\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4249 - accuracy: 0.8103Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69365\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4249 - accuracy: 0.8103 - val_loss: 0.6946 - val_accuracy: 0.5031\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.6061\n",
      "Test AUC for Layer 1: 0.4606\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_196 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_147 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_98 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_197 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_148 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_99 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_198 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_149 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_199 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8311 - accuracy: 0.5193\n",
      "Epoch 1: val_loss improved from inf to 0.67634, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 21ms/step - loss: 0.8346 - accuracy: 0.5197 - val_loss: 0.6763 - val_accuracy: 0.6061\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8010 - accuracy: 0.5223\n",
      "Epoch 2: val_loss improved from 0.67634 to 0.67147, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.7991 - accuracy: 0.5270 - val_loss: 0.6715 - val_accuracy: 0.6061\n",
      "Epoch 3/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.7189 - accuracy: 0.5735\n",
      "Epoch 3: val_loss did not improve from 0.67147\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7235 - accuracy: 0.5723 - val_loss: 0.6722 - val_accuracy: 0.6061\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7384 - accuracy: 0.5723\n",
      "Epoch 4: val_loss did not improve from 0.67147\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7384 - accuracy: 0.5723 - val_loss: 0.6753 - val_accuracy: 0.6061\n",
      "Epoch 5/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6625 - accuracy: 0.6354\n",
      "Epoch 5: val_loss did not improve from 0.67147\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6753 - accuracy: 0.6263 - val_loss: 0.6785 - val_accuracy: 0.6061\n",
      "Epoch 6/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6509 - accuracy: 0.6562\n",
      "Epoch 6: val_loss did not improve from 0.67147\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6461 - accuracy: 0.6584 - val_loss: 0.6809 - val_accuracy: 0.6061\n",
      "Epoch 7/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5896 - accuracy: 0.6875\n",
      "Epoch 7: val_loss did not improve from 0.67147\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5987 - accuracy: 0.6818 - val_loss: 0.6822 - val_accuracy: 0.6061\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6177 - accuracy: 0.6701\n",
      "Epoch 8: val_loss did not improve from 0.67147\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6177 - accuracy: 0.6701 - val_loss: 0.6808 - val_accuracy: 0.6061\n",
      "Epoch 9/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5746 - accuracy: 0.6949\n",
      "Epoch 9: val_loss did not improve from 0.67147\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5666 - accuracy: 0.6993 - val_loss: 0.6786 - val_accuracy: 0.6061\n",
      "Epoch 10/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5124 - accuracy: 0.7500\n",
      "Epoch 10: val_loss did not improve from 0.67147\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5257 - accuracy: 0.7401 - val_loss: 0.6808 - val_accuracy: 0.6061\n",
      "Epoch 11/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5396 - accuracy: 0.7220\n",
      "Epoch 11: val_loss did not improve from 0.67147\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5343 - accuracy: 0.7241 - val_loss: 0.6804 - val_accuracy: 0.6061\n",
      "Epoch 12/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5092 - accuracy: 0.7674\n",
      "Epoch 12: val_loss did not improve from 0.67147\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5167 - accuracy: 0.7489 - val_loss: 0.6790 - val_accuracy: 0.6061\n",
      "Epoch 13/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4940 - accuracy: 0.7482\n",
      "Epoch 13: val_loss did not improve from 0.67147\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4897 - accuracy: 0.7474 - val_loss: 0.6798 - val_accuracy: 0.6061\n",
      "Epoch 14/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4718 - accuracy: 0.7849\n",
      "Epoch 14: val_loss did not improve from 0.67147\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4689 - accuracy: 0.7796 - val_loss: 0.6824 - val_accuracy: 0.6061\n",
      "Epoch 15/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4633 - accuracy: 0.7776\n",
      "Epoch 15: val_loss did not improve from 0.67147\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4732 - accuracy: 0.7664 - val_loss: 0.6855 - val_accuracy: 0.6061\n",
      "Epoch 16/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.4451 - accuracy: 0.7902\n",
      "Epoch 16: val_loss did not improve from 0.67147\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4432 - accuracy: 0.7927 - val_loss: 0.6875 - val_accuracy: 0.6061\n",
      "Epoch 17/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4237 - accuracy: 0.7986Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.67147\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4221 - accuracy: 0.8044 - val_loss: 0.6919 - val_accuracy: 0.6061\n",
      "Epoch 17: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5833\n",
      "Test AUC for Layer 2: 0.5563\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_200 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_150 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_100 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_201 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_151 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_101 (Dropout)       (None, 256)               0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " dense_202 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_152 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_203 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8718 - accuracy: 0.4970\n",
      "Epoch 1: val_loss improved from inf to 0.71687, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.8874 - accuracy: 0.4967 - val_loss: 0.7169 - val_accuracy: 0.4167\n",
      "Epoch 2/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8157 - accuracy: 0.5476\n",
      "Epoch 2: val_loss did not improve from 0.71687\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.8075 - accuracy: 0.5513 - val_loss: 0.7401 - val_accuracy: 0.4167\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.7565 - accuracy: 0.5899\n",
      "Epoch 3: val_loss did not improve from 0.71687\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.7565 - accuracy: 0.5899 - val_loss: 0.7589 - val_accuracy: 0.4167\n",
      "Epoch 4/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.7619 - accuracy: 0.5822\n",
      "Epoch 4: val_loss did not improve from 0.71687\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.7642 - accuracy: 0.5779 - val_loss: 0.7727 - val_accuracy: 0.4167\n",
      "Epoch 5/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6690 - accuracy: 0.6398\n",
      "Epoch 5: val_loss did not improve from 0.71687\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6699 - accuracy: 0.6258 - val_loss: 0.7817 - val_accuracy: 0.4167\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6242 - accuracy: 0.6618\n",
      "Epoch 6: val_loss did not improve from 0.71687\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6242 - accuracy: 0.6618 - val_loss: 0.7907 - val_accuracy: 0.4167\n",
      "Epoch 7/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5894 - accuracy: 0.6875\n",
      "Epoch 7: val_loss did not improve from 0.71687\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5844 - accuracy: 0.6911 - val_loss: 0.7965 - val_accuracy: 0.4167\n",
      "Epoch 8/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6080 - accuracy: 0.6739\n",
      "Epoch 8: val_loss did not improve from 0.71687\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6056 - accuracy: 0.6751 - val_loss: 0.7983 - val_accuracy: 0.4167\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5939 - accuracy: 0.6818\n",
      "Epoch 9: val_loss did not improve from 0.71687\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5939 - accuracy: 0.6818 - val_loss: 0.7919 - val_accuracy: 0.4167\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5625 - accuracy: 0.7190\n",
      "Epoch 10: val_loss did not improve from 0.71687\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5625 - accuracy: 0.7190 - val_loss: 0.7875 - val_accuracy: 0.4167\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5604 - accuracy: 0.7190\n",
      "Epoch 11: val_loss did not improve from 0.71687\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5604 - accuracy: 0.7190 - val_loss: 0.7846 - val_accuracy: 0.4167\n",
      "Epoch 12/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5202 - accuracy: 0.7554\n",
      "Epoch 12: val_loss did not improve from 0.71687\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5183 - accuracy: 0.7577 - val_loss: 0.7836 - val_accuracy: 0.4167\n",
      "Epoch 13/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5107 - accuracy: 0.7319\n",
      "Epoch 13: val_loss did not improve from 0.71687\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5221 - accuracy: 0.7270 - val_loss: 0.7817 - val_accuracy: 0.4167\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5118 - accuracy: 0.7443\n",
      "Epoch 14: val_loss did not improve from 0.71687\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5118 - accuracy: 0.7443 - val_loss: 0.7868 - val_accuracy: 0.4167\n",
      "Epoch 15/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.4665 - accuracy: 0.7619\n",
      "Epoch 15: val_loss did not improve from 0.71687\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4617 - accuracy: 0.7643 - val_loss: 0.7907 - val_accuracy: 0.4167\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4660 - accuracy: 0.7696Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.71687\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4660 - accuracy: 0.7696 - val_loss: 0.7992 - val_accuracy: 0.4259\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4928\n",
      "Test AUC for Layer 3: 0.5294\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5607\n",
      "Average Test AUC across all layers: 0.5154\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_204 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_153 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_102 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_205 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_154 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_103 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_206 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_155 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_207 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8353 - accuracy: 0.5527\n",
      "Epoch 1: val_loss improved from inf to 0.70022, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_1.weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8377 - accuracy: 0.5536 - val_loss: 0.7002 - val_accuracy: 0.3926\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8110 - accuracy: 0.5645\n",
      "Epoch 2: val_loss did not improve from 0.70022\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.8052 - accuracy: 0.5690 - val_loss: 0.7062 - val_accuracy: 0.3926\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7124 - accuracy: 0.6513\n",
      "Epoch 3: val_loss did not improve from 0.70022\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7124 - accuracy: 0.6513 - val_loss: 0.7109 - val_accuracy: 0.3926\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7211 - accuracy: 0.5920\n",
      "Epoch 4: val_loss did not improve from 0.70022\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7211 - accuracy: 0.5920 - val_loss: 0.7161 - val_accuracy: 0.3926\n",
      "Epoch 5/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6342 - accuracy: 0.6827\n",
      "Epoch 5: val_loss did not improve from 0.70022\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6380 - accuracy: 0.6724 - val_loss: 0.7224 - val_accuracy: 0.3926\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6401 - accuracy: 0.6442\n",
      "Epoch 6: val_loss did not improve from 0.70022\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6203 - accuracy: 0.6571 - val_loss: 0.7273 - val_accuracy: 0.3926\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5879 - accuracy: 0.6895\n",
      "Epoch 7: val_loss did not improve from 0.70022\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5903 - accuracy: 0.6897 - val_loss: 0.7327 - val_accuracy: 0.3926\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5272 - accuracy: 0.7441\n",
      "Epoch 8: val_loss did not improve from 0.70022\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5243 - accuracy: 0.7471 - val_loss: 0.7332 - val_accuracy: 0.3926\n",
      "Epoch 9/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4697 - accuracy: 0.7708\n",
      "Epoch 9: val_loss did not improve from 0.70022\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4733 - accuracy: 0.7739 - val_loss: 0.7365 - val_accuracy: 0.3926\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4794 - accuracy: 0.7586\n",
      "Epoch 10: val_loss did not improve from 0.70022\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4794 - accuracy: 0.7586 - val_loss: 0.7402 - val_accuracy: 0.3926\n",
      "Epoch 11/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4436 - accuracy: 0.7995\n",
      "Epoch 11: val_loss did not improve from 0.70022\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4471 - accuracy: 0.7912 - val_loss: 0.7431 - val_accuracy: 0.3926\n",
      "Epoch 12/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4104 - accuracy: 0.8099\n",
      "Epoch 12: val_loss did not improve from 0.70022\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4142 - accuracy: 0.8180 - val_loss: 0.7427 - val_accuracy: 0.3865\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4756 - accuracy: 0.7620\n",
      "Epoch 13: val_loss did not improve from 0.70022\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4595 - accuracy: 0.7720 - val_loss: 0.7480 - val_accuracy: 0.3865\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4011 - accuracy: 0.8262\n",
      "Epoch 14: val_loss did not improve from 0.70022\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4015 - accuracy: 0.8257 - val_loss: 0.7511 - val_accuracy: 0.3804\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3828 - accuracy: 0.8320\n",
      "Epoch 15: val_loss did not improve from 0.70022\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3892 - accuracy: 0.8238 - val_loss: 0.7548 - val_accuracy: 0.3742\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3780 - accuracy: 0.8365Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70022\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3770 - accuracy: 0.8333 - val_loss: 0.7560 - val_accuracy: 0.3681\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.1061\n",
      "Test AUC for Layer 1: 0.3220\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_208 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_156 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_104 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_209 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_157 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_105 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_210 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_158 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_211 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.9116 - accuracy: 0.4911\n",
      "Epoch 1: val_loss improved from inf to 0.66920, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 21ms/step - loss: 0.9197 - accuracy: 0.4861 - val_loss: 0.6692 - val_accuracy: 0.8939\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7999 - accuracy: 0.5670\n",
      "Epoch 2: val_loss improved from 0.66920 to 0.65003, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.8056 - accuracy: 0.5635 - val_loss: 0.6500 - val_accuracy: 0.8939\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7238 - accuracy: 0.6219\n",
      "Epoch 3: val_loss improved from 0.65003 to 0.62894, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.7238 - accuracy: 0.6219 - val_loss: 0.6289 - val_accuracy: 0.8939\n",
      "Epoch 4/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7262 - accuracy: 0.6071\n",
      "Epoch 4: val_loss improved from 0.62894 to 0.61180, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.7275 - accuracy: 0.6058 - val_loss: 0.6118 - val_accuracy: 0.8939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6707 - accuracy: 0.6380\n",
      "Epoch 5: val_loss improved from 0.61180 to 0.59420, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6707 - accuracy: 0.6380 - val_loss: 0.5942 - val_accuracy: 0.8939\n",
      "Epoch 6/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.6310 - accuracy: 0.6673\n",
      "Epoch 6: val_loss improved from 0.59420 to 0.58512, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6195 - accuracy: 0.6686 - val_loss: 0.5851 - val_accuracy: 0.8939\n",
      "Epoch 7/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.6191 - accuracy: 0.6728\n",
      "Epoch 7: val_loss improved from 0.58512 to 0.57156, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6064 - accuracy: 0.6861 - val_loss: 0.5716 - val_accuracy: 0.8939\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5731 - accuracy: 0.7066\n",
      "Epoch 8: val_loss improved from 0.57156 to 0.56173, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5731 - accuracy: 0.7066 - val_loss: 0.5617 - val_accuracy: 0.8939\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5612 - accuracy: 0.7139\n",
      "Epoch 9: val_loss improved from 0.56173 to 0.55085, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5612 - accuracy: 0.7139 - val_loss: 0.5509 - val_accuracy: 0.8939\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5087 - accuracy: 0.7693\n",
      "Epoch 10: val_loss improved from 0.55085 to 0.54421, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5087 - accuracy: 0.7693 - val_loss: 0.5442 - val_accuracy: 0.8939\n",
      "Epoch 11/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5157 - accuracy: 0.7321\n",
      "Epoch 11: val_loss improved from 0.54421 to 0.54250, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5175 - accuracy: 0.7314 - val_loss: 0.5425 - val_accuracy: 0.8939\n",
      "Epoch 12/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4693 - accuracy: 0.8015\n",
      "Epoch 12: val_loss did not improve from 0.54250\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4621 - accuracy: 0.8029 - val_loss: 0.5472 - val_accuracy: 0.8939\n",
      "Epoch 13/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4667 - accuracy: 0.7726\n",
      "Epoch 13: val_loss improved from 0.54250 to 0.54220, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.4786 - accuracy: 0.7635 - val_loss: 0.5422 - val_accuracy: 0.8939\n",
      "Epoch 14/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4577 - accuracy: 0.7829\n",
      "Epoch 14: val_loss improved from 0.54220 to 0.53702, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.4576 - accuracy: 0.7839 - val_loss: 0.5370 - val_accuracy: 0.8939\n",
      "Epoch 15/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4396 - accuracy: 0.7986\n",
      "Epoch 15: val_loss improved from 0.53702 to 0.53098, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.4310 - accuracy: 0.8000 - val_loss: 0.5310 - val_accuracy: 0.8788\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4237 - accuracy: 0.8088\n",
      "Epoch 16: val_loss did not improve from 0.53098\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4237 - accuracy: 0.8088 - val_loss: 0.5370 - val_accuracy: 0.8636\n",
      "Epoch 17/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4096 - accuracy: 0.8108\n",
      "Epoch 17: val_loss did not improve from 0.53098\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4078 - accuracy: 0.8161 - val_loss: 0.5327 - val_accuracy: 0.8636\n",
      "Epoch 18/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3962 - accuracy: 0.8229\n",
      "Epoch 18: val_loss improved from 0.53098 to 0.52837, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3849 - accuracy: 0.8321 - val_loss: 0.5284 - val_accuracy: 0.8636\n",
      "Epoch 19/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.3956 - accuracy: 0.8229\n",
      "Epoch 19: val_loss improved from 0.52837 to 0.52641, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.3973 - accuracy: 0.8219 - val_loss: 0.5264 - val_accuracy: 0.8636\n",
      "Epoch 20/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.3825 - accuracy: 0.8571\n",
      "Epoch 20: val_loss did not improve from 0.52641\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3847 - accuracy: 0.8526 - val_loss: 0.5358 - val_accuracy: 0.8636\n",
      "Epoch 21/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3671 - accuracy: 0.8355\n",
      "Epoch 21: val_loss did not improve from 0.52641\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3621 - accuracy: 0.8409 - val_loss: 0.5364 - val_accuracy: 0.8636\n",
      "Epoch 22/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.3467 - accuracy: 0.8364\n",
      "Epoch 22: val_loss improved from 0.52641 to 0.52487, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3533 - accuracy: 0.8380 - val_loss: 0.5249 - val_accuracy: 0.8636\n",
      "Epoch 23/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.3303 - accuracy: 0.8690\n",
      "Epoch 23: val_loss did not improve from 0.52487\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3333 - accuracy: 0.8672 - val_loss: 0.5507 - val_accuracy: 0.8333\n",
      "Epoch 24/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.3323 - accuracy: 0.8661\n",
      "Epoch 24: val_loss did not improve from 0.52487\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3319 - accuracy: 0.8657 - val_loss: 0.5601 - val_accuracy: 0.8030\n",
      "Epoch 25/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2977 - accuracy: 0.8898\n",
      "Epoch 25: val_loss did not improve from 0.52487\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.2960 - accuracy: 0.8905 - val_loss: 0.5767 - val_accuracy: 0.7576\n",
      "Epoch 26/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.2810 - accuracy: 0.8837\n",
      "Epoch 26: val_loss did not improve from 0.52487\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2806 - accuracy: 0.8891 - val_loss: 0.5910 - val_accuracy: 0.7576\n",
      "Epoch 27/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.2858 - accuracy: 0.8941\n",
      "Epoch 27: val_loss did not improve from 0.52487\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2968 - accuracy: 0.8847 - val_loss: 0.5970 - val_accuracy: 0.7424\n",
      "Epoch 28/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.2655 - accuracy: 0.9149\n",
      "Epoch 28: val_loss did not improve from 0.52487\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2883 - accuracy: 0.9007 - val_loss: 0.6153 - val_accuracy: 0.7273\n",
      "Epoch 29/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.2694 - accuracy: 0.9062\n",
      "Epoch 29: val_loss did not improve from 0.52487\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2727 - accuracy: 0.9080 - val_loss: 0.6356 - val_accuracy: 0.7121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.2811 - accuracy: 0.8915\n",
      "Epoch 30: val_loss did not improve from 0.52487\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2807 - accuracy: 0.8920 - val_loss: 0.6671 - val_accuracy: 0.7273\n",
      "Epoch 31/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.2501 - accuracy: 0.9196\n",
      "Epoch 31: val_loss did not improve from 0.52487\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.2491 - accuracy: 0.9197 - val_loss: 0.7146 - val_accuracy: 0.6818\n",
      "Epoch 32/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.2426 - accuracy: 0.9283\n",
      "Epoch 32: val_loss did not improve from 0.52487\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.2403 - accuracy: 0.9285 - val_loss: 0.7359 - val_accuracy: 0.6818\n",
      "Epoch 33/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.2231 - accuracy: 0.9167\n",
      "Epoch 33: val_loss did not improve from 0.52487\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2327 - accuracy: 0.9124 - val_loss: 0.7329 - val_accuracy: 0.6970\n",
      "Epoch 34/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.2279 - accuracy: 0.9288\n",
      "Epoch 34: val_loss did not improve from 0.52487\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2362 - accuracy: 0.9197 - val_loss: 0.7325 - val_accuracy: 0.7121\n",
      "Epoch 35/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.2206 - accuracy: 0.9210\n",
      "Epoch 35: val_loss did not improve from 0.52487\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2220 - accuracy: 0.9255 - val_loss: 0.7803 - val_accuracy: 0.6515\n",
      "Epoch 36/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.1894 - accuracy: 0.9522\n",
      "Epoch 36: val_loss did not improve from 0.52487\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.1971 - accuracy: 0.9460 - val_loss: 0.8028 - val_accuracy: 0.6364\n",
      "Epoch 37/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.1824 - accuracy: 0.9497Restoring model weights from the end of the best epoch: 22.\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.52487\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.1895 - accuracy: 0.9489 - val_loss: 0.7787 - val_accuracy: 0.6515\n",
      "Epoch 37: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.7037\n",
      "Test AUC for Layer 2: 0.6124\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_212 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_159 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_106 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_213 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_160 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_107 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_214 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_161 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_215 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.9065 - accuracy: 0.4643\n",
      "Epoch 1: val_loss improved from inf to 0.70554, saving model to Merged_OpenAI_MLP/visualizations_mlp/MPC\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.9204 - accuracy: 0.4594 - val_loss: 0.7055 - val_accuracy: 0.2685\n",
      "Epoch 2/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.7424 - accuracy: 0.5640\n",
      "Epoch 2: val_loss did not improve from 0.70554\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.7575 - accuracy: 0.5526 - val_loss: 0.7202 - val_accuracy: 0.2685\n",
      "Epoch 3/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.7440 - accuracy: 0.5848\n",
      "Epoch 3: val_loss did not improve from 0.70554\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.7416 - accuracy: 0.5792 - val_loss: 0.7289 - val_accuracy: 0.2685\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.7144 - accuracy: 0.5872\n",
      "Epoch 4: val_loss did not improve from 0.70554\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.7144 - accuracy: 0.5872 - val_loss: 0.7417 - val_accuracy: 0.2685\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.7007 - accuracy: 0.6085\n",
      "Epoch 5: val_loss did not improve from 0.70554\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.7007 - accuracy: 0.6085 - val_loss: 0.7475 - val_accuracy: 0.2685\n",
      "Epoch 6/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6270 - accuracy: 0.6809\n",
      "Epoch 6: val_loss did not improve from 0.70554\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.6088 - accuracy: 0.6897 - val_loss: 0.7534 - val_accuracy: 0.2685\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6036 - accuracy: 0.6858\n",
      "Epoch 7: val_loss did not improve from 0.70554\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6036 - accuracy: 0.6858 - val_loss: 0.7569 - val_accuracy: 0.2685\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5696 - accuracy: 0.6937\n",
      "Epoch 8: val_loss did not improve from 0.70554\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5696 - accuracy: 0.6937 - val_loss: 0.7661 - val_accuracy: 0.2685\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5617 - accuracy: 0.7031\n",
      "Epoch 9: val_loss did not improve from 0.70554\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5617 - accuracy: 0.7031 - val_loss: 0.7715 - val_accuracy: 0.2685\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5520 - accuracy: 0.7190\n",
      "Epoch 10: val_loss did not improve from 0.70554\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5520 - accuracy: 0.7190 - val_loss: 0.7672 - val_accuracy: 0.2685\n",
      "Epoch 11/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.4997 - accuracy: 0.7589\n",
      "Epoch 11: val_loss did not improve from 0.70554\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5058 - accuracy: 0.7577 - val_loss: 0.7612 - val_accuracy: 0.2778\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5050 - accuracy: 0.7550\n",
      "Epoch 12: val_loss did not improve from 0.70554\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.5050 - accuracy: 0.7550 - val_loss: 0.7615 - val_accuracy: 0.2870\n",
      "Epoch 13/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4812 - accuracy: 0.7697\n",
      "Epoch 13: val_loss did not improve from 0.70554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4857 - accuracy: 0.7710 - val_loss: 0.7506 - val_accuracy: 0.3056\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4306 - accuracy: 0.8069\n",
      "Epoch 14: val_loss did not improve from 0.70554\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4306 - accuracy: 0.8069 - val_loss: 0.7354 - val_accuracy: 0.3889\n",
      "Epoch 15/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4627 - accuracy: 0.7763\n",
      "Epoch 15: val_loss did not improve from 0.70554\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4647 - accuracy: 0.7763 - val_loss: 0.7296 - val_accuracy: 0.4167\n",
      "Epoch 16/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4259 - accuracy: 0.8030Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70554\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4278 - accuracy: 0.8029 - val_loss: 0.7268 - val_accuracy: 0.4537\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4783\n",
      "Test AUC for Layer 3: 0.4882\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4293\n",
      "Average Test AUC across all layers: 0.4742\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS (MPC)\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Merged + S_label\n",
      "Average Accuracy: 0.5607\n",
      "Average AUC: 0.5154\n",
      "  Layer 1 - Accuracy: 0.6061, AUC: 0.4606\n",
      "  Layer 2 - Accuracy: 0.5833, AUC: 0.5563\n",
      "  Layer 3 - Accuracy: 0.4928, AUC: 0.5294\n",
      "\n",
      "Combination: MLP with Merged + L_label\n",
      "Average Accuracy: 0.4293\n",
      "Average AUC: 0.4742\n",
      "  Layer 1 - Accuracy: 0.1061, AUC: 0.3220\n",
      "  Layer 2 - Accuracy: 0.7037, AUC: 0.6124\n",
      "  Layer 3 - Accuracy: 0.4783, AUC: 0.4882\n",
      "MLP summary comparison visualization saved as: Merged_OpenAI_MLP/visualizations_summary/MPC\\mlp_merged_performance_comparison.png\n",
      "MLP layer performance visualization saved as: Merged_OpenAI_MLP/visualizations_summary/MPC\\mlp_merged_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class MergedEmbeddingMLPPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using merged OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with merged OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'Merged_OpenAI_MLP/visualizations_mlp/MPC'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs('Merged_OpenAI_MLP/visualizations_summary/MPC', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with merged OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing merged OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert merged embeddings to numpy arrays\n",
    "        self.data['Merged_embedding'] = self.data['Merged_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_merged_embedding = self.data['Merged_embedding'].iloc[0]\n",
    "        print(f\"Sample Merged embedding shape: {sample_merged_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2019-2021), validation (2021-2021), test (2022-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-05-31'),\n",
    "            'val_start': pd.Timestamp('2021-06-01'),\n",
    "            'val_end': pd.Timestamp('2021-12-31'),\n",
    "            'test_start': pd.Timestamp('2022-01-01'),\n",
    "            'test_end': pd.Timestamp('2022-05-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2019-2021), validation (2022-2022), test (2022-2022)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-12-31'),\n",
    "            'val_start': pd.Timestamp('2022-01-01'),\n",
    "            'val_end': pd.Timestamp('2022-05-31'),\n",
    "            'test_start': pd.Timestamp('2022-06-01'),\n",
    "            'test_end': pd.Timestamp('2022-12-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2019-2022), validation (2022-2022), test (2023-2023)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2022-05-31'),\n",
    "            'val_start': pd.Timestamp('2022-06-01'),\n",
    "            'val_end': pd.Timestamp('2022-12-31'),\n",
    "            'test_start': pd.Timestamp('2023-01-01'),\n",
    "            'test_end': pd.Timestamp('2023-05-31')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data['Merged_embedding'].values)\n",
    "        X_val = np.stack(val_data['Merged_embedding'].values)\n",
    "        X_test = np.stack(test_data['Merged_embedding'].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_mlp_merged_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, label_col):\n",
    "        \"\"\"\n",
    "        Train and evaluate an MLP model for merged embeddings and a specific label column.\n",
    "        \n",
    "        Args:\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        combination_key = f\"MLP|Merged|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training MLP model for Merged Embeddings and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            model = self.create_mlp_model((1536,))\n",
    "            print(f\"Created MLP model for Merged Embeddings ({label_col})\")\n",
    "            model.summary()\n",
    "            \n",
    "            # Setup callbacks\n",
    "            plot_callback = PlotLearningCallback('Merged', label_col, i+1, self.mlp_viz_dir)\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=15,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            model_checkpoint = ModelCheckpoint(\n",
    "                filepath=f\"{self.mlp_viz_dir}/best_mlp_merged_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                monitor='val_loss',\n",
    "                save_weights_only=True,\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            print(f\"Training MLP model...\")\n",
    "            batch_size = 32  # Larger batch size for embeddings\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred_proba = model.predict(X_test)\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "            \n",
    "            # Create final learning curve visualization\n",
    "            self.plot_final_learning_curves(history, label_col, i+1, self.mlp_viz_dir)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': history.history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for merged embeddings with both short-term and long-term labels.\"\"\"\n",
    "        # Define label columns\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        \n",
    "        # Run analysis for each label column\n",
    "        for label_col in label_cols:\n",
    "            self.train_and_evaluate_model(label_col)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS (MPC)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Performance comparison for MLP with merged embeddings\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Plot accuracy and AUC bars\n",
    "        x = np.arange(len(df))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, df['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, df['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with Merged OpenAI Embeddings')\n",
    "        labels = [f\"Merged + {row['Label']}\" for _, row in df.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(df['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(df['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(df['Avg Accuracy'].max(), df['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/MPC', \"mlp_merged_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                layer_data.append(layer_info)\n",
    "        \n",
    "        if not layer_data:\n",
    "            print(\"No layer data available\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title('MLP Accuracy by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title('MLP AUC by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/MPC', \"mlp_merged_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['Merged_OpenAI_MLP/visualizations_mlp/MPC', 'Merged_OpenAI_MLP/visualizations_summary/MPC']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/US_news/SP500_semantic/us_news_semantics_MPC_completed_openai.csv'\n",
    "    \n",
    "    # Initialize the predictor with merged OpenAI embeddings\n",
    "    predictor = MergedEmbeddingMLPPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing OpenAI embedding vectors from string format...\n",
      "Sample Title embedding shape: (1536,)\n",
      "Sample Fulltext embedding shape: (1536,)\n",
      "Loaded 929 climate change news articles spanning from 02/01/2019 to 07/05/2023\n",
      "Class distribution for short-term prediction: {1: 499, 0: 430}\n",
      "Class distribution for long-term prediction: {1: 521, 0: 408}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_216 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_162 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_108 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_217 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_163 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_109 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_218 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_164 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_219 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8853 - accuracy: 0.5156\n",
      "Epoch 1: val_loss improved from inf to 0.68834, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 26ms/step - loss: 0.8855 - accuracy: 0.5153 - val_loss: 0.6883 - val_accuracy: 0.5521\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8380 - accuracy: 0.5430\n",
      "Epoch 2: val_loss improved from 0.68834 to 0.68758, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.8428 - accuracy: 0.5421 - val_loss: 0.6876 - val_accuracy: 0.5521\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7346 - accuracy: 0.5723\n",
      "Epoch 3: val_loss did not improve from 0.68758\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7350 - accuracy: 0.5747 - val_loss: 0.6877 - val_accuracy: 0.5521\n",
      "Epoch 4/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6584 - accuracy: 0.6466\n",
      "Epoch 4: val_loss did not improve from 0.68758\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6750 - accuracy: 0.6284 - val_loss: 0.6880 - val_accuracy: 0.5521\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6134 - accuracy: 0.6743\n",
      "Epoch 5: val_loss did not improve from 0.68758\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6134 - accuracy: 0.6743 - val_loss: 0.6882 - val_accuracy: 0.5521\n",
      "Epoch 6/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6247 - accuracy: 0.6719\n",
      "Epoch 6: val_loss did not improve from 0.68758\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6208 - accuracy: 0.6782 - val_loss: 0.6883 - val_accuracy: 0.5521\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6046 - accuracy: 0.6762\n",
      "Epoch 7: val_loss did not improve from 0.68758\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6046 - accuracy: 0.6762 - val_loss: 0.6886 - val_accuracy: 0.5521\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5491 - accuracy: 0.7088\n",
      "Epoch 8: val_loss did not improve from 0.68758\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5491 - accuracy: 0.7088 - val_loss: 0.6891 - val_accuracy: 0.5521\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5558 - accuracy: 0.6995\n",
      "Epoch 9: val_loss did not improve from 0.68758\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5510 - accuracy: 0.7069 - val_loss: 0.6905 - val_accuracy: 0.5521\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4919 - accuracy: 0.7596\n",
      "Epoch 10: val_loss did not improve from 0.68758\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4937 - accuracy: 0.7586 - val_loss: 0.6922 - val_accuracy: 0.4908\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4677 - accuracy: 0.7931\n",
      "Epoch 11: val_loss did not improve from 0.68758\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4677 - accuracy: 0.7931 - val_loss: 0.6952 - val_accuracy: 0.5092\n",
      "Epoch 12/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4638 - accuracy: 0.7969\n",
      "Epoch 12: val_loss did not improve from 0.68758\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4537 - accuracy: 0.7969 - val_loss: 0.6982 - val_accuracy: 0.4969\n",
      "Epoch 13/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4339 - accuracy: 0.8359\n",
      "Epoch 13: val_loss did not improve from 0.68758\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4279 - accuracy: 0.8295 - val_loss: 0.7021 - val_accuracy: 0.4785\n",
      "Epoch 14/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4027 - accuracy: 0.8333\n",
      "Epoch 14: val_loss did not improve from 0.68758\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4048 - accuracy: 0.8314 - val_loss: 0.7068 - val_accuracy: 0.4847\n",
      "Epoch 15/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3781 - accuracy: 0.8534\n",
      "Epoch 15: val_loss did not improve from 0.68758\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3933 - accuracy: 0.8429 - val_loss: 0.7124 - val_accuracy: 0.4601\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3801 - accuracy: 0.8413\n",
      "Epoch 16: val_loss did not improve from 0.68758\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3728 - accuracy: 0.8506 - val_loss: 0.7165 - val_accuracy: 0.4479\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3497 - accuracy: 0.8659Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.68758\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3497 - accuracy: 0.8659 - val_loss: 0.7199 - val_accuracy: 0.4479\n",
      "Epoch 17: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.6515\n",
      "Test AUC for Layer 1: 0.3933\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_55\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_220 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_165 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_110 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_221 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_166 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_111 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_222 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_167 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_223 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.9541 - accuracy: 0.4717\n",
      "Epoch 1: val_loss improved from inf to 0.68565, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 21ms/step - loss: 0.9522 - accuracy: 0.4715 - val_loss: 0.6857 - val_accuracy: 0.6515\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8040 - accuracy: 0.5551\n",
      "Epoch 2: val_loss improved from 0.68565 to 0.68154, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.8023 - accuracy: 0.5562 - val_loss: 0.6815 - val_accuracy: 0.6515\n",
      "Epoch 3/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.7537 - accuracy: 0.6011\n",
      "Epoch 3: val_loss improved from 0.68154 to 0.67964, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.7644 - accuracy: 0.5956 - val_loss: 0.6796 - val_accuracy: 0.6515\n",
      "Epoch 4/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.7207 - accuracy: 0.6011\n",
      "Epoch 4: val_loss improved from 0.67964 to 0.67838, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6989 - accuracy: 0.6088 - val_loss: 0.6784 - val_accuracy: 0.6515\n",
      "Epoch 5/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.6693 - accuracy: 0.6562\n",
      "Epoch 5: val_loss improved from 0.67838 to 0.67597, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6700 - accuracy: 0.6453 - val_loss: 0.6760 - val_accuracy: 0.6515\n",
      "Epoch 6/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.6316 - accuracy: 0.6360\n",
      "Epoch 6: val_loss did not improve from 0.67597\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6219 - accuracy: 0.6511 - val_loss: 0.6762 - val_accuracy: 0.6515\n",
      "Epoch 7/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5904 - accuracy: 0.6967\n",
      "Epoch 7: val_loss improved from 0.67597 to 0.67587, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.5990 - accuracy: 0.6876 - val_loss: 0.6759 - val_accuracy: 0.6364\n",
      "Epoch 8/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5686 - accuracy: 0.7083\n",
      "Epoch 8: val_loss did not improve from 0.67587\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5659 - accuracy: 0.7124 - val_loss: 0.6759 - val_accuracy: 0.6212\n",
      "Epoch 9/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5319 - accuracy: 0.7292\n",
      "Epoch 9: val_loss improved from 0.67587 to 0.67581, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5324 - accuracy: 0.7285 - val_loss: 0.6758 - val_accuracy: 0.6212\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5300 - accuracy: 0.7460\n",
      "Epoch 10: val_loss did not improve from 0.67581\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5300 - accuracy: 0.7460 - val_loss: 0.6787 - val_accuracy: 0.6061\n",
      "Epoch 11/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.4654 - accuracy: 0.7679\n",
      "Epoch 11: val_loss did not improve from 0.67581\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4656 - accuracy: 0.7693 - val_loss: 0.6817 - val_accuracy: 0.5758\n",
      "Epoch 12/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4465 - accuracy: 0.7760\n",
      "Epoch 12: val_loss did not improve from 0.67581\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4404 - accuracy: 0.7898 - val_loss: 0.6799 - val_accuracy: 0.5606\n",
      "Epoch 13/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4157 - accuracy: 0.8125\n",
      "Epoch 13: val_loss did not improve from 0.67581\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4220 - accuracy: 0.8044 - val_loss: 0.6771 - val_accuracy: 0.5758\n",
      "Epoch 14/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4429 - accuracy: 0.7865\n",
      "Epoch 14: val_loss did not improve from 0.67581\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4399 - accuracy: 0.7869 - val_loss: 0.6768 - val_accuracy: 0.5606\n",
      "Epoch 15/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.3950 - accuracy: 0.8180\n",
      "Epoch 15: val_loss did not improve from 0.67581\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4000 - accuracy: 0.8146 - val_loss: 0.6800 - val_accuracy: 0.5909\n",
      "Epoch 16/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.3852 - accuracy: 0.8585\n",
      "Epoch 16: val_loss did not improve from 0.67581\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3826 - accuracy: 0.8584 - val_loss: 0.6817 - val_accuracy: 0.6061\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3927 - accuracy: 0.8307\n",
      "Epoch 17: val_loss did not improve from 0.67581\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3927 - accuracy: 0.8307 - val_loss: 0.6807 - val_accuracy: 0.6061\n",
      "Epoch 18/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3615 - accuracy: 0.8553\n",
      "Epoch 18: val_loss did not improve from 0.67581\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3506 - accuracy: 0.8628 - val_loss: 0.6800 - val_accuracy: 0.5758\n",
      "Epoch 19/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3231 - accuracy: 0.8750\n",
      "Epoch 19: val_loss improved from 0.67581 to 0.67482, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3269 - accuracy: 0.8730 - val_loss: 0.6748 - val_accuracy: 0.6061\n",
      "Epoch 20/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.3522 - accuracy: 0.8529\n",
      "Epoch 20: val_loss did not improve from 0.67482\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3438 - accuracy: 0.8540 - val_loss: 0.6768 - val_accuracy: 0.5909\n",
      "Epoch 21/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.3013 - accuracy: 0.8897\n",
      "Epoch 21: val_loss did not improve from 0.67482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3066 - accuracy: 0.8861 - val_loss: 0.6788 - val_accuracy: 0.5909\n",
      "Epoch 22/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.3301 - accuracy: 0.8658\n",
      "Epoch 22: val_loss did not improve from 0.67482\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3257 - accuracy: 0.8672 - val_loss: 0.6806 - val_accuracy: 0.5455\n",
      "Epoch 23/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.2897 - accuracy: 0.9007\n",
      "Epoch 23: val_loss did not improve from 0.67482\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2920 - accuracy: 0.8978 - val_loss: 0.6892 - val_accuracy: 0.5303\n",
      "Epoch 24/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.2832 - accuracy: 0.9080\n",
      "Epoch 24: val_loss did not improve from 0.67482\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2827 - accuracy: 0.9036 - val_loss: 0.6966 - val_accuracy: 0.5303\n",
      "Epoch 25/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.2675 - accuracy: 0.9080\n",
      "Epoch 25: val_loss did not improve from 0.67482\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2718 - accuracy: 0.9080 - val_loss: 0.7039 - val_accuracy: 0.5303\n",
      "Epoch 26/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.2364 - accuracy: 0.9271\n",
      "Epoch 26: val_loss did not improve from 0.67482\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2431 - accuracy: 0.9241 - val_loss: 0.7148 - val_accuracy: 0.5455\n",
      "Epoch 27/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2494 - accuracy: 0.9161\n",
      "Epoch 27: val_loss did not improve from 0.67482\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2587 - accuracy: 0.9139 - val_loss: 0.7452 - val_accuracy: 0.4848\n",
      "Epoch 28/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.2593 - accuracy: 0.9167\n",
      "Epoch 28: val_loss did not improve from 0.67482\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2597 - accuracy: 0.9153 - val_loss: 0.7600 - val_accuracy: 0.4848\n",
      "Epoch 29/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.2334 - accuracy: 0.9241\n",
      "Epoch 29: val_loss did not improve from 0.67482\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.2370 - accuracy: 0.9212 - val_loss: 0.7801 - val_accuracy: 0.4848\n",
      "Epoch 30/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.2030 - accuracy: 0.9283\n",
      "Epoch 30: val_loss did not improve from 0.67482\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.2032 - accuracy: 0.9328 - val_loss: 0.8025 - val_accuracy: 0.4848\n",
      "Epoch 31/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2111 - accuracy: 0.9293\n",
      "Epoch 31: val_loss did not improve from 0.67482\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2068 - accuracy: 0.9314 - val_loss: 0.8147 - val_accuracy: 0.4848\n",
      "Epoch 32/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.2166 - accuracy: 0.9276\n",
      "Epoch 32: val_loss did not improve from 0.67482\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2134 - accuracy: 0.9328 - val_loss: 0.8249 - val_accuracy: 0.5000\n",
      "Epoch 33/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.1703 - accuracy: 0.9494\n",
      "Epoch 33: val_loss did not improve from 0.67482\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.1739 - accuracy: 0.9460 - val_loss: 0.8400 - val_accuracy: 0.4848\n",
      "Epoch 34/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.1772 - accuracy: 0.9479Restoring model weights from the end of the best epoch: 19.\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.67482\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.1768 - accuracy: 0.9489 - val_loss: 0.8538 - val_accuracy: 0.5000\n",
      "Epoch 34: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5000\n",
      "Test AUC for Layer 2: 0.4243\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_224 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_168 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_112 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_225 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_169 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_113 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_226 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_170 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_227 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8941 - accuracy: 0.5015\n",
      "Epoch 1: val_loss improved from inf to 0.68542, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.8898 - accuracy: 0.5113 - val_loss: 0.6854 - val_accuracy: 0.6019\n",
      "Epoch 2/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.8389 - accuracy: 0.5284\n",
      "Epoch 2: val_loss improved from 0.68542 to 0.67783, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.8320 - accuracy: 0.5326 - val_loss: 0.6778 - val_accuracy: 0.6019\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.8019 - accuracy: 0.5486\n",
      "Epoch 3: val_loss improved from 0.67783 to 0.67499, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.8019 - accuracy: 0.5486 - val_loss: 0.6750 - val_accuracy: 0.6019\n",
      "Epoch 4/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.7053 - accuracy: 0.6080\n",
      "Epoch 4: val_loss improved from 0.67499 to 0.67472, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7164 - accuracy: 0.5979 - val_loss: 0.6747 - val_accuracy: 0.6019\n",
      "Epoch 5/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6647 - accuracy: 0.6293\n",
      "Epoch 5: val_loss did not improve from 0.67472\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.6579 - accuracy: 0.6352 - val_loss: 0.6769 - val_accuracy: 0.6019\n",
      "Epoch 6/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6477 - accuracy: 0.6378\n",
      "Epoch 6: val_loss did not improve from 0.67472\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6510 - accuracy: 0.6378 - val_loss: 0.6809 - val_accuracy: 0.6019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6282 - accuracy: 0.6780\n",
      "Epoch 7: val_loss did not improve from 0.67472\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6242 - accuracy: 0.6791 - val_loss: 0.6867 - val_accuracy: 0.6019\n",
      "Epoch 8/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.5898 - accuracy: 0.6960\n",
      "Epoch 8: val_loss did not improve from 0.67472\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5820 - accuracy: 0.7017 - val_loss: 0.6928 - val_accuracy: 0.6019\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5530 - accuracy: 0.7137\n",
      "Epoch 9: val_loss did not improve from 0.67472\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5530 - accuracy: 0.7137 - val_loss: 0.7001 - val_accuracy: 0.6019\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5688 - accuracy: 0.6951\n",
      "Epoch 10: val_loss did not improve from 0.67472\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5688 - accuracy: 0.6951 - val_loss: 0.7103 - val_accuracy: 0.6019\n",
      "Epoch 11/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4954 - accuracy: 0.7467\n",
      "Epoch 11: val_loss did not improve from 0.67472\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4981 - accuracy: 0.7510 - val_loss: 0.7191 - val_accuracy: 0.6019\n",
      "Epoch 12/100\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.4740 - accuracy: 0.7641\n",
      "Epoch 12: val_loss did not improve from 0.67472\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4760 - accuracy: 0.7670 - val_loss: 0.7279 - val_accuracy: 0.6019\n",
      "Epoch 13/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4756 - accuracy: 0.7549\n",
      "Epoch 13: val_loss did not improve from 0.67472\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4778 - accuracy: 0.7470 - val_loss: 0.7327 - val_accuracy: 0.6019\n",
      "Epoch 14/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4438 - accuracy: 0.7928\n",
      "Epoch 14: val_loss did not improve from 0.67472\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4314 - accuracy: 0.8043 - val_loss: 0.7363 - val_accuracy: 0.6019\n",
      "Epoch 15/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4387 - accuracy: 0.8092\n",
      "Epoch 15: val_loss did not improve from 0.67472\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4320 - accuracy: 0.8149 - val_loss: 0.7369 - val_accuracy: 0.6019\n",
      "Epoch 16/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4095 - accuracy: 0.8289\n",
      "Epoch 16: val_loss did not improve from 0.67472\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4091 - accuracy: 0.8242 - val_loss: 0.7422 - val_accuracy: 0.6019\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4001 - accuracy: 0.8282\n",
      "Epoch 17: val_loss did not improve from 0.67472\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4001 - accuracy: 0.8282 - val_loss: 0.7463 - val_accuracy: 0.6019\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3877 - accuracy: 0.8269\n",
      "Epoch 18: val_loss did not improve from 0.67472\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3877 - accuracy: 0.8269 - val_loss: 0.7490 - val_accuracy: 0.6019\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3752 - accuracy: 0.8495Restoring model weights from the end of the best epoch: 4.\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.67472\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3752 - accuracy: 0.8495 - val_loss: 0.7506 - val_accuracy: 0.5926\n",
      "Epoch 19: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4638\n",
      "Test AUC for Layer 3: 0.5414\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5384\n",
      "Average Test AUC across all layers: 0.4530\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_228 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_171 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_114 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_229 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_172 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_115 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_230 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_173 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_231 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.9561 - accuracy: 0.4609\n",
      "Epoch 1: val_loss improved from inf to 0.68659, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 26ms/step - loss: 0.9479 - accuracy: 0.4674 - val_loss: 0.6866 - val_accuracy: 0.6258\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7870 - accuracy: 0.5703\n",
      "Epoch 2: val_loss improved from 0.68659 to 0.67750, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.7934 - accuracy: 0.5670 - val_loss: 0.6775 - val_accuracy: 0.6258\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7570 - accuracy: 0.5762\n",
      "Epoch 3: val_loss improved from 0.67750 to 0.66989, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7515 - accuracy: 0.5805 - val_loss: 0.6699 - val_accuracy: 0.6258\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7037 - accuracy: 0.6055\n",
      "Epoch 4: val_loss improved from 0.66989 to 0.66485, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.7042 - accuracy: 0.6034 - val_loss: 0.6648 - val_accuracy: 0.6258\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6428 - accuracy: 0.6609\n",
      "Epoch 5: val_loss improved from 0.66485 to 0.66234, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.6428 - accuracy: 0.6609 - val_loss: 0.6623 - val_accuracy: 0.6258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6286 - accuracy: 0.6590\n",
      "Epoch 6: val_loss improved from 0.66234 to 0.66172, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.6286 - accuracy: 0.6590 - val_loss: 0.6617 - val_accuracy: 0.6258\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5739 - accuracy: 0.7129\n",
      "Epoch 7: val_loss did not improve from 0.66172\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5723 - accuracy: 0.7146 - val_loss: 0.6634 - val_accuracy: 0.6258\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5515 - accuracy: 0.7222\n",
      "Epoch 8: val_loss did not improve from 0.66172\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5515 - accuracy: 0.7222 - val_loss: 0.6671 - val_accuracy: 0.6258\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5261 - accuracy: 0.7452\n",
      "Epoch 9: val_loss did not improve from 0.66172\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5252 - accuracy: 0.7414 - val_loss: 0.6728 - val_accuracy: 0.6258\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5235 - accuracy: 0.7428\n",
      "Epoch 10: val_loss did not improve from 0.66172\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5133 - accuracy: 0.7529 - val_loss: 0.6797 - val_accuracy: 0.6258\n",
      "Epoch 11/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5010 - accuracy: 0.7260\n",
      "Epoch 11: val_loss did not improve from 0.66172\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4940 - accuracy: 0.7433 - val_loss: 0.6880 - val_accuracy: 0.6258\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4637 - accuracy: 0.7912\n",
      "Epoch 12: val_loss did not improve from 0.66172\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4637 - accuracy: 0.7912 - val_loss: 0.6964 - val_accuracy: 0.6258\n",
      "Epoch 13/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4137 - accuracy: 0.8281\n",
      "Epoch 13: val_loss did not improve from 0.66172\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4185 - accuracy: 0.8180 - val_loss: 0.7057 - val_accuracy: 0.6258\n",
      "Epoch 14/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4306 - accuracy: 0.7786\n",
      "Epoch 14: val_loss did not improve from 0.66172\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4249 - accuracy: 0.7778 - val_loss: 0.7167 - val_accuracy: 0.6258\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3944 - accuracy: 0.8242\n",
      "Epoch 15: val_loss did not improve from 0.66172\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3946 - accuracy: 0.8238 - val_loss: 0.7277 - val_accuracy: 0.6258\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4028 - accuracy: 0.8161\n",
      "Epoch 16: val_loss did not improve from 0.66172\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4028 - accuracy: 0.8161 - val_loss: 0.7386 - val_accuracy: 0.6258\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3406 - accuracy: 0.8621\n",
      "Epoch 17: val_loss did not improve from 0.66172\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3406 - accuracy: 0.8621 - val_loss: 0.7451 - val_accuracy: 0.6258\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3527 - accuracy: 0.8467\n",
      "Epoch 18: val_loss did not improve from 0.66172\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3527 - accuracy: 0.8467 - val_loss: 0.7540 - val_accuracy: 0.6258\n",
      "Epoch 19/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3435 - accuracy: 0.8534\n",
      "Epoch 19: val_loss did not improve from 0.66172\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3322 - accuracy: 0.8640 - val_loss: 0.7605 - val_accuracy: 0.6258\n",
      "Epoch 20/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3139 - accuracy: 0.8672\n",
      "Epoch 20: val_loss did not improve from 0.66172\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3125 - accuracy: 0.8678 - val_loss: 0.7655 - val_accuracy: 0.6258\n",
      "Epoch 21/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3120 - accuracy: 0.8918Restoring model weights from the end of the best epoch: 6.\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.66172\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3134 - accuracy: 0.8870 - val_loss: 0.7707 - val_accuracy: 0.6258\n",
      "Epoch 21: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.7879\n",
      "Test AUC for Layer 1: 0.5831\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_232 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_174 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_116 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_233 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_175 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_117 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_234 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_176 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_235 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8968 - accuracy: 0.5104\n",
      "Epoch 1: val_loss improved from inf to 0.74419, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 21ms/step - loss: 0.8906 - accuracy: 0.5139 - val_loss: 0.7442 - val_accuracy: 0.2121\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8240 - accuracy: 0.5825\n",
      "Epoch 2: val_loss did not improve from 0.74419\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.8240 - accuracy: 0.5825 - val_loss: 0.8058 - val_accuracy: 0.2121\n",
      "Epoch 3/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.7830 - accuracy: 0.5954\n",
      "Epoch 3: val_loss did not improve from 0.74419\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7739 - accuracy: 0.5985 - val_loss: 0.8612 - val_accuracy: 0.2121\n",
      "Epoch 4/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6765 - accuracy: 0.6365\n",
      "Epoch 4: val_loss did not improve from 0.74419\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6856 - accuracy: 0.6321 - val_loss: 0.9074 - val_accuracy: 0.2121\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6498 - accuracy: 0.6760\n",
      "Epoch 5: val_loss did not improve from 0.74419\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6533 - accuracy: 0.6686 - val_loss: 0.9606 - val_accuracy: 0.2121\n",
      "Epoch 6/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5995 - accuracy: 0.6941\n",
      "Epoch 6: val_loss did not improve from 0.74419\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5927 - accuracy: 0.7051 - val_loss: 1.0028 - val_accuracy: 0.2121\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6141 - accuracy: 0.6774\n",
      "Epoch 7: val_loss did not improve from 0.74419\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6141 - accuracy: 0.6774 - val_loss: 1.0379 - val_accuracy: 0.2121\n",
      "Epoch 8/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5491 - accuracy: 0.7286\n",
      "Epoch 8: val_loss did not improve from 0.74419\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5387 - accuracy: 0.7314 - val_loss: 1.0771 - val_accuracy: 0.2121\n",
      "Epoch 9/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5112 - accuracy: 0.7500\n",
      "Epoch 9: val_loss did not improve from 0.74419\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5366 - accuracy: 0.7328 - val_loss: 1.1173 - val_accuracy: 0.2121\n",
      "Epoch 10/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4647 - accuracy: 0.7796\n",
      "Epoch 10: val_loss did not improve from 0.74419\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4598 - accuracy: 0.7839 - val_loss: 1.1493 - val_accuracy: 0.2121\n",
      "Epoch 11/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4477 - accuracy: 0.7763\n",
      "Epoch 11: val_loss did not improve from 0.74419\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4430 - accuracy: 0.7825 - val_loss: 1.1779 - val_accuracy: 0.2121\n",
      "Epoch 12/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4216 - accuracy: 0.8212\n",
      "Epoch 12: val_loss did not improve from 0.74419\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4206 - accuracy: 0.8190 - val_loss: 1.2078 - val_accuracy: 0.2121\n",
      "Epoch 13/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4199 - accuracy: 0.8229\n",
      "Epoch 13: val_loss did not improve from 0.74419\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4213 - accuracy: 0.8190 - val_loss: 1.2152 - val_accuracy: 0.2121\n",
      "Epoch 14/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.3789 - accuracy: 0.8290\n",
      "Epoch 14: val_loss did not improve from 0.74419\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3675 - accuracy: 0.8409 - val_loss: 1.2325 - val_accuracy: 0.2121\n",
      "Epoch 15/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3909 - accuracy: 0.8191\n",
      "Epoch 15: val_loss did not improve from 0.74419\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3928 - accuracy: 0.8175 - val_loss: 1.2360 - val_accuracy: 0.2121\n",
      "Epoch 16/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3514 - accuracy: 0.8385Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.74419\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3454 - accuracy: 0.8453 - val_loss: 1.2399 - val_accuracy: 0.2121\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.3704\n",
      "Test AUC for Layer 2: 0.5055\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_236 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_177 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_118 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_237 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_178 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_119 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_238 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_179 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_239 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8480 - accuracy: 0.5179\n",
      "Epoch 1: val_loss improved from inf to 0.67784, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 21ms/step - loss: 0.8513 - accuracy: 0.5033 - val_loss: 0.6778 - val_accuracy: 0.6296\n",
      "Epoch 2/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.7843 - accuracy: 0.5739\n",
      "Epoch 2: val_loss improved from 0.67784 to 0.67230, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7785 - accuracy: 0.5752 - val_loss: 0.6723 - val_accuracy: 0.6296\n",
      "Epoch 3/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.7193 - accuracy: 0.6037\n",
      "Epoch 3: val_loss improved from 0.67230 to 0.66902, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.7173 - accuracy: 0.6045 - val_loss: 0.6690 - val_accuracy: 0.6296\n",
      "Epoch 4/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6611 - accuracy: 0.6440\n",
      "Epoch 4: val_loss improved from 0.66902 to 0.66862, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6679 - accuracy: 0.6431 - val_loss: 0.6686 - val_accuracy: 0.6296\n",
      "Epoch 5/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6487 - accuracy: 0.6491\n",
      "Epoch 5: val_loss improved from 0.66862 to 0.66850, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6641 - accuracy: 0.6352 - val_loss: 0.6685 - val_accuracy: 0.6296\n",
      "Epoch 6/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.6118 - accuracy: 0.6801\n",
      "Epoch 6: val_loss did not improve from 0.66850\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.6037 - accuracy: 0.6884 - val_loss: 0.6690 - val_accuracy: 0.6296\n",
      "Epoch 7/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5466 - accuracy: 0.7079\n",
      "Epoch 7: val_loss did not improve from 0.66850\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5456 - accuracy: 0.7097 - val_loss: 0.6705 - val_accuracy: 0.6296\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5367 - accuracy: 0.7368\n",
      "Epoch 8: val_loss did not improve from 0.66850\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5451 - accuracy: 0.7310 - val_loss: 0.6730 - val_accuracy: 0.6296\n",
      "Epoch 9/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5190 - accuracy: 0.7451\n",
      "Epoch 9: val_loss did not improve from 0.66850\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5217 - accuracy: 0.7403 - val_loss: 0.6766 - val_accuracy: 0.6019\n",
      "Epoch 10/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5054 - accuracy: 0.7514\n",
      "Epoch 10: val_loss did not improve from 0.66850\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5094 - accuracy: 0.7510 - val_loss: 0.6789 - val_accuracy: 0.5741\n",
      "Epoch 11/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4611 - accuracy: 0.7911\n",
      "Epoch 11: val_loss did not improve from 0.66850\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4622 - accuracy: 0.7909 - val_loss: 0.6819 - val_accuracy: 0.5648\n",
      "Epoch 12/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4678 - accuracy: 0.7747\n",
      "Epoch 12: val_loss did not improve from 0.66850\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4658 - accuracy: 0.7763 - val_loss: 0.6821 - val_accuracy: 0.5741\n",
      "Epoch 13/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4153 - accuracy: 0.8273\n",
      "Epoch 13: val_loss did not improve from 0.66850\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4111 - accuracy: 0.8242 - val_loss: 0.6839 - val_accuracy: 0.5463\n",
      "Epoch 14/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3877 - accuracy: 0.8339\n",
      "Epoch 14: val_loss did not improve from 0.66850\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3977 - accuracy: 0.8282 - val_loss: 0.6842 - val_accuracy: 0.5463\n",
      "Epoch 15/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3492 - accuracy: 0.8587\n",
      "Epoch 15: val_loss did not improve from 0.66850\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3534 - accuracy: 0.8562 - val_loss: 0.6787 - val_accuracy: 0.5833\n",
      "Epoch 16/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3717 - accuracy: 0.8372\n",
      "Epoch 16: val_loss did not improve from 0.66850\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3658 - accuracy: 0.8482 - val_loss: 0.6769 - val_accuracy: 0.5741\n",
      "Epoch 17/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3521 - accuracy: 0.8520\n",
      "Epoch 17: val_loss did not improve from 0.66850\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3516 - accuracy: 0.8549 - val_loss: 0.6739 - val_accuracy: 0.6019\n",
      "Epoch 18/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3530 - accuracy: 0.8635\n",
      "Epoch 18: val_loss did not improve from 0.66850\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3508 - accuracy: 0.8682 - val_loss: 0.6740 - val_accuracy: 0.6019\n",
      "Epoch 19/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2938 - accuracy: 0.9030\n",
      "Epoch 19: val_loss did not improve from 0.66850\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3014 - accuracy: 0.8975 - val_loss: 0.6697 - val_accuracy: 0.6019\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3111 - accuracy: 0.8775\n",
      "Epoch 20: val_loss improved from 0.66850 to 0.66354, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.3111 - accuracy: 0.8775 - val_loss: 0.6635 - val_accuracy: 0.6204\n",
      "Epoch 21/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3035 - accuracy: 0.8872\n",
      "Epoch 21: val_loss did not improve from 0.66354\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.3040 - accuracy: 0.8855 - val_loss: 0.6654 - val_accuracy: 0.6111\n",
      "Epoch 22/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2811 - accuracy: 0.8882\n",
      "Epoch 22: val_loss did not improve from 0.66354\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2789 - accuracy: 0.8935 - val_loss: 0.6720 - val_accuracy: 0.6204\n",
      "Epoch 23/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2674 - accuracy: 0.9079\n",
      "Epoch 23: val_loss did not improve from 0.66354\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2555 - accuracy: 0.9161 - val_loss: 0.6789 - val_accuracy: 0.6019\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.9095\n",
      "Epoch 24: val_loss did not improve from 0.66354\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2700 - accuracy: 0.9095 - val_loss: 0.6857 - val_accuracy: 0.6019\n",
      "Epoch 25/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2559 - accuracy: 0.9030\n",
      "Epoch 25: val_loss did not improve from 0.66354\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2626 - accuracy: 0.8988 - val_loss: 0.6941 - val_accuracy: 0.6019\n",
      "Epoch 26/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2159 - accuracy: 0.9326\n",
      "Epoch 26: val_loss did not improve from 0.66354\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2201 - accuracy: 0.9294 - val_loss: 0.7003 - val_accuracy: 0.5833\n",
      "Epoch 27/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2011 - accuracy: 0.9474\n",
      "Epoch 27: val_loss did not improve from 0.66354\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2143 - accuracy: 0.9401 - val_loss: 0.7215 - val_accuracy: 0.5741\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2022 - accuracy: 0.9574\n",
      "Epoch 28: val_loss did not improve from 0.66354\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2022 - accuracy: 0.9574 - val_loss: 0.7403 - val_accuracy: 0.5741\n",
      "Epoch 29/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.1964 - accuracy: 0.9523\n",
      "Epoch 29: val_loss did not improve from 0.66354\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1982 - accuracy: 0.9534 - val_loss: 0.7682 - val_accuracy: 0.5463\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1935 - accuracy: 0.9427\n",
      "Epoch 30: val_loss did not improve from 0.66354\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1935 - accuracy: 0.9427 - val_loss: 0.7792 - val_accuracy: 0.5463\n",
      "Epoch 31/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.1683 - accuracy: 0.9622\n",
      "Epoch 31: val_loss did not improve from 0.66354\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1696 - accuracy: 0.9587 - val_loss: 0.8094 - val_accuracy: 0.5463\n",
      "Epoch 32/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.1745 - accuracy: 0.9589\n",
      "Epoch 32: val_loss did not improve from 0.66354\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1801 - accuracy: 0.9574 - val_loss: 0.8177 - val_accuracy: 0.5278\n",
      "Epoch 33/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.1604 - accuracy: 0.9565\n",
      "Epoch 33: val_loss did not improve from 0.66354\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1601 - accuracy: 0.9561 - val_loss: 0.8305 - val_accuracy: 0.5185\n",
      "Epoch 34/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.1406 - accuracy: 0.9755\n",
      "Epoch 34: val_loss did not improve from 0.66354\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1390 - accuracy: 0.9760 - val_loss: 0.8339 - val_accuracy: 0.5370\n",
      "Epoch 35/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.1618 - accuracy: 0.9616Restoring model weights from the end of the best epoch: 20.\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.66354\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.1626 - accuracy: 0.9614 - val_loss: 0.8583 - val_accuracy: 0.5463\n",
      "Epoch 35: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.3478\n",
      "Test AUC for Layer 3: 0.4600\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5020\n",
      "Average Test AUC across all layers: 0.5162\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_60\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_240 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_180 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_120 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_241 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_181 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_121 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_242 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_182 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_243 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8891 - accuracy: 0.4883\n",
      "Epoch 1: val_loss improved from inf to 0.69838, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 24ms/step - loss: 0.8892 - accuracy: 0.4847 - val_loss: 0.6984 - val_accuracy: 0.4479\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7646 - accuracy: 0.5781\n",
      "Epoch 2: val_loss did not improve from 0.69838\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7676 - accuracy: 0.5747 - val_loss: 0.7044 - val_accuracy: 0.4479\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7381 - accuracy: 0.5594\n",
      "Epoch 3: val_loss did not improve from 0.69838\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7381 - accuracy: 0.5594 - val_loss: 0.7092 - val_accuracy: 0.4479\n",
      "Epoch 4/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6987 - accuracy: 0.5889\n",
      "Epoch 4: val_loss did not improve from 0.69838\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6918 - accuracy: 0.6034 - val_loss: 0.7133 - val_accuracy: 0.4479\n",
      "Epoch 5/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6576 - accuracy: 0.6380\n",
      "Epoch 5: val_loss did not improve from 0.69838\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6667 - accuracy: 0.6322 - val_loss: 0.7158 - val_accuracy: 0.4479\n",
      "Epoch 6/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6482 - accuracy: 0.6198\n",
      "Epoch 6: val_loss did not improve from 0.69838\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6248 - accuracy: 0.6418 - val_loss: 0.7174 - val_accuracy: 0.4479\n",
      "Epoch 7/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5799 - accuracy: 0.6971\n",
      "Epoch 7: val_loss did not improve from 0.69838\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5999 - accuracy: 0.6782 - val_loss: 0.7209 - val_accuracy: 0.4479\n",
      "Epoch 8/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5730 - accuracy: 0.7019\n",
      "Epoch 8: val_loss did not improve from 0.69838\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5707 - accuracy: 0.7031 - val_loss: 0.7195 - val_accuracy: 0.4479\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5437 - accuracy: 0.7139\n",
      "Epoch 9: val_loss did not improve from 0.69838\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5482 - accuracy: 0.7146 - val_loss: 0.7170 - val_accuracy: 0.4479\n",
      "Epoch 10/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5251 - accuracy: 0.7109\n",
      "Epoch 10: val_loss did not improve from 0.69838\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5297 - accuracy: 0.7107 - val_loss: 0.7151 - val_accuracy: 0.4479\n",
      "Epoch 11/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4842 - accuracy: 0.7644\n",
      "Epoch 11: val_loss did not improve from 0.69838\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4783 - accuracy: 0.7644 - val_loss: 0.7125 - val_accuracy: 0.4540\n",
      "Epoch 12/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4631 - accuracy: 0.7861\n",
      "Epoch 12: val_loss did not improve from 0.69838\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4639 - accuracy: 0.7874 - val_loss: 0.7137 - val_accuracy: 0.4540\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4561 - accuracy: 0.7957\n",
      "Epoch 13: val_loss did not improve from 0.69838\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4553 - accuracy: 0.7893 - val_loss: 0.7138 - val_accuracy: 0.4601\n",
      "Epoch 14/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4650 - accuracy: 0.7764\n",
      "Epoch 14: val_loss did not improve from 0.69838\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4650 - accuracy: 0.7778 - val_loss: 0.7099 - val_accuracy: 0.4540\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4630 - accuracy: 0.7854\n",
      "Epoch 15: val_loss did not improve from 0.69838\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4630 - accuracy: 0.7854 - val_loss: 0.7093 - val_accuracy: 0.4601\n",
      "Epoch 16/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4221 - accuracy: 0.8177Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69838\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4029 - accuracy: 0.8295 - val_loss: 0.7081 - val_accuracy: 0.4969\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.3485\n",
      "Test AUC for Layer 1: 0.4762\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_244 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_183 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_122 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_245 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_184 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_123 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_246 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_185 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_247 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8939 - accuracy: 0.4926\n",
      "Epoch 1: val_loss improved from inf to 0.74706, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 22ms/step - loss: 0.8903 - accuracy: 0.4949 - val_loss: 0.7471 - val_accuracy: 0.3485\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8142 - accuracy: 0.5417\n",
      "Epoch 2: val_loss did not improve from 0.74706\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.8148 - accuracy: 0.5401 - val_loss: 0.8021 - val_accuracy: 0.3485\n",
      "Epoch 3/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.7480 - accuracy: 0.5451\n",
      "Epoch 3: val_loss did not improve from 0.74706\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7577 - accuracy: 0.5547 - val_loss: 0.8445 - val_accuracy: 0.3485\n",
      "Epoch 4/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6848 - accuracy: 0.6316\n",
      "Epoch 4: val_loss did not improve from 0.74706\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6910 - accuracy: 0.6321 - val_loss: 0.8898 - val_accuracy: 0.3485\n",
      "Epoch 5/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6487 - accuracy: 0.6497\n",
      "Epoch 5: val_loss did not improve from 0.74706\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6459 - accuracy: 0.6511 - val_loss: 0.9238 - val_accuracy: 0.3485\n",
      "Epoch 6/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6487 - accuracy: 0.6711\n",
      "Epoch 6: val_loss did not improve from 0.74706\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6429 - accuracy: 0.6701 - val_loss: 0.9526 - val_accuracy: 0.3485\n",
      "Epoch 7/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6312 - accuracy: 0.6562\n",
      "Epoch 7: val_loss did not improve from 0.74706\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6274 - accuracy: 0.6599 - val_loss: 0.9710 - val_accuracy: 0.3485\n",
      "Epoch 8/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5739 - accuracy: 0.6997\n",
      "Epoch 8: val_loss did not improve from 0.74706\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5912 - accuracy: 0.6905 - val_loss: 0.9746 - val_accuracy: 0.3485\n",
      "Epoch 9/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5249 - accuracy: 0.7426\n",
      "Epoch 9: val_loss did not improve from 0.74706\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5299 - accuracy: 0.7343 - val_loss: 0.9771 - val_accuracy: 0.3485\n",
      "Epoch 10/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5444 - accuracy: 0.7083\n",
      "Epoch 10: val_loss did not improve from 0.74706\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5503 - accuracy: 0.7007 - val_loss: 0.9771 - val_accuracy: 0.3485\n",
      "Epoch 11/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5154 - accuracy: 0.7500\n",
      "Epoch 11: val_loss did not improve from 0.74706\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5140 - accuracy: 0.7445 - val_loss: 0.9823 - val_accuracy: 0.3485\n",
      "Epoch 12/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5004 - accuracy: 0.7451\n",
      "Epoch 12: val_loss did not improve from 0.74706\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4944 - accuracy: 0.7460 - val_loss: 0.9794 - val_accuracy: 0.3485\n",
      "Epoch 13/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4913 - accuracy: 0.7691\n",
      "Epoch 13: val_loss did not improve from 0.74706\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4843 - accuracy: 0.7766 - val_loss: 0.9891 - val_accuracy: 0.3485\n",
      "Epoch 14/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4836 - accuracy: 0.7599\n",
      "Epoch 14: val_loss did not improve from 0.74706\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4787 - accuracy: 0.7650 - val_loss: 0.9831 - val_accuracy: 0.3485\n",
      "Epoch 15/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4681 - accuracy: 0.7681\n",
      "Epoch 15: val_loss did not improve from 0.74706\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4674 - accuracy: 0.7693 - val_loss: 0.9831 - val_accuracy: 0.3485\n",
      "Epoch 16/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4487 - accuracy: 0.7865Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.74706\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4504 - accuracy: 0.7839 - val_loss: 0.9765 - val_accuracy: 0.3485\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.3981\n",
      "Test AUC for Layer 2: 0.4501\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_248 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_186 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_124 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_249 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_187 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_125 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_250 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_188 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_251 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8904 - accuracy: 0.4792\n",
      "Epoch 1: val_loss improved from inf to 0.68788, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.8860 - accuracy: 0.4807 - val_loss: 0.6879 - val_accuracy: 0.6019\n",
      "Epoch 2/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.7717 - accuracy: 0.5655\n",
      "Epoch 2: val_loss improved from 0.68788 to 0.68347, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.7711 - accuracy: 0.5659 - val_loss: 0.6835 - val_accuracy: 0.6019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.7758 - accuracy: 0.5584\n",
      "Epoch 3: val_loss improved from 0.68347 to 0.68090, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7756 - accuracy: 0.5606 - val_loss: 0.6809 - val_accuracy: 0.6019\n",
      "Epoch 4/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6963 - accuracy: 0.6277\n",
      "Epoch 4: val_loss improved from 0.68090 to 0.67937, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6980 - accuracy: 0.6258 - val_loss: 0.6794 - val_accuracy: 0.6019\n",
      "Epoch 5/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.6636 - accuracy: 0.6384\n",
      "Epoch 5: val_loss improved from 0.67937 to 0.67801, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.6527 - accuracy: 0.6471 - val_loss: 0.6780 - val_accuracy: 0.6019\n",
      "Epoch 6/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.6262 - accuracy: 0.6592\n",
      "Epoch 6: val_loss improved from 0.67801 to 0.67736, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.6296 - accuracy: 0.6538 - val_loss: 0.6774 - val_accuracy: 0.6019\n",
      "Epoch 7/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5970 - accuracy: 0.6890\n",
      "Epoch 7: val_loss did not improve from 0.67736\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5936 - accuracy: 0.6924 - val_loss: 0.6778 - val_accuracy: 0.6019\n",
      "Epoch 8/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5885 - accuracy: 0.6711\n",
      "Epoch 8: val_loss did not improve from 0.67736\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5826 - accuracy: 0.6764 - val_loss: 0.6785 - val_accuracy: 0.6019\n",
      "Epoch 9/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5873 - accuracy: 0.6809\n",
      "Epoch 9: val_loss did not improve from 0.67736\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5800 - accuracy: 0.6871 - val_loss: 0.6798 - val_accuracy: 0.6019\n",
      "Epoch 10/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5408 - accuracy: 0.7296\n",
      "Epoch 10: val_loss did not improve from 0.67736\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5436 - accuracy: 0.7270 - val_loss: 0.6821 - val_accuracy: 0.6019\n",
      "Epoch 11/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5095 - accuracy: 0.7541\n",
      "Epoch 11: val_loss did not improve from 0.67736\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5155 - accuracy: 0.7497 - val_loss: 0.6837 - val_accuracy: 0.6019\n",
      "Epoch 12/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4818 - accuracy: 0.7741\n",
      "Epoch 12: val_loss did not improve from 0.67736\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4898 - accuracy: 0.7656 - val_loss: 0.6858 - val_accuracy: 0.5833\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4897 - accuracy: 0.7710\n",
      "Epoch 13: val_loss did not improve from 0.67736\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4897 - accuracy: 0.7710 - val_loss: 0.6894 - val_accuracy: 0.5833\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4350 - accuracy: 0.8096\n",
      "Epoch 14: val_loss did not improve from 0.67736\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4350 - accuracy: 0.8096 - val_loss: 0.6917 - val_accuracy: 0.5741\n",
      "Epoch 15/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4486 - accuracy: 0.8109\n",
      "Epoch 15: val_loss did not improve from 0.67736\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4555 - accuracy: 0.8016 - val_loss: 0.6970 - val_accuracy: 0.5741\n",
      "Epoch 16/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4605 - accuracy: 0.7763\n",
      "Epoch 16: val_loss did not improve from 0.67736\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4532 - accuracy: 0.7856 - val_loss: 0.6983 - val_accuracy: 0.5741\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4330 - accuracy: 0.8083\n",
      "Epoch 17: val_loss did not improve from 0.67736\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4330 - accuracy: 0.8083 - val_loss: 0.7034 - val_accuracy: 0.5556\n",
      "Epoch 18/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4164 - accuracy: 0.8240\n",
      "Epoch 18: val_loss did not improve from 0.67736\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4110 - accuracy: 0.8322 - val_loss: 0.7086 - val_accuracy: 0.5370\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3905 - accuracy: 0.8349\n",
      "Epoch 19: val_loss did not improve from 0.67736\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3905 - accuracy: 0.8349 - val_loss: 0.7184 - val_accuracy: 0.5278\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3569 - accuracy: 0.8495\n",
      "Epoch 20: val_loss did not improve from 0.67736\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.3569 - accuracy: 0.8495 - val_loss: 0.7274 - val_accuracy: 0.5278\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3532 - accuracy: 0.8642Restoring model weights from the end of the best epoch: 6.\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.67736\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3532 - accuracy: 0.8642 - val_loss: 0.7312 - val_accuracy: 0.5185\n",
      "Epoch 21: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4638\n",
      "Test AUC for Layer 3: 0.6014\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4035\n",
      "Average Test AUC across all layers: 0.5092\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_252 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_189 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_126 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_253 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_190 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_127 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_254 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_191 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_255 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8772 - accuracy: 0.5117\n",
      "Epoch 1: val_loss improved from inf to 0.69021, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8787 - accuracy: 0.5096 - val_loss: 0.6902 - val_accuracy: 0.6196\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7736 - accuracy: 0.5293\n",
      "Epoch 2: val_loss improved from 0.69021 to 0.68542, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7737 - accuracy: 0.5326 - val_loss: 0.6854 - val_accuracy: 0.6258\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7447 - accuracy: 0.5703\n",
      "Epoch 3: val_loss improved from 0.68542 to 0.68372, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.7500 - accuracy: 0.5651 - val_loss: 0.6837 - val_accuracy: 0.6258\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6853 - accuracy: 0.6309\n",
      "Epoch 4: val_loss improved from 0.68372 to 0.68216, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6851 - accuracy: 0.6322 - val_loss: 0.6822 - val_accuracy: 0.6258\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6468 - accuracy: 0.6602\n",
      "Epoch 5: val_loss improved from 0.68216 to 0.68031, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.6484 - accuracy: 0.6609 - val_loss: 0.6803 - val_accuracy: 0.6258\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5880 - accuracy: 0.6801\n",
      "Epoch 6: val_loss improved from 0.68031 to 0.67901, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.5880 - accuracy: 0.6801 - val_loss: 0.6790 - val_accuracy: 0.6258\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6088 - accuracy: 0.6816\n",
      "Epoch 7: val_loss improved from 0.67901 to 0.67749, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.6087 - accuracy: 0.6839 - val_loss: 0.6775 - val_accuracy: 0.6258\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6139 - accuracy: 0.6992\n",
      "Epoch 8: val_loss improved from 0.67749 to 0.67697, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.6146 - accuracy: 0.6973 - val_loss: 0.6770 - val_accuracy: 0.6258\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5778 - accuracy: 0.7012\n",
      "Epoch 9: val_loss did not improve from 0.67697\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5771 - accuracy: 0.7031 - val_loss: 0.6771 - val_accuracy: 0.6258\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5240 - accuracy: 0.7383\n",
      "Epoch 10: val_loss did not improve from 0.67697\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5243 - accuracy: 0.7375 - val_loss: 0.6779 - val_accuracy: 0.6319\n",
      "Epoch 11/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4869 - accuracy: 0.7716\n",
      "Epoch 11: val_loss improved from 0.67697 to 0.67688, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4740 - accuracy: 0.7797 - val_loss: 0.6769 - val_accuracy: 0.6258\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4837 - accuracy: 0.7715\n",
      "Epoch 12: val_loss improved from 0.67688 to 0.67602, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4839 - accuracy: 0.7701 - val_loss: 0.6760 - val_accuracy: 0.6258\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4579 - accuracy: 0.8066\n",
      "Epoch 13: val_loss improved from 0.67602 to 0.67537, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4641 - accuracy: 0.7989 - val_loss: 0.6754 - val_accuracy: 0.6258\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4990 - accuracy: 0.7695\n",
      "Epoch 14: val_loss did not improve from 0.67537\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4996 - accuracy: 0.7682 - val_loss: 0.6770 - val_accuracy: 0.6258\n",
      "Epoch 15/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4446 - accuracy: 0.7861\n",
      "Epoch 15: val_loss did not improve from 0.67537\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4490 - accuracy: 0.7854 - val_loss: 0.6780 - val_accuracy: 0.6319\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4373 - accuracy: 0.8164\n",
      "Epoch 16: val_loss did not improve from 0.67537\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4409 - accuracy: 0.8123 - val_loss: 0.6774 - val_accuracy: 0.6319\n",
      "Epoch 17/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4160 - accuracy: 0.8101\n",
      "Epoch 17: val_loss did not improve from 0.67537\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4217 - accuracy: 0.8046 - val_loss: 0.6781 - val_accuracy: 0.6380\n",
      "Epoch 18/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4266 - accuracy: 0.7917\n",
      "Epoch 18: val_loss did not improve from 0.67537\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4357 - accuracy: 0.7912 - val_loss: 0.6822 - val_accuracy: 0.6135\n",
      "Epoch 19/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4001 - accuracy: 0.8125\n",
      "Epoch 19: val_loss did not improve from 0.67537\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3987 - accuracy: 0.8180 - val_loss: 0.6818 - val_accuracy: 0.6135\n",
      "Epoch 20/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3824 - accuracy: 0.8486\n",
      "Epoch 20: val_loss did not improve from 0.67537\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3664 - accuracy: 0.8582 - val_loss: 0.6814 - val_accuracy: 0.5890\n",
      "Epoch 21/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3753 - accuracy: 0.8438\n",
      "Epoch 21: val_loss did not improve from 0.67537\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3788 - accuracy: 0.8391 - val_loss: 0.6819 - val_accuracy: 0.5890\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3641 - accuracy: 0.8506\n",
      "Epoch 22: val_loss did not improve from 0.67537\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3641 - accuracy: 0.8506 - val_loss: 0.6847 - val_accuracy: 0.5706\n",
      "Epoch 23/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3628 - accuracy: 0.8582\n",
      "Epoch 23: val_loss did not improve from 0.67537\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3718 - accuracy: 0.8506 - val_loss: 0.6897 - val_accuracy: 0.5092\n",
      "Epoch 24/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3181 - accuracy: 0.8654\n",
      "Epoch 24: val_loss did not improve from 0.67537\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3169 - accuracy: 0.8659 - val_loss: 0.6935 - val_accuracy: 0.5092\n",
      "Epoch 25/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3398 - accuracy: 0.8698\n",
      "Epoch 25: val_loss did not improve from 0.67537\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3324 - accuracy: 0.8697 - val_loss: 0.6953 - val_accuracy: 0.4969\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3125 - accuracy: 0.8880\n",
      "Epoch 26: val_loss did not improve from 0.67537\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3097 - accuracy: 0.8851 - val_loss: 0.7011 - val_accuracy: 0.4908\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3101 - accuracy: 0.8793\n",
      "Epoch 27: val_loss did not improve from 0.67537\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3101 - accuracy: 0.8793 - val_loss: 0.7067 - val_accuracy: 0.4785\n",
      "Epoch 28/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2789 - accuracy: 0.9036Restoring model weights from the end of the best epoch: 13.\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.67537\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2731 - accuracy: 0.9100 - val_loss: 0.7116 - val_accuracy: 0.4724\n",
      "Epoch 28: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.7576\n",
      "Test AUC for Layer 1: 0.4533\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_256 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_192 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_128 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_257 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_193 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_129 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_258 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_194 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_259 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8449 - accuracy: 0.5431\n",
      "Epoch 1: val_loss improved from inf to 0.76523, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 22ms/step - loss: 0.8449 - accuracy: 0.5431 - val_loss: 0.7652 - val_accuracy: 0.2121\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7963 - accuracy: 0.5565\n",
      "Epoch 2: val_loss did not improve from 0.76523\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7942 - accuracy: 0.5562 - val_loss: 0.8257 - val_accuracy: 0.2121\n",
      "Epoch 3/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.7457 - accuracy: 0.6036\n",
      "Epoch 3: val_loss did not improve from 0.76523\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7361 - accuracy: 0.6088 - val_loss: 0.8823 - val_accuracy: 0.2121\n",
      "Epoch 4/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6893 - accuracy: 0.6003\n",
      "Epoch 4: val_loss did not improve from 0.76523\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6781 - accuracy: 0.6102 - val_loss: 0.9251 - val_accuracy: 0.2121\n",
      "Epoch 5/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6506 - accuracy: 0.6530\n",
      "Epoch 5: val_loss did not improve from 0.76523\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6503 - accuracy: 0.6569 - val_loss: 0.9581 - val_accuracy: 0.2121\n",
      "Epoch 6/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6070 - accuracy: 0.6684\n",
      "Epoch 6: val_loss did not improve from 0.76523\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6010 - accuracy: 0.6803 - val_loss: 0.9822 - val_accuracy: 0.2121\n",
      "Epoch 7/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5969 - accuracy: 0.6727\n",
      "Epoch 7: val_loss did not improve from 0.76523\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5856 - accuracy: 0.6803 - val_loss: 0.9967 - val_accuracy: 0.2121\n",
      "Epoch 8/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5447 - accuracy: 0.6997\n",
      "Epoch 8: val_loss did not improve from 0.76523\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5573 - accuracy: 0.6949 - val_loss: 1.0057 - val_accuracy: 0.2121\n",
      "Epoch 9/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5059 - accuracy: 0.7451\n",
      "Epoch 9: val_loss did not improve from 0.76523\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5071 - accuracy: 0.7445 - val_loss: 1.0096 - val_accuracy: 0.2121\n",
      "Epoch 10/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5272 - accuracy: 0.7292\n",
      "Epoch 10: val_loss did not improve from 0.76523\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5200 - accuracy: 0.7328 - val_loss: 1.0033 - val_accuracy: 0.2121\n",
      "Epoch 11/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5210 - accuracy: 0.7463\n",
      "Epoch 11: val_loss did not improve from 0.76523\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5136 - accuracy: 0.7460 - val_loss: 1.0040 - val_accuracy: 0.2121\n",
      "Epoch 12/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4912 - accuracy: 0.7664\n",
      "Epoch 12: val_loss did not improve from 0.76523\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4954 - accuracy: 0.7635 - val_loss: 0.9932 - val_accuracy: 0.2121\n",
      "Epoch 13/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4260 - accuracy: 0.8306\n",
      "Epoch 13: val_loss did not improve from 0.76523\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4254 - accuracy: 0.8248 - val_loss: 0.9951 - val_accuracy: 0.2121\n",
      "Epoch 14/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4282 - accuracy: 0.8174\n",
      "Epoch 14: val_loss did not improve from 0.76523\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4285 - accuracy: 0.8204 - val_loss: 0.9973 - val_accuracy: 0.2121\n",
      "Epoch 15/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4344 - accuracy: 0.8141\n",
      "Epoch 15: val_loss did not improve from 0.76523\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4458 - accuracy: 0.8058 - val_loss: 0.9882 - val_accuracy: 0.2121\n",
      "Epoch 16/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3909 - accuracy: 0.8368Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.76523\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3941 - accuracy: 0.8307 - val_loss: 0.9899 - val_accuracy: 0.2121\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.3704\n",
      "Test AUC for Layer 2: 0.4761\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_65\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_260 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_195 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_130 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_261 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_196 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_131 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_262 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_197 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_263 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.9657 - accuracy: 0.4568\n",
      "Epoch 1: val_loss improved from inf to 0.67130, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 21ms/step - loss: 0.9590 - accuracy: 0.4634 - val_loss: 0.6713 - val_accuracy: 0.6296\n",
      "Epoch 2/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.8291 - accuracy: 0.5384\n",
      "Epoch 2: val_loss improved from 0.67130 to 0.66277, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.8172 - accuracy: 0.5513 - val_loss: 0.6628 - val_accuracy: 0.6296\n",
      "Epoch 3/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.7363 - accuracy: 0.5982\n",
      "Epoch 3: val_loss improved from 0.66277 to 0.65922, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.7355 - accuracy: 0.5965 - val_loss: 0.6592 - val_accuracy: 0.6296\n",
      "Epoch 4/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.7305 - accuracy: 0.5966\n",
      "Epoch 4: val_loss improved from 0.65922 to 0.65821, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7304 - accuracy: 0.6019 - val_loss: 0.6582 - val_accuracy: 0.6296\n",
      "Epoch 5/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.7372 - accuracy: 0.6023\n",
      "Epoch 5: val_loss improved from 0.65821 to 0.65812, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.7291 - accuracy: 0.6099 - val_loss: 0.6581 - val_accuracy: 0.6296\n",
      "Epoch 6/100\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.6746 - accuracy: 0.6344\n",
      "Epoch 6: val_loss improved from 0.65812 to 0.65775, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.6722 - accuracy: 0.6391 - val_loss: 0.6577 - val_accuracy: 0.6296\n",
      "Epoch 7/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.6053 - accuracy: 0.6875\n",
      "Epoch 7: val_loss improved from 0.65775 to 0.65717, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.6162 - accuracy: 0.6764 - val_loss: 0.6572 - val_accuracy: 0.6296\n",
      "Epoch 8/100\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.5779 - accuracy: 0.7188\n",
      "Epoch 8: val_loss improved from 0.65717 to 0.65596, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.5766 - accuracy: 0.7164 - val_loss: 0.6560 - val_accuracy: 0.6296\n",
      "Epoch 9/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.5713 - accuracy: 0.6918\n",
      "Epoch 9: val_loss improved from 0.65596 to 0.65400, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.5665 - accuracy: 0.6964 - val_loss: 0.6540 - val_accuracy: 0.6296\n",
      "Epoch 10/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5357 - accuracy: 0.7262\n",
      "Epoch 10: val_loss improved from 0.65400 to 0.65391, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.5344 - accuracy: 0.7257 - val_loss: 0.6539 - val_accuracy: 0.6296\n",
      "Epoch 11/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5285 - accuracy: 0.7173\n",
      "Epoch 11: val_loss did not improve from 0.65391\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5283 - accuracy: 0.7230 - val_loss: 0.6540 - val_accuracy: 0.6296\n",
      "Epoch 12/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5413 - accuracy: 0.7321\n",
      "Epoch 12: val_loss improved from 0.65391 to 0.65374, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.5317 - accuracy: 0.7377 - val_loss: 0.6537 - val_accuracy: 0.6296\n",
      "Epoch 13/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.4707 - accuracy: 0.7917\n",
      "Epoch 13: val_loss improved from 0.65374 to 0.65327, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.4775 - accuracy: 0.7856 - val_loss: 0.6533 - val_accuracy: 0.6296\n",
      "Epoch 14/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.4739 - accuracy: 0.7872\n",
      "Epoch 14: val_loss improved from 0.65327 to 0.65137, saving model to OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.4726 - accuracy: 0.7896 - val_loss: 0.6514 - val_accuracy: 0.6296\n",
      "Epoch 15/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.4366 - accuracy: 0.8036\n",
      "Epoch 15: val_loss did not improve from 0.65137\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4379 - accuracy: 0.8069 - val_loss: 0.6529 - val_accuracy: 0.6296\n",
      "Epoch 16/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.4239 - accuracy: 0.8229\n",
      "Epoch 16: val_loss did not improve from 0.65137\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4301 - accuracy: 0.8162 - val_loss: 0.6528 - val_accuracy: 0.6296\n",
      "Epoch 17/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.4298 - accuracy: 0.8006\n",
      "Epoch 17: val_loss did not improve from 0.65137\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4188 - accuracy: 0.8149 - val_loss: 0.6518 - val_accuracy: 0.6296\n",
      "Epoch 18/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.3922 - accuracy: 0.8393\n",
      "Epoch 18: val_loss did not improve from 0.65137\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.3983 - accuracy: 0.8309 - val_loss: 0.6526 - val_accuracy: 0.6389\n",
      "Epoch 19/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.3976 - accuracy: 0.8239\n",
      "Epoch 19: val_loss did not improve from 0.65137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 15ms/step - loss: 0.4024 - accuracy: 0.8189 - val_loss: 0.6543 - val_accuracy: 0.6481\n",
      "Epoch 20/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.3921 - accuracy: 0.8267\n",
      "Epoch 20: val_loss did not improve from 0.65137\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.3939 - accuracy: 0.8242 - val_loss: 0.6566 - val_accuracy: 0.6389\n",
      "Epoch 21/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3579 - accuracy: 0.8684\n",
      "Epoch 21: val_loss did not improve from 0.65137\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3570 - accuracy: 0.8628 - val_loss: 0.6657 - val_accuracy: 0.6389\n",
      "Epoch 22/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3278 - accuracy: 0.8734\n",
      "Epoch 22: val_loss did not improve from 0.65137\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3323 - accuracy: 0.8708 - val_loss: 0.6643 - val_accuracy: 0.6296\n",
      "Epoch 23/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.3151 - accuracy: 0.8824\n",
      "Epoch 23: val_loss did not improve from 0.65137\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.3162 - accuracy: 0.8815 - val_loss: 0.6703 - val_accuracy: 0.6389\n",
      "Epoch 24/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3229 - accuracy: 0.8618\n",
      "Epoch 24: val_loss did not improve from 0.65137\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3388 - accuracy: 0.8589 - val_loss: 0.6868 - val_accuracy: 0.6296\n",
      "Epoch 25/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3107 - accuracy: 0.8764\n",
      "Epoch 25: val_loss did not improve from 0.65137\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3107 - accuracy: 0.8775 - val_loss: 0.6946 - val_accuracy: 0.6574\n",
      "Epoch 26/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3018 - accuracy: 0.8832\n",
      "Epoch 26: val_loss did not improve from 0.65137\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3008 - accuracy: 0.8842 - val_loss: 0.6984 - val_accuracy: 0.6204\n",
      "Epoch 27/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3020 - accuracy: 0.8849\n",
      "Epoch 27: val_loss did not improve from 0.65137\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3025 - accuracy: 0.8855 - val_loss: 0.7075 - val_accuracy: 0.6296\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2862 - accuracy: 0.8921\n",
      "Epoch 28: val_loss did not improve from 0.65137\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2862 - accuracy: 0.8921 - val_loss: 0.7171 - val_accuracy: 0.6204\n",
      "Epoch 29/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2820 - accuracy: 0.8964Restoring model weights from the end of the best epoch: 14.\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.65137\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2769 - accuracy: 0.8975 - val_loss: 0.7378 - val_accuracy: 0.6204\n",
      "Epoch 29: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.2899\n",
      "Test AUC for Layer 3: 0.4684\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4726\n",
      "Average Test AUC across all layers: 0.4659\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS CVX)\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Title + S_label\n",
      "Average Accuracy: 0.5384\n",
      "Average AUC: 0.4530\n",
      "  Layer 1 - Accuracy: 0.6515, AUC: 0.3933\n",
      "  Layer 2 - Accuracy: 0.5000, AUC: 0.4243\n",
      "  Layer 3 - Accuracy: 0.4638, AUC: 0.5414\n",
      "\n",
      "Combination: MLP with Title + L_label\n",
      "Average Accuracy: 0.5020\n",
      "Average AUC: 0.5162\n",
      "  Layer 1 - Accuracy: 0.7879, AUC: 0.5831\n",
      "  Layer 2 - Accuracy: 0.3704, AUC: 0.5055\n",
      "  Layer 3 - Accuracy: 0.3478, AUC: 0.4600\n",
      "\n",
      "Combination: MLP with Full text + S_label\n",
      "Average Accuracy: 0.4035\n",
      "Average AUC: 0.5092\n",
      "  Layer 1 - Accuracy: 0.3485, AUC: 0.4762\n",
      "  Layer 2 - Accuracy: 0.3981, AUC: 0.4501\n",
      "  Layer 3 - Accuracy: 0.4638, AUC: 0.6014\n",
      "\n",
      "Combination: MLP with Full text + L_label\n",
      "Average Accuracy: 0.4726\n",
      "Average AUC: 0.4659\n",
      "  Layer 1 - Accuracy: 0.7576, AUC: 0.4533\n",
      "  Layer 2 - Accuracy: 0.3704, AUC: 0.4761\n",
      "  Layer 3 - Accuracy: 0.2899, AUC: 0.4684\n",
      "MLP summary comparison visualization saved as: OpenAI_MLP/visualizations_summary/CVX\\mlp_performance_comparison.png\n",
      "MLP layer performance visualization saved as: OpenAI_MLP/visualizations_summary/CVX\\mlp_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'OpenAI_MLP' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "from xgboost import callback\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, model_type, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class ClimateNewsOpenAIPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'OpenAI_MLP/visualizations_mlp/CVX'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs('OpenAI_MLP/visualizations_summary/CVX', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert embeddings to numpy arrays\n",
    "        self.data['Title_embedding'] = self.data['Title_embedding_vector'].apply(parse_embedding)\n",
    "        self.data['Fulltext_embedding'] = self.data['Full_text_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_title_embedding = self.data['Title_embedding'].iloc[0]\n",
    "        sample_fulltext_embedding = self.data['Fulltext_embedding'].iloc[0]\n",
    "        \n",
    "        print(f\"Sample Title embedding shape: {sample_title_embedding.shape}\")\n",
    "        print(f\"Sample Fulltext embedding shape: {sample_fulltext_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2019-2021), validation (2021-2021), test (2022-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-05-31'),\n",
    "            'val_start': pd.Timestamp('2021-06-01'),\n",
    "            'val_end': pd.Timestamp('2021-12-31'),\n",
    "            'test_start': pd.Timestamp('2022-01-01'),\n",
    "            'test_end': pd.Timestamp('2022-05-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2019-2021), validation (2022-2022), test (2022-2022)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-12-31'),\n",
    "            'val_start': pd.Timestamp('2022-01-01'),\n",
    "            'val_end': pd.Timestamp('2022-05-31'),\n",
    "            'test_start': pd.Timestamp('2022-06-01'),\n",
    "            'test_end': pd.Timestamp('2022-12-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2019-2022), validation (2022-2022), test (2023-2023)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2022-05-31'),\n",
    "            'val_start': pd.Timestamp('2022-06-01'),\n",
    "            'val_end': pd.Timestamp('2022-12-31'),\n",
    "            'test_start': pd.Timestamp('2023-01-01'),\n",
    "            'test_end': pd.Timestamp('2023-05-31')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, text_col, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            text_col: The column containing the embeddings ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data[text_col].values)\n",
    "        X_val = np.stack(val_data[text_col].values)\n",
    "        X_test = np.stack(test_data[text_col].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, model_type, text_col, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            model_type: Model type (MLP)\n",
    "            text_col: Text column used\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_{model_type.lower()}_{display_text.replace(' ', '_')}_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, text_col, label_col, model_type):\n",
    "        \"\"\"\n",
    "        Train and evaluate a model for a specific text column and label column.\n",
    "        \n",
    "        Args:\n",
    "            text_col: The embedding column to use ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "            model_type: The model type to use ('MLP')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        combination_key = f\"{model_type}|{display_text}|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training {model_type} model for {display_text} and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        visualization_dir = self.mlp_viz_dir if model_type == 'MLP' else self.xgb_viz_dir\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, text_col, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            if model_type == 'MLP':\n",
    "                # MLP model training\n",
    "                model = self.create_mlp_model((1536,))\n",
    "                print(f\"Created MLP model for {display_text} ({label_col})\")\n",
    "                model.summary()\n",
    "                \n",
    "                # Setup callbacks\n",
    "                plot_callback = PlotLearningCallback(model_type, display_text, label_col, i+1, visualization_dir)\n",
    "                early_stopping = EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=15,\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                model_checkpoint = ModelCheckpoint(\n",
    "                    filepath=f\"{visualization_dir}/best_{model_type.lower()}_{display_text.lower().replace(' ', '_')}_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                    monitor='val_loss',\n",
    "                    save_weights_only=True,\n",
    "                    save_best_only=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Train model\n",
    "                print(f\"Training MLP model...\")\n",
    "                batch_size = 32  # Larger batch size for embeddings\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                y_pred_proba = model.predict(X_test)\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                # Create final learning curve visualization\n",
    "                self.plot_final_learning_curves(history, model_type, text_col, label_col, i+1, visualization_dir)\n",
    "                \n",
    "                # Store training history\n",
    "                training_history = history.history\n",
    "                \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'model_type': model_type,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': training_history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for all combinations of text inputs, label columns, and model types.\"\"\"\n",
    "        # Define all combinations\n",
    "        embedding_cols = ['Title_embedding', 'Fulltext_embedding']\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        model_types = ['MLP']\n",
    "        \n",
    "        # Run analysis for each combination\n",
    "        for model_type in model_types:\n",
    "            for embedding_col in embedding_cols:\n",
    "                for label_col in label_cols:\n",
    "                    self.train_and_evaluate_model(embedding_col, label_col, model_type)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS CVX)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Model': model_type,\n",
    "                'Text': text_col,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing all model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Split by model type\n",
    "        mlp_data = df[df['Model'] == 'MLP']\n",
    "        \n",
    "        # 1. Performance comparison for MLP\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot accuracy and AUC bars for MLP\n",
    "        x = np.arange(len(mlp_data))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, mlp_data['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, mlp_data['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('MLP Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with OpenAI Embeddings')\n",
    "        labels = [f\"{row['Text']} + {row['Label']}\" for _, row in mlp_data.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(mlp_data['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(mlp_data['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(mlp_data['Avg Accuracy'].max(), mlp_data['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP/visualizations_summary/CVX', \"mlp_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        mlp_layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Model': model_type,\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                if model_type == 'MLP':\n",
    "                    mlp_layer_data.append(layer_info)\n",
    "        \n",
    "        # Create visualizations for each model type\n",
    "        self._create_model_layer_visualization(mlp_layer_data, 'MLP')\n",
    "    \n",
    "    def _create_model_layer_visualization(self, layer_data, model_type):\n",
    "        \"\"\"Create layer-specific visualizations for a given model type.\"\"\"\n",
    "        if not layer_data:\n",
    "            print(f\"No layer data available for {model_type}\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} Accuracy by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} AUC by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP/visualizations_summary/CVX', f\"{model_type.lower()}_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"{model_type} layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['OpenAI_MLP/visualizations_mlp/CVX',\n",
    "                      'OpenAI_MLP/visualizations_summary/CVX']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/US_news/SP500_semantic/us_news_semantics_CVX_completed_openai.csv'\n",
    "    \n",
    "    # Initialize the predictor with OpenAI embeddings\n",
    "    predictor = ClimateNewsOpenAIPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'OpenAI_MLP' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing merged OpenAI embedding vectors from string format...\n",
      "Sample Merged embedding shape: (1536,)\n",
      "Loaded 929 climate change news articles spanning from 02/01/2019 to 07/05/2023\n",
      "Class distribution for short-term prediction: {1: 499, 0: 430}\n",
      "Class distribution for long-term prediction: {1: 521, 0: 408}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_264 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_198 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_132 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_265 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_199 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_133 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_266 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_200 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_267 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8972 - accuracy: 0.4805\n",
      "Epoch 1: val_loss improved from inf to 0.69002, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8939 - accuracy: 0.4828 - val_loss: 0.6900 - val_accuracy: 0.5521\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7971 - accuracy: 0.5332\n",
      "Epoch 2: val_loss improved from 0.69002 to 0.68858, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7968 - accuracy: 0.5345 - val_loss: 0.6886 - val_accuracy: 0.5521\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7672 - accuracy: 0.5651\n",
      "Epoch 3: val_loss did not improve from 0.68858\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7672 - accuracy: 0.5651 - val_loss: 0.6886 - val_accuracy: 0.5521\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6931 - accuracy: 0.6172\n",
      "Epoch 4: val_loss did not improve from 0.68858\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6905 - accuracy: 0.6207 - val_loss: 0.6895 - val_accuracy: 0.5521\n",
      "Epoch 5/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6149 - accuracy: 0.6490\n",
      "Epoch 5: val_loss did not improve from 0.68858\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6244 - accuracy: 0.6513 - val_loss: 0.6913 - val_accuracy: 0.5521\n",
      "Epoch 6/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6664 - accuracy: 0.6536\n",
      "Epoch 6: val_loss did not improve from 0.68858\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6496 - accuracy: 0.6648 - val_loss: 0.6932 - val_accuracy: 0.5521\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6109 - accuracy: 0.6897\n",
      "Epoch 7: val_loss did not improve from 0.68858\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6109 - accuracy: 0.6897 - val_loss: 0.6962 - val_accuracy: 0.5521\n",
      "Epoch 8/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5025 - accuracy: 0.7380\n",
      "Epoch 8: val_loss did not improve from 0.68858\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5315 - accuracy: 0.7261 - val_loss: 0.6983 - val_accuracy: 0.5521\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5756 - accuracy: 0.6992\n",
      "Epoch 9: val_loss did not improve from 0.68858\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5756 - accuracy: 0.6992 - val_loss: 0.7011 - val_accuracy: 0.5521\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5390 - accuracy: 0.7067\n",
      "Epoch 10: val_loss did not improve from 0.68858\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5107 - accuracy: 0.7337 - val_loss: 0.7039 - val_accuracy: 0.5521\n",
      "Epoch 11/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4951 - accuracy: 0.7474\n",
      "Epoch 11: val_loss did not improve from 0.68858\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5026 - accuracy: 0.7471 - val_loss: 0.7062 - val_accuracy: 0.5521\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4976 - accuracy: 0.7529\n",
      "Epoch 12: val_loss did not improve from 0.68858\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4976 - accuracy: 0.7529 - val_loss: 0.7081 - val_accuracy: 0.5521\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4484 - accuracy: 0.7692\n",
      "Epoch 13: val_loss did not improve from 0.68858\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4565 - accuracy: 0.7701 - val_loss: 0.7102 - val_accuracy: 0.5521\n",
      "Epoch 14/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4267 - accuracy: 0.7943\n",
      "Epoch 14: val_loss did not improve from 0.68858\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4277 - accuracy: 0.8027 - val_loss: 0.7130 - val_accuracy: 0.5521\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4116 - accuracy: 0.7969\n",
      "Epoch 15: val_loss did not improve from 0.68858\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4141 - accuracy: 0.7950 - val_loss: 0.7172 - val_accuracy: 0.5521\n",
      "Epoch 16/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4343 - accuracy: 0.8047\n",
      "Epoch 16: val_loss did not improve from 0.68858\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4169 - accuracy: 0.8103 - val_loss: 0.7193 - val_accuracy: 0.5521\n",
      "Epoch 17/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3960 - accuracy: 0.8301Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.68858\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3989 - accuracy: 0.8295 - val_loss: 0.7219 - val_accuracy: 0.5521\n",
      "Epoch 17: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.6515\n",
      "Test AUC for Layer 1: 0.5814\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_67\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_268 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_201 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_134 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_269 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_202 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_135 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_270 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_203 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_271 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.9234 - accuracy: 0.5047\n",
      "Epoch 1: val_loss improved from inf to 0.69500, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 22ms/step - loss: 0.9382 - accuracy: 0.4964 - val_loss: 0.6950 - val_accuracy: 0.4697\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7625 - accuracy: 0.5670\n",
      "Epoch 2: val_loss did not improve from 0.69500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7666 - accuracy: 0.5679 - val_loss: 0.7012 - val_accuracy: 0.3485\n",
      "Epoch 3/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7504 - accuracy: 0.5625\n",
      "Epoch 3: val_loss did not improve from 0.69500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7497 - accuracy: 0.5606 - val_loss: 0.7084 - val_accuracy: 0.3485\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7391 - accuracy: 0.6044\n",
      "Epoch 4: val_loss did not improve from 0.69500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7391 - accuracy: 0.6044 - val_loss: 0.7133 - val_accuracy: 0.3485\n",
      "Epoch 5/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.6625 - accuracy: 0.6268\n",
      "Epoch 5: val_loss did not improve from 0.69500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6503 - accuracy: 0.6409 - val_loss: 0.7198 - val_accuracy: 0.3485\n",
      "Epoch 6/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6343 - accuracy: 0.6597\n",
      "Epoch 6: val_loss did not improve from 0.69500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6313 - accuracy: 0.6628 - val_loss: 0.7250 - val_accuracy: 0.3485\n",
      "Epoch 7/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.6286 - accuracy: 0.6562\n",
      "Epoch 7: val_loss did not improve from 0.69500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6332 - accuracy: 0.6584 - val_loss: 0.7318 - val_accuracy: 0.3485\n",
      "Epoch 8/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5914 - accuracy: 0.6892\n",
      "Epoch 8: val_loss did not improve from 0.69500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5902 - accuracy: 0.6876 - val_loss: 0.7399 - val_accuracy: 0.3485\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5460 - accuracy: 0.7168\n",
      "Epoch 9: val_loss did not improve from 0.69500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5460 - accuracy: 0.7168 - val_loss: 0.7458 - val_accuracy: 0.3485\n",
      "Epoch 10/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5492 - accuracy: 0.7031\n",
      "Epoch 10: val_loss did not improve from 0.69500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5466 - accuracy: 0.7080 - val_loss: 0.7529 - val_accuracy: 0.3485\n",
      "Epoch 11/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5391 - accuracy: 0.7361\n",
      "Epoch 11: val_loss did not improve from 0.69500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5323 - accuracy: 0.7445 - val_loss: 0.7556 - val_accuracy: 0.3485\n",
      "Epoch 12/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5311 - accuracy: 0.7292\n",
      "Epoch 12: val_loss did not improve from 0.69500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5313 - accuracy: 0.7343 - val_loss: 0.7592 - val_accuracy: 0.3485\n",
      "Epoch 13/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4755 - accuracy: 0.7812\n",
      "Epoch 13: val_loss did not improve from 0.69500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4770 - accuracy: 0.7796 - val_loss: 0.7678 - val_accuracy: 0.3485\n",
      "Epoch 14/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4495 - accuracy: 0.7961\n",
      "Epoch 14: val_loss did not improve from 0.69500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4590 - accuracy: 0.7898 - val_loss: 0.7730 - val_accuracy: 0.3485\n",
      "Epoch 15/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4334 - accuracy: 0.8033\n",
      "Epoch 15: val_loss did not improve from 0.69500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4471 - accuracy: 0.7883 - val_loss: 0.7758 - val_accuracy: 0.3333\n",
      "Epoch 16/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.4239 - accuracy: 0.8110Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4235 - accuracy: 0.8102 - val_loss: 0.7754 - val_accuracy: 0.3182\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.4444\n",
      "Test AUC for Layer 2: 0.4157\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_272 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_204 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_136 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_273 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_205 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_137 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_274 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_206 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_275 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.8760 - accuracy: 0.5031\n",
      "Epoch 1: val_loss improved from inf to 0.67975, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.8711 - accuracy: 0.4993 - val_loss: 0.6798 - val_accuracy: 0.6019\n",
      "Epoch 2/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8265 - accuracy: 0.5060\n",
      "Epoch 2: val_loss improved from 0.67975 to 0.67480, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.8279 - accuracy: 0.5113 - val_loss: 0.6748 - val_accuracy: 0.6019\n",
      "Epoch 3/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.7191 - accuracy: 0.5982\n",
      "Epoch 3: val_loss improved from 0.67480 to 0.67361, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.7200 - accuracy: 0.5979 - val_loss: 0.6736 - val_accuracy: 0.6019\n",
      "Epoch 4/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6815 - accuracy: 0.6136\n",
      "Epoch 4: val_loss did not improve from 0.67361\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.6792 - accuracy: 0.6138 - val_loss: 0.6740 - val_accuracy: 0.6019\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6890 - accuracy: 0.6245\n",
      "Epoch 5: val_loss did not improve from 0.67361\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6890 - accuracy: 0.6245 - val_loss: 0.6743 - val_accuracy: 0.6019\n",
      "Epoch 6/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6501 - accuracy: 0.6612\n",
      "Epoch 6: val_loss did not improve from 0.67361\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6357 - accuracy: 0.6738 - val_loss: 0.6752 - val_accuracy: 0.6019\n",
      "Epoch 7/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6003 - accuracy: 0.6861\n",
      "Epoch 7: val_loss did not improve from 0.67361\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5981 - accuracy: 0.6911 - val_loss: 0.6766 - val_accuracy: 0.6019\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5751 - accuracy: 0.6951\n",
      "Epoch 8: val_loss did not improve from 0.67361\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5751 - accuracy: 0.6951 - val_loss: 0.6771 - val_accuracy: 0.6019\n",
      "Epoch 9/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5400 - accuracy: 0.7232\n",
      "Epoch 9: val_loss did not improve from 0.67361\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5495 - accuracy: 0.7164 - val_loss: 0.6777 - val_accuracy: 0.6019\n",
      "Epoch 10/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5237 - accuracy: 0.7473\n",
      "Epoch 10: val_loss did not improve from 0.67361\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5222 - accuracy: 0.7497 - val_loss: 0.6779 - val_accuracy: 0.6019\n",
      "Epoch 11/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5057 - accuracy: 0.7568\n",
      "Epoch 11: val_loss did not improve from 0.67361\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5040 - accuracy: 0.7577 - val_loss: 0.6782 - val_accuracy: 0.6019\n",
      "Epoch 12/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5222 - accuracy: 0.7470\n",
      "Epoch 12: val_loss did not improve from 0.67361\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5235 - accuracy: 0.7457 - val_loss: 0.6785 - val_accuracy: 0.6019\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4680 - accuracy: 0.7696\n",
      "Epoch 13: val_loss did not improve from 0.67361\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4680 - accuracy: 0.7696 - val_loss: 0.6802 - val_accuracy: 0.6019\n",
      "Epoch 14/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4781 - accuracy: 0.7642\n",
      "Epoch 14: val_loss did not improve from 0.67361\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4835 - accuracy: 0.7617 - val_loss: 0.6827 - val_accuracy: 0.6019\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4448 - accuracy: 0.8043\n",
      "Epoch 15: val_loss did not improve from 0.67361\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4448 - accuracy: 0.8043 - val_loss: 0.6865 - val_accuracy: 0.6019\n",
      "Epoch 16/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4494 - accuracy: 0.7880\n",
      "Epoch 16: val_loss did not improve from 0.67361\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4493 - accuracy: 0.7883 - val_loss: 0.6888 - val_accuracy: 0.6019\n",
      "Epoch 17/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.4412 - accuracy: 0.8095\n",
      "Epoch 17: val_loss did not improve from 0.67361\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4429 - accuracy: 0.8109 - val_loss: 0.6909 - val_accuracy: 0.5926\n",
      "Epoch 18/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4180 - accuracy: 0.8139Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.67361\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4154 - accuracy: 0.8162 - val_loss: 0.7009 - val_accuracy: 0.5833\n",
      "Epoch 18: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4638\n",
      "Test AUC for Layer 3: 0.5971\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5199\n",
      "Average Test AUC across all layers: 0.5314\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_276 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_207 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_138 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_277 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_208 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_139 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_278 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_209 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_279 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.9022 - accuracy: 0.5098\n",
      "Epoch 1: val_loss improved from inf to 0.67585, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.9074 - accuracy: 0.5057 - val_loss: 0.6758 - val_accuracy: 0.6258\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7990 - accuracy: 0.5840\n",
      "Epoch 2: val_loss improved from 0.67585 to 0.66680, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.8012 - accuracy: 0.5805 - val_loss: 0.6668 - val_accuracy: 0.6258\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7544 - accuracy: 0.6172\n",
      "Epoch 3: val_loss improved from 0.66680 to 0.66185, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7532 - accuracy: 0.6149 - val_loss: 0.6619 - val_accuracy: 0.6258\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7570 - accuracy: 0.5664\n",
      "Epoch 4: val_loss improved from 0.66185 to 0.65983, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7536 - accuracy: 0.5690 - val_loss: 0.6598 - val_accuracy: 0.6258\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7249 - accuracy: 0.6094\n",
      "Epoch 5: val_loss improved from 0.65983 to 0.65923, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7241 - accuracy: 0.6073 - val_loss: 0.6592 - val_accuracy: 0.6258\n",
      "Epoch 6/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6237 - accuracy: 0.6758\n",
      "Epoch 6: val_loss improved from 0.65923 to 0.65919, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6193 - accuracy: 0.6782 - val_loss: 0.6592 - val_accuracy: 0.6258\n",
      "Epoch 7/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5771 - accuracy: 0.6973\n",
      "Epoch 7: val_loss improved from 0.65919 to 0.65890, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.5854 - accuracy: 0.6916 - val_loss: 0.6589 - val_accuracy: 0.6258\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5742 - accuracy: 0.6836\n",
      "Epoch 8: val_loss improved from 0.65890 to 0.65825, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5747 - accuracy: 0.6820 - val_loss: 0.6583 - val_accuracy: 0.6258\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5881 - accuracy: 0.7148\n",
      "Epoch 9: val_loss improved from 0.65825 to 0.65734, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.5854 - accuracy: 0.7184 - val_loss: 0.6573 - val_accuracy: 0.6258\n",
      "Epoch 10/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5350 - accuracy: 0.7333\n",
      "Epoch 10: val_loss improved from 0.65734 to 0.65639, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5342 - accuracy: 0.7299 - val_loss: 0.6564 - val_accuracy: 0.6258\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4762 - accuracy: 0.7734\n",
      "Epoch 11: val_loss improved from 0.65639 to 0.65597, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4763 - accuracy: 0.7759 - val_loss: 0.6560 - val_accuracy: 0.6258\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4922 - accuracy: 0.7773\n",
      "Epoch 12: val_loss improved from 0.65597 to 0.65588, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4949 - accuracy: 0.7759 - val_loss: 0.6559 - val_accuracy: 0.6258\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4479 - accuracy: 0.7891\n",
      "Epoch 13: val_loss improved from 0.65588 to 0.65500, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4485 - accuracy: 0.7893 - val_loss: 0.6550 - val_accuracy: 0.6258\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4342 - accuracy: 0.8123\n",
      "Epoch 14: val_loss improved from 0.65500 to 0.65417, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4342 - accuracy: 0.8123 - val_loss: 0.6542 - val_accuracy: 0.6258\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4000 - accuracy: 0.8418\n",
      "Epoch 15: val_loss did not improve from 0.65417\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3990 - accuracy: 0.8429 - val_loss: 0.6542 - val_accuracy: 0.6258\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4172 - accuracy: 0.8161\n",
      "Epoch 16: val_loss did not improve from 0.65417\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4172 - accuracy: 0.8161 - val_loss: 0.6562 - val_accuracy: 0.6319\n",
      "Epoch 17/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3855 - accuracy: 0.8307\n",
      "Epoch 17: val_loss did not improve from 0.65417\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3954 - accuracy: 0.8276 - val_loss: 0.6586 - val_accuracy: 0.6258\n",
      "Epoch 18/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3786 - accuracy: 0.8317\n",
      "Epoch 18: val_loss did not improve from 0.65417\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3804 - accuracy: 0.8276 - val_loss: 0.6587 - val_accuracy: 0.6196\n",
      "Epoch 19/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3776 - accuracy: 0.8255\n",
      "Epoch 19: val_loss did not improve from 0.65417\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3748 - accuracy: 0.8333 - val_loss: 0.6598 - val_accuracy: 0.6074\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3537 - accuracy: 0.8563\n",
      "Epoch 20: val_loss did not improve from 0.65417\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3537 - accuracy: 0.8563 - val_loss: 0.6611 - val_accuracy: 0.5890\n",
      "Epoch 21/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3248 - accuracy: 0.8854\n",
      "Epoch 21: val_loss did not improve from 0.65417\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3175 - accuracy: 0.8851 - val_loss: 0.6604 - val_accuracy: 0.6012\n",
      "Epoch 22/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2956 - accuracy: 0.8932\n",
      "Epoch 22: val_loss did not improve from 0.65417\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3115 - accuracy: 0.8831 - val_loss: 0.6591 - val_accuracy: 0.6074\n",
      "Epoch 23/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3065 - accuracy: 0.8966\n",
      "Epoch 23: val_loss did not improve from 0.65417\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3132 - accuracy: 0.8908 - val_loss: 0.6597 - val_accuracy: 0.6074\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 0.2943 - accuracy: 0.8831\n",
      "Epoch 24: val_loss did not improve from 0.65417\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2943 - accuracy: 0.8831 - val_loss: 0.6616 - val_accuracy: 0.5890\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.8946\n",
      "Epoch 25: val_loss did not improve from 0.65417\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.2905 - accuracy: 0.8946 - val_loss: 0.6609 - val_accuracy: 0.5951\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2616 - accuracy: 0.9023\n",
      "Epoch 26: val_loss did not improve from 0.65417\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2616 - accuracy: 0.9023 - val_loss: 0.6600 - val_accuracy: 0.6074\n",
      "Epoch 27/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2561 - accuracy: 0.9271\n",
      "Epoch 27: val_loss did not improve from 0.65417\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2776 - accuracy: 0.9119 - val_loss: 0.6554 - val_accuracy: 0.6258\n",
      "Epoch 28/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2689 - accuracy: 0.9014\n",
      "Epoch 28: val_loss did not improve from 0.65417\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2648 - accuracy: 0.9042 - val_loss: 0.6572 - val_accuracy: 0.6258\n",
      "Epoch 29/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2448 - accuracy: 0.9167Restoring model weights from the end of the best epoch: 14.\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.65417\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2462 - accuracy: 0.9195 - val_loss: 0.6612 - val_accuracy: 0.6196\n",
      "Epoch 29: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.7879\n",
      "Test AUC for Layer 1: 0.6332\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_280 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_210 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_140 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_281 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_211 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_141 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_282 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_212 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_283 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8633 - accuracy: 0.5030\n",
      "Epoch 1: val_loss improved from inf to 0.74546, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 21ms/step - loss: 0.8598 - accuracy: 0.5066 - val_loss: 0.7455 - val_accuracy: 0.2121\n",
      "Epoch 2/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.7809 - accuracy: 0.5578\n",
      "Epoch 2: val_loss did not improve from 0.74546\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.7840 - accuracy: 0.5533 - val_loss: 0.8028 - val_accuracy: 0.2121\n",
      "Epoch 3/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7582 - accuracy: 0.5610\n",
      "Epoch 3: val_loss did not improve from 0.74546\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7566 - accuracy: 0.5606 - val_loss: 0.8596 - val_accuracy: 0.2121\n",
      "Epoch 4/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.7153 - accuracy: 0.5955\n",
      "Epoch 4: val_loss did not improve from 0.74546\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7223 - accuracy: 0.5927 - val_loss: 0.9083 - val_accuracy: 0.2121\n",
      "Epoch 5/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6720 - accuracy: 0.6406\n",
      "Epoch 5: val_loss did not improve from 0.74546\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6676 - accuracy: 0.6482 - val_loss: 0.9520 - val_accuracy: 0.2121\n",
      "Epoch 6/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6276 - accuracy: 0.6493\n",
      "Epoch 6: val_loss did not improve from 0.74546\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6203 - accuracy: 0.6555 - val_loss: 0.9905 - val_accuracy: 0.2121\n",
      "Epoch 7/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.6083 - accuracy: 0.6875\n",
      "Epoch 7: val_loss did not improve from 0.74546\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5909 - accuracy: 0.6949 - val_loss: 1.0270 - val_accuracy: 0.2121\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5905 - accuracy: 0.6876\n",
      "Epoch 8: val_loss did not improve from 0.74546\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5905 - accuracy: 0.6876 - val_loss: 1.0543 - val_accuracy: 0.2121\n",
      "Epoch 9/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5723 - accuracy: 0.6793\n",
      "Epoch 9: val_loss did not improve from 0.74546\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5730 - accuracy: 0.6774 - val_loss: 1.0758 - val_accuracy: 0.2121\n",
      "Epoch 10/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5617 - accuracy: 0.6997\n",
      "Epoch 10: val_loss did not improve from 0.74546\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5610 - accuracy: 0.7080 - val_loss: 1.0986 - val_accuracy: 0.2121\n",
      "Epoch 11/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5111 - accuracy: 0.7665\n",
      "Epoch 11: val_loss did not improve from 0.74546\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5065 - accuracy: 0.7606 - val_loss: 1.1116 - val_accuracy: 0.2121\n",
      "Epoch 12/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4559 - accuracy: 0.7904\n",
      "Epoch 12: val_loss did not improve from 0.74546\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4728 - accuracy: 0.7781 - val_loss: 1.1302 - val_accuracy: 0.2121\n",
      "Epoch 13/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4806 - accuracy: 0.7865\n",
      "Epoch 13: val_loss did not improve from 0.74546\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4771 - accuracy: 0.7912 - val_loss: 1.1665 - val_accuracy: 0.2121\n",
      "Epoch 14/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4557 - accuracy: 0.7986\n",
      "Epoch 14: val_loss did not improve from 0.74546\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4478 - accuracy: 0.7985 - val_loss: 1.1679 - val_accuracy: 0.2121\n",
      "Epoch 15/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4364 - accuracy: 0.8142\n",
      "Epoch 15: val_loss did not improve from 0.74546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4364 - accuracy: 0.8117 - val_loss: 1.1720 - val_accuracy: 0.2121\n",
      "Epoch 16/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4415 - accuracy: 0.7951Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.74546\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4402 - accuracy: 0.7956 - val_loss: 1.1890 - val_accuracy: 0.2121\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.3704\n",
      "Test AUC for Layer 2: 0.4485\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_284 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_213 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_142 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_285 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_214 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_143 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_286 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_215 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_287 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.9414 - accuracy: 0.4926\n",
      "Epoch 1: val_loss improved from inf to 0.70722, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 21ms/step - loss: 0.9398 - accuracy: 0.4900 - val_loss: 0.7072 - val_accuracy: 0.3704\n",
      "Epoch 2/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8080 - accuracy: 0.5417\n",
      "Epoch 2: val_loss did not improve from 0.70722\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.8056 - accuracy: 0.5499 - val_loss: 0.7115 - val_accuracy: 0.3704\n",
      "Epoch 3/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.7699 - accuracy: 0.5477\n",
      "Epoch 3: val_loss did not improve from 0.70722\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.7673 - accuracy: 0.5526 - val_loss: 0.7117 - val_accuracy: 0.3704\n",
      "Epoch 4/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.7197 - accuracy: 0.5883\n",
      "Epoch 4: val_loss did not improve from 0.70722\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.7185 - accuracy: 0.5899 - val_loss: 0.7089 - val_accuracy: 0.3704\n",
      "Epoch 5/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6541 - accuracy: 0.6480\n",
      "Epoch 5: val_loss improved from 0.70722 to 0.70497, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.6445 - accuracy: 0.6538 - val_loss: 0.7050 - val_accuracy: 0.3611\n",
      "Epoch 6/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6438 - accuracy: 0.6630\n",
      "Epoch 6: val_loss improved from 0.70497 to 0.69700, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6423 - accuracy: 0.6618 - val_loss: 0.6970 - val_accuracy: 0.4630\n",
      "Epoch 7/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.6139 - accuracy: 0.6682\n",
      "Epoch 7: val_loss improved from 0.69700 to 0.69125, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.6070 - accuracy: 0.6724 - val_loss: 0.6912 - val_accuracy: 0.5463\n",
      "Epoch 8/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5509 - accuracy: 0.7083\n",
      "Epoch 8: val_loss improved from 0.69125 to 0.68339, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.5617 - accuracy: 0.7097 - val_loss: 0.6834 - val_accuracy: 0.6204\n",
      "Epoch 9/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5706 - accuracy: 0.7083\n",
      "Epoch 9: val_loss improved from 0.68339 to 0.67945, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.5726 - accuracy: 0.7031 - val_loss: 0.6794 - val_accuracy: 0.6111\n",
      "Epoch 10/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5438 - accuracy: 0.7247\n",
      "Epoch 10: val_loss improved from 0.67945 to 0.67510, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.5431 - accuracy: 0.7284 - val_loss: 0.6751 - val_accuracy: 0.6019\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5211 - accuracy: 0.7417\n",
      "Epoch 11: val_loss improved from 0.67510 to 0.67421, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.5211 - accuracy: 0.7417 - val_loss: 0.6742 - val_accuracy: 0.6019\n",
      "Epoch 12/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.5042 - accuracy: 0.7528\n",
      "Epoch 12: val_loss improved from 0.67421 to 0.67316, saving model to Merged_OpenAI_MLP/visualizations_mlp/CVX\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.5101 - accuracy: 0.7510 - val_loss: 0.6732 - val_accuracy: 0.6204\n",
      "Epoch 13/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4853 - accuracy: 0.7699\n",
      "Epoch 13: val_loss did not improve from 0.67316\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4844 - accuracy: 0.7683 - val_loss: 0.6742 - val_accuracy: 0.6111\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4561 - accuracy: 0.7936\n",
      "Epoch 14: val_loss did not improve from 0.67316\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4561 - accuracy: 0.7936 - val_loss: 0.6753 - val_accuracy: 0.6111\n",
      "Epoch 15/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4501 - accuracy: 0.8043\n",
      "Epoch 15: val_loss did not improve from 0.67316\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4531 - accuracy: 0.8016 - val_loss: 0.6782 - val_accuracy: 0.6111\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4273 - accuracy: 0.8016\n",
      "Epoch 16: val_loss did not improve from 0.67316\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4273 - accuracy: 0.8016 - val_loss: 0.6790 - val_accuracy: 0.6111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4173 - accuracy: 0.8224\n",
      "Epoch 17: val_loss did not improve from 0.67316\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4153 - accuracy: 0.8242 - val_loss: 0.6828 - val_accuracy: 0.5926\n",
      "Epoch 18/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3827 - accuracy: 0.8370\n",
      "Epoch 18: val_loss did not improve from 0.67316\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3830 - accuracy: 0.8375 - val_loss: 0.6867 - val_accuracy: 0.5833\n",
      "Epoch 19/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3956 - accuracy: 0.8261\n",
      "Epoch 19: val_loss did not improve from 0.67316\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3951 - accuracy: 0.8256 - val_loss: 0.6973 - val_accuracy: 0.5463\n",
      "Epoch 20/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3842 - accuracy: 0.8438\n",
      "Epoch 20: val_loss did not improve from 0.67316\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3857 - accuracy: 0.8429 - val_loss: 0.7084 - val_accuracy: 0.5185\n",
      "Epoch 21/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3558 - accuracy: 0.8601\n",
      "Epoch 21: val_loss did not improve from 0.67316\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3549 - accuracy: 0.8628 - val_loss: 0.7133 - val_accuracy: 0.4907\n",
      "Epoch 22/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.3310 - accuracy: 0.8821\n",
      "Epoch 22: val_loss did not improve from 0.67316\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3301 - accuracy: 0.8828 - val_loss: 0.7213 - val_accuracy: 0.5093\n",
      "Epoch 23/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.3457 - accuracy: 0.8631\n",
      "Epoch 23: val_loss did not improve from 0.67316\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.3436 - accuracy: 0.8695 - val_loss: 0.7356 - val_accuracy: 0.5278\n",
      "Epoch 24/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.3245 - accuracy: 0.8693\n",
      "Epoch 24: val_loss did not improve from 0.67316\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.3238 - accuracy: 0.8682 - val_loss: 0.7427 - val_accuracy: 0.5093\n",
      "Epoch 25/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3263 - accuracy: 0.8723\n",
      "Epoch 25: val_loss did not improve from 0.67316\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3275 - accuracy: 0.8708 - val_loss: 0.7632 - val_accuracy: 0.5463\n",
      "Epoch 26/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3075 - accuracy: 0.8845\n",
      "Epoch 26: val_loss did not improve from 0.67316\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3094 - accuracy: 0.8828 - val_loss: 0.7781 - val_accuracy: 0.5093\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2757 - accuracy: 0.9068Restoring model weights from the end of the best epoch: 12.\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.67316\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.2757 - accuracy: 0.9068 - val_loss: 0.7873 - val_accuracy: 0.5093\n",
      "Epoch 27: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.2754\n",
      "Test AUC for Layer 3: 0.3674\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4779\n",
      "Average Test AUC across all layers: 0.4830\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS (CVX)\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Merged + S_label\n",
      "Average Accuracy: 0.5199\n",
      "Average AUC: 0.5314\n",
      "  Layer 1 - Accuracy: 0.6515, AUC: 0.5814\n",
      "  Layer 2 - Accuracy: 0.4444, AUC: 0.4157\n",
      "  Layer 3 - Accuracy: 0.4638, AUC: 0.5971\n",
      "\n",
      "Combination: MLP with Merged + L_label\n",
      "Average Accuracy: 0.4779\n",
      "Average AUC: 0.4830\n",
      "  Layer 1 - Accuracy: 0.7879, AUC: 0.6332\n",
      "  Layer 2 - Accuracy: 0.3704, AUC: 0.4485\n",
      "  Layer 3 - Accuracy: 0.2754, AUC: 0.3674\n",
      "MLP summary comparison visualization saved as: Merged_OpenAI_MLP/visualizations_summary/CVX\\mlp_merged_performance_comparison.png\n",
      "MLP layer performance visualization saved as: Merged_OpenAI_MLP/visualizations_summary/CVX\\mlp_merged_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class MergedEmbeddingMLPPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using merged OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with merged OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'Merged_OpenAI_MLP/visualizations_mlp/CVX'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs('Merged_OpenAI_MLP/visualizations_summary/CVX', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with merged OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing merged OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert merged embeddings to numpy arrays\n",
    "        self.data['Merged_embedding'] = self.data['Merged_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_merged_embedding = self.data['Merged_embedding'].iloc[0]\n",
    "        print(f\"Sample Merged embedding shape: {sample_merged_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2019-2021), validation (2021-2021), test (2022-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-05-31'),\n",
    "            'val_start': pd.Timestamp('2021-06-01'),\n",
    "            'val_end': pd.Timestamp('2021-12-31'),\n",
    "            'test_start': pd.Timestamp('2022-01-01'),\n",
    "            'test_end': pd.Timestamp('2022-05-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2019-2021), validation (2022-2022), test (2022-2022)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-12-31'),\n",
    "            'val_start': pd.Timestamp('2022-01-01'),\n",
    "            'val_end': pd.Timestamp('2022-05-31'),\n",
    "            'test_start': pd.Timestamp('2022-06-01'),\n",
    "            'test_end': pd.Timestamp('2022-12-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2019-2022), validation (2022-2022), test (2023-2023)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2022-05-31'),\n",
    "            'val_start': pd.Timestamp('2022-06-01'),\n",
    "            'val_end': pd.Timestamp('2022-12-31'),\n",
    "            'test_start': pd.Timestamp('2023-01-01'),\n",
    "            'test_end': pd.Timestamp('2023-05-31')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data['Merged_embedding'].values)\n",
    "        X_val = np.stack(val_data['Merged_embedding'].values)\n",
    "        X_test = np.stack(test_data['Merged_embedding'].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_mlp_merged_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, label_col):\n",
    "        \"\"\"\n",
    "        Train and evaluate an MLP model for merged embeddings and a specific label column.\n",
    "        \n",
    "        Args:\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        combination_key = f\"MLP|Merged|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training MLP model for Merged Embeddings and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            model = self.create_mlp_model((1536,))\n",
    "            print(f\"Created MLP model for Merged Embeddings ({label_col})\")\n",
    "            model.summary()\n",
    "            \n",
    "            # Setup callbacks\n",
    "            plot_callback = PlotLearningCallback('Merged', label_col, i+1, self.mlp_viz_dir)\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=15,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            model_checkpoint = ModelCheckpoint(\n",
    "                filepath=f\"{self.mlp_viz_dir}/best_mlp_merged_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                monitor='val_loss',\n",
    "                save_weights_only=True,\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            print(f\"Training MLP model...\")\n",
    "            batch_size = 32  # Larger batch size for embeddings\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred_proba = model.predict(X_test)\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "            \n",
    "            # Create final learning curve visualization\n",
    "            self.plot_final_learning_curves(history, label_col, i+1, self.mlp_viz_dir)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': history.history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for merged embeddings with both short-term and long-term labels.\"\"\"\n",
    "        # Define label columns\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        \n",
    "        # Run analysis for each label column\n",
    "        for label_col in label_cols:\n",
    "            self.train_and_evaluate_model(label_col)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS (CVX)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Performance comparison for MLP with merged embeddings\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Plot accuracy and AUC bars\n",
    "        x = np.arange(len(df))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, df['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, df['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with Merged OpenAI Embeddings')\n",
    "        labels = [f\"Merged + {row['Label']}\" for _, row in df.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(df['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(df['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(df['Avg Accuracy'].max(), df['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/CVX', \"mlp_merged_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                layer_data.append(layer_info)\n",
    "        \n",
    "        if not layer_data:\n",
    "            print(\"No layer data available\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title('MLP Accuracy by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title('MLP AUC by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/CVX', \"mlp_merged_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['Merged_OpenAI_MLP/visualizations_mlp/CVX', 'Merged_OpenAI_MLP/visualizations_summary/CVX']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/US_news/SP500_semantic/us_news_semantics_CVX_completed_openai.csv'\n",
    "    \n",
    "    # Initialize the predictor with merged OpenAI embeddings\n",
    "    predictor = MergedEmbeddingMLPPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing OpenAI embedding vectors from string format...\n",
      "Sample Title embedding shape: (1536,)\n",
      "Sample Fulltext embedding shape: (1536,)\n",
      "Loaded 929 climate change news articles spanning from 02/01/2019 to 07/05/2023\n",
      "Class distribution for short-term prediction: {0: 469, 1: 460}\n",
      "Class distribution for long-term prediction: {1: 504, 0: 425}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_288 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_216 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_144 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_289 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_217 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_145 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_290 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_218 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_291 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8406 - accuracy: 0.5273\n",
      "Epoch 1: val_loss improved from inf to 0.70107, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_title_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8404 - accuracy: 0.5287 - val_loss: 0.7011 - val_accuracy: 0.4969\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7212 - accuracy: 0.5996\n",
      "Epoch 2: val_loss did not improve from 0.70107\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7171 - accuracy: 0.6015 - val_loss: 0.7181 - val_accuracy: 0.4969\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7463 - accuracy: 0.5843\n",
      "Epoch 3: val_loss did not improve from 0.70107\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7463 - accuracy: 0.5843 - val_loss: 0.7386 - val_accuracy: 0.4969\n",
      "Epoch 4/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6598 - accuracy: 0.6276\n",
      "Epoch 4: val_loss did not improve from 0.70107\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6555 - accuracy: 0.6456 - val_loss: 0.7596 - val_accuracy: 0.4969\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6358 - accuracy: 0.6437\n",
      "Epoch 5: val_loss did not improve from 0.70107\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6358 - accuracy: 0.6437 - val_loss: 0.7806 - val_accuracy: 0.4969\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6523 - accuracy: 0.6562\n",
      "Epoch 6: val_loss did not improve from 0.70107\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6305 - accuracy: 0.6686 - val_loss: 0.7971 - val_accuracy: 0.4969\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5660 - accuracy: 0.7126\n",
      "Epoch 7: val_loss did not improve from 0.70107\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5660 - accuracy: 0.7126 - val_loss: 0.8098 - val_accuracy: 0.4969\n",
      "Epoch 8/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5913 - accuracy: 0.7057\n",
      "Epoch 8: val_loss did not improve from 0.70107\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5633 - accuracy: 0.7261 - val_loss: 0.8209 - val_accuracy: 0.4969\n",
      "Epoch 9/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4900 - accuracy: 0.7524\n",
      "Epoch 9: val_loss did not improve from 0.70107\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5020 - accuracy: 0.7452 - val_loss: 0.8271 - val_accuracy: 0.4969\n",
      "Epoch 10/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4687 - accuracy: 0.7917\n",
      "Epoch 10: val_loss did not improve from 0.70107\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4856 - accuracy: 0.7778 - val_loss: 0.8343 - val_accuracy: 0.4969\n",
      "Epoch 11/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4523 - accuracy: 0.7933\n",
      "Epoch 11: val_loss did not improve from 0.70107\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4705 - accuracy: 0.7778 - val_loss: 0.8373 - val_accuracy: 0.4969\n",
      "Epoch 12/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4167 - accuracy: 0.8173\n",
      "Epoch 12: val_loss did not improve from 0.70107\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4296 - accuracy: 0.8065 - val_loss: 0.8402 - val_accuracy: 0.4969\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4058 - accuracy: 0.8245\n",
      "Epoch 13: val_loss did not improve from 0.70107\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4028 - accuracy: 0.8333 - val_loss: 0.8388 - val_accuracy: 0.4969\n",
      "Epoch 14/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3813 - accuracy: 0.8516\n",
      "Epoch 14: val_loss did not improve from 0.70107\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3884 - accuracy: 0.8372 - val_loss: 0.8459 - val_accuracy: 0.4969\n",
      "Epoch 15/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4064 - accuracy: 0.8053\n",
      "Epoch 15: val_loss did not improve from 0.70107\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4031 - accuracy: 0.8142 - val_loss: 0.8466 - val_accuracy: 0.4969\n",
      "Epoch 16/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3650 - accuracy: 0.8542Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70107\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3590 - accuracy: 0.8659 - val_loss: 0.8460 - val_accuracy: 0.4969\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.4545\n",
      "Test AUC for Layer 1: 0.5491\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_73\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_292 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_219 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_146 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_293 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_220 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_147 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_294 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_221 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_295 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8811 - accuracy: 0.4821\n",
      "Epoch 1: val_loss improved from inf to 0.68913, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_title_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 22ms/step - loss: 0.8772 - accuracy: 0.4847 - val_loss: 0.6891 - val_accuracy: 0.5455\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7677 - accuracy: 0.5565\n",
      "Epoch 2: val_loss did not improve from 0.68913\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7640 - accuracy: 0.5606 - val_loss: 0.6893 - val_accuracy: 0.5455\n",
      "Epoch 3/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6750 - accuracy: 0.6365\n",
      "Epoch 3: val_loss did not improve from 0.68913\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6809 - accuracy: 0.6380 - val_loss: 0.6913 - val_accuracy: 0.5455\n",
      "Epoch 4/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6503 - accuracy: 0.6382\n",
      "Epoch 4: val_loss did not improve from 0.68913\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6464 - accuracy: 0.6438 - val_loss: 0.6938 - val_accuracy: 0.5455\n",
      "Epoch 5/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6298 - accuracy: 0.6628\n",
      "Epoch 5: val_loss did not improve from 0.68913\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6276 - accuracy: 0.6540 - val_loss: 0.6967 - val_accuracy: 0.5455\n",
      "Epoch 6/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5830 - accuracy: 0.6967\n",
      "Epoch 6: val_loss did not improve from 0.68913\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5824 - accuracy: 0.6905 - val_loss: 0.6984 - val_accuracy: 0.5455\n",
      "Epoch 7/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5765 - accuracy: 0.7066\n",
      "Epoch 7: val_loss did not improve from 0.68913\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5689 - accuracy: 0.7139 - val_loss: 0.6989 - val_accuracy: 0.5455\n",
      "Epoch 8/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5312 - accuracy: 0.7418\n",
      "Epoch 8: val_loss did not improve from 0.68913\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5329 - accuracy: 0.7372 - val_loss: 0.7009 - val_accuracy: 0.5455\n",
      "Epoch 9/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4805 - accuracy: 0.7684\n",
      "Epoch 9: val_loss did not improve from 0.68913\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4901 - accuracy: 0.7679 - val_loss: 0.7021 - val_accuracy: 0.5455\n",
      "Epoch 10/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4803 - accuracy: 0.7639\n",
      "Epoch 10: val_loss did not improve from 0.68913\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4843 - accuracy: 0.7635 - val_loss: 0.7041 - val_accuracy: 0.5455\n",
      "Epoch 11/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4388 - accuracy: 0.8076\n",
      "Epoch 11: val_loss did not improve from 0.68913\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4436 - accuracy: 0.8044 - val_loss: 0.7068 - val_accuracy: 0.5455\n",
      "Epoch 12/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4346 - accuracy: 0.8177\n",
      "Epoch 12: val_loss did not improve from 0.68913\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4430 - accuracy: 0.8102 - val_loss: 0.7077 - val_accuracy: 0.5455\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3878 - accuracy: 0.8423\n",
      "Epoch 13: val_loss did not improve from 0.68913\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3878 - accuracy: 0.8423 - val_loss: 0.7097 - val_accuracy: 0.5455\n",
      "Epoch 14/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.3862 - accuracy: 0.8419\n",
      "Epoch 14: val_loss did not improve from 0.68913\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4044 - accuracy: 0.8263 - val_loss: 0.7125 - val_accuracy: 0.5455\n",
      "Epoch 15/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.3656 - accuracy: 0.8529\n",
      "Epoch 15: val_loss did not improve from 0.68913\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3711 - accuracy: 0.8482 - val_loss: 0.7144 - val_accuracy: 0.5606\n",
      "Epoch 16/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3539 - accuracy: 0.8698Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.68913\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3637 - accuracy: 0.8613 - val_loss: 0.7170 - val_accuracy: 0.5303\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5278\n",
      "Test AUC for Layer 2: 0.5944\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (S_label)\n",
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_296 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_222 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_148 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_297 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_223 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_149 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_298 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_224 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_299 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.8631 - accuracy: 0.5327\n",
      "Epoch 1: val_loss improved from inf to 0.69578, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_title_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.8646 - accuracy: 0.5313 - val_loss: 0.6958 - val_accuracy: 0.4722\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.7707 - accuracy: 0.5646\n",
      "Epoch 2: val_loss did not improve from 0.69578\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7707 - accuracy: 0.5646 - val_loss: 0.6998 - val_accuracy: 0.4722\n",
      "Epoch 3/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.7508 - accuracy: 0.5810\n",
      "Epoch 3: val_loss did not improve from 0.69578\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.7441 - accuracy: 0.5779 - val_loss: 0.7027 - val_accuracy: 0.4722\n",
      "Epoch 4/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.6658 - accuracy: 0.6503\n",
      "Epoch 4: val_loss did not improve from 0.69578\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6576 - accuracy: 0.6578 - val_loss: 0.7060 - val_accuracy: 0.4722\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6636 - accuracy: 0.6245\n",
      "Epoch 5: val_loss did not improve from 0.69578\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6636 - accuracy: 0.6245 - val_loss: 0.7097 - val_accuracy: 0.4722\n",
      "Epoch 6/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5739 - accuracy: 0.7068\n",
      "Epoch 6: val_loss did not improve from 0.69578\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5851 - accuracy: 0.6964 - val_loss: 0.7140 - val_accuracy: 0.4722\n",
      "Epoch 7/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5802 - accuracy: 0.6920\n",
      "Epoch 7: val_loss did not improve from 0.69578\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5961 - accuracy: 0.6804 - val_loss: 0.7175 - val_accuracy: 0.4722\n",
      "Epoch 8/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.5730 - accuracy: 0.7173\n",
      "Epoch 8: val_loss did not improve from 0.69578\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5682 - accuracy: 0.7204 - val_loss: 0.7178 - val_accuracy: 0.4722\n",
      "Epoch 9/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5126 - accuracy: 0.7432\n",
      "Epoch 9: val_loss did not improve from 0.69578\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5185 - accuracy: 0.7364 - val_loss: 0.7204 - val_accuracy: 0.4722\n",
      "Epoch 10/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.4653 - accuracy: 0.7723\n",
      "Epoch 10: val_loss did not improve from 0.69578\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4558 - accuracy: 0.7830 - val_loss: 0.7222 - val_accuracy: 0.4722\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4582 - accuracy: 0.7870\n",
      "Epoch 11: val_loss did not improve from 0.69578\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4582 - accuracy: 0.7870 - val_loss: 0.7283 - val_accuracy: 0.4722\n",
      "Epoch 12/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.3952 - accuracy: 0.8338\n",
      "Epoch 12: val_loss did not improve from 0.69578\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4001 - accuracy: 0.8296 - val_loss: 0.7365 - val_accuracy: 0.4722\n",
      "Epoch 13/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4340 - accuracy: 0.7894\n",
      "Epoch 13: val_loss did not improve from 0.69578\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4330 - accuracy: 0.7909 - val_loss: 0.7365 - val_accuracy: 0.4630\n",
      "Epoch 14/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.3921 - accuracy: 0.8259\n",
      "Epoch 14: val_loss did not improve from 0.69578\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.3941 - accuracy: 0.8242 - val_loss: 0.7397 - val_accuracy: 0.4722\n",
      "Epoch 15/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.3724 - accuracy: 0.8466\n",
      "Epoch 15: val_loss did not improve from 0.69578\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3829 - accuracy: 0.8362 - val_loss: 0.7433 - val_accuracy: 0.4630\n",
      "Epoch 16/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3371 - accuracy: 0.8668Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69578\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3351 - accuracy: 0.8655 - val_loss: 0.7541 - val_accuracy: 0.4630\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5507\n",
      "Test AUC for Layer 3: 0.4601\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5110\n",
      "Average Test AUC across all layers: 0.5345\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Title and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_300 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_225 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_150 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_301 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_226 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_151 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_302 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_227 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_303 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.9108 - accuracy: 0.5098\n",
      "Epoch 1: val_loss improved from inf to 0.69786, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 24ms/step - loss: 0.9149 - accuracy: 0.5057 - val_loss: 0.6979 - val_accuracy: 0.4479\n",
      "Epoch 2/100\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.7473 - accuracy: 0.6049\n",
      "Epoch 2: val_loss did not improve from 0.69786\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7520 - accuracy: 0.5977 - val_loss: 0.7004 - val_accuracy: 0.4479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7838 - accuracy: 0.5430\n",
      "Epoch 3: val_loss did not improve from 0.69786\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7810 - accuracy: 0.5441 - val_loss: 0.7014 - val_accuracy: 0.4479\n",
      "Epoch 4/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.7168 - accuracy: 0.5793\n",
      "Epoch 4: val_loss did not improve from 0.69786\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7256 - accuracy: 0.5766 - val_loss: 0.7023 - val_accuracy: 0.4479\n",
      "Epoch 5/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6841 - accuracy: 0.6120\n",
      "Epoch 5: val_loss did not improve from 0.69786\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6566 - accuracy: 0.6322 - val_loss: 0.7019 - val_accuracy: 0.4479\n",
      "Epoch 6/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5903 - accuracy: 0.6953\n",
      "Epoch 6: val_loss did not improve from 0.69786\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6047 - accuracy: 0.6877 - val_loss: 0.7004 - val_accuracy: 0.4479\n",
      "Epoch 7/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5871 - accuracy: 0.6797\n",
      "Epoch 7: val_loss improved from 0.69786 to 0.69755, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5861 - accuracy: 0.6916 - val_loss: 0.6975 - val_accuracy: 0.4601\n",
      "Epoch 8/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5154 - accuracy: 0.7578\n",
      "Epoch 8: val_loss improved from 0.69755 to 0.69551, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5116 - accuracy: 0.7605 - val_loss: 0.6955 - val_accuracy: 0.4908\n",
      "Epoch 9/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5733 - accuracy: 0.6953\n",
      "Epoch 9: val_loss improved from 0.69551 to 0.69368, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.5744 - accuracy: 0.6954 - val_loss: 0.6937 - val_accuracy: 0.4663\n",
      "Epoch 10/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5008 - accuracy: 0.7539\n",
      "Epoch 10: val_loss improved from 0.69368 to 0.69208, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4982 - accuracy: 0.7567 - val_loss: 0.6921 - val_accuracy: 0.5153\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4754 - accuracy: 0.7715\n",
      "Epoch 11: val_loss improved from 0.69208 to 0.69051, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4721 - accuracy: 0.7759 - val_loss: 0.6905 - val_accuracy: 0.5153\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4598 - accuracy: 0.7754\n",
      "Epoch 12: val_loss improved from 0.69051 to 0.68979, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4575 - accuracy: 0.7739 - val_loss: 0.6898 - val_accuracy: 0.5644\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4166 - accuracy: 0.7988\n",
      "Epoch 13: val_loss improved from 0.68979 to 0.68957, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_title_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.4139 - accuracy: 0.8008 - val_loss: 0.6896 - val_accuracy: 0.5583\n",
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3917 - accuracy: 0.8281\n",
      "Epoch 14: val_loss did not improve from 0.68957\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3923 - accuracy: 0.8276 - val_loss: 0.6897 - val_accuracy: 0.5644\n",
      "Epoch 15/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3842 - accuracy: 0.8413\n",
      "Epoch 15: val_loss did not improve from 0.68957\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3736 - accuracy: 0.8525 - val_loss: 0.6903 - val_accuracy: 0.5706\n",
      "Epoch 16/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3802 - accuracy: 0.8486\n",
      "Epoch 16: val_loss did not improve from 0.68957\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3872 - accuracy: 0.8391 - val_loss: 0.6920 - val_accuracy: 0.5706\n",
      "Epoch 17/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3648 - accuracy: 0.8341\n",
      "Epoch 17: val_loss did not improve from 0.68957\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3601 - accuracy: 0.8429 - val_loss: 0.6928 - val_accuracy: 0.5644\n",
      "Epoch 18/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3387 - accuracy: 0.8848\n",
      "Epoch 18: val_loss did not improve from 0.68957\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3368 - accuracy: 0.8870 - val_loss: 0.6939 - val_accuracy: 0.5337\n",
      "Epoch 19/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3379 - accuracy: 0.8809\n",
      "Epoch 19: val_loss did not improve from 0.68957\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3378 - accuracy: 0.8812 - val_loss: 0.6959 - val_accuracy: 0.5215\n",
      "Epoch 20/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3220 - accuracy: 0.8809\n",
      "Epoch 20: val_loss did not improve from 0.68957\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3243 - accuracy: 0.8774 - val_loss: 0.6976 - val_accuracy: 0.5092\n",
      "Epoch 21/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3049 - accuracy: 0.8894\n",
      "Epoch 21: val_loss did not improve from 0.68957\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3036 - accuracy: 0.8927 - val_loss: 0.7012 - val_accuracy: 0.5031\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3002 - accuracy: 0.8908\n",
      "Epoch 22: val_loss did not improve from 0.68957\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3002 - accuracy: 0.8908 - val_loss: 0.7029 - val_accuracy: 0.5031\n",
      "Epoch 23/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2438 - accuracy: 0.9349\n",
      "Epoch 23: val_loss did not improve from 0.68957\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2486 - accuracy: 0.9234 - val_loss: 0.7056 - val_accuracy: 0.5153\n",
      "Epoch 24/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2597 - accuracy: 0.9062\n",
      "Epoch 24: val_loss did not improve from 0.68957\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2498 - accuracy: 0.9157 - val_loss: 0.7088 - val_accuracy: 0.5215\n",
      "Epoch 25/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.2226 - accuracy: 0.9447\n",
      "Epoch 25: val_loss did not improve from 0.68957\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2350 - accuracy: 0.9330 - val_loss: 0.7128 - val_accuracy: 0.5337\n",
      "Epoch 26/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.2498 - accuracy: 0.9193\n",
      "Epoch 26: val_loss did not improve from 0.68957\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2507 - accuracy: 0.9138 - val_loss: 0.7184 - val_accuracy: 0.5337\n",
      "Epoch 27/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2266 - accuracy: 0.9238\n",
      "Epoch 27: val_loss did not improve from 0.68957\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2271 - accuracy: 0.9234 - val_loss: 0.7252 - val_accuracy: 0.5337\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2274 - accuracy: 0.9291Restoring model weights from the end of the best epoch: 13.\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.68957\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.2274 - accuracy: 0.9291 - val_loss: 0.7345 - val_accuracy: 0.5153\n",
      "Epoch 28: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.7576\n",
      "Test AUC for Layer 1: 0.5322\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_76\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_304 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_228 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_152 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_305 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_229 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_153 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_306 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_230 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_307 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.9567 - accuracy: 0.4812\n",
      "Epoch 1: val_loss improved from inf to 0.75362, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_title_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 21ms/step - loss: 0.9570 - accuracy: 0.4788 - val_loss: 0.7536 - val_accuracy: 0.1667\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.9001 - accuracy: 0.5208\n",
      "Epoch 2: val_loss did not improve from 0.75362\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.9036 - accuracy: 0.5197 - val_loss: 0.8069 - val_accuracy: 0.1667\n",
      "Epoch 3/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8482 - accuracy: 0.5060\n",
      "Epoch 3: val_loss did not improve from 0.75362\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.8524 - accuracy: 0.5080 - val_loss: 0.8624 - val_accuracy: 0.1667\n",
      "Epoch 4/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.7130 - accuracy: 0.6250\n",
      "Epoch 4: val_loss did not improve from 0.75362\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6871 - accuracy: 0.6350 - val_loss: 0.9015 - val_accuracy: 0.1667\n",
      "Epoch 5/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6827 - accuracy: 0.6324\n",
      "Epoch 5: val_loss did not improve from 0.75362\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6803 - accuracy: 0.6321 - val_loss: 0.9409 - val_accuracy: 0.1667\n",
      "Epoch 6/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6393 - accuracy: 0.6615\n",
      "Epoch 6: val_loss did not improve from 0.75362\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6501 - accuracy: 0.6482 - val_loss: 0.9789 - val_accuracy: 0.1667\n",
      "Epoch 7/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5945 - accuracy: 0.6823\n",
      "Epoch 7: val_loss did not improve from 0.75362\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6018 - accuracy: 0.6905 - val_loss: 1.0189 - val_accuracy: 0.1667\n",
      "Epoch 8/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5719 - accuracy: 0.6944\n",
      "Epoch 8: val_loss did not improve from 0.75362\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5635 - accuracy: 0.7022 - val_loss: 1.0475 - val_accuracy: 0.1667\n",
      "Epoch 9/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5593 - accuracy: 0.7205\n",
      "Epoch 9: val_loss did not improve from 0.75362\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5488 - accuracy: 0.7226 - val_loss: 1.0716 - val_accuracy: 0.1667\n",
      "Epoch 10/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4854 - accuracy: 0.7535\n",
      "Epoch 10: val_loss did not improve from 0.75362\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4813 - accuracy: 0.7562 - val_loss: 1.0912 - val_accuracy: 0.1667\n",
      "Epoch 11/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4924 - accuracy: 0.7517\n",
      "Epoch 11: val_loss did not improve from 0.75362\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5005 - accuracy: 0.7489 - val_loss: 1.0978 - val_accuracy: 0.1667\n",
      "Epoch 12/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4921 - accuracy: 0.7647\n",
      "Epoch 12: val_loss did not improve from 0.75362\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4864 - accuracy: 0.7679 - val_loss: 1.1083 - val_accuracy: 0.1667\n",
      "Epoch 13/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4287 - accuracy: 0.8141\n",
      "Epoch 13: val_loss did not improve from 0.75362\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4238 - accuracy: 0.8161 - val_loss: 1.1236 - val_accuracy: 0.1667\n",
      "Epoch 14/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4482 - accuracy: 0.8015\n",
      "Epoch 14: val_loss did not improve from 0.75362\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4452 - accuracy: 0.8015 - val_loss: 1.1311 - val_accuracy: 0.1667\n",
      "Epoch 15/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4110 - accuracy: 0.8108\n",
      "Epoch 15: val_loss did not improve from 0.75362\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4102 - accuracy: 0.8044 - val_loss: 1.1242 - val_accuracy: 0.1667\n",
      "Epoch 16/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3879 - accuracy: 0.8299Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.75362\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3858 - accuracy: 0.8292 - val_loss: 1.1329 - val_accuracy: 0.1667\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.3519\n",
      "Test AUC for Layer 2: 0.4180\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Title (L_label)\n",
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_308 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_231 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_154 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_309 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_232 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_155 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_310 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_233 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_311 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.9231 - accuracy: 0.4812\n",
      "Epoch 1: val_loss improved from inf to 0.72223, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_title_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 21ms/step - loss: 0.9212 - accuracy: 0.4820 - val_loss: 0.7222 - val_accuracy: 0.3519\n",
      "Epoch 2/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8314 - accuracy: 0.5327\n",
      "Epoch 2: val_loss did not improve from 0.72223\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.8248 - accuracy: 0.5340 - val_loss: 0.7465 - val_accuracy: 0.3519\n",
      "Epoch 3/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.7529 - accuracy: 0.5893\n",
      "Epoch 3: val_loss did not improve from 0.72223\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.7667 - accuracy: 0.5712 - val_loss: 0.7590 - val_accuracy: 0.3519\n",
      "Epoch 4/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.7116 - accuracy: 0.5982\n",
      "Epoch 4: val_loss did not improve from 0.72223\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.7144 - accuracy: 0.5979 - val_loss: 0.7664 - val_accuracy: 0.3519\n",
      "Epoch 5/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6995 - accuracy: 0.6033\n",
      "Epoch 5: val_loss did not improve from 0.72223\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6985 - accuracy: 0.6032 - val_loss: 0.7708 - val_accuracy: 0.3519\n",
      "Epoch 6/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6534 - accuracy: 0.6435\n",
      "Epoch 6: val_loss did not improve from 0.72223\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.6576 - accuracy: 0.6431 - val_loss: 0.7672 - val_accuracy: 0.3519\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5890 - accuracy: 0.6764\n",
      "Epoch 7: val_loss did not improve from 0.72223\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5890 - accuracy: 0.6764 - val_loss: 0.7592 - val_accuracy: 0.3519\n",
      "Epoch 8/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5433 - accuracy: 0.7303\n",
      "Epoch 8: val_loss did not improve from 0.72223\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5574 - accuracy: 0.7217 - val_loss: 0.7518 - val_accuracy: 0.3519\n",
      "Epoch 9/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5286 - accuracy: 0.7500\n",
      "Epoch 9: val_loss did not improve from 0.72223\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5183 - accuracy: 0.7577 - val_loss: 0.7441 - val_accuracy: 0.3611\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5140 - accuracy: 0.7563\n",
      "Epoch 10: val_loss did not improve from 0.72223\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5140 - accuracy: 0.7563 - val_loss: 0.7378 - val_accuracy: 0.3333\n",
      "Epoch 11/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4800 - accuracy: 0.7714\n",
      "Epoch 11: val_loss did not improve from 0.72223\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4873 - accuracy: 0.7723 - val_loss: 0.7307 - val_accuracy: 0.3519\n",
      "Epoch 12/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4779 - accuracy: 0.7977\n",
      "Epoch 12: val_loss did not improve from 0.72223\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4780 - accuracy: 0.7909 - val_loss: 0.7270 - val_accuracy: 0.3981\n",
      "Epoch 13/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4317 - accuracy: 0.8026\n",
      "Epoch 13: val_loss did not improve from 0.72223\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4448 - accuracy: 0.7949 - val_loss: 0.7243 - val_accuracy: 0.4444\n",
      "Epoch 14/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4350 - accuracy: 0.8092\n",
      "Epoch 14: val_loss did not improve from 0.72223\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4394 - accuracy: 0.8029 - val_loss: 0.7247 - val_accuracy: 0.4630\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3958 - accuracy: 0.8362\n",
      "Epoch 15: val_loss did not improve from 0.72223\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3958 - accuracy: 0.8362 - val_loss: 0.7264 - val_accuracy: 0.4722\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3793 - accuracy: 0.8469Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.72223\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3793 - accuracy: 0.8469 - val_loss: 0.7249 - val_accuracy: 0.4907\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Title_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5652\n",
      "Test AUC for Layer 3: 0.5897\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5582\n",
      "Average Test AUC across all layers: 0.5133\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_78\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_312 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_234 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_156 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_313 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_235 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_157 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_314 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_236 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_315 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.9079 - accuracy: 0.5312\n",
      "Epoch 1: val_loss improved from inf to 0.69371, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.9161 - accuracy: 0.5268 - val_loss: 0.6937 - val_accuracy: 0.4969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7959 - accuracy: 0.5645\n",
      "Epoch 2: val_loss did not improve from 0.69371\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7912 - accuracy: 0.5651 - val_loss: 0.6959 - val_accuracy: 0.4969\n",
      "Epoch 3/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.7523 - accuracy: 0.5792\n",
      "Epoch 3: val_loss did not improve from 0.69371\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7559 - accuracy: 0.5728 - val_loss: 0.6993 - val_accuracy: 0.4969\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7348 - accuracy: 0.6035\n",
      "Epoch 4: val_loss did not improve from 0.69371\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7319 - accuracy: 0.6034 - val_loss: 0.7022 - val_accuracy: 0.4969\n",
      "Epoch 5/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6355 - accuracy: 0.6641\n",
      "Epoch 5: val_loss did not improve from 0.69371\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6350 - accuracy: 0.6609 - val_loss: 0.7052 - val_accuracy: 0.4969\n",
      "Epoch 6/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6322 - accuracy: 0.6827\n",
      "Epoch 6: val_loss did not improve from 0.69371\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6229 - accuracy: 0.6992 - val_loss: 0.7082 - val_accuracy: 0.4969\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5778 - accuracy: 0.7241\n",
      "Epoch 7: val_loss did not improve from 0.69371\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5778 - accuracy: 0.7241 - val_loss: 0.7121 - val_accuracy: 0.4969\n",
      "Epoch 8/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5717 - accuracy: 0.6995\n",
      "Epoch 8: val_loss did not improve from 0.69371\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5600 - accuracy: 0.7031 - val_loss: 0.7156 - val_accuracy: 0.4969\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5886 - accuracy: 0.7011\n",
      "Epoch 9: val_loss did not improve from 0.69371\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5886 - accuracy: 0.7011 - val_loss: 0.7190 - val_accuracy: 0.4969\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4941 - accuracy: 0.7428\n",
      "Epoch 10: val_loss did not improve from 0.69371\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4872 - accuracy: 0.7548 - val_loss: 0.7213 - val_accuracy: 0.4969\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4692 - accuracy: 0.7695\n",
      "Epoch 11: val_loss did not improve from 0.69371\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4756 - accuracy: 0.7586 - val_loss: 0.7236 - val_accuracy: 0.4969\n",
      "Epoch 12/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4673 - accuracy: 0.7676\n",
      "Epoch 12: val_loss did not improve from 0.69371\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4714 - accuracy: 0.7644 - val_loss: 0.7262 - val_accuracy: 0.4969\n",
      "Epoch 13/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4114 - accuracy: 0.8341\n",
      "Epoch 13: val_loss did not improve from 0.69371\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4138 - accuracy: 0.8314 - val_loss: 0.7279 - val_accuracy: 0.4969\n",
      "Epoch 14/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4718 - accuracy: 0.7734\n",
      "Epoch 14: val_loss did not improve from 0.69371\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4561 - accuracy: 0.7835 - val_loss: 0.7336 - val_accuracy: 0.4969\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4430 - accuracy: 0.8066\n",
      "Epoch 15: val_loss did not improve from 0.69371\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4424 - accuracy: 0.8103 - val_loss: 0.7354 - val_accuracy: 0.4969\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4213 - accuracy: 0.8066Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69371\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4232 - accuracy: 0.8065 - val_loss: 0.7358 - val_accuracy: 0.4969\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.4545\n",
      "Test AUC for Layer 1: 0.5250\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_79\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_316 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_237 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_158 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_317 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_238 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_159 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_318 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_239 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_319 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.9183 - accuracy: 0.5015\n",
      "Epoch 1: val_loss improved from inf to 0.68905, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 21ms/step - loss: 0.9169 - accuracy: 0.5036 - val_loss: 0.6891 - val_accuracy: 0.5455\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8522 - accuracy: 0.5299\n",
      "Epoch 2: val_loss did not improve from 0.68905\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.8522 - accuracy: 0.5299 - val_loss: 0.6915 - val_accuracy: 0.5455\n",
      "Epoch 3/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.7430 - accuracy: 0.6111\n",
      "Epoch 3: val_loss did not improve from 0.68905\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7455 - accuracy: 0.6146 - val_loss: 0.6974 - val_accuracy: 0.5455\n",
      "Epoch 4/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.6826 - accuracy: 0.6268\n",
      "Epoch 4: val_loss did not improve from 0.68905\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6818 - accuracy: 0.6292 - val_loss: 0.7039 - val_accuracy: 0.5455\n",
      "Epoch 5/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6491 - accuracy: 0.6285\n",
      "Epoch 5: val_loss did not improve from 0.68905\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6563 - accuracy: 0.6248 - val_loss: 0.7122 - val_accuracy: 0.5455\n",
      "Epoch 6/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6530 - accuracy: 0.6389\n",
      "Epoch 6: val_loss did not improve from 0.68905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6601 - accuracy: 0.6365 - val_loss: 0.7195 - val_accuracy: 0.5455\n",
      "Epoch 7/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6185 - accuracy: 0.6649\n",
      "Epoch 7: val_loss did not improve from 0.68905\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6369 - accuracy: 0.6569 - val_loss: 0.7246 - val_accuracy: 0.5455\n",
      "Epoch 8/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5772 - accuracy: 0.6908\n",
      "Epoch 8: val_loss did not improve from 0.68905\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5765 - accuracy: 0.6964 - val_loss: 0.7284 - val_accuracy: 0.5455\n",
      "Epoch 9/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5583 - accuracy: 0.7089\n",
      "Epoch 9: val_loss did not improve from 0.68905\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5516 - accuracy: 0.7153 - val_loss: 0.7329 - val_accuracy: 0.5455\n",
      "Epoch 10/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5099 - accuracy: 0.7483\n",
      "Epoch 10: val_loss did not improve from 0.68905\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5065 - accuracy: 0.7518 - val_loss: 0.7384 - val_accuracy: 0.5455\n",
      "Epoch 11/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5045 - accuracy: 0.7396\n",
      "Epoch 11: val_loss did not improve from 0.68905\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5137 - accuracy: 0.7358 - val_loss: 0.7419 - val_accuracy: 0.5455\n",
      "Epoch 12/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5329 - accuracy: 0.7426\n",
      "Epoch 12: val_loss did not improve from 0.68905\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5260 - accuracy: 0.7416 - val_loss: 0.7434 - val_accuracy: 0.5455\n",
      "Epoch 13/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4762 - accuracy: 0.7760\n",
      "Epoch 13: val_loss did not improve from 0.68905\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4766 - accuracy: 0.7679 - val_loss: 0.7483 - val_accuracy: 0.5455\n",
      "Epoch 14/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4765 - accuracy: 0.7639\n",
      "Epoch 14: val_loss did not improve from 0.68905\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4782 - accuracy: 0.7650 - val_loss: 0.7536 - val_accuracy: 0.5455\n",
      "Epoch 15/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4631 - accuracy: 0.7778\n",
      "Epoch 15: val_loss did not improve from 0.68905\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4533 - accuracy: 0.7825 - val_loss: 0.7560 - val_accuracy: 0.5455\n",
      "Epoch 16/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3985 - accuracy: 0.8355Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.68905\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3998 - accuracy: 0.8350 - val_loss: 0.7654 - val_accuracy: 0.5455\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5278\n",
      "Test AUC for Layer 2: 0.2907\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (S_label)\n",
      "Model: \"sequential_80\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_320 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_240 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_160 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_321 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_241 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_161 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_322 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_242 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_323 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.9146 - accuracy: 0.5014\n",
      "Epoch 1: val_loss improved from inf to 0.70678, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.9113 - accuracy: 0.5033 - val_loss: 0.7068 - val_accuracy: 0.4722\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.7796 - accuracy: 0.5593\n",
      "Epoch 2: val_loss did not improve from 0.70678\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7796 - accuracy: 0.5593 - val_loss: 0.7243 - val_accuracy: 0.4722\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.7724 - accuracy: 0.5659\n",
      "Epoch 3: val_loss did not improve from 0.70678\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.7724 - accuracy: 0.5659 - val_loss: 0.7435 - val_accuracy: 0.4722\n",
      "Epoch 4/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.7229 - accuracy: 0.6236\n",
      "Epoch 4: val_loss did not improve from 0.70678\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.7215 - accuracy: 0.6245 - val_loss: 0.7615 - val_accuracy: 0.4722\n",
      "Epoch 5/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6652 - accuracy: 0.6414\n",
      "Epoch 5: val_loss did not improve from 0.70678\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6734 - accuracy: 0.6378 - val_loss: 0.7801 - val_accuracy: 0.4722\n",
      "Epoch 6/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6544 - accuracy: 0.6480\n",
      "Epoch 6: val_loss did not improve from 0.70678\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.6404 - accuracy: 0.6511 - val_loss: 0.7890 - val_accuracy: 0.4722\n",
      "Epoch 7/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6103 - accuracy: 0.6780\n",
      "Epoch 7: val_loss did not improve from 0.70678\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6123 - accuracy: 0.6751 - val_loss: 0.8004 - val_accuracy: 0.4722\n",
      "Epoch 8/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6109 - accuracy: 0.6612\n",
      "Epoch 8: val_loss did not improve from 0.70678\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5943 - accuracy: 0.6791 - val_loss: 0.8085 - val_accuracy: 0.4722\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5783 - accuracy: 0.6964\n",
      "Epoch 9: val_loss did not improve from 0.70678\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5783 - accuracy: 0.6964 - val_loss: 0.8198 - val_accuracy: 0.4722\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5307 - accuracy: 0.7377\n",
      "Epoch 10: val_loss did not improve from 0.70678\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5307 - accuracy: 0.7377 - val_loss: 0.8308 - val_accuracy: 0.4722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5181 - accuracy: 0.7269\n",
      "Epoch 11: val_loss did not improve from 0.70678\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5192 - accuracy: 0.7270 - val_loss: 0.8280 - val_accuracy: 0.4722\n",
      "Epoch 12/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5018 - accuracy: 0.7319\n",
      "Epoch 12: val_loss did not improve from 0.70678\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5080 - accuracy: 0.7297 - val_loss: 0.8401 - val_accuracy: 0.4722\n",
      "Epoch 13/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4783 - accuracy: 0.7841\n",
      "Epoch 13: val_loss did not improve from 0.70678\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4741 - accuracy: 0.7856 - val_loss: 0.8468 - val_accuracy: 0.4722\n",
      "Epoch 14/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4839 - accuracy: 0.7615\n",
      "Epoch 14: val_loss did not improve from 0.70678\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4863 - accuracy: 0.7643 - val_loss: 0.8555 - val_accuracy: 0.4722\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4356 - accuracy: 0.7923\n",
      "Epoch 15: val_loss did not improve from 0.70678\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4356 - accuracy: 0.7923 - val_loss: 0.8533 - val_accuracy: 0.4722\n",
      "Epoch 16/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4382 - accuracy: 0.7977Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70678\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4462 - accuracy: 0.7963 - val_loss: 0.8517 - val_accuracy: 0.4722\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5507\n",
      "Test AUC for Layer 3: 0.4516\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5110\n",
      "Average Test AUC across all layers: 0.4224\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Full text and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_324 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_243 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_162 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_325 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_244 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_163 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_326 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_245 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_327 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.9017 - accuracy: 0.4883\n",
      "Epoch 1: val_loss improved from inf to 0.69002, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8997 - accuracy: 0.4923 - val_loss: 0.6900 - val_accuracy: 0.5521\n",
      "Epoch 2/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8042 - accuracy: 0.5391\n",
      "Epoch 2: val_loss improved from 0.69002 to 0.68811, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.8035 - accuracy: 0.5421 - val_loss: 0.6881 - val_accuracy: 0.5521\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7451 - accuracy: 0.5781\n",
      "Epoch 3: val_loss improved from 0.68811 to 0.68729, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7439 - accuracy: 0.5805 - val_loss: 0.6873 - val_accuracy: 0.5521\n",
      "Epoch 4/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6717 - accuracy: 0.6426\n",
      "Epoch 4: val_loss did not improve from 0.68729\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6721 - accuracy: 0.6437 - val_loss: 0.6878 - val_accuracy: 0.5521\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6485 - accuracy: 0.6367\n",
      "Epoch 5: val_loss did not improve from 0.68729\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6442 - accuracy: 0.6398 - val_loss: 0.6893 - val_accuracy: 0.5521\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6224 - accuracy: 0.6782\n",
      "Epoch 6: val_loss did not improve from 0.68729\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6224 - accuracy: 0.6782 - val_loss: 0.6915 - val_accuracy: 0.5521\n",
      "Epoch 7/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5966 - accuracy: 0.6901\n",
      "Epoch 7: val_loss did not improve from 0.68729\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6205 - accuracy: 0.6724 - val_loss: 0.6947 - val_accuracy: 0.5521\n",
      "Epoch 8/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5629 - accuracy: 0.7057\n",
      "Epoch 8: val_loss did not improve from 0.68729\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5720 - accuracy: 0.7011 - val_loss: 0.6981 - val_accuracy: 0.5521\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5465 - accuracy: 0.7184\n",
      "Epoch 9: val_loss did not improve from 0.68729\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5465 - accuracy: 0.7184 - val_loss: 0.7025 - val_accuracy: 0.5521\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5197 - accuracy: 0.7236\n",
      "Epoch 10: val_loss did not improve from 0.68729\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5211 - accuracy: 0.7299 - val_loss: 0.7078 - val_accuracy: 0.5521\n",
      "Epoch 11/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4987 - accuracy: 0.7676\n",
      "Epoch 11: val_loss did not improve from 0.68729\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4959 - accuracy: 0.7701 - val_loss: 0.7153 - val_accuracy: 0.5521\n",
      "Epoch 12/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4752 - accuracy: 0.7969\n",
      "Epoch 12: val_loss did not improve from 0.68729\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4769 - accuracy: 0.7950 - val_loss: 0.7202 - val_accuracy: 0.5521\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4632 - accuracy: 0.7871\n",
      "Epoch 13: val_loss did not improve from 0.68729\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4615 - accuracy: 0.7912 - val_loss: 0.7253 - val_accuracy: 0.5521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4329 - accuracy: 0.8105\n",
      "Epoch 14: val_loss did not improve from 0.68729\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4306 - accuracy: 0.8123 - val_loss: 0.7326 - val_accuracy: 0.5521\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4473 - accuracy: 0.8008\n",
      "Epoch 15: val_loss did not improve from 0.68729\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4493 - accuracy: 0.7989 - val_loss: 0.7381 - val_accuracy: 0.5521\n",
      "Epoch 16/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4212 - accuracy: 0.8105\n",
      "Epoch 16: val_loss did not improve from 0.68729\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4190 - accuracy: 0.8123 - val_loss: 0.7443 - val_accuracy: 0.5521\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4138 - accuracy: 0.8257\n",
      "Epoch 17: val_loss did not improve from 0.68729\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4138 - accuracy: 0.8257 - val_loss: 0.7525 - val_accuracy: 0.5521\n",
      "Epoch 18/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3789 - accuracy: 0.8464Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.68729\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3725 - accuracy: 0.8467 - val_loss: 0.7561 - val_accuracy: 0.5521\n",
      "Epoch 18: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.8333\n",
      "Test AUC for Layer 1: 0.3521\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_82\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_328 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_246 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_164 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_329 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_247 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_165 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_330 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_248 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_331 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8859 - accuracy: 0.4970\n",
      "Epoch 1: val_loss improved from inf to 0.72310, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 21ms/step - loss: 0.8827 - accuracy: 0.4978 - val_loss: 0.7231 - val_accuracy: 0.1667\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7902 - accuracy: 0.5521\n",
      "Epoch 2: val_loss did not improve from 0.72310\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7838 - accuracy: 0.5577 - val_loss: 0.7532 - val_accuracy: 0.1667\n",
      "Epoch 3/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7399 - accuracy: 0.5908\n",
      "Epoch 3: val_loss did not improve from 0.72310\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.7413 - accuracy: 0.5898 - val_loss: 0.7842 - val_accuracy: 0.1667\n",
      "Epoch 4/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.7447 - accuracy: 0.6024\n",
      "Epoch 4: val_loss did not improve from 0.72310\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7296 - accuracy: 0.6073 - val_loss: 0.8114 - val_accuracy: 0.1667\n",
      "Epoch 5/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6906 - accuracy: 0.6283\n",
      "Epoch 5: val_loss did not improve from 0.72310\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6995 - accuracy: 0.6219 - val_loss: 0.8406 - val_accuracy: 0.1667\n",
      "Epoch 6/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6246 - accuracy: 0.6711\n",
      "Epoch 6: val_loss did not improve from 0.72310\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6217 - accuracy: 0.6686 - val_loss: 0.8699 - val_accuracy: 0.1667\n",
      "Epoch 7/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6088 - accuracy: 0.6694\n",
      "Epoch 7: val_loss did not improve from 0.72310\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6117 - accuracy: 0.6672 - val_loss: 0.8964 - val_accuracy: 0.1667\n",
      "Epoch 8/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5808 - accuracy: 0.6962\n",
      "Epoch 8: val_loss did not improve from 0.72310\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5929 - accuracy: 0.6905 - val_loss: 0.9248 - val_accuracy: 0.1667\n",
      "Epoch 9/100\n",
      "20/22 [==========================>...] - ETA: 0s - loss: 0.5472 - accuracy: 0.7250\n",
      "Epoch 9: val_loss did not improve from 0.72310\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.5541 - accuracy: 0.7197 - val_loss: 0.9504 - val_accuracy: 0.1667\n",
      "Epoch 10/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5233 - accuracy: 0.7401\n",
      "Epoch 10: val_loss did not improve from 0.72310\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5203 - accuracy: 0.7416 - val_loss: 0.9790 - val_accuracy: 0.1667\n",
      "Epoch 11/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.5198 - accuracy: 0.7253\n",
      "Epoch 11: val_loss did not improve from 0.72310\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5198 - accuracy: 0.7285 - val_loss: 1.0067 - val_accuracy: 0.1667\n",
      "Epoch 12/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4897 - accuracy: 0.7862\n",
      "Epoch 12: val_loss did not improve from 0.72310\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4845 - accuracy: 0.7869 - val_loss: 1.0349 - val_accuracy: 0.1667\n",
      "Epoch 13/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4720 - accuracy: 0.7516\n",
      "Epoch 13: val_loss did not improve from 0.72310\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4704 - accuracy: 0.7547 - val_loss: 1.0499 - val_accuracy: 0.1667\n",
      "Epoch 14/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4727 - accuracy: 0.7865\n",
      "Epoch 14: val_loss did not improve from 0.72310\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4804 - accuracy: 0.7752 - val_loss: 1.0664 - val_accuracy: 0.1667\n",
      "Epoch 15/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4483 - accuracy: 0.7993\n",
      "Epoch 15: val_loss did not improve from 0.72310\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4502 - accuracy: 0.7971 - val_loss: 1.0827 - val_accuracy: 0.1667\n",
      "Epoch 16/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4294 - accuracy: 0.7845Restoring model weights from the end of the best epoch: 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: val_loss did not improve from 0.72310\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4279 - accuracy: 0.7854 - val_loss: 1.0947 - val_accuracy: 0.1667\n",
      "Epoch 16: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.3519\n",
      "Test AUC for Layer 2: 0.4929\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Full text (L_label)\n",
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_332 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_249 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_166 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_333 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_250 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_167 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_334 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_251 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_335 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.9090 - accuracy: 0.5312\n",
      "Epoch 1: val_loss improved from inf to 0.68436, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.9112 - accuracy: 0.5300 - val_loss: 0.6844 - val_accuracy: 0.6481\n",
      "Epoch 2/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.8485 - accuracy: 0.5284\n",
      "Epoch 2: val_loss improved from 0.68436 to 0.67832, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.8551 - accuracy: 0.5233 - val_loss: 0.6783 - val_accuracy: 0.6481\n",
      "Epoch 3/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.7190 - accuracy: 0.5910\n",
      "Epoch 3: val_loss improved from 0.67832 to 0.67213, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.7244 - accuracy: 0.5899 - val_loss: 0.6721 - val_accuracy: 0.6481\n",
      "Epoch 4/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.7006 - accuracy: 0.6128\n",
      "Epoch 4: val_loss improved from 0.67213 to 0.66710, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6961 - accuracy: 0.6152 - val_loss: 0.6671 - val_accuracy: 0.6481\n",
      "Epoch 5/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6673 - accuracy: 0.6209\n",
      "Epoch 5: val_loss improved from 0.66710 to 0.66233, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6674 - accuracy: 0.6232 - val_loss: 0.6623 - val_accuracy: 0.6481\n",
      "Epoch 6/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6787 - accuracy: 0.6576\n",
      "Epoch 6: val_loss improved from 0.66233 to 0.65789, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6744 - accuracy: 0.6591 - val_loss: 0.6579 - val_accuracy: 0.6481\n",
      "Epoch 7/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.5886 - accuracy: 0.7102\n",
      "Epoch 7: val_loss improved from 0.65789 to 0.65580, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.5901 - accuracy: 0.7031 - val_loss: 0.6558 - val_accuracy: 0.6481\n",
      "Epoch 8/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5910 - accuracy: 0.6957\n",
      "Epoch 8: val_loss improved from 0.65580 to 0.65305, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.5926 - accuracy: 0.6964 - val_loss: 0.6531 - val_accuracy: 0.6481\n",
      "Epoch 9/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5538 - accuracy: 0.7242\n",
      "Epoch 9: val_loss improved from 0.65305 to 0.65154, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.5503 - accuracy: 0.7244 - val_loss: 0.6515 - val_accuracy: 0.6481\n",
      "Epoch 10/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5456 - accuracy: 0.7054\n",
      "Epoch 10: val_loss did not improve from 0.65154\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5609 - accuracy: 0.6897 - val_loss: 0.6517 - val_accuracy: 0.6481\n",
      "Epoch 11/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4974 - accuracy: 0.7323\n",
      "Epoch 11: val_loss improved from 0.65154 to 0.64943, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.4973 - accuracy: 0.7350 - val_loss: 0.6494 - val_accuracy: 0.6481\n",
      "Epoch 12/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4830 - accuracy: 0.7685\n",
      "Epoch 12: val_loss improved from 0.64943 to 0.64911, saving model to OpenAI_MLP/visualizations_mlp/COP\\best_mlp_full_text_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.4834 - accuracy: 0.7696 - val_loss: 0.6491 - val_accuracy: 0.6574\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4578 - accuracy: 0.7830\n",
      "Epoch 13: val_loss did not improve from 0.64911\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4578 - accuracy: 0.7830 - val_loss: 0.6495 - val_accuracy: 0.6574\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4540 - accuracy: 0.7856\n",
      "Epoch 14: val_loss did not improve from 0.64911\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4540 - accuracy: 0.7856 - val_loss: 0.6496 - val_accuracy: 0.6574\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4420 - accuracy: 0.8069\n",
      "Epoch 15: val_loss did not improve from 0.64911\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4420 - accuracy: 0.8069 - val_loss: 0.6492 - val_accuracy: 0.6574\n",
      "Epoch 16/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4157 - accuracy: 0.7993\n",
      "Epoch 16: val_loss did not improve from 0.64911\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4254 - accuracy: 0.7963 - val_loss: 0.6496 - val_accuracy: 0.6759\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4263 - accuracy: 0.8016\n",
      "Epoch 17: val_loss did not improve from 0.64911\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4281 - accuracy: 0.8003 - val_loss: 0.6520 - val_accuracy: 0.6667\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4240 - accuracy: 0.8123\n",
      "Epoch 18: val_loss did not improve from 0.64911\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4240 - accuracy: 0.8123 - val_loss: 0.6544 - val_accuracy: 0.6574\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3725 - accuracy: 0.8349\n",
      "Epoch 19: val_loss did not improve from 0.64911\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3725 - accuracy: 0.8349 - val_loss: 0.6615 - val_accuracy: 0.6481\n",
      "Epoch 20/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3542 - accuracy: 0.8388\n",
      "Epoch 20: val_loss did not improve from 0.64911\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3673 - accuracy: 0.8349 - val_loss: 0.6628 - val_accuracy: 0.6389\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3521 - accuracy: 0.8602\n",
      "Epoch 21: val_loss did not improve from 0.64911\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3521 - accuracy: 0.8602 - val_loss: 0.6739 - val_accuracy: 0.6111\n",
      "Epoch 22/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3254 - accuracy: 0.8618\n",
      "Epoch 22: val_loss did not improve from 0.64911\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3286 - accuracy: 0.8602 - val_loss: 0.6769 - val_accuracy: 0.6019\n",
      "Epoch 23/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3530 - accuracy: 0.8520\n",
      "Epoch 23: val_loss did not improve from 0.64911\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3449 - accuracy: 0.8602 - val_loss: 0.6893 - val_accuracy: 0.6111\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3007 - accuracy: 0.8788\n",
      "Epoch 24: val_loss did not improve from 0.64911\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3007 - accuracy: 0.8788 - val_loss: 0.7005 - val_accuracy: 0.5648\n",
      "Epoch 25/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3144 - accuracy: 0.8816\n",
      "Epoch 25: val_loss did not improve from 0.64911\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3170 - accuracy: 0.8828 - val_loss: 0.7206 - val_accuracy: 0.5648\n",
      "Epoch 26/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2796 - accuracy: 0.8947\n",
      "Epoch 26: val_loss did not improve from 0.64911\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2715 - accuracy: 0.9015 - val_loss: 0.7499 - val_accuracy: 0.5648\n",
      "Epoch 27/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2771 - accuracy: 0.9030Restoring model weights from the end of the best epoch: 12.\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.64911\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2794 - accuracy: 0.9055 - val_loss: 0.7611 - val_accuracy: 0.5741\n",
      "Epoch 27: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_Full_text_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4348\n",
      "Test AUC for Layer 3: 0.3991\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5400\n",
      "Average Test AUC across all layers: 0.4147\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS COP)\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Title + S_label\n",
      "Average Accuracy: 0.5110\n",
      "Average AUC: 0.5345\n",
      "  Layer 1 - Accuracy: 0.4545, AUC: 0.5491\n",
      "  Layer 2 - Accuracy: 0.5278, AUC: 0.5944\n",
      "  Layer 3 - Accuracy: 0.5507, AUC: 0.4601\n",
      "\n",
      "Combination: MLP with Title + L_label\n",
      "Average Accuracy: 0.5582\n",
      "Average AUC: 0.5133\n",
      "  Layer 1 - Accuracy: 0.7576, AUC: 0.5322\n",
      "  Layer 2 - Accuracy: 0.3519, AUC: 0.4180\n",
      "  Layer 3 - Accuracy: 0.5652, AUC: 0.5897\n",
      "\n",
      "Combination: MLP with Full text + S_label\n",
      "Average Accuracy: 0.5110\n",
      "Average AUC: 0.4224\n",
      "  Layer 1 - Accuracy: 0.4545, AUC: 0.5250\n",
      "  Layer 2 - Accuracy: 0.5278, AUC: 0.2907\n",
      "  Layer 3 - Accuracy: 0.5507, AUC: 0.4516\n",
      "\n",
      "Combination: MLP with Full text + L_label\n",
      "Average Accuracy: 0.5400\n",
      "Average AUC: 0.4147\n",
      "  Layer 1 - Accuracy: 0.8333, AUC: 0.3521\n",
      "  Layer 2 - Accuracy: 0.3519, AUC: 0.4929\n",
      "  Layer 3 - Accuracy: 0.4348, AUC: 0.3991\n",
      "MLP summary comparison visualization saved as: OpenAI_MLP/visualizations_summary/COP\\mlp_performance_comparison.png\n",
      "MLP layer performance visualization saved as: OpenAI_MLP/visualizations_summary/COP\\mlp_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'OpenAI_MLP' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "from xgboost import callback\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, model_type, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class ClimateNewsOpenAIPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'OpenAI_MLP/visualizations_mlp/COP'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs('OpenAI_MLP/visualizations_summary/COP', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert embeddings to numpy arrays\n",
    "        self.data['Title_embedding'] = self.data['Title_embedding_vector'].apply(parse_embedding)\n",
    "        self.data['Fulltext_embedding'] = self.data['Full_text_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_title_embedding = self.data['Title_embedding'].iloc[0]\n",
    "        sample_fulltext_embedding = self.data['Fulltext_embedding'].iloc[0]\n",
    "        \n",
    "        print(f\"Sample Title embedding shape: {sample_title_embedding.shape}\")\n",
    "        print(f\"Sample Fulltext embedding shape: {sample_fulltext_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2019-2021), validation (2021-2021), test (2022-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-05-31'),\n",
    "            'val_start': pd.Timestamp('2021-06-01'),\n",
    "            'val_end': pd.Timestamp('2021-12-31'),\n",
    "            'test_start': pd.Timestamp('2022-01-01'),\n",
    "            'test_end': pd.Timestamp('2022-05-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2019-2021), validation (2022-2022), test (2022-2022)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-12-31'),\n",
    "            'val_start': pd.Timestamp('2022-01-01'),\n",
    "            'val_end': pd.Timestamp('2022-05-31'),\n",
    "            'test_start': pd.Timestamp('2022-06-01'),\n",
    "            'test_end': pd.Timestamp('2022-12-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2019-2022), validation (2022-2022), test (2023-2023)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2022-05-31'),\n",
    "            'val_start': pd.Timestamp('2022-06-01'),\n",
    "            'val_end': pd.Timestamp('2022-12-31'),\n",
    "            'test_start': pd.Timestamp('2023-01-01'),\n",
    "            'test_end': pd.Timestamp('2023-05-31')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, text_col, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            text_col: The column containing the embeddings ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data[text_col].values)\n",
    "        X_val = np.stack(val_data[text_col].values)\n",
    "        X_test = np.stack(test_data[text_col].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, model_type, text_col, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            model_type: Model type (MLP)\n",
    "            text_col: Text column used\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: {model_type} with {display_text} ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_{model_type.lower()}_{display_text.replace(' ', '_')}_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, text_col, label_col, model_type):\n",
    "        \"\"\"\n",
    "        Train and evaluate a model for a specific text column and label column.\n",
    "        \n",
    "        Args:\n",
    "            text_col: The embedding column to use ('Title_embedding' or 'Fulltext_embedding')\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "            model_type: The model type to use ('MLP')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        display_text = 'Title' if 'Title' in text_col else 'Full text'\n",
    "        combination_key = f\"{model_type}|{display_text}|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training {model_type} model for {display_text} and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        visualization_dir = self.mlp_viz_dir if model_type == 'MLP' else self.xgb_viz_dir\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, text_col, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            if model_type == 'MLP':\n",
    "                # MLP model training\n",
    "                model = self.create_mlp_model((1536,))\n",
    "                print(f\"Created MLP model for {display_text} ({label_col})\")\n",
    "                model.summary()\n",
    "                \n",
    "                # Setup callbacks\n",
    "                plot_callback = PlotLearningCallback(model_type, display_text, label_col, i+1, visualization_dir)\n",
    "                early_stopping = EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=15,\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                model_checkpoint = ModelCheckpoint(\n",
    "                    filepath=f\"{visualization_dir}/best_{model_type.lower()}_{display_text.lower().replace(' ', '_')}_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                    monitor='val_loss',\n",
    "                    save_weights_only=True,\n",
    "                    save_best_only=True,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Train model\n",
    "                print(f\"Training MLP model...\")\n",
    "                batch_size = 32  # Larger batch size for embeddings\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                y_pred_proba = model.predict(X_test)\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                # Create final learning curve visualization\n",
    "                self.plot_final_learning_curves(history, model_type, text_col, label_col, i+1, visualization_dir)\n",
    "                \n",
    "                # Store training history\n",
    "                training_history = history.history\n",
    "                \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'model_type': model_type,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': training_history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for all combinations of text inputs, label columns, and model types.\"\"\"\n",
    "        # Define all combinations\n",
    "        embedding_cols = ['Title_embedding', 'Fulltext_embedding']\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        model_types = ['MLP']\n",
    "        \n",
    "        # Run analysis for each combination\n",
    "        for model_type in model_types:\n",
    "            for embedding_col in embedding_cols:\n",
    "                for label_col in label_cols:\n",
    "                    self.train_and_evaluate_model(embedding_col, label_col, model_type)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS COP)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Model': model_type,\n",
    "                'Text': text_col,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing all model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Split by model type\n",
    "        mlp_data = df[df['Model'] == 'MLP']\n",
    "        \n",
    "        # 1. Performance comparison for MLP\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot accuracy and AUC bars for MLP\n",
    "        x = np.arange(len(mlp_data))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, mlp_data['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, mlp_data['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('MLP Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with OpenAI Embeddings')\n",
    "        labels = [f\"{row['Text']} + {row['Label']}\" for _, row in mlp_data.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(mlp_data['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(mlp_data['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(mlp_data['Avg Accuracy'].max(), mlp_data['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP/visualizations_summary/COP', \"mlp_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        mlp_layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Model': model_type,\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                if model_type == 'MLP':\n",
    "                    mlp_layer_data.append(layer_info)\n",
    "        \n",
    "        # Create visualizations for each model type\n",
    "        self._create_model_layer_visualization(mlp_layer_data, 'MLP')\n",
    "    \n",
    "    def _create_model_layer_visualization(self, layer_data, model_type):\n",
    "        \"\"\"Create layer-specific visualizations for a given model type.\"\"\"\n",
    "        if not layer_data:\n",
    "            print(f\"No layer data available for {model_type}\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} Accuracy by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title(f'{model_type} AUC by Model Combination and Layer with OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('OpenAI_MLP/visualizations_summary/COP', f\"{model_type.lower()}_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"{model_type} layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['OpenAI_MLP/visualizations_mlp/COP',\n",
    "                      'OpenAI_MLP/visualizations_summary/COP']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/US_news/SP500_semantic/us_news_semantics_COP_completed_openai.csv'\n",
    "    \n",
    "    # Initialize the predictor with OpenAI embeddings\n",
    "    predictor = ClimateNewsOpenAIPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'OpenAI_MLP' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Automatic date parsing failed. Trying manual conversion...\n",
      "Parsing merged OpenAI embedding vectors from string format...\n",
      "Sample Merged embedding shape: (1536,)\n",
      "Loaded 929 climate change news articles spanning from 02/01/2019 to 07/05/2023\n",
      "Class distribution for short-term prediction: {0: 469, 1: 460}\n",
      "Class distribution for long-term prediction: {1: 504, 0: 425}\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and S_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_84\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_336 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_252 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_168 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_337 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_253 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_169 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_338 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_254 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_339 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.8120 - accuracy: 0.5312\n",
      "Epoch 1: val_loss improved from inf to 0.69516, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_S_label_layer_1.weights.h5\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.8090 - accuracy: 0.5307 - val_loss: 0.6952 - val_accuracy: 0.5031\n",
      "Epoch 2/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.7980 - accuracy: 0.5521\n",
      "Epoch 2: val_loss did not improve from 0.69516\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7942 - accuracy: 0.5517 - val_loss: 0.7023 - val_accuracy: 0.5031\n",
      "Epoch 3/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7244 - accuracy: 0.5879\n",
      "Epoch 3: val_loss did not improve from 0.69516\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.7237 - accuracy: 0.5900 - val_loss: 0.7131 - val_accuracy: 0.5031\n",
      "Epoch 4/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6506 - accuracy: 0.6380\n",
      "Epoch 4: val_loss did not improve from 0.69516\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6755 - accuracy: 0.6341 - val_loss: 0.7285 - val_accuracy: 0.5031\n",
      "Epoch 5/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6599 - accuracy: 0.6211\n",
      "Epoch 5: val_loss did not improve from 0.69516\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6595 - accuracy: 0.6245 - val_loss: 0.7453 - val_accuracy: 0.5031\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5929 - accuracy: 0.6897\n",
      "Epoch 6: val_loss did not improve from 0.69516\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5929 - accuracy: 0.6897 - val_loss: 0.7631 - val_accuracy: 0.5031\n",
      "Epoch 7/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5606 - accuracy: 0.6927\n",
      "Epoch 7: val_loss did not improve from 0.69516\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5574 - accuracy: 0.6992 - val_loss: 0.7830 - val_accuracy: 0.5031\n",
      "Epoch 8/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5854 - accuracy: 0.7135\n",
      "Epoch 8: val_loss did not improve from 0.69516\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5682 - accuracy: 0.7069 - val_loss: 0.8029 - val_accuracy: 0.5031\n",
      "Epoch 9/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4822 - accuracy: 0.7760\n",
      "Epoch 9: val_loss did not improve from 0.69516\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5110 - accuracy: 0.7510 - val_loss: 0.8244 - val_accuracy: 0.5031\n",
      "Epoch 10/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5239 - accuracy: 0.7091\n",
      "Epoch 10: val_loss did not improve from 0.69516\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5152 - accuracy: 0.7280 - val_loss: 0.8433 - val_accuracy: 0.5031\n",
      "Epoch 11/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5686 - accuracy: 0.7188\n",
      "Epoch 11: val_loss did not improve from 0.69516\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5475 - accuracy: 0.7318 - val_loss: 0.8595 - val_accuracy: 0.5031\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4748 - accuracy: 0.7931\n",
      "Epoch 12: val_loss did not improve from 0.69516\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4748 - accuracy: 0.7931 - val_loss: 0.8785 - val_accuracy: 0.5031\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4528 - accuracy: 0.7871\n",
      "Epoch 13: val_loss did not improve from 0.69516\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4524 - accuracy: 0.7874 - val_loss: 0.9010 - val_accuracy: 0.5031\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4112 - accuracy: 0.8199\n",
      "Epoch 14: val_loss did not improve from 0.69516\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4112 - accuracy: 0.8199 - val_loss: 0.9208 - val_accuracy: 0.5031\n",
      "Epoch 15/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4119 - accuracy: 0.8073\n",
      "Epoch 15: val_loss did not improve from 0.69516\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4000 - accuracy: 0.8295 - val_loss: 0.9380 - val_accuracy: 0.5031\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4090 - accuracy: 0.8008Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69516\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4090 - accuracy: 0.8008 - val_loss: 0.9544 - val_accuracy: 0.5031\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.5455\n",
      "Test AUC for Layer 1: 0.5324\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_85\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_340 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_255 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_170 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_341 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_256 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_171 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_342 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_257 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_343 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8903 - accuracy: 0.4940\n",
      "Epoch 1: val_loss improved from inf to 0.69018, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 21ms/step - loss: 0.8966 - accuracy: 0.4949 - val_loss: 0.6902 - val_accuracy: 0.5455\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7660 - accuracy: 0.5723\n",
      "Epoch 2: val_loss improved from 0.69018 to 0.68870, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_S_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.7660 - accuracy: 0.5723 - val_loss: 0.6887 - val_accuracy: 0.5455\n",
      "Epoch 3/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7523 - accuracy: 0.5759\n",
      "Epoch 3: val_loss did not improve from 0.68870\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7481 - accuracy: 0.5766 - val_loss: 0.6888 - val_accuracy: 0.5455\n",
      "Epoch 4/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6891 - accuracy: 0.6168\n",
      "Epoch 4: val_loss did not improve from 0.68870\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6875 - accuracy: 0.6088 - val_loss: 0.6891 - val_accuracy: 0.5455\n",
      "Epoch 5/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.6410 - accuracy: 0.6232\n",
      "Epoch 5: val_loss did not improve from 0.68870\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6365 - accuracy: 0.6350 - val_loss: 0.6900 - val_accuracy: 0.5455\n",
      "Epoch 6/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.6258 - accuracy: 0.6595\n",
      "Epoch 6: val_loss did not improve from 0.68870\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6334 - accuracy: 0.6467 - val_loss: 0.6915 - val_accuracy: 0.5455\n",
      "Epoch 7/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.6160 - accuracy: 0.6510\n",
      "Epoch 7: val_loss did not improve from 0.68870\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6042 - accuracy: 0.6613 - val_loss: 0.6927 - val_accuracy: 0.5455\n",
      "Epoch 8/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5976 - accuracy: 0.6858\n",
      "Epoch 8: val_loss did not improve from 0.68870\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5805 - accuracy: 0.6934 - val_loss: 0.6942 - val_accuracy: 0.5455\n",
      "Epoch 9/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5263 - accuracy: 0.7292\n",
      "Epoch 9: val_loss did not improve from 0.68870\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5344 - accuracy: 0.7226 - val_loss: 0.6964 - val_accuracy: 0.5455\n",
      "Epoch 10/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.5297 - accuracy: 0.7274\n",
      "Epoch 10: val_loss did not improve from 0.68870\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5347 - accuracy: 0.7285 - val_loss: 0.6980 - val_accuracy: 0.5455\n",
      "Epoch 11/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5100 - accuracy: 0.7445\n",
      "Epoch 11: val_loss did not improve from 0.68870\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5028 - accuracy: 0.7547 - val_loss: 0.7011 - val_accuracy: 0.5455\n",
      "Epoch 12/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.4968 - accuracy: 0.7545\n",
      "Epoch 12: val_loss did not improve from 0.68870\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4950 - accuracy: 0.7562 - val_loss: 0.7038 - val_accuracy: 0.5455\n",
      "Epoch 13/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4888 - accuracy: 0.7648\n",
      "Epoch 13: val_loss did not improve from 0.68870\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4848 - accuracy: 0.7664 - val_loss: 0.7050 - val_accuracy: 0.5455\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4416 - accuracy: 0.7927\n",
      "Epoch 14: val_loss did not improve from 0.68870\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4416 - accuracy: 0.7927 - val_loss: 0.7108 - val_accuracy: 0.5455\n",
      "Epoch 15/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.4536 - accuracy: 0.7911\n",
      "Epoch 15: val_loss did not improve from 0.68870\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4539 - accuracy: 0.7942 - val_loss: 0.7159 - val_accuracy: 0.5455\n",
      "Epoch 16/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3993 - accuracy: 0.8306\n",
      "Epoch 16: val_loss did not improve from 0.68870\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4024 - accuracy: 0.8277 - val_loss: 0.7201 - val_accuracy: 0.5303\n",
      "Epoch 17/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4085 - accuracy: 0.8264Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.68870\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4151 - accuracy: 0.8219 - val_loss: 0.7267 - val_accuracy: 0.5303\n",
      "Epoch 17: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.5278\n",
      "Test AUC for Layer 2: 0.4180\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (S_label)\n",
      "Model: \"sequential_86\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_344 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_258 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_172 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_345 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_259 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_173 (Dropout)       (None, 256)               0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " dense_346 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_260 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_347 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8978 - accuracy: 0.4926\n",
      "Epoch 1: val_loss improved from inf to 0.70019, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_S_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 21ms/step - loss: 0.8937 - accuracy: 0.4980 - val_loss: 0.7002 - val_accuracy: 0.4722\n",
      "Epoch 2/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.7984 - accuracy: 0.5426\n",
      "Epoch 2: val_loss did not improve from 0.70019\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.7882 - accuracy: 0.5513 - val_loss: 0.7105 - val_accuracy: 0.4722\n",
      "Epoch 3/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.7769 - accuracy: 0.5571\n",
      "Epoch 3: val_loss did not improve from 0.70019\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.7722 - accuracy: 0.5593 - val_loss: 0.7240 - val_accuracy: 0.4722\n",
      "Epoch 4/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.7519 - accuracy: 0.5740\n",
      "Epoch 4: val_loss did not improve from 0.70019\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.7577 - accuracy: 0.5792 - val_loss: 0.7384 - val_accuracy: 0.4722\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.7141 - accuracy: 0.5992\n",
      "Epoch 5: val_loss did not improve from 0.70019\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.7141 - accuracy: 0.5992 - val_loss: 0.7566 - val_accuracy: 0.4722\n",
      "Epoch 6/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6284 - accuracy: 0.6612\n",
      "Epoch 6: val_loss did not improve from 0.70019\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6294 - accuracy: 0.6618 - val_loss: 0.7741 - val_accuracy: 0.4722\n",
      "Epoch 7/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.6302 - accuracy: 0.6546\n",
      "Epoch 7: val_loss did not improve from 0.70019\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6306 - accuracy: 0.6498 - val_loss: 0.7891 - val_accuracy: 0.4722\n",
      "Epoch 8/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5989 - accuracy: 0.6974\n",
      "Epoch 8: val_loss did not improve from 0.70019\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5919 - accuracy: 0.6937 - val_loss: 0.8025 - val_accuracy: 0.4722\n",
      "Epoch 9/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.5692 - accuracy: 0.6918\n",
      "Epoch 9: val_loss did not improve from 0.70019\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5715 - accuracy: 0.6844 - val_loss: 0.8217 - val_accuracy: 0.4722\n",
      "Epoch 10/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5658 - accuracy: 0.7056\n",
      "Epoch 10: val_loss did not improve from 0.70019\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5619 - accuracy: 0.7017 - val_loss: 0.8368 - val_accuracy: 0.4722\n",
      "Epoch 11/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.5564 - accuracy: 0.7188\n",
      "Epoch 11: val_loss did not improve from 0.70019\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5554 - accuracy: 0.7190 - val_loss: 0.8475 - val_accuracy: 0.4722\n",
      "Epoch 12/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4769 - accuracy: 0.7741\n",
      "Epoch 12: val_loss did not improve from 0.70019\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4874 - accuracy: 0.7683 - val_loss: 0.8558 - val_accuracy: 0.4722\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5111 - accuracy: 0.7457\n",
      "Epoch 13: val_loss did not improve from 0.70019\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5111 - accuracy: 0.7457 - val_loss: 0.8639 - val_accuracy: 0.4722\n",
      "Epoch 14/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4592 - accuracy: 0.7780\n",
      "Epoch 14: val_loss did not improve from 0.70019\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4513 - accuracy: 0.7856 - val_loss: 0.8706 - val_accuracy: 0.4722\n",
      "Epoch 15/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4622 - accuracy: 0.7770\n",
      "Epoch 15: val_loss did not improve from 0.70019\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4654 - accuracy: 0.7750 - val_loss: 0.8829 - val_accuracy: 0.4722\n",
      "Epoch 16/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4300 - accuracy: 0.8098Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.70019\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4336 - accuracy: 0.8069 - val_loss: 0.8795 - val_accuracy: 0.4722\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_S_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.5507\n",
      "Test AUC for Layer 3: 0.5289\n",
      "\n",
      "Average Test Accuracy across all layers: 0.5413\n",
      "Average Test AUC across all layers: 0.4931\n",
      "\n",
      "================================================================================\n",
      "Training MLP model for Merged Embeddings and L_label\n",
      "================================================================================\n",
      "\n",
      "Layer 1:\n",
      "Training period: 01/01/2019 - 31/05/2021\n",
      "Validation period: 01/06/2021 - 31/12/2021\n",
      "Testing period: 01/01/2022 - 31/05/2022\n",
      "Training data: 522 samples\n",
      "Validation data: 163 samples\n",
      "Test data: 66 samples\n",
      "Embeddings shapes - Train: (522, 1536), Val: (163, 1536), Test: (66, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_87\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_348 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_261 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_174 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_349 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_262 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_175 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_350 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_263 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_351 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.9322 - accuracy: 0.4824\n",
      "Epoch 1: val_loss improved from inf to 0.69588, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_1.weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 2s 26ms/step - loss: 0.9361 - accuracy: 0.4770 - val_loss: 0.6959 - val_accuracy: 0.4479\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8716 - accuracy: 0.4828\n",
      "Epoch 2: val_loss did not improve from 0.69588\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.8716 - accuracy: 0.4828 - val_loss: 0.6990 - val_accuracy: 0.4479\n",
      "Epoch 3/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.7809 - accuracy: 0.5697\n",
      "Epoch 3: val_loss did not improve from 0.69588\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7787 - accuracy: 0.5690 - val_loss: 0.7014 - val_accuracy: 0.4479\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7415 - accuracy: 0.5785\n",
      "Epoch 4: val_loss did not improve from 0.69588\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7415 - accuracy: 0.5785 - val_loss: 0.7027 - val_accuracy: 0.4479\n",
      "Epoch 5/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6679 - accuracy: 0.6514\n",
      "Epoch 5: val_loss did not improve from 0.69588\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6657 - accuracy: 0.6552 - val_loss: 0.7037 - val_accuracy: 0.4479\n",
      "Epoch 6/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6391 - accuracy: 0.6406\n",
      "Epoch 6: val_loss did not improve from 0.69588\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6526 - accuracy: 0.6379 - val_loss: 0.7043 - val_accuracy: 0.4479\n",
      "Epoch 7/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5836 - accuracy: 0.7067\n",
      "Epoch 7: val_loss did not improve from 0.69588\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5788 - accuracy: 0.7031 - val_loss: 0.7054 - val_accuracy: 0.4479\n",
      "Epoch 8/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5679 - accuracy: 0.7115\n",
      "Epoch 8: val_loss did not improve from 0.69588\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5744 - accuracy: 0.7050 - val_loss: 0.7042 - val_accuracy: 0.4479\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5369 - accuracy: 0.7280\n",
      "Epoch 9: val_loss did not improve from 0.69588\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5369 - accuracy: 0.7280 - val_loss: 0.7041 - val_accuracy: 0.4540\n",
      "Epoch 10/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5351 - accuracy: 0.7188\n",
      "Epoch 10: val_loss did not improve from 0.69588\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5382 - accuracy: 0.7222 - val_loss: 0.7035 - val_accuracy: 0.4601\n",
      "Epoch 11/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5264 - accuracy: 0.7452\n",
      "Epoch 11: val_loss did not improve from 0.69588\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5212 - accuracy: 0.7414 - val_loss: 0.7034 - val_accuracy: 0.4601\n",
      "Epoch 12/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5497 - accuracy: 0.7163\n",
      "Epoch 12: val_loss did not improve from 0.69588\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5310 - accuracy: 0.7280 - val_loss: 0.7048 - val_accuracy: 0.4785\n",
      "Epoch 13/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4588 - accuracy: 0.7754\n",
      "Epoch 13: val_loss did not improve from 0.69588\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4616 - accuracy: 0.7701 - val_loss: 0.7050 - val_accuracy: 0.4724\n",
      "Epoch 14/100\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4562 - accuracy: 0.7981\n",
      "Epoch 14: val_loss did not improve from 0.69588\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4739 - accuracy: 0.7893 - val_loss: 0.7057 - val_accuracy: 0.4479\n",
      "Epoch 15/100\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4376 - accuracy: 0.7969\n",
      "Epoch 15: val_loss did not improve from 0.69588\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.4347 - accuracy: 0.7989 - val_loss: 0.7055 - val_accuracy: 0.4294\n",
      "Epoch 16/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3958 - accuracy: 0.8255Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69588\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4219 - accuracy: 0.8161 - val_loss: 0.7047 - val_accuracy: 0.4356\n",
      "Epoch 16: early stopping\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_1.png\n",
      "Test Accuracy for Layer 1: 0.1667\n",
      "Test AUC for Layer 1: 0.3157\n",
      "\n",
      "Layer 2:\n",
      "Training period: 01/01/2019 - 31/12/2021\n",
      "Validation period: 01/01/2022 - 31/05/2022\n",
      "Testing period: 01/06/2022 - 31/12/2022\n",
      "Training data: 685 samples\n",
      "Validation data: 66 samples\n",
      "Test data: 108 samples\n",
      "Embeddings shapes - Train: (685, 1536), Val: (66, 1536), Test: (108, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_88\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_352 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_264 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_176 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_353 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_265 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_177 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_354 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_266 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_355 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.9185 - accuracy: 0.4673\n",
      "Epoch 1: val_loss improved from inf to 0.68105, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 2s 21ms/step - loss: 0.9134 - accuracy: 0.4730 - val_loss: 0.6811 - val_accuracy: 0.8333\n",
      "Epoch 2/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.8185 - accuracy: 0.5372\n",
      "Epoch 2: val_loss improved from 0.68105 to 0.66410, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.8162 - accuracy: 0.5387 - val_loss: 0.6641 - val_accuracy: 0.8333\n",
      "Epoch 3/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.7488 - accuracy: 0.5878\n",
      "Epoch 3: val_loss improved from 0.66410 to 0.65443, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.7499 - accuracy: 0.5869 - val_loss: 0.6544 - val_accuracy: 0.8333\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7135 - accuracy: 0.6088\n",
      "Epoch 4: val_loss improved from 0.65443 to 0.64153, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.7135 - accuracy: 0.6088 - val_loss: 0.6415 - val_accuracy: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6428 - accuracy: 0.6526\n",
      "Epoch 5: val_loss improved from 0.64153 to 0.63253, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6428 - accuracy: 0.6526 - val_loss: 0.6325 - val_accuracy: 0.8333\n",
      "Epoch 6/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6463 - accuracy: 0.6458\n",
      "Epoch 6: val_loss improved from 0.63253 to 0.62551, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6440 - accuracy: 0.6467 - val_loss: 0.6255 - val_accuracy: 0.8333\n",
      "Epoch 7/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.6014 - accuracy: 0.6786\n",
      "Epoch 7: val_loss improved from 0.62551 to 0.61753, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5948 - accuracy: 0.6832 - val_loss: 0.6175 - val_accuracy: 0.8333\n",
      "Epoch 8/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5521 - accuracy: 0.7143\n",
      "Epoch 8: val_loss improved from 0.61753 to 0.61652, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.5480 - accuracy: 0.7182 - val_loss: 0.6165 - val_accuracy: 0.8333\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5377 - accuracy: 0.7182\n",
      "Epoch 9: val_loss improved from 0.61652 to 0.61475, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.5377 - accuracy: 0.7182 - val_loss: 0.6148 - val_accuracy: 0.8333\n",
      "Epoch 10/100\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.5384 - accuracy: 0.7321\n",
      "Epoch 10: val_loss improved from 0.61475 to 0.61365, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5431 - accuracy: 0.7299 - val_loss: 0.6137 - val_accuracy: 0.8485\n",
      "Epoch 11/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.5280 - accuracy: 0.7390\n",
      "Epoch 11: val_loss improved from 0.61365 to 0.60551, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.5243 - accuracy: 0.7358 - val_loss: 0.6055 - val_accuracy: 0.8485\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5092 - accuracy: 0.7445\n",
      "Epoch 12: val_loss improved from 0.60551 to 0.59845, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5092 - accuracy: 0.7445 - val_loss: 0.5984 - val_accuracy: 0.8485\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4808 - accuracy: 0.7737\n",
      "Epoch 13: val_loss improved from 0.59845 to 0.59786, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.4808 - accuracy: 0.7737 - val_loss: 0.5979 - val_accuracy: 0.8485\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4858 - accuracy: 0.7635\n",
      "Epoch 14: val_loss did not improve from 0.59786\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4858 - accuracy: 0.7635 - val_loss: 0.5982 - val_accuracy: 0.8485\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4463 - accuracy: 0.7927\n",
      "Epoch 15: val_loss improved from 0.59786 to 0.59177, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_2.weights.h5\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.4463 - accuracy: 0.7927 - val_loss: 0.5918 - val_accuracy: 0.8485\n",
      "Epoch 16/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.4381 - accuracy: 0.8180\n",
      "Epoch 16: val_loss did not improve from 0.59177\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4403 - accuracy: 0.8131 - val_loss: 0.5939 - val_accuracy: 0.8333\n",
      "Epoch 17/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.4134 - accuracy: 0.8247\n",
      "Epoch 17: val_loss did not improve from 0.59177\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4080 - accuracy: 0.8336 - val_loss: 0.5953 - val_accuracy: 0.8485\n",
      "Epoch 18/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3750 - accuracy: 0.8542\n",
      "Epoch 18: val_loss did not improve from 0.59177\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3848 - accuracy: 0.8467 - val_loss: 0.6015 - val_accuracy: 0.7879\n",
      "Epoch 19/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3547 - accuracy: 0.8594\n",
      "Epoch 19: val_loss did not improve from 0.59177\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3533 - accuracy: 0.8569 - val_loss: 0.5970 - val_accuracy: 0.8030\n",
      "Epoch 20/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3722 - accuracy: 0.8455\n",
      "Epoch 20: val_loss did not improve from 0.59177\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3681 - accuracy: 0.8511 - val_loss: 0.5986 - val_accuracy: 0.7576\n",
      "Epoch 21/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.3446 - accuracy: 0.8732\n",
      "Epoch 21: val_loss did not improve from 0.59177\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3477 - accuracy: 0.8701 - val_loss: 0.6041 - val_accuracy: 0.7273\n",
      "Epoch 22/100\n",
      "19/22 [========================>.....] - ETA: 0s - loss: 0.3471 - accuracy: 0.8553\n",
      "Epoch 22: val_loss did not improve from 0.59177\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3484 - accuracy: 0.8584 - val_loss: 0.6122 - val_accuracy: 0.6970\n",
      "Epoch 23/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3115 - accuracy: 0.8767\n",
      "Epoch 23: val_loss did not improve from 0.59177\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3105 - accuracy: 0.8803 - val_loss: 0.6169 - val_accuracy: 0.6515\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3133 - accuracy: 0.8745\n",
      "Epoch 24: val_loss did not improve from 0.59177\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3133 - accuracy: 0.8745 - val_loss: 0.6320 - val_accuracy: 0.6212\n",
      "Epoch 25/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.2875 - accuracy: 0.9191\n",
      "Epoch 25: val_loss did not improve from 0.59177\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.2913 - accuracy: 0.9139 - val_loss: 0.6512 - val_accuracy: 0.6212\n",
      "Epoch 26/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.2995 - accuracy: 0.8993\n",
      "Epoch 26: val_loss did not improve from 0.59177\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2944 - accuracy: 0.9036 - val_loss: 0.6786 - val_accuracy: 0.5758\n",
      "Epoch 27/100\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.3100 - accuracy: 0.8872\n",
      "Epoch 27: val_loss did not improve from 0.59177\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3046 - accuracy: 0.8905 - val_loss: 0.7063 - val_accuracy: 0.5909\n",
      "Epoch 28/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.2689 - accuracy: 0.9007\n",
      "Epoch 28: val_loss did not improve from 0.59177\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2707 - accuracy: 0.9007 - val_loss: 0.7131 - val_accuracy: 0.5909\n",
      "Epoch 29/100\n",
      "17/22 [======================>.......] - ETA: 0s - loss: 0.2648 - accuracy: 0.9099\n",
      "Epoch 29: val_loss did not improve from 0.59177\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2763 - accuracy: 0.9080 - val_loss: 0.7311 - val_accuracy: 0.6061\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2582 - accuracy: 0.9139Restoring model weights from the end of the best epoch: 15.\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.59177\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.2582 - accuracy: 0.9139 - val_loss: 0.7649 - val_accuracy: 0.5758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: early stopping\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_2.png\n",
      "Test Accuracy for Layer 2: 0.6667\n",
      "Test AUC for Layer 2: 0.5256\n",
      "\n",
      "Layer 3:\n",
      "Training period: 01/01/2019 - 31/05/2022\n",
      "Validation period: 01/06/2022 - 31/12/2022\n",
      "Testing period: 01/01/2023 - 31/05/2023\n",
      "Training data: 751 samples\n",
      "Validation data: 108 samples\n",
      "Test data: 69 samples\n",
      "Embeddings shapes - Train: (751, 1536), Val: (108, 1536), Test: (69, 1536)\n",
      "Creating MLP model with input shape: (1536,)\n",
      "MLP model created\n",
      "Created MLP model for Merged Embeddings (L_label)\n",
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_356 (Dense)           (None, 512)               786944    \n",
      "                                                                 \n",
      " batch_normalization_267 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_178 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_357 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_268 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_179 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_358 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_269 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_359 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 954881 (3.64 MB)\n",
      "Trainable params: 953089 (3.64 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n",
      "Training MLP model...\n",
      "Epoch 1/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.8353 - accuracy: 0.5312\n",
      "Epoch 1: val_loss improved from inf to 0.67344, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 2s 20ms/step - loss: 0.8372 - accuracy: 0.5313 - val_loss: 0.6734 - val_accuracy: 0.6481\n",
      "Epoch 2/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.7395 - accuracy: 0.5714\n",
      "Epoch 2: val_loss improved from 0.67344 to 0.66291, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.7378 - accuracy: 0.5712 - val_loss: 0.6629 - val_accuracy: 0.6481\n",
      "Epoch 3/100\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.7420 - accuracy: 0.5670\n",
      "Epoch 3: val_loss improved from 0.66291 to 0.65672, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.7405 - accuracy: 0.5699 - val_loss: 0.6567 - val_accuracy: 0.6481\n",
      "Epoch 4/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6509 - accuracy: 0.6630\n",
      "Epoch 4: val_loss improved from 0.65672 to 0.65295, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6547 - accuracy: 0.6578 - val_loss: 0.6529 - val_accuracy: 0.6481\n",
      "Epoch 5/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6374 - accuracy: 0.6577\n",
      "Epoch 5: val_loss improved from 0.65295 to 0.65114, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6391 - accuracy: 0.6551 - val_loss: 0.6511 - val_accuracy: 0.6481\n",
      "Epoch 6/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6013 - accuracy: 0.6676\n",
      "Epoch 6: val_loss improved from 0.65114 to 0.64912, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.6019 - accuracy: 0.6631 - val_loss: 0.6491 - val_accuracy: 0.6481\n",
      "Epoch 7/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5745 - accuracy: 0.7079\n",
      "Epoch 7: val_loss improved from 0.64912 to 0.64871, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.5763 - accuracy: 0.7071 - val_loss: 0.6487 - val_accuracy: 0.6481\n",
      "Epoch 8/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5550 - accuracy: 0.7201\n",
      "Epoch 8: val_loss did not improve from 0.64871\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5554 - accuracy: 0.7190 - val_loss: 0.6493 - val_accuracy: 0.6481\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5593 - accuracy: 0.7137\n",
      "Epoch 9: val_loss did not improve from 0.64871\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5593 - accuracy: 0.7137 - val_loss: 0.6493 - val_accuracy: 0.6481\n",
      "Epoch 10/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5243 - accuracy: 0.7533\n",
      "Epoch 10: val_loss did not improve from 0.64871\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5295 - accuracy: 0.7443 - val_loss: 0.6490 - val_accuracy: 0.6481\n",
      "Epoch 11/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.5085 - accuracy: 0.7467\n",
      "Epoch 11: val_loss did not improve from 0.64871\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5130 - accuracy: 0.7497 - val_loss: 0.6491 - val_accuracy: 0.6481\n",
      "Epoch 12/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4723 - accuracy: 0.7840\n",
      "Epoch 12: val_loss did not improve from 0.64871\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4742 - accuracy: 0.7816 - val_loss: 0.6505 - val_accuracy: 0.6481\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4706 - accuracy: 0.7683\n",
      "Epoch 13: val_loss did not improve from 0.64871\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4706 - accuracy: 0.7683 - val_loss: 0.6493 - val_accuracy: 0.6481\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4614 - accuracy: 0.7696\n",
      "Epoch 14: val_loss did not improve from 0.64871\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4614 - accuracy: 0.7696 - val_loss: 0.6491 - val_accuracy: 0.6481\n",
      "Epoch 15/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.4258 - accuracy: 0.8158\n",
      "Epoch 15: val_loss did not improve from 0.64871\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4257 - accuracy: 0.8123 - val_loss: 0.6492 - val_accuracy: 0.6574\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4064 - accuracy: 0.8149\n",
      "Epoch 16: val_loss improved from 0.64871 to 0.64704, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.4064 - accuracy: 0.8149 - val_loss: 0.6470 - val_accuracy: 0.6574\n",
      "Epoch 17/100\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.4246 - accuracy: 0.8109\n",
      "Epoch 17: val_loss did not improve from 0.64704\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4233 - accuracy: 0.8069 - val_loss: 0.6478 - val_accuracy: 0.6481\n",
      "Epoch 18/100\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.3667 - accuracy: 0.8375\n",
      "Epoch 18: val_loss improved from 0.64704 to 0.64683, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.3686 - accuracy: 0.8389 - val_loss: 0.6468 - val_accuracy: 0.6759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.3742 - accuracy: 0.8523\n",
      "Epoch 19: val_loss improved from 0.64683 to 0.64502, saving model to Merged_OpenAI_MLP/visualizations_mlp/COP\\best_mlp_merged_L_label_layer_3.weights.h5\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.3762 - accuracy: 0.8509 - val_loss: 0.6450 - val_accuracy: 0.6944\n",
      "Epoch 20/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3582 - accuracy: 0.8587\n",
      "Epoch 20: val_loss did not improve from 0.64502\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3552 - accuracy: 0.8615 - val_loss: 0.6482 - val_accuracy: 0.6852\n",
      "Epoch 21/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3410 - accuracy: 0.8668\n",
      "Epoch 21: val_loss did not improve from 0.64502\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3402 - accuracy: 0.8628 - val_loss: 0.6497 - val_accuracy: 0.6759\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3338 - accuracy: 0.8735\n",
      "Epoch 22: val_loss did not improve from 0.64502\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3338 - accuracy: 0.8735 - val_loss: 0.6555 - val_accuracy: 0.6852\n",
      "Epoch 23/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.3097 - accuracy: 0.8849\n",
      "Epoch 23: val_loss did not improve from 0.64502\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3030 - accuracy: 0.8921 - val_loss: 0.6593 - val_accuracy: 0.6759\n",
      "Epoch 24/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3359 - accuracy: 0.8709\n",
      "Epoch 24: val_loss did not improve from 0.64502\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3347 - accuracy: 0.8708 - val_loss: 0.6652 - val_accuracy: 0.6667\n",
      "Epoch 25/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2704 - accuracy: 0.9046\n",
      "Epoch 25: val_loss did not improve from 0.64502\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2708 - accuracy: 0.9001 - val_loss: 0.6780 - val_accuracy: 0.6574\n",
      "Epoch 26/100\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.2797 - accuracy: 0.9006\n",
      "Epoch 26: val_loss did not improve from 0.64502\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2774 - accuracy: 0.9015 - val_loss: 0.6862 - val_accuracy: 0.6296\n",
      "Epoch 27/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2722 - accuracy: 0.8980\n",
      "Epoch 27: val_loss did not improve from 0.64502\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2742 - accuracy: 0.9001 - val_loss: 0.7027 - val_accuracy: 0.6204\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2738 - accuracy: 0.8975\n",
      "Epoch 28: val_loss did not improve from 0.64502\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2738 - accuracy: 0.8975 - val_loss: 0.7170 - val_accuracy: 0.6111\n",
      "Epoch 29/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2289 - accuracy: 0.9441\n",
      "Epoch 29: val_loss did not improve from 0.64502\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2319 - accuracy: 0.9374 - val_loss: 0.7157 - val_accuracy: 0.6204\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2373 - accuracy: 0.9281\n",
      "Epoch 30: val_loss did not improve from 0.64502\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2373 - accuracy: 0.9281 - val_loss: 0.7214 - val_accuracy: 0.6204\n",
      "Epoch 31/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.2263 - accuracy: 0.9321\n",
      "Epoch 31: val_loss did not improve from 0.64502\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2291 - accuracy: 0.9308 - val_loss: 0.7406 - val_accuracy: 0.6296\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2214 - accuracy: 0.9294\n",
      "Epoch 32: val_loss did not improve from 0.64502\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2214 - accuracy: 0.9294 - val_loss: 0.7463 - val_accuracy: 0.5926\n",
      "Epoch 33/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.2048 - accuracy: 0.9441\n",
      "Epoch 33: val_loss did not improve from 0.64502\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2159 - accuracy: 0.9374 - val_loss: 0.7523 - val_accuracy: 0.5833\n",
      "Epoch 34/100\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.1973 - accuracy: 0.9539Restoring model weights from the end of the best epoch: 19.\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.64502\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2015 - accuracy: 0.9481 - val_loss: 0.7745 - val_accuracy: 0.5648\n",
      "Epoch 34: early stopping\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Saved learning curves: final_mlp_merged_L_label_layer_3.png\n",
      "Test Accuracy for Layer 3: 0.4928\n",
      "Test AUC for Layer 3: 0.4940\n",
      "\n",
      "Average Test Accuracy across all layers: 0.4420\n",
      "Average Test AUC across all layers: 0.4451\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS (COP)\n",
      "================================================================================\n",
      "\n",
      "Combination: MLP with Merged + S_label\n",
      "Average Accuracy: 0.5413\n",
      "Average AUC: 0.4931\n",
      "  Layer 1 - Accuracy: 0.5455, AUC: 0.5324\n",
      "  Layer 2 - Accuracy: 0.5278, AUC: 0.4180\n",
      "  Layer 3 - Accuracy: 0.5507, AUC: 0.5289\n",
      "\n",
      "Combination: MLP with Merged + L_label\n",
      "Average Accuracy: 0.4420\n",
      "Average AUC: 0.4451\n",
      "  Layer 1 - Accuracy: 0.1667, AUC: 0.3157\n",
      "  Layer 2 - Accuracy: 0.6667, AUC: 0.5256\n",
      "  Layer 3 - Accuracy: 0.4928, AUC: 0.4940\n",
      "MLP summary comparison visualization saved as: Merged_OpenAI_MLP/visualizations_summary/COP\\mlp_merged_performance_comparison.png\n",
      "MLP layer performance visualization saved as: Merged_OpenAI_MLP/visualizations_summary/COP\\mlp_merged_layer_performance.png\n",
      "\n",
      "Analysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import ast  # For safely parsing string representations of arrays\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to plot training history after each epoch\n",
    "class PlotLearningCallback(Callback):\n",
    "    def __init__(self, text_col, label_col, layer_num, plot_dir):\n",
    "        super().__init__()\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.layer_num = layer_num\n",
    "        self.plot_dir = plot_dir\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        # Keep track of metrics for each epoch\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "            \n",
    "        # Collect metrics\n",
    "        self.train_acc.append(logs.get('accuracy', logs.get('acc', 0)))\n",
    "        self.val_acc.append(logs.get('val_accuracy', logs.get('val_acc', 0)))\n",
    "        self.train_loss.append(logs.get('loss', 0))\n",
    "        self.val_loss.append(logs.get('val_loss', 0))\n",
    "\n",
    "class MergedEmbeddingMLPPredictor:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the stock predictor for climate change news analysis using merged OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file containing climate change news data with merged OpenAI embeddings\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.data = None\n",
    "        self.layers = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for visualizations\n",
    "        self.mlp_viz_dir = 'Merged_OpenAI_MLP/visualizations_mlp/COP'\n",
    "        os.makedirs(self.mlp_viz_dir, exist_ok=True)\n",
    "        os.makedirs('Merged_OpenAI_MLP/visualizations_summary/COP', exist_ok=True)\n",
    "            \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the CSV data containing climate change news with merged OpenAI embeddings.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        try:\n",
    "            # Try pandas' automatic date parsing first with dayfirst=True\n",
    "            self.data['Publication date'] = pd.to_datetime(self.data['Publication date'], dayfirst=True)\n",
    "            self.data['Predicting date Short'] = pd.to_datetime(self.data['Predicting date Short'], dayfirst=True)\n",
    "            self.data['Predicting date Long'] = pd.to_datetime(self.data['Predicting date Long'], dayfirst=True)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Automatic date parsing failed. Trying manual conversion...\")\n",
    "            \n",
    "            # Helper function to handle different date formats\n",
    "            def parse_date_column(column):\n",
    "                result = []\n",
    "                for date_str in column:\n",
    "                    try:\n",
    "                        # Try to parse as DD/MM/YYYY\n",
    "                        date = pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            # Try to parse as YYYY-MM-DD\n",
    "                            date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "                        except ValueError:\n",
    "                            # As a last resort, let pandas guess with dayfirst=True\n",
    "                            date = pd.to_datetime(date_str, dayfirst=True)\n",
    "                    result.append(date)\n",
    "                return pd.Series(result)\n",
    "            \n",
    "            self.data['Publication date'] = parse_date_column(self.data['Publication date'])\n",
    "            self.data['Predicting date Short'] = parse_date_column(self.data['Predicting date Short'])\n",
    "            self.data['Predicting date Long'] = parse_date_column(self.data['Predicting date Long'])\n",
    "        \n",
    "        # Sort by publication date\n",
    "        self.data = self.data.sort_values('Publication date')\n",
    "        \n",
    "        # Parse embedding vectors from string format to numpy arrays\n",
    "        print(\"Parsing merged OpenAI embedding vectors from string format...\")\n",
    "        \n",
    "        # Function to safely convert string representation of array to numpy array\n",
    "        def parse_embedding(embedding_str):\n",
    "            if pd.isna(embedding_str):\n",
    "                # Return zeros array if embedding is missing\n",
    "                return np.zeros(1536)\n",
    "            try:\n",
    "                # Try parsing as a string representation of a list\n",
    "                embedding_list = ast.literal_eval(embedding_str)\n",
    "                return np.array(embedding_list, dtype=np.float32)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Error parsing embedding: {embedding_str[:50]}...\")\n",
    "                return np.zeros(1536)\n",
    "        \n",
    "        # Convert merged embeddings to numpy arrays\n",
    "        self.data['Merged_embedding'] = self.data['Merged_embedding_vector'].apply(parse_embedding)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        sample_merged_embedding = self.data['Merged_embedding'].iloc[0]\n",
    "        print(f\"Sample Merged embedding shape: {sample_merged_embedding.shape}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} climate change news articles spanning from \"\n",
    "              f\"{self.data['Publication date'].min().strftime('%d/%m/%Y')} \"\n",
    "              f\"to {self.data['Publication date'].max().strftime('%d/%m/%Y')}\")\n",
    "        print(f\"Class distribution for short-term prediction: {self.data['S_label'].value_counts().to_dict()}\")\n",
    "        print(f\"Class distribution for long-term prediction: {self.data['L_label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def define_time_windows(self):\n",
    "        \"\"\"Define the time windows for the sliding window approach.\"\"\"\n",
    "        # First layer: train (2019-2021), validation (2021-2021), test (2022-2022)\n",
    "        layer1 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-05-31'),\n",
    "            'val_start': pd.Timestamp('2021-06-01'),\n",
    "            'val_end': pd.Timestamp('2021-12-31'),\n",
    "            'test_start': pd.Timestamp('2022-01-01'),\n",
    "            'test_end': pd.Timestamp('2022-05-31')\n",
    "        }\n",
    "        \n",
    "        # Second layer: train (2019-2021), validation (2022-2022), test (2022-2022)\n",
    "        layer2 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2021-12-31'),\n",
    "            'val_start': pd.Timestamp('2022-01-01'),\n",
    "            'val_end': pd.Timestamp('2022-05-31'),\n",
    "            'test_start': pd.Timestamp('2022-06-01'),\n",
    "            'test_end': pd.Timestamp('2022-12-31')\n",
    "        }\n",
    "        \n",
    "        # Third layer: train (2019-2022), validation (2022-2022), test (2023-2023)\n",
    "        layer3 = {\n",
    "            'train_start': pd.Timestamp('2019-01-01'),\n",
    "            'train_end': pd.Timestamp('2022-05-31'),\n",
    "            'val_start': pd.Timestamp('2022-06-01'),\n",
    "            'val_end': pd.Timestamp('2022-12-31'),\n",
    "            'test_start': pd.Timestamp('2023-01-01'),\n",
    "            'test_end': pd.Timestamp('2023-05-31')\n",
    "        }\n",
    "        \n",
    "        self.layers = [layer1, layer2, layer3]\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, layer, label_col):\n",
    "        \"\"\"\n",
    "        Split the data into training, validation, and test sets based on the defined time windows.\n",
    "        Extract OpenAI embeddings for text data.\n",
    "        \n",
    "        Args:\n",
    "            layer: The time window layer\n",
    "            label_col: The column containing the labels ('S_label' or 'L_label')\n",
    "            \n",
    "        Returns:\n",
    "            train_data, val_data, test_data with embeddings\n",
    "        \"\"\"\n",
    "        train_mask = (self.data['Publication date'] >= layer['train_start']) & (self.data['Publication date'] <= layer['train_end'])\n",
    "        val_mask = (self.data['Publication date'] > layer['val_start']) & (self.data['Publication date'] <= layer['val_end'])\n",
    "        test_mask = (self.data['Publication date'] > layer['test_start']) & (self.data['Publication date'] <= layer['test_end'])\n",
    "        \n",
    "        train_data = self.data[train_mask]\n",
    "        val_data = self.data[val_mask]\n",
    "        test_data = self.data[test_mask]\n",
    "        \n",
    "        print(f\"Training data: {len(train_data)} samples\")\n",
    "        print(f\"Validation data: {len(val_data)} samples\")\n",
    "        print(f\"Test data: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        X_train = np.stack(train_data['Merged_embedding'].values)\n",
    "        X_val = np.stack(val_data['Merged_embedding'].values)\n",
    "        X_test = np.stack(test_data['Merged_embedding'].values)\n",
    "        \n",
    "        # Get labels\n",
    "        y_train = train_data[label_col].values\n",
    "        y_val = val_data[label_col].values\n",
    "        y_test = test_data[label_col].values\n",
    "        \n",
    "        print(f\"Embeddings shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        return (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data\n",
    "    \n",
    "    def create_mlp_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Create a simple MLP model with dense layers (256, 128) for document-level embeddings.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of the input data (1536,)\n",
    "            \n",
    "        Returns:\n",
    "            Compiled MLP model\n",
    "        \"\"\"\n",
    "        print(f\"Creating MLP model with input shape: {input_shape}\")\n",
    "        \n",
    "        # Create a feed-forward neural network\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(512, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layer\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.00005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(\"MLP model created\")\n",
    "        return model\n",
    "    \n",
    "    def plot_final_learning_curves(self, history, label_col, layer_num, visualization_dir):\n",
    "        \"\"\"\n",
    "        Plot final learning curves after training is complete for MLP.\n",
    "        \n",
    "        Args:\n",
    "            history: Training history\n",
    "            label_col: Label column used\n",
    "            layer_num: Layer number\n",
    "            visualization_dir: Directory to save visualizations\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Get history dictionary safely\n",
    "        history_dict = {}\n",
    "        if hasattr(history, 'history'):\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine the correct metric names\n",
    "        acc_metric = 'accuracy' if 'accuracy' in history_dict else 'acc'\n",
    "        val_acc_metric = 'val_accuracy' if 'val_accuracy' in history_dict else 'val_acc'\n",
    "        \n",
    "        # Plot accuracy\n",
    "        if acc_metric in history_dict and val_acc_metric in history_dict:\n",
    "            ax1.plot(history_dict[acc_metric], label='Training Accuracy', color='blue')\n",
    "            ax1.plot(history_dict[val_acc_metric], label='Validation Accuracy', color='orange')\n",
    "        ax1.set_title(f'Final Accuracy Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot loss\n",
    "        if 'loss' in history_dict and 'val_loss' in history_dict:\n",
    "            ax2.plot(history_dict['loss'], label='Training Loss', color='blue')\n",
    "            ax2.plot(history_dict['val_loss'], label='Validation Loss', color='orange')\n",
    "        ax2.set_title(f'Final Loss Curves: MLP with Merged Embeddings ({label_col}, Layer {layer_num})')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"final_mlp_merged_{label_col}_layer_{layer_num}.png\"\n",
    "        plt.savefig(f\"{visualization_dir}/{filename}\")\n",
    "        print(f\"Saved learning curves: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def train_and_evaluate_model(self, label_col):\n",
    "        \"\"\"\n",
    "        Train and evaluate an MLP model for merged embeddings and a specific label column.\n",
    "        \n",
    "        Args:\n",
    "            label_col: The label column to use ('S_label' or 'L_label')\n",
    "        \"\"\"\n",
    "        # Store results\n",
    "        combination_key = f\"MLP|Merged|{label_col}\"\n",
    "        self.results[combination_key] = {\n",
    "            'accuracy': [],\n",
    "            'auc': [],\n",
    "            'layer_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training MLP model for Merged Embeddings and {label_col}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(f\"Training period: {layer['train_start'].strftime('%d/%m/%Y')} - {layer['train_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Validation period: {layer['val_start'].strftime('%d/%m/%Y')} - {layer['val_end'].strftime('%d/%m/%Y')}\")\n",
    "            print(f\"Testing period: {layer['test_start'].strftime('%d/%m/%Y')} - {layer['test_end'].strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "            # Split data and get embeddings\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test), train_data, val_data, test_data = self.split_data(\n",
    "                layer, label_col)\n",
    "            \n",
    "            # Check if there are enough samples and classes\n",
    "            if len(X_train) < 10 or len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                print(f\"Skipping layer {i+1} due to insufficient data or class imbalance\")\n",
    "                continue\n",
    "            \n",
    "            # Create and train model\n",
    "            model = self.create_mlp_model((1536,))\n",
    "            print(f\"Created MLP model for Merged Embeddings ({label_col})\")\n",
    "            model.summary()\n",
    "            \n",
    "            # Setup callbacks\n",
    "            plot_callback = PlotLearningCallback('Merged', label_col, i+1, self.mlp_viz_dir)\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=15,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            model_checkpoint = ModelCheckpoint(\n",
    "                filepath=f\"{self.mlp_viz_dir}/best_mlp_merged_{label_col}_layer_{i+1}.weights.h5\",\n",
    "                monitor='val_loss',\n",
    "                save_weights_only=True,\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            print(f\"Training MLP model...\")\n",
    "            batch_size = 32  # Larger batch size for embeddings\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=100,  # Increase max epochs, early stopping will prevent overfitting\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[early_stopping, model_checkpoint, plot_callback],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred_proba = model.predict(X_test)\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "            \n",
    "            # Create final learning curve visualization\n",
    "            self.plot_final_learning_curves(history, label_col, i+1, self.mlp_viz_dir)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            self.results[combination_key]['accuracy'].append(accuracy)\n",
    "            self.results[combination_key]['auc'].append(auc)\n",
    "            \n",
    "            print(f\"Test Accuracy for Layer {i+1}: {accuracy:.4f}\")\n",
    "            print(f\"Test AUC for Layer {i+1}: {auc:.4f}\")\n",
    "            \n",
    "            # Store layer results for visualization\n",
    "            layer_result = {\n",
    "                'layer': i+1,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'history': history.history\n",
    "            }\n",
    "            \n",
    "            self.results[combination_key]['layer_results'].append(layer_result)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_accuracy = np.mean(self.results[combination_key]['accuracy']) if self.results[combination_key]['accuracy'] else 0\n",
    "        avg_auc = np.mean(self.results[combination_key]['auc']) if self.results[combination_key]['auc'] else 0\n",
    "        \n",
    "        print(f\"\\nAverage Test Accuracy across all layers: {avg_accuracy:.4f}\")\n",
    "        print(f\"Average Test AUC across all layers: {avg_auc:.4f}\")\n",
    "        \n",
    "        self.results[combination_key]['avg_accuracy'] = avg_accuracy\n",
    "        self.results[combination_key]['avg_auc'] = avg_auc\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_all_combinations(self):\n",
    "        \"\"\"Run the analysis for merged embeddings with both short-term and long-term labels.\"\"\"\n",
    "        # Define label columns\n",
    "        label_cols = ['S_label', 'L_label']\n",
    "        \n",
    "        # Run analysis for each label column\n",
    "        for label_col in label_cols:\n",
    "            self.train_and_evaluate_model(label_col)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF RESULTS (COP)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create a summary table for easier comparison\n",
    "        summary_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            avg_accuracy = results.get('avg_accuracy', 0)\n",
    "            avg_auc = results.get('avg_auc', 0)\n",
    "            \n",
    "            print(f\"\\nCombination: {model_type} with {text_col} + {label_col}\")\n",
    "            print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "            \n",
    "            # Print layer-specific results\n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                print(f\"  Layer {i+1} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Combination': f\"{text_col} + {label_col}\",\n",
    "                'Avg Accuracy': avg_accuracy,\n",
    "                'Avg AUC': avg_auc,\n",
    "                'Label': label_col\n",
    "            })\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        self.create_summary_visualization(summary_data)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_summary_visualization(self, summary_data):\n",
    "        \"\"\"Create a summary visualization comparing model combinations.\"\"\"\n",
    "        if not summary_data:\n",
    "            print(\"No data available for summary visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Convert metrics to float for plotting\n",
    "        df['Avg Accuracy'] = df['Avg Accuracy'].astype(float)\n",
    "        df['Avg AUC'] = df['Avg AUC'].astype(float)\n",
    "        \n",
    "        # Performance comparison for MLP with merged embeddings\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Plot accuracy and AUC bars\n",
    "        x = np.arange(len(df))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, df['Avg Accuracy'], width, label='Average Accuracy', color='skyblue')\n",
    "        plt.bar(x + width/2, df['Avg AUC'], width, label='Average AUC', color='salmon')\n",
    "        \n",
    "        plt.xlabel('Model Combination')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison of MLP Models with Merged OpenAI Embeddings')\n",
    "        labels = [f\"Merged + {row['Label']}\" for _, row in df.iterrows()]\n",
    "        plt.xticks(x, labels, rotation=0)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(df['Avg Accuracy']):\n",
    "            plt.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for i, v in enumerate(df['Avg AUC']):\n",
    "            plt.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.ylim(0, max(df['Avg Accuracy'].max(), df['Avg AUC'].max()) + 0.1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/COP', \"mlp_merged_performance_comparison.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP summary comparison visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Layer-specific performance visualizations\n",
    "        self.create_layer_performance_visualizations()\n",
    "    \n",
    "    def create_layer_performance_visualizations(self):\n",
    "        \"\"\"Create visualizations showing performance across different layers for MLP models.\"\"\"\n",
    "        # Prepare data for visualization\n",
    "        layer_data = []\n",
    "        \n",
    "        for combination, results in self.results.items():\n",
    "            model_type, text_col, label_col = combination.split('|')\n",
    "            \n",
    "            for i, accuracy in enumerate(results.get('accuracy', [])):\n",
    "                auc = results.get('auc', [])[i]\n",
    "                layer_info = {\n",
    "                    'Text': text_col,\n",
    "                    'Label': label_col,\n",
    "                    'Layer': f\"Layer {i+1}\",\n",
    "                    'Layer_num': i+1,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC': auc,\n",
    "                    'Combination': f\"{text_col} + {label_col}\"\n",
    "                }\n",
    "                \n",
    "                layer_data.append(layer_info)\n",
    "        \n",
    "        if not layer_data:\n",
    "            print(\"No layer data available\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(layer_data)\n",
    "        \n",
    "        # Create visualization - Accuracy by layer\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # 1. Accuracy by combination and layer\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x='Combination', y='Accuracy', hue='Layer', data=df)\n",
    "        plt.title('MLP Accuracy by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['Accuracy'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 2. AUC by combination and layer\n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x='Combination', y='AUC', hue='Layer', data=df)\n",
    "        plt.title('MLP AUC by Model Combination and Layer with Merged OpenAI Embeddings')\n",
    "        plt.ylim(0, max(0.8, df['AUC'].max() + 0.05))\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_path = os.path.join('Merged_OpenAI_MLP/visualizations_summary/COP', \"mlp_merged_layer_performance.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"MLP layer performance visualization saved as: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure visualization directories exist\n",
    "    for directory in ['Merged_OpenAI_MLP/visualizations_mlp/COP', 'Merged_OpenAI_MLP/visualizations_summary/COP']:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    csv_path = 'E:/User/Documents/Documents (UK)/Cardiff (PhD)/First Year/climate change/US_news/SP500_semantic/us_news_semantics_COP_completed_openai.csv'\n",
    "    \n",
    "    # Initialize the predictor with merged OpenAI embeddings\n",
    "    predictor = MergedEmbeddingMLPPredictor(csv_path)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    predictor.load_data().define_time_windows().run_all_combinations()\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to 'Merged_OpenAI_MLP' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_python3",
   "language": "python",
   "name": "my_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
